[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v1",
                "updated": "2025-05-27T11:55:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v3",
                "updated": "2025-05-27T05:11:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    5,
                    11,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v2",
                "updated": "2025-05-27T03:57:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    57,
                    47,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v4",
                "updated": "2025-05-25T05:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    5,
                    26,
                    2,
                    6,
                    145,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v1",
                "updated": "2025-05-24T17:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v3",
                "updated": "2025-05-24T17:39:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    39,
                    32,
                    5,
                    144,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "18 pages, 14 figures. Accepted by ICML 2025. The code is available at\n  https://github.com/wenhao728/AsymRnR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16366v2",
                "updated": "2025-05-24T17:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    4,
                    26,
                    5,
                    144,
                    0
                ],
                "published": "2024-04-25T07:09:05Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    9,
                    5,
                    3,
                    116,
                    0
                ],
                "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection"
                },
                "summary": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jinke Shi"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Haishuai Wang"
                    },
                    {
                        "name": "Jiajun Bu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Bu"
                },
                "author": "Jiajun Bu",
                "arxiv_comment": "Accepted by IEEE TNNLS (14 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20334v1",
                "updated": "2025-05-24T10:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query"
                },
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v2",
                "updated": "2025-05-24T09:33:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    33,
                    35,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18610v1",
                "updated": "2025-05-24T09:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T09:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs"
                },
                "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."
                },
                "authors": [
                    {
                        "name": "Tengxuan Liu"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Jiayi Yang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18577v1",
                "updated": "2025-05-24T07:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T07:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance"
                },
                "summary": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dongsuk Oh"
                    },
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Eunjee Na"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Hyunkyu Choi"
                    },
                    {
                        "name": "Seonghyeon Jang"
                    },
                    {
                        "name": "Hanjin Choi"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18554v1",
                "updated": "2025-05-24T06:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:45:16Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads"
                },
                "summary": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively."
                },
                "authors": [
                    {
                        "name": "Jaewon Kwon"
                    },
                    {
                        "name": "Yongju Lee"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hongju Kal"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "arxiv_affiliation": "Yonsei University, Seoul, Republic of Korea",
                "author": "Won Woo Ro",
                "arxiv_comment": "Accepted to ISCA '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v2",
                "updated": "2025-05-24T04:37:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    37,
                    34,
                    5,
                    144,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v1",
                "updated": "2025-05-23T18:46:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20325v1",
                "updated": "2025-05-23T18:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence"
                },
                "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques."
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v3",
                "updated": "2025-05-23T17:02:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v3",
                "updated": "2025-05-23T16:36:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    36,
                    12,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18013v1",
                "updated": "2025-05-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "title": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence"
                },
                "summary": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively."
                },
                "authors": [
                    {
                        "name": "Hanze Zhang"
                    },
                    {
                        "name": "Kaiming Wang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17934v1",
                "updated": "2025-05-23T14:12:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T14:12:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "title": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications"
                },
                "summary": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Łukasz Szustak"
                    },
                    {
                        "name": "László Környei"
                    },
                    {
                        "name": "Flavio Cesar Cunha Galeazzo"
                    },
                    {
                        "name": "Paweł Bratek"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Bratek"
                },
                "author": "Paweł Bratek",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18231v1",
                "updated": "2025-05-23T12:40:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache"
                },
                "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17787v1",
                "updated": "2025-05-23T12:00:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:00:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration."
                },
                "authors": [
                    {
                        "name": "Peilin Chen"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang",
                "arxiv_comment": "Accepted to GLSVLSI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v2",
                "updated": "2025-05-23T11:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    59,
                    22,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v4",
                "updated": "2025-05-23T10:45:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    45,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17694v1",
                "updated": "2025-05-23T10:03:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T10:03:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding"
                },
                "summary": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Rui Ning"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v3",
                "updated": "2025-05-23T10:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    1,
                    57,
                    4,
                    143,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v3",
                "updated": "2025-05-23T09:33:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    33,
                    32,
                    4,
                    143,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "ACL25 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v2",
                "updated": "2025-05-23T08:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    12,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v2",
                "updated": "2025-05-23T07:07:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    7,
                    7,
                    29,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v2",
                "updated": "2025-05-23T04:58:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    58,
                    47,
                    4,
                    143,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values"
                },
                "summary": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Mohammadreza Nemati"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v1",
                "updated": "2025-05-22T22:54:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17272v1",
                "updated": "2025-05-22T20:39:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T20:39:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zebra-Llama: Towards Extremely Efficient Hybrid Models"
                },
                "summary": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance."
                },
                "authors": [
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v2",
                "updated": "2025-05-22T20:10:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    10,
                    16,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v1",
                "updated": "2025-05-22T17:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16271v1",
                "updated": "2025-05-22T06:04:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T06:04:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms"
                },
                "summary": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime."
                },
                "authors": [
                    {
                        "name": "A. Chilingarian"
                    }
                ],
                "author_detail": {
                    "name": "A. Chilingarian"
                },
                "author": "A. Chilingarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16210v1",
                "updated": "2025-05-22T04:23:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T04:23:19Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."
                },
                "authors": [
                    {
                        "name": "Zhihang Cai"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Zheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wei"
                },
                "author": "Zheng Wei",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v1",
                "updated": "2025-05-22T03:26:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v1",
                "updated": "2025-05-22T03:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16076v1",
                "updated": "2025-05-21T23:23:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T23:23:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models"
                },
                "summary": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url."
                },
                "authors": [
                    {
                        "name": "Jinhua Liang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v1",
                "updated": "2025-05-21T22:13:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v2",
                "updated": "2025-05-21T20:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    52,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache. Accepted\n  at CVPR eLVM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v2",
                "updated": "2025-05-21T20:42:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    42,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15558v1",
                "updated": "2025-05-21T14:17:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:17:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Robo-DM: Data Management For Large Robot Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robo-DM: Data Management For Large Robot Datasets"
                },
                "summary": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Letian Fu"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Yanxiang Zhang"
                    },
                    {
                        "name": "Lawrence Yunliang Chen"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kush Hari"
                    },
                    {
                        "name": "Ashwin Balakrishna"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Pannag R Sanketi"
                    },
                    {
                        "name": "John Kubiatowicz"
                    },
                    {
                        "name": "Ken Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Ken Goldberg"
                },
                "author": "Ken Goldberg",
                "arxiv_comment": "Best paper finalist of IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15535v1",
                "updated": "2025-05-21T13:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:56:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead"
                },
                "summary": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations."
                },
                "authors": [
                    {
                        "name": "Michał Wichrowski"
                    },
                    {
                        "name": "Mohsen Rezaee-Hajidehi"
                    },
                    {
                        "name": "Jože Korelc"
                    },
                    {
                        "name": "Martin Kronbichler"
                    },
                    {
                        "name": "Stanisław Stupkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Stanisław Stupkiewicz"
                },
                "author": "Stanisław Stupkiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M60, 74B20, 74S05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15531v1",
                "updated": "2025-05-21T13:52:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:52:45Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "title": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency"
                },
                "summary": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chaofan Ma"
                },
                "author": "Chaofan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v2",
                "updated": "2025-05-21T10:38:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v2",
                "updated": "2025-05-21T10:38:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Accepted by ICML 2025. Code is available:\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v2",
                "updated": "2025-05-21T10:37:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    37,
                    50,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v1",
                "updated": "2025-05-21T10:20:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15269v1",
                "updated": "2025-05-21T08:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T08:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval"
                },
                "summary": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v2",
                "updated": "2025-05-21T06:45:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    45,
                    58,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15859v1",
                "updated": "2025-05-21T04:32:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T04:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoData: A Multi-Agent System for Open Web Data Collection"
                },
                "summary": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData."
                },
                "authors": [
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Yiyue Qian"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Xiaoye Qian"
                    },
                    {
                        "name": "Feifan Bai"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Xuwei Luo"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v2",
                "updated": "2025-05-21T01:34:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    34,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14992v1",
                "updated": "2025-05-21T00:40:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T00:40:05Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "title": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models"
                },
                "summary": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v1",
                "updated": "2025-05-20T23:12:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v3",
                "updated": "2025-05-20T18:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    49,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14427v1",
                "updated": "2025-05-20T14:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:38:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out"
                },
                "summary": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "Lin Cheng"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14398v1",
                "updated": "2025-05-20T14:14:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation"
                },
                "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "Data and code are available at https://peterbaile.github.io/lag/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14085v1",
                "updated": "2025-05-20T08:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T08:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration"
                },
                "summary": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity."
                },
                "authors": [
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "14 pages, 7 figures including subplots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13002v2",
                "updated": "2025-05-20T07:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    34,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T11:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    41,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures"
                },
                "summary": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase."
                },
                "authors": [
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Bongjoon Hyun"
                    },
                    {
                        "name": "Youngjin Kwon"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14010v1",
                "updated": "2025-05-20T07:04:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T07:04:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache"
                },
                "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Pengwen Dai"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Dianjie Lu"
                    },
                    {
                        "name": "Guijuan Zhang"
                    },
                    {
                        "name": "Youshan Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v3",
                "updated": "2025-05-20T04:52:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    52,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Akash Das"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v1",
                "updated": "2025-05-20T03:21:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v2",
                "updated": "2025-05-19T20:37:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    37,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12463v2",
                "updated": "2025-05-19T19:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2024-05-21T02:39:45Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    2,
                    39,
                    45,
                    1,
                    142,
                    0
                ],
                "title": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge"
                },
                "summary": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software."
                },
                "authors": [
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13140v1",
                "updated": "2025-05-19T14:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:09:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow"
                },
                "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Takahiro Maeda"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Norimichi Ukita"
                    },
                    {
                        "name": "Kris Kitani"
                    }
                ],
                "author_detail": {
                    "name": "Kris Kitani"
                },
                "author": "Kris Kitani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v1",
                "updated": "2025-05-19T13:36:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13094v1",
                "updated": "2025-05-19T13:25:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:25:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation"
                },
                "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."
                },
                "authors": [
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Runxuan Yang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.21505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21505v1",
                "updated": "2025-05-27T17:59:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:59:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective"
                },
                "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Shimao Zhang"
                    },
                    {
                        "name": "Zhejian Lai"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Shuaijie She"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Jiajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Chen"
                },
                "author": "Jiajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21503v1",
                "updated": "2025-05-27T17:59:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:59:50Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    50,
                    1,
                    147,
                    0
                ],
                "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs\n  via Catfish Agent for Clinical Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs\n  via Catfish Agent for Clinical Decision Making"
                },
                "summary": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Qiao Yan"
                    },
                    {
                        "name": "Zhenghao Xing"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    },
                    {
                        "name": "Xiaowei Hu"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21499v1",
                "updated": "2025-05-27T17:59:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    5,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:59:05Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    5,
                    1,
                    147,
                    0
                ],
                "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery"
                },
                "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."
                },
                "authors": [
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Rupeng Zhang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21495v1",
                "updated": "2025-05-27T17:58:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    58,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:58:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    58,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an\n  open-source device for Multimodal robot Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an\n  open-source device for Multimodal robot Perception"
                },
                "summary": "Robust robot manipulation in unstructured environments often requires\nunderstanding object properties that extend beyond geometry, such as material\nor compliance-properties that can be challenging to infer using vision alone.\nMultimodal haptic sensing provides a promising avenue for inferring such\nproperties, yet progress has been constrained by the lack of large, diverse,\nand realistic haptic datasets. In this work, we introduce the CLAMP device, a\nlow-cost (<\\$200) sensorized reacher-grabber designed to collect large-scale,\nin-the-wild multimodal haptic data from non-expert users in everyday settings.\nWe deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP\ndataset, the largest open-source multimodal haptic dataset to date, comprising\n12.3 million datapoints across 5357 household objects. Using this dataset, we\ntrain a haptic encoder that can infer material and compliance object properties\nfrom multimodal haptic data. We leverage this encoder to create the CLAMP\nmodel, a visuo-haptic perception model for material recognition that\ngeneralizes to novel objects and three robot embodiments with minimal\nfinetuning. We also demonstrate the effectiveness of our model in three\nreal-world robot manipulation tasks: sorting recyclable and non-recyclable\nwaste, retrieving objects from a cluttered bag, and distinguishing overripe\nfrom ripe bananas. Our results show that large-scale, in-the-wild haptic data\ncollection can unlock new capabilities for generalizable robot manipulation.\nWebsite: https://emprise.cs.cornell.edu/clamp/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust robot manipulation in unstructured environments often requires\nunderstanding object properties that extend beyond geometry, such as material\nor compliance-properties that can be challenging to infer using vision alone.\nMultimodal haptic sensing provides a promising avenue for inferring such\nproperties, yet progress has been constrained by the lack of large, diverse,\nand realistic haptic datasets. In this work, we introduce the CLAMP device, a\nlow-cost (<\\$200) sensorized reacher-grabber designed to collect large-scale,\nin-the-wild multimodal haptic data from non-expert users in everyday settings.\nWe deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP\ndataset, the largest open-source multimodal haptic dataset to date, comprising\n12.3 million datapoints across 5357 household objects. Using this dataset, we\ntrain a haptic encoder that can infer material and compliance object properties\nfrom multimodal haptic data. We leverage this encoder to create the CLAMP\nmodel, a visuo-haptic perception model for material recognition that\ngeneralizes to novel objects and three robot embodiments with minimal\nfinetuning. We also demonstrate the effectiveness of our model in three\nreal-world robot manipulation tasks: sorting recyclable and non-recyclable\nwaste, retrieving objects from a cluttered bag, and distinguishing overripe\nfrom ripe bananas. Our results show that large-scale, in-the-wild haptic data\ncollection can unlock new capabilities for generalizable robot manipulation.\nWebsite: https://emprise.cs.cornell.edu/clamp/"
                },
                "authors": [
                    {
                        "name": "Pranav N. Thakkar"
                    },
                    {
                        "name": "Shubhangi Sinha"
                    },
                    {
                        "name": "Karan Baijal"
                    },
                    {
                        "name": "Yuhan"
                    },
                    {
                        "name": "Bian"
                    },
                    {
                        "name": "Leah Lackey"
                    },
                    {
                        "name": "Ben Dodson"
                    },
                    {
                        "name": "Heisen Kong"
                    },
                    {
                        "name": "Jueun Kwon"
                    },
                    {
                        "name": "Amber Li"
                    },
                    {
                        "name": "Yifei Hu"
                    },
                    {
                        "name": "Alexios Rekoutis"
                    },
                    {
                        "name": "Tom Silver"
                    },
                    {
                        "name": "Tapomayukh Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Tapomayukh Bhattacharjee"
                },
                "arxiv_affiliation": "Anjelica",
                "author": "Tapomayukh Bhattacharjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21493v1",
                "updated": "2025-05-27T17:56:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    56,
                    27,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:56:27Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    56,
                    27,
                    1,
                    147,
                    0
                ],
                "title": "Reinforcing General Reasoning without Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing General Reasoning without Verifiers"
                },
                "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree."
                },
                "authors": [
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Anya Sims"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Chao Du"
                    }
                ],
                "author_detail": {
                    "name": "Chao Du"
                },
                "author": "Chao Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21492v1",
                "updated": "2025-05-27T17:56:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    56,
                    11,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:56:11Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    56,
                    11,
                    1,
                    147,
                    0
                ],
                "title": "Dust removal timescale in galaxies across cosmic time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dust removal timescale in galaxies across cosmic time"
                },
                "summary": "Understanding the evolution of dust in galaxies is crucial because it affects\nthe dynamics and cooling of gas, star formation, and chemical evolution. Recent\nwork on dust removal in galaxies indicates timescales of gigayears, with old\nstellar populations and AGNs as the primary drivers of this process. However,\nmost statistically significant studies are focused on low redshifts $z < 0.4$.\nHere, we determine the dust removal timescale in galaxies over a wide range of\nredshifts, up to $z \\sim 5$. We use publicly available catalogue data of\ninfrared-selected galaxies, observed by \\textit{Herschel}. Using the inferred\ndust masses, stellar masses, and stellar ages, we calculate the dust removal\ntimescale in a sample of more than 120,000 galaxies. We find that, with\nincreasing redshift, the dust removal timescale decreases from 1.8 Gyr at\nredshift $z \\sim 0.05$ to less than 500\\,Myr at $z > 3$. Galaxies at higher\nredshifts undergo more efficient dust removal than galaxies at lower redshift,\nlikely driven by AGN activity, supernova shocks, and astration. These findings\nindicate that dust removal evolves over cosmic time, reflecting the changing\nmechanisms regulating dust content of galaxies as the Universe evolves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the evolution of dust in galaxies is crucial because it affects\nthe dynamics and cooling of gas, star formation, and chemical evolution. Recent\nwork on dust removal in galaxies indicates timescales of gigayears, with old\nstellar populations and AGNs as the primary drivers of this process. However,\nmost statistically significant studies are focused on low redshifts $z < 0.4$.\nHere, we determine the dust removal timescale in galaxies over a wide range of\nredshifts, up to $z \\sim 5$. We use publicly available catalogue data of\ninfrared-selected galaxies, observed by \\textit{Herschel}. Using the inferred\ndust masses, stellar masses, and stellar ages, we calculate the dust removal\ntimescale in a sample of more than 120,000 galaxies. We find that, with\nincreasing redshift, the dust removal timescale decreases from 1.8 Gyr at\nredshift $z \\sim 0.05$ to less than 500\\,Myr at $z > 3$. Galaxies at higher\nredshifts undergo more efficient dust removal than galaxies at lower redshift,\nlikely driven by AGN activity, supernova shocks, and astration. These findings\nindicate that dust removal evolves over cosmic time, reflecting the changing\nmechanisms regulating dust content of galaxies as the Universe evolves."
                },
                "authors": [
                    {
                        "name": "Aleksandra Leśniewska"
                    },
                    {
                        "name": "Jens Hjorth"
                    },
                    {
                        "name": "Christa Gall"
                    }
                ],
                "author_detail": {
                    "name": "Christa Gall"
                },
                "author": "Christa Gall",
                "arxiv_comment": "submitted to A&A, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21486v1",
                "updated": "2025-05-27T17:53:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    53,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:53:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    53,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive\n  Logic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive\n  Logic Programming"
                },
                "summary": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jiemin Wu"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21482v1",
                "updated": "2025-05-27T17:52:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    52,
                    42,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:52:42Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    52,
                    42,
                    1,
                    147,
                    0
                ],
                "title": "Tissue-specific predictive performance: A unified estimation and\n  inference framework for multi-category screening tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue-specific predictive performance: A unified estimation and\n  inference framework for multi-category screening tests"
                },
                "summary": "Multi-Cancer Early Detection (MCED) testing with tissue localization aims to\ndetect and identify multiple cancer types from a single blood sample. Such\ntests have the potential to aid clinical decisions and significantly improve\nhealth outcomes. Despite this promise, MCED testing has not yet achieved\nregulatory approval, reimbursement or broad clinical adoption. One major reason\nfor this shortcoming is uncertainty about test performance resulting from the\nreporting of clinically obtuse metrics. Traditionally, MCED tests report\naggregate measures of test performance, disregarding cancer type, that obscure\nbiological variability and underlying differences in the test's behavior,\nlimiting insight into true effectiveness. Clinically informative evaluation of\nan MCED test's performance requires metrics that are specific to cancer types.\nIn the context of a case-control sampling design, this paper derives analytical\nmethods that estimate cancer-specific intrinsic accuracy, tissue localization\nreadout-specific predictive value and the marginal test classification\ndistribution, each with corresponding confidence interval formulae. A\nsimulation study is presented that evaluates performance of the proposed\nmethodology and provides guidance for implementation. An application to a\npublished MCED test dataset is given. These statistical approaches allow for\nestimation and inference for the pointed metric of an MCED test that allow its\nevaluation to support a potential role in early cancer detection. This\nframework enables more precise clinical decision-making, supports optimized\ntrial designs across classical, digital, AI-driven, and hybrid stratified\ndiagnostic screening platforms, and facilitates informed healthcare decisions\nby clinicians, policymakers, regulators, scientists, and patients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cancer Early Detection (MCED) testing with tissue localization aims to\ndetect and identify multiple cancer types from a single blood sample. Such\ntests have the potential to aid clinical decisions and significantly improve\nhealth outcomes. Despite this promise, MCED testing has not yet achieved\nregulatory approval, reimbursement or broad clinical adoption. One major reason\nfor this shortcoming is uncertainty about test performance resulting from the\nreporting of clinically obtuse metrics. Traditionally, MCED tests report\naggregate measures of test performance, disregarding cancer type, that obscure\nbiological variability and underlying differences in the test's behavior,\nlimiting insight into true effectiveness. Clinically informative evaluation of\nan MCED test's performance requires metrics that are specific to cancer types.\nIn the context of a case-control sampling design, this paper derives analytical\nmethods that estimate cancer-specific intrinsic accuracy, tissue localization\nreadout-specific predictive value and the marginal test classification\ndistribution, each with corresponding confidence interval formulae. A\nsimulation study is presented that evaluates performance of the proposed\nmethodology and provides guidance for implementation. An application to a\npublished MCED test dataset is given. These statistical approaches allow for\nestimation and inference for the pointed metric of an MCED test that allow its\nevaluation to support a potential role in early cancer detection. This\nframework enables more precise clinical decision-making, supports optimized\ntrial designs across classical, digital, AI-driven, and hybrid stratified\ndiagnostic screening platforms, and facilitates informed healthcare decisions\nby clinicians, policymakers, regulators, scientists, and patients."
                },
                "authors": [
                    {
                        "name": "A. Gregory DiRienzo"
                    },
                    {
                        "name": "Elie Massaad"
                    },
                    {
                        "name": "Hutan Ashrafian"
                    }
                ],
                "author_detail": {
                    "name": "Hutan Ashrafian"
                },
                "author": "Hutan Ashrafian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21479v1",
                "updated": "2025-05-27T17:51:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    51,
                    18,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:51:18Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    51,
                    18,
                    1,
                    147,
                    0
                ],
                "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Language Models Consequentialist or Deontological Moral Reasoners?"
                },
                "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens ."
                },
                "authors": [
                    {
                        "name": "Keenan Samway"
                    },
                    {
                        "name": "Max Kleiman-Weiner"
                    },
                    {
                        "name": "David Guzman Piedrahita"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21478v1",
                "updated": "2025-05-27T17:50:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    50,
                    47,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:50:47Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    50,
                    47,
                    1,
                    147,
                    0
                ],
                "title": "Policy Optimized Text-to-Image Pipeline Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Optimized Text-to-Image Pipeline Design"
                },
                "summary": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Uri Gadot"
                    },
                    {
                        "name": "Rinon Gal"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Shie Mannor"
                    }
                ],
                "author_detail": {
                    "name": "Shie Mannor"
                },
                "author": "Shie Mannor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20547v4",
                "updated": "2025-05-27T17:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    47,
                    15,
                    1,
                    147,
                    0
                ],
                "published": "2024-09-30T17:48:22Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    48,
                    22,
                    0,
                    274,
                    0
                ],
                "title": "Annealing Flow Generative Models Towards Sampling High-Dimensional and\n  Multi-Modal Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annealing Flow Generative Models Towards Sampling High-Dimensional and\n  Multi-Modal Distributions"
                },
                "summary": "Sampling from high-dimensional, multi-modal distributions remains a\nfundamental challenge across domains such as statistical Bayesian inference and\nphysics-based machine learning. In this paper, we propose Annealing Flow (AF),\na method built on Continuous Normalizing Flow (CNF) for sampling from\nhigh-dimensional and multi-modal distributions. AF is trained with a dynamic\nOptimal Transport (OT) objective incorporating Wasserstein regularization, and\nguided by annealing procedures, facilitating effective exploration of modes in\nhigh-dimensional spaces. Compared to recent NF methods, AF greatly improves\ntraining efficiency and stability, with minimal reliance on MC assistance. We\ndemonstrate the superior performance of AF compared to state-of-the-art methods\nthrough experiments on various challenging distributions and real-world\ndatasets, particularly in high-dimensional and multi-modal settings. We also\nhighlight AF potential for sampling the least favorable distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from high-dimensional, multi-modal distributions remains a\nfundamental challenge across domains such as statistical Bayesian inference and\nphysics-based machine learning. In this paper, we propose Annealing Flow (AF),\na method built on Continuous Normalizing Flow (CNF) for sampling from\nhigh-dimensional and multi-modal distributions. AF is trained with a dynamic\nOptimal Transport (OT) objective incorporating Wasserstein regularization, and\nguided by annealing procedures, facilitating effective exploration of modes in\nhigh-dimensional spaces. Compared to recent NF methods, AF greatly improves\ntraining efficiency and stability, with minimal reliance on MC assistance. We\ndemonstrate the superior performance of AF compared to state-of-the-art methods\nthrough experiments on various challenging distributions and real-world\ndatasets, particularly in high-dimensional and multi-modal settings. We also\nhighlight AF potential for sampling the least favorable distributions."
                },
                "authors": [
                    {
                        "name": "Dongze Wu"
                    },
                    {
                        "name": "Yao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yao Xie"
                },
                "author": "Yao Xie",
                "arxiv_comment": "This paper has been accepted to ICML 2025 and will appear in the\n  Proceedings of Machine Learning Research (PMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21472v1",
                "updated": "2025-05-27T17:45:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:45:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive\n  Attention Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucination in Large Vision-Language Models via Adaptive\n  Attention Calibration"
                },
                "summary": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination."
                },
                "authors": [
                    {
                        "name": "Mehrdad Fazli"
                    },
                    {
                        "name": "Bowen Wei"
                    },
                    {
                        "name": "Ziwei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Zhu"
                },
                "author": "Ziwei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21473v1",
                "updated": "2025-05-27T17:45:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:45:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction"
                },
                "summary": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yiheng Liu"
                    },
                    {
                        "name": "Liao Qu"
                    },
                    {
                        "name": "Huichao Zhang"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Yiming Gao"
                    },
                    {
                        "name": "Hu Ye"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Daniel K. Du"
                    },
                    {
                        "name": "Shu Cheng"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Xinglong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinglong Wu"
                },
                "author": "Xinglong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21471v1",
                "updated": "2025-05-27T17:45:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:45:04Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    4,
                    1,
                    147,
                    0
                ],
                "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration"
                },
                "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications."
                },
                "authors": [
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Zhennan Wan"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "30 pages, 9 figures. Code and data are available at\n  https://github.com/THUNLP-MT/ExtAgents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21469v1",
                "updated": "2025-05-27T17:42:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    42,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:42:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    42,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "PropMolFlow: Property-guided Molecule Generation with Geometry-Complete\n  Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropMolFlow: Property-guided Molecule Generation with Geometry-Complete\n  Flow Matching"
                },
                "summary": "Molecule generation is advancing rapidly in chemical discovery and drug\ndesign. Flow matching methods have recently set the state of the art (SOTA) in\nunconditional molecule generation, surpassing score-based diffusion models.\nHowever, diffusion models still lead in property-guided generation. In this\nwork, we introduce PropMolFlow, a novel approach for property-guided molecule\ngeneration based on geometry-complete SE(3)-equivariant flow matching.\nIntegrating five different property embedding methods with a Gaussian expansion\nof scalar properties, PropMolFlow outperforms previous SOTA diffusion models in\nconditional molecule generation across various properties while preserving the\nstability and validity of the generated molecules, consistent with its\nunconditional counterpart. Additionally, it enables faster inference with\nsignificantly fewer time steps compared to baseline models. We highlight the\nimportance of validating the properties of generated molecules through DFT\ncalculations performed at the same level of theory as the training data.\nSpecifically, our analysis identifies properties that require DFT validation\nand others where a pretrained SE(3) geometric vector perceptron regressors\nprovide sufficiently accurate predictions on generated molecules. Furthermore,\nwe introduce a new property metric designed to assess the model's ability to\npropose molecules with underrepresented property values, assessing its capacity\nfor out-of-distribution generalization. Our findings reveal shortcomings in\nexisting structural metrics, which mistakenly validate open-shell molecules or\nmolecules with invalid valence-charge configurations, underscoring the need for\nimproved evaluation frameworks. Overall, this work paves the way for developing\ntargeted property-guided generation methods, enhancing the design of molecular\ngenerative models for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule generation is advancing rapidly in chemical discovery and drug\ndesign. Flow matching methods have recently set the state of the art (SOTA) in\nunconditional molecule generation, surpassing score-based diffusion models.\nHowever, diffusion models still lead in property-guided generation. In this\nwork, we introduce PropMolFlow, a novel approach for property-guided molecule\ngeneration based on geometry-complete SE(3)-equivariant flow matching.\nIntegrating five different property embedding methods with a Gaussian expansion\nof scalar properties, PropMolFlow outperforms previous SOTA diffusion models in\nconditional molecule generation across various properties while preserving the\nstability and validity of the generated molecules, consistent with its\nunconditional counterpart. Additionally, it enables faster inference with\nsignificantly fewer time steps compared to baseline models. We highlight the\nimportance of validating the properties of generated molecules through DFT\ncalculations performed at the same level of theory as the training data.\nSpecifically, our analysis identifies properties that require DFT validation\nand others where a pretrained SE(3) geometric vector perceptron regressors\nprovide sufficiently accurate predictions on generated molecules. Furthermore,\nwe introduce a new property metric designed to assess the model's ability to\npropose molecules with underrepresented property values, assessing its capacity\nfor out-of-distribution generalization. Our findings reveal shortcomings in\nexisting structural metrics, which mistakenly validate open-shell molecules or\nmolecules with invalid valence-charge configurations, underscoring the need for\nimproved evaluation frameworks. Overall, this work paves the way for developing\ntargeted property-guided generation methods, enhancing the design of molecular\ngenerative models for diverse applications."
                },
                "authors": [
                    {
                        "name": "Cheng Zeng"
                    },
                    {
                        "name": "Jirui Jin"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Mark Transtrum"
                    },
                    {
                        "name": "Ellad B. Tadmor"
                    },
                    {
                        "name": "Richard G. Hennig"
                    },
                    {
                        "name": "Adrian Roitberg"
                    },
                    {
                        "name": "Stefano Martiniani"
                    },
                    {
                        "name": "Mingjie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Liu"
                },
                "author": "Mingjie Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21952v2",
                "updated": "2025-05-27T17:41:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    41,
                    42,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-29T11:12:44Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    44,
                    1,
                    303,
                    0
                ],
                "title": "On the Robustness of Adversarial Training Against Uncertainty Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Adversarial Training Against Uncertainty Attacks"
                },
                "summary": "In learning problems, the noise inherent to the task at hand hinders the\npossibility to infer without a certain degree of uncertainty. Quantifying this\nuncertainty, regardless of its wide use, assumes high relevance for\nsecurity-sensitive applications. Within these scenarios, it becomes fundamental\nto guarantee good (i.e., trustworthy) uncertainty measures, which downstream\nmodules can securely employ to drive the final decision-making process.\nHowever, an attacker may be interested in forcing the system to produce either\n(i) highly uncertain outputs jeopardizing the system's availability or (ii) low\nuncertainty estimates, making the system accept uncertain samples that would\ninstead require a careful inspection (e.g., human intervention). Therefore, it\nbecomes fundamental to understand how to obtain robust uncertainty estimates\nagainst these kinds of attacks. In this work, we reveal both empirically and\ntheoretically that defending against adversarial examples, i.e., carefully\nperturbed samples that cause misclassification, additionally guarantees a more\nsecure, trustworthy uncertainty estimate under common attack scenarios without\nthe need for an ad-hoc defense strategy. To support our claims, we evaluate\nmultiple adversarial-robust models from the publicly available benchmark\nRobustBench on the CIFAR-10 and ImageNet datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In learning problems, the noise inherent to the task at hand hinders the\npossibility to infer without a certain degree of uncertainty. Quantifying this\nuncertainty, regardless of its wide use, assumes high relevance for\nsecurity-sensitive applications. Within these scenarios, it becomes fundamental\nto guarantee good (i.e., trustworthy) uncertainty measures, which downstream\nmodules can securely employ to drive the final decision-making process.\nHowever, an attacker may be interested in forcing the system to produce either\n(i) highly uncertain outputs jeopardizing the system's availability or (ii) low\nuncertainty estimates, making the system accept uncertain samples that would\ninstead require a careful inspection (e.g., human intervention). Therefore, it\nbecomes fundamental to understand how to obtain robust uncertainty estimates\nagainst these kinds of attacks. In this work, we reveal both empirically and\ntheoretically that defending against adversarial examples, i.e., carefully\nperturbed samples that cause misclassification, additionally guarantees a more\nsecure, trustworthy uncertainty estimate under common attack scenarios without\nthe need for an ad-hoc defense strategy. To support our claims, we evaluate\nmultiple adversarial-robust models from the publicly available benchmark\nRobustBench on the CIFAR-10 and ImageNet datasets."
                },
                "authors": [
                    {
                        "name": "Emanuele Ledda"
                    },
                    {
                        "name": "Giovanni Scodeller"
                    },
                    {
                        "name": "Daniele Angioni"
                    },
                    {
                        "name": "Giorgio Piras"
                    },
                    {
                        "name": "Antonio Emanuele Cinà"
                    },
                    {
                        "name": "Giorgio Fumera"
                    },
                    {
                        "name": "Battista Biggio"
                    },
                    {
                        "name": "Fabio Roli"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Roli"
                },
                "author": "Fabio Roli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21468v1",
                "updated": "2025-05-27T17:41:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    41,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    41,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Causal Posterior Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Posterior Estimation"
                },
                "summary": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field."
                },
                "authors": [
                    {
                        "name": "Simon Dirmeier"
                    },
                    {
                        "name": "Antonietta Mira"
                    }
                ],
                "author_detail": {
                    "name": "Antonietta Mira"
                },
                "author": "Antonietta Mira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00675v2",
                "updated": "2025-05-27T17:38:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    38,
                    40,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-01T17:31:33Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    31,
                    33,
                    3,
                    121,
                    0
                ],
                "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future\n  Directions"
                },
                "summary": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs)-based agents. While prior surveys have focused on memory\napplications with LLMs (e.g., enabling personalized memory in conversational\nagents), they often overlook the atomic operations that underlie memory\ndynamics. In this survey, we first categorize memory representations into\nparametric and contextual forms, and then introduce six fundamental memory\noperations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and\nCompression. We map these operations to the most relevant research topics\nacross long-term, long-context, parametric modification, and multi-source\nmemory. By reframing memory systems through the lens of atomic operations and\nrepresentation types, this survey provides a structured and dynamic perspective\non research, benchmark datasets, and tools related to memory in AI, clarifying\nthe functional interplay in LLMs based agents while outlining promising\ndirections for future research\\footnote{The paper list, datasets, methods and\ntools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs)-based agents. While prior surveys have focused on memory\napplications with LLMs (e.g., enabling personalized memory in conversational\nagents), they often overlook the atomic operations that underlie memory\ndynamics. In this survey, we first categorize memory representations into\nparametric and contextual forms, and then introduce six fundamental memory\noperations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and\nCompression. We map these operations to the most relevant research topics\nacross long-term, long-context, parametric modification, and multi-source\nmemory. By reframing memory systems through the lens of atomic operations and\nrepresentation types, this survey provides a structured and dynamic perspective\non research, benchmark datasets, and tools related to memory in AI, clarifying\nthe functional interplay in LLMs based agents while outlining promising\ndirections for future research\\footnote{The paper list, datasets, methods and\ntools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}."
                },
                "authors": [
                    {
                        "name": "Yiming Du"
                    },
                    {
                        "name": "Wenyu Huang"
                    },
                    {
                        "name": "Danna Zheng"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Sebastien Montella"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13398v2",
                "updated": "2025-05-27T17:37:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    37,
                    58,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-19T03:14:11Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    3,
                    14,
                    11,
                    2,
                    50,
                    0
                ],
                "title": "GeLLMO: Generalizing Large Language Models for Multi-property Molecule\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeLLMO: Generalizing Large Language Models for Multi-property Molecule\n  Optimization"
                },
                "summary": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce MuMOInstruct, the first high-quality\ninstruction-tuning dataset specifically focused on complex multi-property\nmolecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a\nseries of instruction-tuned LLMs for molecule optimization. Extensive\nevaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that\nGeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also\nexhibit outstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of GeLLMOs as foundational models for\nmolecule optimization, thereby tackling novel optimization tasks without\nresource-intensive retraining. MuMOInstruct, models, and code are accessible\nthrough https://github.com/ninglab/GeLLMO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce MuMOInstruct, the first high-quality\ninstruction-tuning dataset specifically focused on complex multi-property\nmolecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a\nseries of instruction-tuned LLMs for molecule optimization. Extensive\nevaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that\nGeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also\nexhibit outstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of GeLLMOs as foundational models for\nmolecule optimization, thereby tackling novel optimization tasks without\nresource-intensive retraining. MuMOInstruct, models, and code are accessible\nthrough https://github.com/ninglab/GeLLMO."
                },
                "authors": [
                    {
                        "name": "Vishal Dey"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "arxiv_comment": "Accepted to ACL Main 2025. Vishal Dey and Xiao Hu contributed equally\n  to this paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21458v1",
                "updated": "2025-05-27T17:30:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    30,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:30:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    30,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "Do LLMs Need to Think in One Language? Correlation between Latent\n  Language and Task Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Need to Think in One Language? Correlation between Latent\n  Language and Task Performance"
                },
                "summary": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance."
                },
                "authors": [
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Tatsuya Hiraoka"
                    },
                    {
                        "name": "Hiroto Otake"
                    },
                    {
                        "name": "Hiroki Ouchi"
                    },
                    {
                        "name": "Masaru Isonuma"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Taro Watanabe"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Yu Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Yu Takagi"
                },
                "author": "Yu Takagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21453v1",
                "updated": "2025-05-27T17:25:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    25,
                    25,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:25:25Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    25,
                    25,
                    1,
                    147,
                    0
                ],
                "title": "An Integrated Time-Varying Ornstein-Uhlenbeck Process for Jointly\n  Modeling Individual and Population-Level Dynamics of Golden Eagles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Integrated Time-Varying Ornstein-Uhlenbeck Process for Jointly\n  Modeling Individual and Population-Level Dynamics of Golden Eagles"
                },
                "summary": "With technological advancements, the quantity and quality of animal movement\ndata has increased greatly. Currently, there is no existing movement model that\ncan be used to describe full year of migratory species data that leverages both\nindividual movement data and species distribution data. Herein we propose a\nfull-year stochastic differential equation model for jointly modeling both\nindividual movement data and species distribution data. We show that this joint\nmodel, under certain assumptions, results in efficient computation of the\nspatio-temporal dynamics of the entire population, and thus provides\nstraightforward inference on the species distribution data. We illustrate this\nmodel with 215 bird-years of golden eagle movement in western North America and\ndata from eBird for the species distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With technological advancements, the quantity and quality of animal movement\ndata has increased greatly. Currently, there is no existing movement model that\ncan be used to describe full year of migratory species data that leverages both\nindividual movement data and species distribution data. Herein we propose a\nfull-year stochastic differential equation model for jointly modeling both\nindividual movement data and species distribution data. We show that this joint\nmodel, under certain assumptions, results in efficient computation of the\nspatio-temporal dynamics of the entire population, and thus provides\nstraightforward inference on the species distribution data. We illustrate this\nmodel with 215 bird-years of golden eagle movement in western North America and\ndata from eBird for the species distribution."
                },
                "authors": [
                    {
                        "name": "Michael L. Shull"
                    },
                    {
                        "name": "Ephraim M. Hanks"
                    },
                    {
                        "name": "James C. Russell"
                    },
                    {
                        "name": "Robert K. Murphy"
                    },
                    {
                        "name": "Frances E. Buderman"
                    }
                ],
                "author_detail": {
                    "name": "Frances E. Buderman"
                },
                "author": "Frances E. Buderman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09192v2",
                "updated": "2025-05-27T17:24:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    24,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-13T11:32:09Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    32,
                    9,
                    3,
                    44,
                    0
                ],
                "title": "Thinking beyond the anthropomorphic paradigm benefits LLM research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking beyond the anthropomorphic paradigm benefits LLM research"
                },
                "summary": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof research articles to present empirical evidence of the prevalence and growth\nof anthropomorphic terminology in research on large language models (LLMs). We\nargue for challenging the deeper assumptions reflected in this terminology --\nwhich, though often useful, may inadvertently constrain LLM development -- and\nbroadening beyond them to open new pathways for understanding and improving\nLLMs. Specifically, we identify and examine five anthropomorphic assumptions\nthat shape research across the LLM development lifecycle. For each assumption\n(e.g., that LLMs must use natural language for reasoning, or that they should\nbe evaluated on benchmarks originally meant for humans), we demonstrate\nempirical, non-anthropomorphic alternatives that remain under-explored yet\noffer promising directions for LLM research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof research articles to present empirical evidence of the prevalence and growth\nof anthropomorphic terminology in research on large language models (LLMs). We\nargue for challenging the deeper assumptions reflected in this terminology --\nwhich, though often useful, may inadvertently constrain LLM development -- and\nbroadening beyond them to open new pathways for understanding and improving\nLLMs. Specifically, we identify and examine five anthropomorphic assumptions\nthat shape research across the LLM development lifecycle. For each assumption\n(e.g., that LLMs must use natural language for reasoning, or that they should\nbe evaluated on benchmarks originally meant for humans), we demonstrate\nempirical, non-anthropomorphic alternatives that remain under-explored yet\noffer promising directions for LLM research and development."
                },
                "authors": [
                    {
                        "name": "Lujain Ibrahim"
                    },
                    {
                        "name": "Myra Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Myra Cheng"
                },
                "author": "Myra Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21451v1",
                "updated": "2025-05-27T17:23:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    23,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:23:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    23,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of\n  Violent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words Like Knives: Backstory-Personalized Modeling and Detection of\n  Violent Communication"
                },
                "summary": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection."
                },
                "authors": [
                    {
                        "name": "Jocelyn Shen"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Cynthia Breazeal"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Hae Won Park"
                    }
                ],
                "author_detail": {
                    "name": "Hae Won Park"
                },
                "author": "Hae Won Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21448v1",
                "updated": "2025-05-27T17:20:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    20,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    20,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniSync: Towards Universal Lip Synchronization via Diffusion\n  Transformers"
                },
                "summary": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos."
                },
                "authors": [
                    {
                        "name": "Ziqiao Peng"
                    },
                    {
                        "name": "Jiwen Liu"
                    },
                    {
                        "name": "Haoxian Zhang"
                    },
                    {
                        "name": "Xiaoqiang Liu"
                    },
                    {
                        "name": "Songlin Tang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Hongyan Liu"
                    },
                    {
                        "name": "Jun He"
                    }
                ],
                "author_detail": {
                    "name": "Jun He"
                },
                "author": "Jun He",
                "arxiv_comment": "https://ziqiaopeng.github.io/OmniSync/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21447v1",
                "updated": "2025-05-27T17:19:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    19,
                    41,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:19:41Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    19,
                    41,
                    1,
                    147,
                    0
                ],
                "title": "A Bayesian approach to the survivor average causal effect in\n  cluster-randomized crossover trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian approach to the survivor average causal effect in\n  cluster-randomized crossover trials"
                },
                "summary": "In cluster-randomized crossover (CRXO) trials, groups of individuals are\nrandomly assigned to two or more sequences of alternating treatments. Since\nclusters act as their own control, the CRXO design is typically more\nstatistically efficient than the usual parallel-arm trial. CRXO trials are\nincreasingly popular in many areas of health research where the number of\navailable clusters is limited. Further, in trials among severely ill patients,\nresearchers often want to assess the effect of treatments on secondary\nnon-terminal outcomes, but frequently in these studies, there are patients who\ndo not survive to have these measurements fully recorded. In this paper, we\nprovide a causal inference framework and treatment effect estimation methods\nfor addressing truncation by death in the setting of CRXO trials. We target the\nsurvivor average causal effect (SACE) estimand, a well-defined subgroup\ntreatment effect obtained via principal stratification. We propose novel\nstructural and standard modeling assumptions to enable SACE identification\nfollowed by estimation within a Bayesian paradigm. We evaluate the small-sample\nperformance of our proposed Bayesian approach for the estimation of the SACE in\nCRXO trial settings via simulation studies. We apply our methods to a\npreviously conducted two-period cross-sectional CRXO study examining the impact\nof proton pump inhibitors compared to histamine-2 receptor blockers on length\nof hospitalization among adults requiring invasive mechanical ventilation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cluster-randomized crossover (CRXO) trials, groups of individuals are\nrandomly assigned to two or more sequences of alternating treatments. Since\nclusters act as their own control, the CRXO design is typically more\nstatistically efficient than the usual parallel-arm trial. CRXO trials are\nincreasingly popular in many areas of health research where the number of\navailable clusters is limited. Further, in trials among severely ill patients,\nresearchers often want to assess the effect of treatments on secondary\nnon-terminal outcomes, but frequently in these studies, there are patients who\ndo not survive to have these measurements fully recorded. In this paper, we\nprovide a causal inference framework and treatment effect estimation methods\nfor addressing truncation by death in the setting of CRXO trials. We target the\nsurvivor average causal effect (SACE) estimand, a well-defined subgroup\ntreatment effect obtained via principal stratification. We propose novel\nstructural and standard modeling assumptions to enable SACE identification\nfollowed by estimation within a Bayesian paradigm. We evaluate the small-sample\nperformance of our proposed Bayesian approach for the estimation of the SACE in\nCRXO trial settings via simulation studies. We apply our methods to a\npreviously conducted two-period cross-sectional CRXO study examining the impact\nof proton pump inhibitors compared to histamine-2 receptor blockers on length\nof hospitalization among adults requiring invasive mechanical ventilation."
                },
                "authors": [
                    {
                        "name": "Dane Isenberg"
                    },
                    {
                        "name": "Michael O. Harhay"
                    },
                    {
                        "name": "Andrew B. Forbes"
                    },
                    {
                        "name": "Paul J. Young"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Nandita Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Nandita Mitra"
                },
                "author": "Nandita Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19184v2",
                "updated": "2025-05-27T17:17:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    17,
                    17,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-25T15:06:17Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    15,
                    6,
                    17,
                    6,
                    145,
                    0
                ],
                "title": "When Two LLMs Debate, Both Think They'll Win",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Two LLMs Debate, Both Think They'll Win"
                },
                "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."
                },
                "authors": [
                    {
                        "name": "Pradyumna Shyama Prasad"
                    },
                    {
                        "name": "Minh Nhat Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Nhat Nguyen"
                },
                "author": "Minh Nhat Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21444v1",
                "updated": "2025-05-27T17:16:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    16,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:16:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    16,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "Can Large Reasoning Models Self-Train?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Reasoning Models Self-Train?"
                },
                "summary": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Fahim Tajwar"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Jeff Schneider"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "arxiv_comment": "Project website: https://self-rewarding-llm-training.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01082v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01082v6",
                "updated": "2025-05-27T17:15:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    15,
                    3,
                    1,
                    147,
                    0
                ],
                "published": "2024-07-01T08:37:25Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    37,
                    25,
                    0,
                    183,
                    0
                ],
                "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs"
                },
                "summary": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its considerable impact on improving text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its considerable impact on improving text generation quality."
                },
                "authors": [
                    {
                        "name": "Minh Nhat Nguyen"
                    },
                    {
                        "name": "Andrew Baker"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Allen Roush"
                    },
                    {
                        "name": "Andreas Kirsch"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "arxiv_comment": "Oral presentation at ICLR 2025. Camera-ready version available at\n  https://iclr.cc/virtual/2025/poster/30358",
                "arxiv_journal_ref": "In Proceedings of the 2025 International Conference on Learning\n  Representations (ICLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01082v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01082v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13010v2",
                "updated": "2025-05-27T17:05:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    5,
                    15,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-18T16:29:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    29,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering:\n  Bridging the Gap Between LLMs and Evolving Medical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering:\n  Bridging the Gap Between LLMs and Evolving Medical Knowledge"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Rezaei"
                    },
                    {
                        "name": "Reza Saadati Fard"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Milad Lankarany"
                    }
                ],
                "author_detail": {
                    "name": "Milad Lankarany"
                },
                "author": "Milad Lankarany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v1",
                "updated": "2025-05-27T17:04:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11927v2",
                "updated": "2025-05-27T16:59:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    59,
                    37,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-16T16:13:55Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    13,
                    55,
                    0,
                    351,
                    0
                ],
                "title": "Transparent and Coherent Procedural Mistake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparent and Coherent Procedural Mistake Detection"
                },
                "summary": "Procedural mistake detection (PMD) is a challenging problem of classifying\nwhether a human user (observed through egocentric video) has successfully\nexecuted a task (specified by a procedural text). Despite significant recent\nefforts, machine performance in the wild remains nonviable, and the reasoning\nprocesses underlying this performance are opaque. As such, we extend PMD to\nrequire generating visual self-dialog rationales to inform decisions. Given the\nimpressive, mature image understanding capabilities observed in recent\nvision-and-language models (VLMs), we curate a suitable benchmark dataset for\nPMD based on individual frames. As our reformulation enables unprecedented\ntransparency, we leverage a natural language inference (NLI) model to formulate\ntwo automated metrics for the coherence of generated rationales. We establish\nbaselines for this reframed task, showing that while VLMs struggle\noff-the-shelf, their accuracy, coherence, and efficiency can be improved by\nincorporating these metrics into common inference and fine-tuning methods-\nthough not without tradeoff. Lastly, our multi-faceted metrics visualize common\noutcomes, highlighting areas for further improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural mistake detection (PMD) is a challenging problem of classifying\nwhether a human user (observed through egocentric video) has successfully\nexecuted a task (specified by a procedural text). Despite significant recent\nefforts, machine performance in the wild remains nonviable, and the reasoning\nprocesses underlying this performance are opaque. As such, we extend PMD to\nrequire generating visual self-dialog rationales to inform decisions. Given the\nimpressive, mature image understanding capabilities observed in recent\nvision-and-language models (VLMs), we curate a suitable benchmark dataset for\nPMD based on individual frames. As our reformulation enables unprecedented\ntransparency, we leverage a natural language inference (NLI) model to formulate\ntwo automated metrics for the coherence of generated rationales. We establish\nbaselines for this reframed task, showing that while VLMs struggle\noff-the-shelf, their accuracy, coherence, and efficiency can be improved by\nincorporating these metrics into common inference and fine-tuning methods-\nthough not without tradeoff. Lastly, our multi-faceted metrics visualize common\noutcomes, highlighting areas for further improvement."
                },
                "authors": [
                    {
                        "name": "Shane Storks"
                    },
                    {
                        "name": "Itamar Bar-Yossef"
                    },
                    {
                        "name": "Yayuan Li"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Jason J. Corso"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21427v1",
                "updated": "2025-05-27T16:57:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    57,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:57:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    57,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Policy Induction: Predicting Startup Success via Explainable\n  Memory-Augmented In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Induction: Predicting Startup Success via Explainable\n  Memory-Augmented In-Context Learning"
                },
                "summary": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms."
                },
                "authors": [
                    {
                        "name": "Xianling Mu"
                    },
                    {
                        "name": "Joseph Ternasky"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05159v2",
                "updated": "2025-05-27T16:54:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    54,
                    17,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-07T18:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    41,
                    21,
                    4,
                    38,
                    0
                ],
                "title": "A Lightweight Method to Disrupt Memorized Sequences in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Method to Disrupt Memorized Sequences in LLM"
                },
                "summary": "As language models scale, their performance improves dramatically across a\nwide range of tasks, but so does their tendency to memorize and regurgitate\nparts of their training data verbatim. This tradeoff poses serious legal,\nethical, and safety concerns, especially in real-world deployments. Existing\nmitigation techniques, such as differential privacy or model unlearning, often\nrequire retraining or access to internal weights making them impractical for\nmost users. In this work, we introduce TokenSwap, a lightweight, post-hoc\ndefense designed for realistic settings where the user can only access\ntoken-level outputs. Our key insight is that while large models are necessary\nfor high task performance, small models (e.g., DistilGPT-2) are often\nsufficient to assign fluent, grammatically plausible probabilities to common\nfunction words - and crucially, they memorize far less. By selectively swapping\ntoken probabilities between models, TokenSwap preserves the capabilities of\nlarge models while reducing their propensity for verbatim reproduction.\nEvaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact\nmemorization with negligible task degradation. Our method offers a practical,\naccessible solution for mitigating memorized generation in deployed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models scale, their performance improves dramatically across a\nwide range of tasks, but so does their tendency to memorize and regurgitate\nparts of their training data verbatim. This tradeoff poses serious legal,\nethical, and safety concerns, especially in real-world deployments. Existing\nmitigation techniques, such as differential privacy or model unlearning, often\nrequire retraining or access to internal weights making them impractical for\nmost users. In this work, we introduce TokenSwap, a lightweight, post-hoc\ndefense designed for realistic settings where the user can only access\ntoken-level outputs. Our key insight is that while large models are necessary\nfor high task performance, small models (e.g., DistilGPT-2) are often\nsufficient to assign fluent, grammatically plausible probabilities to common\nfunction words - and crucially, they memorize far less. By selectively swapping\ntoken probabilities between models, TokenSwap preserves the capabilities of\nlarge models while reducing their propensity for verbatim reproduction.\nEvaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact\nmemorization with negligible task degradation. Our method offers a practical,\naccessible solution for mitigating memorized generation in deployed LLMs."
                },
                "authors": [
                    {
                        "name": "Parjanya Prajakta Prashant"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Babak Salimi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Salimi"
                },
                "author": "Babak Salimi",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08313v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08313v4",
                "updated": "2025-05-27T16:54:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    54,
                    13,
                    1,
                    147,
                    0
                ],
                "published": "2024-08-15T17:59:57Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Symbolic Graphics Programs?"
                },
                "summary": "Against the backdrop of enthusiasm for large language models (LLMs), there is\na growing need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer semantic\nquestions about the images or 3D geometries without a vision encoder. To\nsemantically understand the symbolic programs, LLMs would need to possess the\nability to \"imagine\" and reason how the corresponding graphics content would\nlook with only the symbolic description of the local curvatures and strokes. We\nuse this task to evaluate LLMs by creating a large benchmark for the semantic\nvisual understanding of symbolic graphics programs, built procedurally with\nminimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against the backdrop of enthusiasm for large language models (LLMs), there is\na growing need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer semantic\nquestions about the images or 3D geometries without a vision encoder. To\nsemantically understand the symbolic programs, LLMs would need to possess the\nability to \"imagine\" and reason how the corresponding graphics content would\nlook with only the symbolic description of the local curvatures and strokes. We\nuse this task to evaluate LLMs by creating a large benchmark for the semantic\nvisual understanding of symbolic graphics programs, built procedurally with\nminimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks."
                },
                "authors": [
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "ICLR 2025 Spotlight (v4: 47 pages, 26 figures, project page:\n  https://sgp-bench.github.io/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08313v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08313v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11618v2",
                "updated": "2025-05-27T16:52:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    52,
                    19,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-16T18:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    32,
                    35,
                    4,
                    136,
                    0
                ],
                "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models:\n  Capabilities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models:\n  Capabilities and Challenges"
                },
                "summary": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs."
                },
                "authors": [
                    {
                        "name": "Pengrui Quan"
                    },
                    {
                        "name": "Brian Wang"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Liying Han"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21419v2",
                "updated": "2025-05-28T02:17:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    17,
                    40,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T16:43:45Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    43,
                    45,
                    1,
                    147,
                    0
                ],
                "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG\n  LLMs"
                },
                "summary": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Kenneth P. Birman"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth P. Birman"
                },
                "author": "Kenneth P. Birman",
                "arxiv_doi": "10.1145/3721146.3721958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in EuroMLSys2025",
                "arxiv_journal_ref": "2025, Association for Computing Machinery, Proceedings of the 5th\n  Workshop on Machine Learning and Systems, 139-147, 9, series EuroMLSys '25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21418v1",
                "updated": "2025-05-27T16:43:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    43,
                    31,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:43:31Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    43,
                    31,
                    1,
                    147,
                    0
                ],
                "title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused\n  Ultrasound Ablation Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused\n  Ultrasound Ablation Surgery"
                },
                "summary": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains."
                },
                "authors": [
                    {
                        "name": "Lina Zhao"
                    },
                    {
                        "name": "Jiaxing Bai"
                    },
                    {
                        "name": "Zihao Bian"
                    },
                    {
                        "name": "Qingyue Chen"
                    },
                    {
                        "name": "Yafang Li"
                    },
                    {
                        "name": "Guangbo Li"
                    },
                    {
                        "name": "Min He"
                    },
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Zongjiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zongjiu Zhang"
                },
                "author": "Zongjiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20993v2",
                "updated": "2025-05-27T16:41:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-30T14:57:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "Efficiently Scaling LLM Reasoning with Certaindex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Scaling LLM Reasoning with Certaindex"
                },
                "summary": "Test-time reasoning algorithms such as chain-of-thought, self-consistency,\nand MCTS enhance LLM problem-solving but can wastefully generate many tokens\nwithout improving accuracy. At the same time, we observe that these algorithms\nexhibit answer stabilization: their intermediate solutions often cease to\nchange after a certain point, and further investment of compute does not change\ntheir final answer. To quantify this phenomenon, we introduce Certaindex, an\nalgorithm-agnostic metric measuring this evolving stability, signaling when\nfurther computation is unlikely to alter the final result. Certaindex is\nlightweight, can accelerate reasoning program inference via early exit, and\nfurther enables dynamic token allocation, gang scheduling, and many\nopportunities when integrated with real-world LLM serving systems. To quantify\nreal-world benefits, we built Certaindex as a scheduler into Dynasor, our\nreasoning-aware LLM serving system, and demonstrate up to 50% compute savings\nand 3.3x higher throughput in real workloads with no accuracy drop. Our code is\navailable at https://github.com/hao-ai-lab/Dynasor.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time reasoning algorithms such as chain-of-thought, self-consistency,\nand MCTS enhance LLM problem-solving but can wastefully generate many tokens\nwithout improving accuracy. At the same time, we observe that these algorithms\nexhibit answer stabilization: their intermediate solutions often cease to\nchange after a certain point, and further investment of compute does not change\ntheir final answer. To quantify this phenomenon, we introduce Certaindex, an\nalgorithm-agnostic metric measuring this evolving stability, signaling when\nfurther computation is unlikely to alter the final result. Certaindex is\nlightweight, can accelerate reasoning program inference via early exit, and\nfurther enables dynamic token allocation, gang scheduling, and many\nopportunities when integrated with real-world LLM serving systems. To quantify\nreal-world benefits, we built Certaindex as a scheduler into Dynasor, our\nreasoning-aware LLM serving system, and demonstrate up to 50% compute savings\nand 3.3x higher throughput in real workloads with no accuracy drop. Our code is\navailable at https://github.com/hao-ai-lab/Dynasor.git"
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Zheyu Fu"
                    },
                    {
                        "name": "Zhongdongming Dai"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Yian Ma"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21413v1",
                "updated": "2025-05-27T16:41:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    19,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:41:19Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    19,
                    1,
                    147,
                    0
                ],
                "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation"
                },
                "summary": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning."
                },
                "authors": [
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Zirui Wu"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "arxiv_comment": "Code is available at https://github.com/xxxiaol/RefTool",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09598v2",
                "updated": "2025-05-27T16:40:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    40,
                    26,
                    1,
                    147,
                    0
                ],
                "published": "2025-03-12T17:59:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation"
                },
                "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world interactions. We curated EchoMist, the\nfirst comprehensive benchmark for implicit misinformation, where false\nassumptions are embedded in the query to LLMs. EchoMist targets circulated,\nharmful, and ever-evolving implicit misinformation from diverse sources,\nincluding realistic human-AI conversations and social media interactions.\nThrough extensive empirical studies on 15 state-of-the-art LLMs, we find that\ncurrent models perform alarmingly poorly on this task, often failing to detect\nfalse premises and generating counterfactual explanations. We also investigate\ntwo mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability\nto counter implicit misinformation. Our findings indicate that EchoMist remains\na persistent challenge and underscore the critical need to safeguard against\nthe risk of implicit misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world interactions. We curated EchoMist, the\nfirst comprehensive benchmark for implicit misinformation, where false\nassumptions are embedded in the query to LLMs. EchoMist targets circulated,\nharmful, and ever-evolving implicit misinformation from diverse sources,\nincluding realistic human-AI conversations and social media interactions.\nThrough extensive empirical studies on 15 state-of-the-art LLMs, we find that\ncurrent models perform alarmingly poorly on this task, often failing to detect\nfalse premises and generating counterfactual explanations. We also investigate\ntwo mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability\nto counter implicit misinformation. Our findings indicate that EchoMist remains\na persistent challenge and underscore the critical need to safeguard against\nthe risk of implicit misinformation."
                },
                "authors": [
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21411v1",
                "updated": "2025-05-27T16:40:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    40,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:40:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    40,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity"
                },
                "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B."
                },
                "authors": [
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xiaosong Li"
                    },
                    {
                        "name": "Fangcheng Liu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Hui Zang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Binfan Zheng"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Youliang Yan"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Peifeng Qin"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "arxiv_affiliation": "and Other Contributors",
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21409v1",
                "updated": "2025-05-27T16:33:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    33,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:33:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    33,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from\n  Large Language Models"
                },
                "summary": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality."
                },
                "authors": [
                    {
                        "name": "Dario Satriani"
                    },
                    {
                        "name": "Enzo Veltri"
                    },
                    {
                        "name": "Donatello Santoro"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21400v1",
                "updated": "2025-05-27T16:24:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    20,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:24:20Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    20,
                    1,
                    147,
                    0
                ],
                "title": "A Convergence Theory for Diffusion Language Models: An\n  Information-Theoretic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Convergence Theory for Diffusion Language Models: An\n  Information-Theoretic Perspective"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Changxiao Cai"
                    }
                ],
                "author_detail": {
                    "name": "Changxiao Cai"
                },
                "author": "Changxiao Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21399v1",
                "updated": "2025-05-27T16:24:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    2,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:24:02Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    2,
                    1,
                    147,
                    0
                ],
                "title": "Factual Self-Awareness in Language Models: Representation, Robustness,\n  and Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factual Self-Awareness in Language Models: Representation, Robustness,\n  and Scaling"
                },
                "summary": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability."
                },
                "authors": [
                    {
                        "name": "Hovhannes Tamoyan"
                    },
                    {
                        "name": "Subhabrata Dutta"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21397v1",
                "updated": "2025-05-27T16:23:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:23:53Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    53,
                    1,
                    147,
                    0
                ],
                "title": "DecisionFlow: Advancing Large Language Model as Principled Decision\n  Maker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecisionFlow: Advancing Large Language Model as Principled Decision\n  Maker"
                },
                "summary": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow."
                },
                "authors": [
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Shanyong Wang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11189v3",
                "updated": "2025-05-27T16:23:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    46,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-15T13:48:39Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    13,
                    48,
                    39,
                    6,
                    350,
                    0
                ],
                "title": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters"
                },
                "summary": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions with active NPCs have been a focus, price negotiations\nbetween merchant NPCs and players remain underexplored. First, passive pricing\nrefers to the limited ability of merchants to modify predefined item prices.\nSecond, passive communication means that merchants can only interact with\nplayers in a scripted manner. To tackle these issues and create an active\nmerchant NPC, we propose a merchant framework based on large language models\n(LLMs), called MART, which consists of an appraiser module and a negotiator\nmodule. We conducted two experiments to explore various implementation options\nunder different training methods and LLM sizes, considering a range of possible\ngame environments. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions with active NPCs have been a focus, price negotiations\nbetween merchant NPCs and players remain underexplored. First, passive pricing\nrefers to the limited ability of merchants to modify predefined item prices.\nSecond, passive communication means that merchants can only interact with\nplayers in a scripted manner. To tackle these issues and create an active\nmerchant NPC, we propose a merchant framework based on large language models\n(LLMs), called MART, which consists of an appraiser module and a negotiator\nmodule. We conducted two experiments to explore various implementation options\nunder different training methods and LLM sizes, considering a range of possible\ngame environments. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Accepted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21396v1",
                "updated": "2025-05-27T16:23:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    42,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:23:42Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    42,
                    1,
                    147,
                    0
                ],
                "title": "Improving Research Idea Generation Through Data: An Empirical\n  Investigation in Social Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Research Idea Generation Through Data: An Empirical\n  Investigation in Social Science"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings."
                },
                "authors": [
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xinyi Dong"
                    },
                    {
                        "name": "Xinyang Gao"
                    },
                    {
                        "name": "Yansong Feng"
                    },
                    {
                        "name": "Xun Pang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Pang"
                },
                "author": "Xun Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07910v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07910v8",
                "updated": "2025-05-27T16:21:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    21,
                    8,
                    1,
                    147,
                    0
                ],
                "published": "2024-05-13T16:42:55Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    16,
                    42,
                    55,
                    0,
                    134,
                    0
                ],
                "title": "Improving Causal Inference with Measurement Errors in Exposures and\n  Confounders: A New Method and Its Application to Air Pollution Exposure\n  Assessment and Epidemiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Causal Inference with Measurement Errors in Exposures and\n  Confounders: A New Method and Its Application to Air Pollution Exposure\n  Assessment and Epidemiology"
                },
                "summary": "When exposure measurement error (EME), confounder measurement error (CME), or\nboth are present, health effect estimates regarding exposure mixtures and\ncritical exposure time-window may not represent the true effects. For example,\nin air pollution epidemiology, modeled estimates for multiple air pollutants\nand meteorological factors may serve as surrogates for exposures and\nconfounders. Methods for simultaneously addressing EME and CME remain\nunderstudied. We developed a two-stage causal effect modeling framework to\nestimate average exposure/treatment effects (AEE) by addressing EME and CME. We\nidentified conditions under which AEE is identifiable with minimal bias given\nlinear or non-linear potential outcomes models and developed a new method,\nreferred to as multi-dimensional regression calibration (MRC). The first stage\nof the framework estimates MRC models. The second stage estimates AEE by using\ng-computation with MR-Calibrated variables. Simulation analyses confirmed the\nbias-correction capability. As an application, we analyzed the association\nbetween air pollution and COVID-19 mortality in Cook County, Illinois. We\ndeveloped machine learning-based 500m-gridded daily estimates of air pollutants\nand meteorological factors in a way for what we refer to as doubly\nEME&CME-correction. Using distributed lag variables, a one interquartile range\n(22.7ppb) increase in 3-week O3 exposure below 70ppb was associated with an\n135.3% (95% CI: 68.4, 233.0) increase in COVID-19 mortality risk, comparable to\nthat for PM2.5 exposure, which contradicts the previously reported no\nassociation for 3-week O3 in Cook County. At low levels, reducing pollution may\nhave helped prevent premature deaths from COVID-19. Our new framework can\naddress measurement error in multiple covariates simultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When exposure measurement error (EME), confounder measurement error (CME), or\nboth are present, health effect estimates regarding exposure mixtures and\ncritical exposure time-window may not represent the true effects. For example,\nin air pollution epidemiology, modeled estimates for multiple air pollutants\nand meteorological factors may serve as surrogates for exposures and\nconfounders. Methods for simultaneously addressing EME and CME remain\nunderstudied. We developed a two-stage causal effect modeling framework to\nestimate average exposure/treatment effects (AEE) by addressing EME and CME. We\nidentified conditions under which AEE is identifiable with minimal bias given\nlinear or non-linear potential outcomes models and developed a new method,\nreferred to as multi-dimensional regression calibration (MRC). The first stage\nof the framework estimates MRC models. The second stage estimates AEE by using\ng-computation with MR-Calibrated variables. Simulation analyses confirmed the\nbias-correction capability. As an application, we analyzed the association\nbetween air pollution and COVID-19 mortality in Cook County, Illinois. We\ndeveloped machine learning-based 500m-gridded daily estimates of air pollutants\nand meteorological factors in a way for what we refer to as doubly\nEME&CME-correction. Using distributed lag variables, a one interquartile range\n(22.7ppb) increase in 3-week O3 exposure below 70ppb was associated with an\n135.3% (95% CI: 68.4, 233.0) increase in COVID-19 mortality risk, comparable to\nthat for PM2.5 exposure, which contradicts the previously reported no\nassociation for 3-week O3 in Cook County. At low levels, reducing pollution may\nhave helped prevent premature deaths from COVID-19. Our new framework can\naddress measurement error in multiple covariates simultaneously."
                },
                "authors": [
                    {
                        "name": "Honghyok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Honghyok Kim"
                },
                "author": "Honghyok Kim",
                "arxiv_comment": "Revisited/Revised",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07910v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07910v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21392v1",
                "updated": "2025-05-27T16:20:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    20,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:20:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    20,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "Kilonova modelling and parameter inference: Understanding uncertainties\n  and evaluating compatibility between observations and models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonova modelling and parameter inference: Understanding uncertainties\n  and evaluating compatibility between observations and models"
                },
                "summary": "In the study of optical transients, parameter inference is the process of\nextracting physical information, i.e. constraints on the source's\ncharacteristics, by comparing the observed lightcurves to the predictions of\ndifferent models and finding the model and parameter combination that make the\nclosest match. In the developing field of the study of kilonovae (KNe),\nsystematic uncertainties in modelling are still very large, and many models\nstruggle to fit satisfactorily the whole multi-wavelength dataset of the\nAT2017gfo kilonova, associated to the Binary Neutron Star (BNS) merger\nGW170817. In a multi-messenger context, we sometime observe tensions between\nKN-only inference results and constraints from other messengers. In order to\ndiscuss the compatibility of KN models with observations and with the\ninformation derived from other messengers, we detail the process of Bayesian\nparameter inference, identifying the many sources of uncertainty embedded in KN\nanalyses. We highlight the systematic error margin hyperparameter $\\sigma_{\\rm\nsys}$, which can be exploited as a metric for a model's goodness-of-fit. We\nthen discuss how to assess the performance of parameter inference analyses by\nquantifying the information gain using the Kullback-Leibler divergence between\nprior and posterior. Using the example of the Bu2019lm model with the NMMA\nBayesian inference framework, we showcase the expected performance that\ndedicated KN follow-ups with telescope networks could reasonably reach,\nhighlighting the different factors (observational cadence, error margins) that\ninfluence such inference performances. We finally apply our KN analysis to the\ndataset of AT2017gfo to validate our performance predictions and discuss the\ncomplementarity of multi-messenger approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the study of optical transients, parameter inference is the process of\nextracting physical information, i.e. constraints on the source's\ncharacteristics, by comparing the observed lightcurves to the predictions of\ndifferent models and finding the model and parameter combination that make the\nclosest match. In the developing field of the study of kilonovae (KNe),\nsystematic uncertainties in modelling are still very large, and many models\nstruggle to fit satisfactorily the whole multi-wavelength dataset of the\nAT2017gfo kilonova, associated to the Binary Neutron Star (BNS) merger\nGW170817. In a multi-messenger context, we sometime observe tensions between\nKN-only inference results and constraints from other messengers. In order to\ndiscuss the compatibility of KN models with observations and with the\ninformation derived from other messengers, we detail the process of Bayesian\nparameter inference, identifying the many sources of uncertainty embedded in KN\nanalyses. We highlight the systematic error margin hyperparameter $\\sigma_{\\rm\nsys}$, which can be exploited as a metric for a model's goodness-of-fit. We\nthen discuss how to assess the performance of parameter inference analyses by\nquantifying the information gain using the Kullback-Leibler divergence between\nprior and posterior. Using the example of the Bu2019lm model with the NMMA\nBayesian inference framework, we showcase the expected performance that\ndedicated KN follow-ups with telescope networks could reasonably reach,\nhighlighting the different factors (observational cadence, error margins) that\ninfluence such inference performances. We finally apply our KN analysis to the\ndataset of AT2017gfo to validate our performance predictions and discuss the\ncomplementarity of multi-messenger approaches."
                },
                "authors": [
                    {
                        "name": "Thomas Hussenot-Desenonges"
                    },
                    {
                        "name": "Marion Pillas"
                    },
                    {
                        "name": "Sarah Antier"
                    },
                    {
                        "name": "Patrice Hello"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    }
                ],
                "author_detail": {
                    "name": "Peter T. H. Pang"
                },
                "arxiv_affiliation": "Institute for Gravitational and Subatomic Physics",
                "author": "Peter T. H. Pang",
                "arxiv_comment": "23 pages, 16 figures, 6 tables (1 sidewaystable)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01834v2",
                "updated": "2025-05-27T16:17:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    17,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2024-11-04T06:07:53Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    7,
                    53,
                    0,
                    309,
                    0
                ],
                "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback"
                },
                "summary": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs."
                },
                "authors": [
                    {
                        "name": "Guan-Ting Lin"
                    },
                    {
                        "name": "Prashanth Gurunath Shivakumar"
                    },
                    {
                        "name": "Aditya Gourav"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Ankur Gandhe"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Ivan Bulyko"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Bulyko"
                },
                "author": "Ivan Bulyko",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21385v1",
                "updated": "2025-05-27T16:12:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    12,
                    43,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:12:43Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    12,
                    43,
                    1,
                    147,
                    0
                ],
                "title": "Dynamic Vision from EEG Brain Recordings: How much does EEG know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Vision from EEG Brain Recordings: How much does EEG know?"
                },
                "summary": "Reconstructing and understanding dynamic visual information (video) from\nbrain EEG recordings is challenging due to the non-stationary nature of EEG\nsignals, their low signal-to-noise ratio (SNR), and the limited availability of\nEEG-Video stimulus datasets. Most recent studies have focused on reconstructing\nstatic images from EEG recordings. In this work, we propose a framework to\nreconstruct dynamic visual stimuli from EEG data and conduct an in-depth study\nof the information encoded in EEG signals. Our approach first trains a feature\nextraction network using a triplet-based contrastive learning strategy within\nan EEG-video generation framework. The extracted EEG features are then used for\nvideo synthesis with a modified StyleGAN-ADA, which incorporates temporal\ninformation as conditioning. Additionally, we analyze how different brain\nregions contribute to processing dynamic visual stimuli. Through several\nempirical studies, we evaluate the effectiveness of our framework and\ninvestigate how much dynamic visual information can be inferred from EEG\nsignals. The inferences we derive through our extensive studies would be of\nimmense value to future research on extracting visual dynamics from EEG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing and understanding dynamic visual information (video) from\nbrain EEG recordings is challenging due to the non-stationary nature of EEG\nsignals, their low signal-to-noise ratio (SNR), and the limited availability of\nEEG-Video stimulus datasets. Most recent studies have focused on reconstructing\nstatic images from EEG recordings. In this work, we propose a framework to\nreconstruct dynamic visual stimuli from EEG data and conduct an in-depth study\nof the information encoded in EEG signals. Our approach first trains a feature\nextraction network using a triplet-based contrastive learning strategy within\nan EEG-video generation framework. The extracted EEG features are then used for\nvideo synthesis with a modified StyleGAN-ADA, which incorporates temporal\ninformation as conditioning. Additionally, we analyze how different brain\nregions contribute to processing dynamic visual stimuli. Through several\nempirical studies, we evaluate the effectiveness of our framework and\ninvestigate how much dynamic visual information can be inferred from EEG\nsignals. The inferences we derive through our extensive studies would be of\nimmense value to future research on extracting visual dynamics from EEG."
                },
                "authors": [
                    {
                        "name": "Prajwal Singh"
                    },
                    {
                        "name": "Anupam Sharma"
                    },
                    {
                        "name": "Pankaj Pandey"
                    },
                    {
                        "name": "Krishna Miyapuram"
                    },
                    {
                        "name": "Shanmuganathan Raman"
                    }
                ],
                "author_detail": {
                    "name": "Shanmuganathan Raman"
                },
                "author": "Shanmuganathan Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21382v1",
                "updated": "2025-05-27T16:10:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    10,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:10:53Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    10,
                    53,
                    1,
                    147,
                    0
                ],
                "title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation\n  of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation\n  of Foundation Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions."
                },
                "authors": [
                    {
                        "name": "Nastaran Saadati"
                    },
                    {
                        "name": "Zhanhong Jiang"
                    },
                    {
                        "name": "Joshua R. Waite"
                    },
                    {
                        "name": "Shreyan Ganguly"
                    },
                    {
                        "name": "Aditya Balu"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Soumik Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Soumik Sarkar"
                },
                "author": "Soumik Sarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21378v1",
                "updated": "2025-05-27T16:07:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    7,
                    33,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:07:33Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    7,
                    33,
                    1,
                    147,
                    0
                ],
                "title": "Analyzing values about gendered language reform in LLMs' revisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing values about gendered language reform in LLMs' revisions"
                },
                "summary": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment."
                },
                "authors": [
                    {
                        "name": "Jules Watson"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Raymond Liu"
                    },
                    {
                        "name": "Suzanne Stevenson"
                    },
                    {
                        "name": "Barend Beekhuizen"
                    }
                ],
                "author_detail": {
                    "name": "Barend Beekhuizen"
                },
                "author": "Barend Beekhuizen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05203v2",
                "updated": "2025-05-27T16:06:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    6,
                    13,
                    1,
                    147,
                    0
                ],
                "published": "2025-03-07T07:48:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    48,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "Path Pooling: Training-Free Structure Enhancement for Efficient\n  Knowledge Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path Pooling: Training-Free Structure Enhancement for Efficient\n  Knowledge Graph Retrieval-Augmented Generation"
                },
                "summary": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, training-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, training-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost."
                },
                "authors": [
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21372v1",
                "updated": "2025-05-27T16:01:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    49,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:01:49Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    49,
                    1,
                    147,
                    0
                ],
                "title": "Improving LLM-based Global Optimization with Search Space Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM-based Global Optimization with Search Space Partitioning"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies."
                },
                "authors": [
                    {
                        "name": "Andrej Schwanke"
                    },
                    {
                        "name": "Lyubomir Ivanov"
                    },
                    {
                        "name": "David Salinas"
                    },
                    {
                        "name": "Fabio Ferreira"
                    },
                    {
                        "name": "Aaron Klein"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Arber Zela"
                    }
                ],
                "author_detail": {
                    "name": "Arber Zela"
                },
                "author": "Arber Zela",
                "arxiv_comment": "25 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21371v1",
                "updated": "2025-05-27T16:01:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:01:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "When Experimental Economics Meets Large Language Models: Tactics with\n  Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Experimental Economics Meets Large Language Models: Tactics with\n  Evidence"
                },
                "summary": "Advancements in large language models (LLMs) have sparked a growing interest\nin measuring and understanding their behavior through experimental economics.\nHowever, there is still a lack of established guidelines for designing economic\nexperiments for LLMs. By combining principles from experimental economics with\ninsights from LLM research in artificial intelligence, we outline and discuss\neight practical tactics for conducting experiments with LLMs. We further\nperform two sets of experiments to demonstrate the significance of these\ntactics. Our study enhances the design, replicability, and generalizability of\nLLM experiments, and broadens the scope of experimental economics in the\ndigital age.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have sparked a growing interest\nin measuring and understanding their behavior through experimental economics.\nHowever, there is still a lack of established guidelines for designing economic\nexperiments for LLMs. By combining principles from experimental economics with\ninsights from LLM research in artificial intelligence, we outline and discuss\neight practical tactics for conducting experiments with LLMs. We further\nperform two sets of experiments to demonstrate the significance of these\ntactics. Our study enhances the design, replicability, and generalizability of\nLLM experiments, and broadens the scope of experimental economics in the\ndigital age."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Shuhuai Zhang"
                    },
                    {
                        "name": "Jianuo Gai"
                    },
                    {
                        "name": "Tracy Xiao Liu"
                    },
                    {
                        "name": "Songfa Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Songfa Zhong"
                },
                "author": "Songfa Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21362v1",
                "updated": "2025-05-27T15:52:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    52,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:52:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    52,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs.\n  Dialogue History",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs.\n  Dialogue History"
                },
                "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation."
                },
                "authors": [
                    {
                        "name": "Qishuai Zhong"
                    },
                    {
                        "name": "Zongmin Li"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17266v2",
                "updated": "2025-05-27T15:50:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    50,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-22T20:24:08Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    24,
                    8,
                    3,
                    142,
                    0
                ],
                "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT\n  Reasoning"
                },
                "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost."
                },
                "authors": [
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21354v1",
                "updated": "2025-05-27T15:47:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    47,
                    10,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:47:10Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    47,
                    10,
                    1,
                    147,
                    0
                ],
                "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning"
                },
                "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies."
                },
                "authors": [
                    {
                        "name": "Bidyarthi Paul"
                    },
                    {
                        "name": "Jalisha Jashim Era"
                    },
                    {
                        "name": "Mirazur Rahman Zim"
                    },
                    {
                        "name": "Tahmid Sattar Aothoi"
                    },
                    {
                        "name": "Faisal Muhammad Shah"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Muhammad Shah"
                },
                "author": "Faisal Muhammad Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21342v2",
                "updated": "2025-05-28T10:15:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    10,
                    15,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T15:34:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    34,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims"
                },
                "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code."
                },
                "authors": [
                    {
                        "name": "Valentin Knappich"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    },
                    {
                        "name": "Anna Hätty"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15949v2",
                "updated": "2025-05-27T15:32:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    32,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2024-08-28T17:08:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Probing Lorentz invariance with a high-energy neutrino flare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Lorentz invariance with a high-energy neutrino flare"
                },
                "summary": "Time-of-flight measurements of high-energy astrophysical neutrinos can be\nused to probe Lorentz invariance, a pillar of modern physics. If\nLorentz-invariance violation (LIV) occurs, it could cause neutrinos to slow\ndown, with the delay scaling linearly or quadratically with their energy. We\nintroduce non-parametric statistical methods designed to detect LIV-induced\ndistortions in the temporal structure of a high-energy neutrino flare as it\ntravels to Earth from a distant astrophysical source, independently of the\nintrinsic timing properties of the source. Our approach, illustrated using the\n2014/2015 TeV-PeV neutrino flare from the blazar TXS 0506+056 detected by\nIceCube, finds that the LIV energy scale must exceed 10^{14} GeV (linear) or\n10^9 GeV (quadratic). Our methods provide a robust means to investigate LIV by\nfocusing solely on a neutrino flare, without relying on electromagnetic\ncounterparts, and account for realistic energy and directional uncertainties.\nFor completeness, we compare our limits inferred from TXS 0506+056 to the\nsensitivity inferred from multi-messenger detection of tentative coincidences\nbetween neutrinos and electromagnetic emission from active galactic nuclei and\ntidal disruption events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-of-flight measurements of high-energy astrophysical neutrinos can be\nused to probe Lorentz invariance, a pillar of modern physics. If\nLorentz-invariance violation (LIV) occurs, it could cause neutrinos to slow\ndown, with the delay scaling linearly or quadratically with their energy. We\nintroduce non-parametric statistical methods designed to detect LIV-induced\ndistortions in the temporal structure of a high-energy neutrino flare as it\ntravels to Earth from a distant astrophysical source, independently of the\nintrinsic timing properties of the source. Our approach, illustrated using the\n2014/2015 TeV-PeV neutrino flare from the blazar TXS 0506+056 detected by\nIceCube, finds that the LIV energy scale must exceed 10^{14} GeV (linear) or\n10^9 GeV (quadratic). Our methods provide a robust means to investigate LIV by\nfocusing solely on a neutrino flare, without relying on electromagnetic\ncounterparts, and account for realistic energy and directional uncertainties.\nFor completeness, we compare our limits inferred from TXS 0506+056 to the\nsensitivity inferred from multi-messenger detection of tentative coincidences\nbetween neutrinos and electromagnetic emission from active galactic nuclei and\ntidal disruption events."
                },
                "authors": [
                    {
                        "name": "Mauricio Bustamante"
                    },
                    {
                        "name": "John Ellis"
                    },
                    {
                        "name": "Rostislav Konoplich"
                    },
                    {
                        "name": "Alexander S. Sakharov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander S. Sakharov"
                },
                "author": "Alexander S. Sakharov",
                "arxiv_comment": "19 pages, 7 figures, plus an appendix, version accepted for\n  publication in Physical Review D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11642v2",
                "updated": "2025-05-27T15:31:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    31,
                    58,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-16T19:08:29Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    19,
                    8,
                    29,
                    4,
                    136,
                    0
                ],
                "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks\n  Through Mutual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks\n  Through Mutual Reasoning"
                },
                "summary": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."
                },
                "authors": [
                    {
                        "name": "Falong Fan"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "arxiv_comment": "This paper has been accepted to IEEE IRI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06832v3",
                "updated": "2025-05-27T15:29:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    29,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-05T20:45:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    45,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach"
                },
                "summary": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods. The code is publicly\navailable at https://github.com/TIML-Group/Robust-MoE-Dual-Model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods. The code is publicly\navailable at https://github.com/TIML-Group/Robust-MoE-Dual-Model."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Ziqing Hu"
                    },
                    {
                        "name": "Ren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ren Wang"
                },
                "author": "Ren Wang",
                "arxiv_comment": "15 pages, 7 figures, accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21334v1",
                "updated": "2025-05-27T15:28:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    28,
                    45,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:28:45Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    28,
                    45,
                    1,
                    147,
                    0
                ],
                "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoliTom: Holistic Token Merging for Fast Video Large Language Models"
                },
                "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference."
                },
                "authors": [
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21333v1",
                "updated": "2025-05-27T15:27:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    27,
                    46,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:27:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    27,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios."
                },
                "authors": [
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Huanqian Wang"
                    },
                    {
                        "name": "Wulin Xie"
                    },
                    {
                        "name": "Huanyao Zhang"
                    },
                    {
                        "name": "Lijie Zhao"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Zhuoer Wen"
                    },
                    {
                        "name": "Wenting Liu"
                    },
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Bohan Zeng"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Yang"
                },
                "author": "Wenjing Yang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07101v2",
                "updated": "2025-05-27T15:24:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    24,
                    36,
                    1,
                    147,
                    0
                ],
                "published": "2024-09-11T08:45:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    45,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Statistical Finite Elements via Interacting Particle Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Finite Elements via Interacting Particle Langevin Dynamics"
                },
                "summary": "In this paper, we develop a class of interacting particle Langevin algorithms\nto solve inverse problems for partial differential equations (PDEs). In\nparticular, we leverage the statistical finite elements (statFEM) formulation\nto obtain a finite-dimensional latent variable statistical model where the\nparameter is that of the (discretised) forward map and the latent variable is\nthe statFEM solution of the PDE which is assumed to be partially observed. We\nthen adapt a recently proposed expectation-maximisation like scheme,\ninteracting particle Langevin algorithm (IPLA), for this problem and obtain a\njoint estimation procedure for the parameters and the latent variables. We\nconsider three main examples: (i) estimating the forcing for linear Poisson\nPDE, (ii) estimating diffusivity for linear Poisson PDE, and (iii) estimating\nthe forcing for nonlinear Poisson PDE. We provide computational complexity\nestimates for forcing estimation in the linear case. We also provide\ncomprehensive numerical experiments and preconditioning strategies that\nsignificantly improve the performance, showing that the proposed class of\nmethods can be the choice for parameter inference in PDE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we develop a class of interacting particle Langevin algorithms\nto solve inverse problems for partial differential equations (PDEs). In\nparticular, we leverage the statistical finite elements (statFEM) formulation\nto obtain a finite-dimensional latent variable statistical model where the\nparameter is that of the (discretised) forward map and the latent variable is\nthe statFEM solution of the PDE which is assumed to be partially observed. We\nthen adapt a recently proposed expectation-maximisation like scheme,\ninteracting particle Langevin algorithm (IPLA), for this problem and obtain a\njoint estimation procedure for the parameters and the latent variables. We\nconsider three main examples: (i) estimating the forcing for linear Poisson\nPDE, (ii) estimating diffusivity for linear Poisson PDE, and (iii) estimating\nthe forcing for nonlinear Poisson PDE. We provide computational complexity\nestimates for forcing estimation in the linear case. We also provide\ncomprehensive numerical experiments and preconditioning strategies that\nsignificantly improve the performance, showing that the proposed class of\nmethods can be the choice for parameter inference in PDE models."
                },
                "authors": [
                    {
                        "name": "Alex Glyn-Davies"
                    },
                    {
                        "name": "Connor Duffin"
                    },
                    {
                        "name": "Ieva Kazlauskaite"
                    },
                    {
                        "name": "Mark Girolami"
                    },
                    {
                        "name": "Ö. Deniz Akyildiz"
                    }
                ],
                "author_detail": {
                    "name": "Ö. Deniz Akyildiz"
                },
                "author": "Ö. Deniz Akyildiz",
                "arxiv_comment": "26 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21324v1",
                "updated": "2025-05-27T15:22:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    22,
                    1,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:22:01Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    22,
                    1,
                    1,
                    147,
                    0
                ],
                "title": "Leveraging large language models and traditional machine learning\n  ensembles for ADHD detection from narrative transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models and traditional machine learning\n  ensembles for ADHD detection from narrative transcripts"
                },
                "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhu"
                    },
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Noah Marchuck"
                    },
                    {
                        "name": "Abeed Sarker"
                    },
                    {
                        "name": "Yun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Wang"
                },
                "author": "Yun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21318v1",
                "updated": "2025-05-27T15:15:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    15,
                    44,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:15:44Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    15,
                    44,
                    1,
                    147,
                    0
                ],
                "title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular\n  Chemical Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular\n  Chemical Operations"
                },
                "summary": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "He Cao"
                    },
                    {
                        "name": "Bin Feng"
                    },
                    {
                        "name": "Yanjun Shao"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Yu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu Li"
                },
                "author": "Yu Li",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21317v1",
                "updated": "2025-05-27T15:15:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    15,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:15:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    15,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for\n  Improving Transcriptomics Representations through Morphological Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for\n  Improving Transcriptomics Representations through Morphological Features"
                },
                "summary": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks."
                },
                "authors": [
                    {
                        "name": "Ihab Bendidi"
                    },
                    {
                        "name": "Yassir El Mesbahi"
                    },
                    {
                        "name": "Alisandra K. Denton"
                    },
                    {
                        "name": "Karush Suri"
                    },
                    {
                        "name": "Kian Kenyon-Dean"
                    },
                    {
                        "name": "Auguste Genovesio"
                    },
                    {
                        "name": "Emmanuel Noutahi"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Noutahi"
                },
                "author": "Emmanuel Noutahi",
                "arxiv_comment": "ICML 2025 Main Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21315v1",
                "updated": "2025-05-27T15:13:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    13,
                    8,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:13:08Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    13,
                    8,
                    1,
                    147,
                    0
                ],
                "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead"
                },
                "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages."
                },
                "authors": [
                    {
                        "name": "Jesujoba O. Alabi"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "arxiv_comment": "Working paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21304v1",
                "updated": "2025-05-27T15:06:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    6,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:06:04Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    6,
                    4,
                    1,
                    147,
                    0
                ],
                "title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with\n  Limited Participants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with\n  Limited Participants"
                },
                "summary": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort."
                },
                "authors": [
                    {
                        "name": "Louis Jalouzot"
                    },
                    {
                        "name": "Alexis Thual"
                    },
                    {
                        "name": "Yair Lakretz"
                    },
                    {
                        "name": "Christophe Pallier"
                    },
                    {
                        "name": "Bertrand Thirion"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Thirion"
                },
                "author": "Bertrand Thirion",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21301v1",
                "updated": "2025-05-27T15:04:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    4,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:04:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    4,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate\n  Categories in Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate\n  Categories in Italian"
                },
                "summary": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research."
                },
                "authors": [
                    {
                        "name": "Andrea Pedrotti"
                    },
                    {
                        "name": "Giulia Rambelli"
                    },
                    {
                        "name": "Caterina Villani"
                    },
                    {
                        "name": "Marianna Bolognesi"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Bolognesi"
                },
                "author": "Marianna Bolognesi",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21298v1",
                "updated": "2025-05-27T15:01:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    1,
                    6,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    1,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Large Language Models Miss the Multi-Agent Mark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Miss the Multi-Agent Mark"
                },
                "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities."
                },
                "authors": [
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Gabriele La Malfa"
                    },
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Elizabeth Black"
                    },
                    {
                        "name": "Micheal Luck"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21297v1",
                "updated": "2025-05-27T15:00:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    0,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:00:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    0,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset"
                },
                "summary": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar."
                },
                "authors": [
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Bingcheng Dong"
                    },
                    {
                        "name": "Xudong Zhou"
                    },
                    {
                        "name": "Ning Shang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20947v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20947v4",
                "updated": "2025-05-27T14:59:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    59,
                    5,
                    1,
                    147,
                    0
                ],
                "published": "2024-05-31T15:44:33Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    44,
                    33,
                    4,
                    152,
                    0
                ],
                "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OR-Bench: An Over-Refusal Benchmark for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models."
                },
                "authors": [
                    {
                        "name": "Justin Cui"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "arxiv_comment": "Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20947v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20947v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00715v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00715v6",
                "updated": "2025-05-27T14:58:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    58,
                    40,
                    1,
                    147,
                    0
                ],
                "published": "2024-04-25T15:34:53Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    15,
                    34,
                    53,
                    3,
                    116,
                    0
                ],
                "title": "Towards Adapting Open-Source Large Language Models for Expert-Level\n  Clinical Note Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adapting Open-Source Large Language Models for Expert-Level\n  Clinical Note Generation"
                },
                "summary": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pretraining,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(92.8%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic matched physician-authored notes in real-world readiness\nscore. We highlight key considerations for future clinical note-generation\ntasks, emphasizing the importance of pre-defining a \"best practice\" note\nformat, rather than relying on LLMs to determine this for clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pretraining,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(92.8%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic matched physician-authored notes in real-world readiness\nscore. We highlight key considerations for future clinical note-generation\ntasks, emphasizing the importance of pre-defining a \"best practice\" note\nformat, rather than relying on LLMs to determine this for clinical practice."
                },
                "authors": [
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Chufan Gao"
                    },
                    {
                        "name": "Bolun Liu"
                    },
                    {
                        "name": "Qiping Xu"
                    },
                    {
                        "name": "Guleid Hussein"
                    },
                    {
                        "name": "Mohamad El Labban"
                    },
                    {
                        "name": "Kingsley Iheasirim"
                    },
                    {
                        "name": "Hariprasad Korsapati"
                    },
                    {
                        "name": "Chuck Outcalt"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00715v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00715v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16892v2",
                "updated": "2025-05-27T14:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    58,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-22T16:50:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "FlashBack: Consistency Model-Accelerated Shared Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack: Consistency Model-Accelerated Shared Autonomy"
                },
                "summary": "Shared autonomy is an enabling technology that provides users with control\nauthority over robots that would otherwise be difficult if not impossible to\ndirectly control. Yet, standard methods make assumptions that limit their\nadoption in practice-for example, prior knowledge of the user's goals or the\nobjective (i.e., reward) function that they wish to optimize, knowledge of the\nuser's policy, or query-level access to the user during training.\nDiffusion-based approaches to shared autonomy do not make such assumptions and\ninstead only require access to demonstrations of desired behaviors, while\nallowing the user to maintain control authority. However, these advantages have\ncome at the expense of high computational complexity, which has made real-time\nshared autonomy all but impossible. To overcome this limitation, we propose\nConsistency Shared Autonomy (CSA), a shared autonomy framework that employs a\nconsistency model-based formulation of diffusion. Key to CSA is that it employs\nthe distilled probability flow of ordinary differential equations (PF ODE) to\ngenerate high-fidelity samples in a single step. This results in inference\nspeeds significantly than what is possible with previous diffusion-based\napproaches to shared autonomy, enabling real-time assistance in complex domains\nwith only a single function evaluation. Further, by intervening on flawed\nactions at intermediate states of the PF ODE, CSA enables varying levels of\nassistance. We evaluate CSA on a variety of challenging simulated and\nreal-world robot control problems, demonstrating significant improvements over\nstate-of-the-art methods both in terms of task performance and computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared autonomy is an enabling technology that provides users with control\nauthority over robots that would otherwise be difficult if not impossible to\ndirectly control. Yet, standard methods make assumptions that limit their\nadoption in practice-for example, prior knowledge of the user's goals or the\nobjective (i.e., reward) function that they wish to optimize, knowledge of the\nuser's policy, or query-level access to the user during training.\nDiffusion-based approaches to shared autonomy do not make such assumptions and\ninstead only require access to demonstrations of desired behaviors, while\nallowing the user to maintain control authority. However, these advantages have\ncome at the expense of high computational complexity, which has made real-time\nshared autonomy all but impossible. To overcome this limitation, we propose\nConsistency Shared Autonomy (CSA), a shared autonomy framework that employs a\nconsistency model-based formulation of diffusion. Key to CSA is that it employs\nthe distilled probability flow of ordinary differential equations (PF ODE) to\ngenerate high-fidelity samples in a single step. This results in inference\nspeeds significantly than what is possible with previous diffusion-based\napproaches to shared autonomy, enabling real-time assistance in complex domains\nwith only a single function evaluation. Further, by intervening on flawed\nactions at intermediate states of the PF ODE, CSA enables varying levels of\nassistance. We evaluate CSA on a variety of challenging simulated and\nreal-world robot control problems, demonstrating significant improvements over\nstate-of-the-art methods both in terms of task performance and computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Luzhe Sun"
                    },
                    {
                        "name": "Jingtian Ji"
                    },
                    {
                        "name": "Xiangshan Tan"
                    },
                    {
                        "name": "Matthew R. Walter"
                    }
                ],
                "author_detail": {
                    "name": "Matthew R. Walter"
                },
                "author": "Matthew R. Walter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12134v2",
                "updated": "2025-05-27T14:54:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    51,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-17T18:52:29Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    52,
                    29,
                    0,
                    48,
                    0
                ],
                "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nrequire full-model fine-tuning and suffer from catastrophic forgetting,\nlimiting their applicability to state-of-the-art LLMs that already perform well\nin zero-shot settings with a proper instruction. To address this challenge, we\npropose a novel approach for continuous-space reasoning that does not require\nmodifying the LLM. Specifically, we employ a lightweight fixed assistant model\nto speculatively generate instance-specific soft thought tokens as the initial\nchain of thoughts, which are then mapped into the LLM's representation space\nvia a trainable projection module. Experimental results on five reasoning\nbenchmarks demonstrate that our method enhances LLM reasoning performance\nthrough supervised, parameter-efficient fine-tuning. Source code is available\nat https://github.com/xuyige/SoftCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nrequire full-model fine-tuning and suffer from catastrophic forgetting,\nlimiting their applicability to state-of-the-art LLMs that already perform well\nin zero-shot settings with a proper instruction. To address this challenge, we\npropose a novel approach for continuous-space reasoning that does not require\nmodifying the LLM. Specifically, we employ a lightweight fixed assistant model\nto speculatively generate instance-specific soft thought tokens as the initial\nchain of thoughts, which are then mapped into the LLM's representation space\nvia a trainable projection module. Experimental results on five reasoning\nbenchmarks demonstrate that our method enhances LLM reasoning performance\nthrough supervised, parameter-efficient fine-tuning. Source code is available\nat https://github.com/xuyige/SoftCoT."
                },
                "authors": [
                    {
                        "name": "Yige Xu"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Zhiwei Zeng"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "arxiv_comment": "Camera-ready for ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21291v1",
                "updated": "2025-05-27T14:54:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    49,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:54:49Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    49,
                    1,
                    147,
                    0
                ],
                "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large\n  Language Model-Enhanced Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large\n  Language Model-Enhanced Framework"
                },
                "summary": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics."
                },
                "authors": [
                    {
                        "name": "Saman Marandi"
                    },
                    {
                        "name": "Yu-Shu Hu"
                    },
                    {
                        "name": "Mohammad Modarres"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Modarres"
                },
                "author": "Mohammad Modarres",
                "arxiv_comment": "22 Pages, 11 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21289v1",
                "updated": "2025-05-27T14:54:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    24,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:54:24Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    24,
                    1,
                    147,
                    0
                ],
                "title": "LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning"
                },
                "summary": "Large pre-trained models are commonly adapted to downstream tasks using\nparameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),\nwhich injects small trainable low-rank matrices instead of updating all\nweights. While LoRA dramatically reduces trainable parameters with little\noverhead, it can still underperform full fine-tuning in accuracy and often\nconverges more slowly. We introduce LoFT, a novel low-rank adaptation method\nthat behaves like full fine-tuning by aligning the optimizer's internal\ndynamics with those of updating all model weights. LoFT not only learns weight\nupdates in a low-rank subspace (like LoRA) but also properly projects the\noptimizer's first and second moments (Adam's momentum and variance) into the\nsame subspace, mirroring full-model updates. By aligning the low-rank update\nitself with the full update, LoFT eliminates the need for tuning extra\nhyperparameters, e.g., LoRA scaling factor $\\alpha$. Empirically, this approach\nsubstantially narrows the performance gap between adapter-based tuning and full\nfine-tuning and consistently outperforms standard LoRA-style methods, all\nwithout increasing inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained models are commonly adapted to downstream tasks using\nparameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),\nwhich injects small trainable low-rank matrices instead of updating all\nweights. While LoRA dramatically reduces trainable parameters with little\noverhead, it can still underperform full fine-tuning in accuracy and often\nconverges more slowly. We introduce LoFT, a novel low-rank adaptation method\nthat behaves like full fine-tuning by aligning the optimizer's internal\ndynamics with those of updating all model weights. LoFT not only learns weight\nupdates in a low-rank subspace (like LoRA) but also properly projects the\noptimizer's first and second moments (Adam's momentum and variance) into the\nsame subspace, mirroring full-model updates. By aligning the low-rank update\nitself with the full update, LoFT eliminates the need for tuning extra\nhyperparameters, e.g., LoRA scaling factor $\\alpha$. Empirically, this approach\nsubstantially narrows the performance gap between adapter-based tuning and full\nfine-tuning and consistently outperforms standard LoRA-style methods, all\nwithout increasing inference cost."
                },
                "authors": [
                    {
                        "name": "Nurbek Tastan"
                    },
                    {
                        "name": "Stefanos Laskaridis"
                    },
                    {
                        "name": "Martin Takac"
                    },
                    {
                        "name": "Karthik Nandakumar"
                    },
                    {
                        "name": "Samuel Horvath"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Horvath"
                },
                "author": "Samuel Horvath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21286v1",
                "updated": "2025-05-27T14:53:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    53,
                    25,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:53:25Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    53,
                    25,
                    1,
                    147,
                    0
                ],
                "title": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services\n  Powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services\n  Powered by Large Language Models"
                },
                "summary": "Agentic AI, often powered by large language models (LLMs), is becoming\nincreasingly popular and adopted to support autonomous reasoning,\ndecision-making, and task execution across various domains. While agentic AI\nholds great promise, its deployment as services for easy access raises critical\nchallenges in pricing, due to high infrastructure and computation costs,\nmulti-dimensional and task-dependent Quality of Service (QoS), and growing\nconcerns around liability in high-stakes applications. In this work, we propose\nPACT, a Pricing framework for cloud-based Agentic AI services through a\nContract-Theoretic approach, which models QoS along both objective (e.g.,\nresponse time) and subjective (e.g., user satisfaction) dimensions. PACT\naccounts for computational, infrastructure, and potential liability costs for\nthe service provider, while ensuring incentive compatibility and individual\nrationality for the user under information asymmetry. Through contract-based\nselection, users receive tailored service offerings aligned with their needs.\nNumerical evaluations demonstrate that PACT improves QoS alignment between\nusers and providers and offers a scalable, liable approach to pricing agentic\nAI services in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI, often powered by large language models (LLMs), is becoming\nincreasingly popular and adopted to support autonomous reasoning,\ndecision-making, and task execution across various domains. While agentic AI\nholds great promise, its deployment as services for easy access raises critical\nchallenges in pricing, due to high infrastructure and computation costs,\nmulti-dimensional and task-dependent Quality of Service (QoS), and growing\nconcerns around liability in high-stakes applications. In this work, we propose\nPACT, a Pricing framework for cloud-based Agentic AI services through a\nContract-Theoretic approach, which models QoS along both objective (e.g.,\nresponse time) and subjective (e.g., user satisfaction) dimensions. PACT\naccounts for computational, infrastructure, and potential liability costs for\nthe service provider, while ensuring incentive compatibility and individual\nrationality for the user under information asymmetry. Through contract-based\nselection, users receive tailored service offerings aligned with their needs.\nNumerical evaluations demonstrate that PACT improves QoS alignment between\nusers and providers and offers a scalable, liable approach to pricing agentic\nAI services in the future."
                },
                "authors": [
                    {
                        "name": "Ya-Ting Yang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13789v2",
                "updated": "2025-05-27T14:52:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    52,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-17T17:29:23Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    29,
                    23,
                    3,
                    291,
                    0
                ],
                "title": "Neutron-neutron distribution of the triton from pionless EFT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron-neutron distribution of the triton from pionless EFT"
                },
                "summary": "We compute the neutron-neutron relative-energy distribution of the triton\nfollowing the hard knockout of the proton in pionless effective field theory.\nThis distribution can be used to study universality as well as to obtain\ninformation on the neutron-neutron interaction. Especially, one can infer the\nscattering length from fitting theory predictions for the shape of the\ndistribution to experimental data. To obtain the distribution for the triton,\nwe first solve the ground-state three-body problem using momentum-space Faddeev\nequations. Next, we include the neutron-neutron final-state interaction by\napplying the corresponding M{\\o}ller operator to the ground state. We present\nleading-order (LO) and next-to-leading order (NLO) pionless effective field\ntheory results with quantified uncertainties. At NLO, we include the effective\nranges semi-perturbatively. We conclude, that pionless EFT works reliably as\nexpected and that the neutron-neutron distribution of the triton shows a\nsignificant sensitivity to the scattering length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute the neutron-neutron relative-energy distribution of the triton\nfollowing the hard knockout of the proton in pionless effective field theory.\nThis distribution can be used to study universality as well as to obtain\ninformation on the neutron-neutron interaction. Especially, one can infer the\nscattering length from fitting theory predictions for the shape of the\ndistribution to experimental data. To obtain the distribution for the triton,\nwe first solve the ground-state three-body problem using momentum-space Faddeev\nequations. Next, we include the neutron-neutron final-state interaction by\napplying the corresponding M{\\o}ller operator to the ground state. We present\nleading-order (LO) and next-to-leading order (NLO) pionless effective field\ntheory results with quantified uncertainties. At NLO, we include the effective\nranges semi-perturbatively. We conclude, that pionless EFT works reliably as\nexpected and that the neutron-neutron distribution of the triton shows a\nsignificant sensitivity to the scattering length."
                },
                "authors": [
                    {
                        "name": "Tanja Kirchner"
                    },
                    {
                        "name": "Matthias Göbel"
                    },
                    {
                        "name": "Hans-Werner Hammer"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Werner Hammer"
                },
                "author": "Hans-Werner Hammer",
                "arxiv_doi": "10.1103/PhysRevC.111.044002",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevC.111.044002",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 7 figures. Further explanations, discussions and appendices\n  with figures added. Results unchanged. Accepted for publication in Phys. Rev.\n  C",
                "arxiv_journal_ref": "Phys. Rev. C 111, 044002 (2025)",
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21277v1",
                "updated": "2025-05-27T14:48:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    48,
                    44,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:48:44Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    48,
                    44,
                    1,
                    147,
                    0
                ],
                "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space"
                },
                "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
                },
                "authors": [
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Shouwei Ruan"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Xingxing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wei"
                },
                "author": "Xingxing Wei",
                "arxiv_comment": "19 pages, 20 figures, accepted by ACL 2025, Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03181v2",
                "updated": "2025-05-27T14:46:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    46,
                    3,
                    1,
                    147,
                    0
                ],
                "published": "2024-07-03T15:01:18Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    15,
                    1,
                    18,
                    2,
                    185,
                    0
                ],
                "title": "Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT\n  Refinement in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT\n  Refinement in LLMs"
                },
                "summary": "Requiring a large language model (LLM) to generate intermediary reasoning\nsteps, known as Chain of Thought (CoT), has been shown to be an effective way\nof boosting performance. Previous approaches have focused on generating\nmultiple independent CoTs, combining them through ensembling or other post-hoc\nstrategies to enhance reasoning. In this work, we introduce a novel approach\nwhere LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought\n(DCoT) within a single inference step, which is fundamentally different from\nprior work that primarily operate on parallel CoT generations. DCoT allows LLMs\nto gain the ability to perform within-inference refinement of reasoning chains\nwithout requiring external feedback. Through a rigorous set of experiments\nspanning a wide range of tasks that require various reasoning types, we show\nthat fine-tuning on DCoT improves performance over the CoT baseline across\nmodel families and scales (1.3B to 70B). These improvements are particularly\nimpactful for tasks with a large result state space, such as those involving\nnumeric answers. Our work is also significant because both quantitative\nanalyses and manual evaluations reveal the observed gains stem from the models'\nability to refine an initial reasoning chain by generating a second, improved\nchain within the same inference step, demonstrating previously elusive\nself-improvement. Our code and data are publicly available at\nhttps://github.com/UKPLab/acl2025-diverse-cot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requiring a large language model (LLM) to generate intermediary reasoning\nsteps, known as Chain of Thought (CoT), has been shown to be an effective way\nof boosting performance. Previous approaches have focused on generating\nmultiple independent CoTs, combining them through ensembling or other post-hoc\nstrategies to enhance reasoning. In this work, we introduce a novel approach\nwhere LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought\n(DCoT) within a single inference step, which is fundamentally different from\nprior work that primarily operate on parallel CoT generations. DCoT allows LLMs\nto gain the ability to perform within-inference refinement of reasoning chains\nwithout requiring external feedback. Through a rigorous set of experiments\nspanning a wide range of tasks that require various reasoning types, we show\nthat fine-tuning on DCoT improves performance over the CoT baseline across\nmodel families and scales (1.3B to 70B). These improvements are particularly\nimpactful for tasks with a large result state space, such as those involving\nnumeric answers. Our work is also significant because both quantitative\nanalyses and manual evaluations reveal the observed gains stem from the models'\nability to refine an initial reasoning chain by generating a second, improved\nchain within the same inference step, demonstrating previously elusive\nself-improvement. Our code and data are publicly available at\nhttps://github.com/UKPLab/acl2025-diverse-cot."
                },
                "authors": [
                    {
                        "name": "Haritz Puerto"
                    },
                    {
                        "name": "Tilek Chubakov"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21263v1",
                "updated": "2025-05-27T14:40:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    40,
                    25,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:40:25Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    40,
                    25,
                    1,
                    147,
                    0
                ],
                "title": "JavaSith: A Client-Side Framework for Analyzing Potentially Malicious\n  Extensions in Browsers, VS Code, and NPM Packages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JavaSith: A Client-Side Framework for Analyzing Potentially Malicious\n  Extensions in Browsers, VS Code, and NPM Packages"
                },
                "summary": "Modern software supply chains face an increasing threat from malicious code\nhidden in trusted components such as browser extensions, IDE extensions, and\nopen-source packages. This paper introduces JavaSith, a novel client-side\nframework for analyzing potentially malicious extensions in web browsers,\nVisual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a\nruntime sandbox that emulates browser/Node.js extension APIs (with a ``time\nmachine'' to accelerate time-based triggers) with static analysis and a local\nlarge language model (LLM) to assess risk from code and metadata. We present\nthe design and architecture of JavaSith, including techniques for intercepting\nextension behavior over simulated time and extracting suspicious patterns.\nThrough case studies on real-world attacks (such as a supply-chain compromise\nof a Chrome extension and malicious VSCode extensions installing cryptominers),\nwe demonstrate how JavaSith can catch stealthy malicious behaviors that evade\ntraditional detection. We evaluate the framework's effectiveness and discuss\nits limitations and future enhancements. JavaSith's client-side approach\nempowers end-users/organizations to vet extensions and packages before\ntrustingly integrating them into their environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software supply chains face an increasing threat from malicious code\nhidden in trusted components such as browser extensions, IDE extensions, and\nopen-source packages. This paper introduces JavaSith, a novel client-side\nframework for analyzing potentially malicious extensions in web browsers,\nVisual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a\nruntime sandbox that emulates browser/Node.js extension APIs (with a ``time\nmachine'' to accelerate time-based triggers) with static analysis and a local\nlarge language model (LLM) to assess risk from code and metadata. We present\nthe design and architecture of JavaSith, including techniques for intercepting\nextension behavior over simulated time and extracting suspicious patterns.\nThrough case studies on real-world attacks (such as a supply-chain compromise\nof a Chrome extension and malicious VSCode extensions installing cryptominers),\nwe demonstrate how JavaSith can catch stealthy malicious behaviors that evade\ntraditional detection. We evaluate the framework's effectiveness and discuss\nits limitations and future enhancements. JavaSith's client-side approach\nempowers end-users/organizations to vet extensions and packages before\ntrustingly integrating them into their environments."
                },
                "authors": [
                    {
                        "name": "Avihay Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Avihay Cohen"
                },
                "author": "Avihay Cohen",
                "arxiv_comment": "28 pages , 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21256v1",
                "updated": "2025-05-27T14:37:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    37,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:37:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    37,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "What triggers type Ia supernovae: Prompt detonations from primordial\n  black holes or companion stars?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What triggers type Ia supernovae: Prompt detonations from primordial\n  black holes or companion stars?"
                },
                "summary": "We set up and perform collision rate simulations between dark matter in the\nform of asteroid-mass primordial black holes (PBHs) and white dwarf stars.\nThese encounters trigger prompt detonations and could be the key to solving the\nignition mystery of type Ia supernovae. Our framework is flexible enough to\ncover the full range of progenitor white dwarf masses, host galaxy stellar\nmasses, galactocentric radial offsets, and cosmic time. The rate distribution\npattern is consistent with exhaustive literature observational determinations\nfor a slightly extended log-normal PBH mass spectrum. Most strikingly, the so\nfar unexplained brightness distribution comes out without finetuning. We find\nno severe contradictions, except that the inferred PBH mass scale is\nunpredicted from first principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We set up and perform collision rate simulations between dark matter in the\nform of asteroid-mass primordial black holes (PBHs) and white dwarf stars.\nThese encounters trigger prompt detonations and could be the key to solving the\nignition mystery of type Ia supernovae. Our framework is flexible enough to\ncover the full range of progenitor white dwarf masses, host galaxy stellar\nmasses, galactocentric radial offsets, and cosmic time. The rate distribution\npattern is consistent with exhaustive literature observational determinations\nfor a slightly extended log-normal PBH mass spectrum. Most strikingly, the so\nfar unexplained brightness distribution comes out without finetuning. We find\nno severe contradictions, except that the inferred PBH mass scale is\nunpredicted from first principles."
                },
                "authors": [
                    {
                        "name": "Heinrich Steigerwald"
                    }
                ],
                "author_detail": {
                    "name": "Heinrich Steigerwald"
                },
                "author": "Heinrich Steigerwald",
                "arxiv_comment": "5 pages + 9 supplemental material, accepted by PRD Letters, comments\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11484v2",
                "updated": "2025-05-27T14:35:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    35,
                    29,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-16T17:47:50Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    17,
                    47,
                    50,
                    4,
                    136,
                    0
                ],
                "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning"
                },
                "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT."
                },
                "authors": [
                    {
                        "name": "Yige Xu"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Zhiwei Zeng"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10479v2",
                "updated": "2025-05-27T14:29:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    29,
                    54,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-14T13:15:34Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    15,
                    34,
                    0,
                    288,
                    0
                ],
                "title": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs"
                },
                "summary": "The rapid advancement of large language models has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate the strategic reasoning capabilities of LLMs, game\ntheory, with its concise structure, has become the preferred approach for many\nresearchers. However, current research typically focuses on a limited selection\nof games, resulting in low coverage of game types. Additionally, classic game\nscenarios carry risks of data leakage, and the benchmarks used often lack\nextensibility, rendering them inadequate for evaluating state-of-the-art\nmodels. To address these challenges, we propose TMGBench, characterized by\ncomprehensive game type coverage, diverse scenarios and flexible game\norganization. Specifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games in our\nbenchmark; we also synthetize diverse, higher-quality game scenarios for each\nclassic game, which we refer to as story-based games. Lastly, to provide a\nsustainable evaluation framework adaptable to increasingly powerful LLMs, we\ntreat the aforementioned games as atomic units and organize them into more\ncomplex forms through sequential, parallel, and nested structures. We conducted\na comprehensive evaluation of mainstream LLMs, covering tests on rational\nreasoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in\ncomplex game forms. The results revealed LLMs still have flaws in the accuracy\nand consistency of strategic reasoning processes, and their levels of mastery\nover Theory-of-Mind also vary. Additionally, SOTA models like o3-mini, Qwen3\nand deepseek-reasoner, were also evaluated across the sequential, parallel, and\nnested game structures while the results highlighted the challenges posed by\nTMGBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate the strategic reasoning capabilities of LLMs, game\ntheory, with its concise structure, has become the preferred approach for many\nresearchers. However, current research typically focuses on a limited selection\nof games, resulting in low coverage of game types. Additionally, classic game\nscenarios carry risks of data leakage, and the benchmarks used often lack\nextensibility, rendering them inadequate for evaluating state-of-the-art\nmodels. To address these challenges, we propose TMGBench, characterized by\ncomprehensive game type coverage, diverse scenarios and flexible game\norganization. Specifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games in our\nbenchmark; we also synthetize diverse, higher-quality game scenarios for each\nclassic game, which we refer to as story-based games. Lastly, to provide a\nsustainable evaluation framework adaptable to increasingly powerful LLMs, we\ntreat the aforementioned games as atomic units and organize them into more\ncomplex forms through sequential, parallel, and nested structures. We conducted\na comprehensive evaluation of mainstream LLMs, covering tests on rational\nreasoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in\ncomplex game forms. The results revealed LLMs still have flaws in the accuracy\nand consistency of strategic reasoning processes, and their levels of mastery\nover Theory-of-Mind also vary. Additionally, SOTA models like o3-mini, Qwen3\nand deepseek-reasoner, were also evaluated across the sequential, parallel, and\nnested game structures while the results highlighted the challenges posed by\nTMGBench."
                },
                "authors": [
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yu Guo"
                    },
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21242v1",
                "updated": "2025-05-27T14:23:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    23,
                    3,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:23:03Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    23,
                    3,
                    1,
                    147,
                    0
                ],
                "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary\n  Adaptation in High OOV Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary\n  Adaptation in High OOV Settings"
                },
                "summary": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark."
                },
                "authors": [
                    {
                        "name": "Gunjan Balde"
                    },
                    {
                        "name": "Soumyadeep Roy"
                    },
                    {
                        "name": "Mainack Mondal"
                    },
                    {
                        "name": "Niloy Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Niloy Ganguly"
                },
                "author": "Niloy Ganguly",
                "arxiv_comment": "16 pages. Accepted for publication in the Findings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21239v1",
                "updated": "2025-05-27T14:19:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    35,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:19:35Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    35,
                    1,
                    147,
                    0
                ],
                "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners"
                },
                "summary": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD"
                },
                "authors": [
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Chentao Song"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Qing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Huang"
                },
                "author": "Qing Huang",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21236v1",
                "updated": "2025-05-27T14:19:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    6,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:19:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Breaking the Performance Ceiling in Complex Reinforcement Learning\n  requires Inference Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Performance Ceiling in Complex Reinforcement Learning\n  requires Inference Strategies"
                },
                "summary": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inf-marl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inf-marl."
                },
                "authors": [
                    {
                        "name": "Felix Chalumeau"
                    },
                    {
                        "name": "Daniel Rajaonarivonivelomanantsoa"
                    },
                    {
                        "name": "Ruan de Kock"
                    },
                    {
                        "name": "Claude Formanek"
                    },
                    {
                        "name": "Sasha Abramowitz"
                    },
                    {
                        "name": "Oumayma Mahjoub"
                    },
                    {
                        "name": "Wiem Khlifi"
                    },
                    {
                        "name": "Simon Du Toit"
                    },
                    {
                        "name": "Louay Ben Nessir"
                    },
                    {
                        "name": "Refiloe Shabe"
                    },
                    {
                        "name": "Arnol Fokam"
                    },
                    {
                        "name": "Siddarth Singh"
                    },
                    {
                        "name": "Ulrich Mbou Sob"
                    },
                    {
                        "name": "Arnu Pretorius"
                    }
                ],
                "author_detail": {
                    "name": "Arnu Pretorius"
                },
                "author": "Arnu Pretorius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19547v2",
                "updated": "2025-05-27T14:17:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    17,
                    24,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T06:11:05Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    6,
                    11,
                    5,
                    0,
                    146,
                    0
                ],
                "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution\n  Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution\n  Generalization"
                },
                "summary": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning."
                },
                "authors": [
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Yuchen Fang"
                    },
                    {
                        "name": "Yifan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhang"
                },
                "author": "Yifan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21233v1",
                "updated": "2025-05-27T14:16:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    16,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:16:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    16,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "CROP: Contextual Region-Oriented Visual Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROP: Contextual Region-Oriented Visual Token Pruning"
                },
                "summary": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Qianrun Wei"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18942v2",
                "updated": "2025-05-27T14:15:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    15,
                    31,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-25T02:28:40Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    2,
                    28,
                    40,
                    6,
                    145,
                    0
                ],
                "title": "Language Models Surface the Unwritten Code of Science and Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Surface the Unwritten Code of Science and Society"
                },
                "summary": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI."
                },
                "authors": [
                    {
                        "name": "Honglin Bao"
                    },
                    {
                        "name": "Siyang Wu"
                    },
                    {
                        "name": "Jiwoong Choi"
                    },
                    {
                        "name": "Yingrong Mao"
                    },
                    {
                        "name": "James A. Evans"
                    }
                ],
                "author_detail": {
                    "name": "James A. Evans"
                },
                "author": "James A. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21231v1",
                "updated": "2025-05-27T14:15:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    15,
                    19,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:15:19Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    15,
                    19,
                    1,
                    147,
                    0
                ],
                "title": "Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning"
                },
                "summary": "Occlusion Boundary Estimation (OBE) identifies boundaries arising from both\ninter-object occlusions and self-occlusion within individual objects,\ndistinguishing intrinsic object edges from occlusion-induced contours to\nimprove scene understanding and 3D reconstruction capacity. This is closely\nrelated to Monocular Depth Estimation (MDE), which infers depth from a single\nimage, as occlusion boundaries provide critical geometric cues for resolving\ndepth ambiguities, while depth priors can conversely refine occlusion reasoning\nin complex scenes. In this paper, we propose a novel network, MoDOT, that first\njointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale\nstrip convolution module, leverages mid-level OB features to significantly\nenhance depth prediction. Additionally, we introduce an occlusion-aware loss\nfunction, OBDCL, which encourages sharper and more accurate depth boundaries.\nExtensive experiments on both real and synthetic datasets demonstrate the\nmutual benefits of jointly estimating depth and OB, and highlight the\neffectiveness of our model design. Our method achieves the state-of-the-art\n(SOTA) on both our proposed synthetic datasets and one popular real dataset,\nNYUD-v2, significantly outperforming multi-task baselines. Besides, without\ndomain adaptation, results on real-world depth transfer are comparable to the\ncompetitors, while preserving sharp occlusion boundaries for geometric\nfidelity. We will release our code, pre-trained models, and datasets to support\nfuture research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occlusion Boundary Estimation (OBE) identifies boundaries arising from both\ninter-object occlusions and self-occlusion within individual objects,\ndistinguishing intrinsic object edges from occlusion-induced contours to\nimprove scene understanding and 3D reconstruction capacity. This is closely\nrelated to Monocular Depth Estimation (MDE), which infers depth from a single\nimage, as occlusion boundaries provide critical geometric cues for resolving\ndepth ambiguities, while depth priors can conversely refine occlusion reasoning\nin complex scenes. In this paper, we propose a novel network, MoDOT, that first\njointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale\nstrip convolution module, leverages mid-level OB features to significantly\nenhance depth prediction. Additionally, we introduce an occlusion-aware loss\nfunction, OBDCL, which encourages sharper and more accurate depth boundaries.\nExtensive experiments on both real and synthetic datasets demonstrate the\nmutual benefits of jointly estimating depth and OB, and highlight the\neffectiveness of our model design. Our method achieves the state-of-the-art\n(SOTA) on both our proposed synthetic datasets and one popular real dataset,\nNYUD-v2, significantly outperforming multi-task baselines. Besides, without\ndomain adaptation, results on real-world depth transfer are comparable to the\ncompetitors, while preserving sharp occlusion boundaries for geometric\nfidelity. We will release our code, pre-trained models, and datasets to support\nfuture research in this direction."
                },
                "authors": [
                    {
                        "name": "Lintao Xu"
                    },
                    {
                        "name": "Yinghao Wang"
                    },
                    {
                        "name": "Chaohui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chaohui Wang"
                },
                "author": "Chaohui Wang",
                "arxiv_comment": "7 pages, 4 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04183v2",
                "updated": "2025-05-27T14:15:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    15,
                    14,
                    1,
                    147,
                    0
                ],
                "published": "2024-09-06T10:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding"
                },
                "summary": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "ACL 2025 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18389v2",
                "updated": "2025-05-27T14:13:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    13,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-23T21:33:16Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    21,
                    33,
                    16,
                    4,
                    143,
                    0
                ],
                "title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN"
                },
                "summary": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types"
                },
                "authors": [
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Reshma Prasad"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Under submission to an IEEE journal, copyright may change without\n  notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21218v1",
                "updated": "2025-05-27T14:06:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    6,
                    15,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:06:15Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    6,
                    15,
                    1,
                    147,
                    0
                ],
                "title": "Pretrained LLMs Learn Multiple Types of Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained LLMs Learn Multiple Types of Uncertainty"
                },
                "summary": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction."
                },
                "authors": [
                    {
                        "name": "Roi Cohen"
                    },
                    {
                        "name": "Omri Fahn"
                    },
                    {
                        "name": "Gerard de Melo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard de Melo"
                },
                "author": "Gerard de Melo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.21505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21505v1",
                "updated": "2025-05-27T17:59:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:59:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective"
                },
                "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Shimao Zhang"
                    },
                    {
                        "name": "Zhejian Lai"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Shuaijie She"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Jiajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Chen"
                },
                "author": "Jiajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21503v1",
                "updated": "2025-05-27T17:59:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:59:50Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    50,
                    1,
                    147,
                    0
                ],
                "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs\n  via Catfish Agent for Clinical Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs\n  via Catfish Agent for Clinical Decision Making"
                },
                "summary": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Qiao Yan"
                    },
                    {
                        "name": "Zhenghao Xing"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    },
                    {
                        "name": "Xiaowei Hu"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21499v1",
                "updated": "2025-05-27T17:59:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    5,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:59:05Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    59,
                    5,
                    1,
                    147,
                    0
                ],
                "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery"
                },
                "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."
                },
                "authors": [
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Rupeng Zhang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21493v1",
                "updated": "2025-05-27T17:56:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    56,
                    27,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:56:27Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    56,
                    27,
                    1,
                    147,
                    0
                ],
                "title": "Reinforcing General Reasoning without Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing General Reasoning without Verifiers"
                },
                "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree."
                },
                "authors": [
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Anya Sims"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Chao Du"
                    }
                ],
                "author_detail": {
                    "name": "Chao Du"
                },
                "author": "Chao Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21486v1",
                "updated": "2025-05-27T17:53:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    53,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:53:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    53,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive\n  Logic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive\n  Logic Programming"
                },
                "summary": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jiemin Wu"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21479v1",
                "updated": "2025-05-27T17:51:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    51,
                    18,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:51:18Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    51,
                    18,
                    1,
                    147,
                    0
                ],
                "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Language Models Consequentialist or Deontological Moral Reasoners?"
                },
                "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens ."
                },
                "authors": [
                    {
                        "name": "Keenan Samway"
                    },
                    {
                        "name": "Max Kleiman-Weiner"
                    },
                    {
                        "name": "David Guzman Piedrahita"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21478v1",
                "updated": "2025-05-27T17:50:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    50,
                    47,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:50:47Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    50,
                    47,
                    1,
                    147,
                    0
                ],
                "title": "Policy Optimized Text-to-Image Pipeline Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Optimized Text-to-Image Pipeline Design"
                },
                "summary": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Uri Gadot"
                    },
                    {
                        "name": "Rinon Gal"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Shie Mannor"
                    }
                ],
                "author_detail": {
                    "name": "Shie Mannor"
                },
                "author": "Shie Mannor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19915v2",
                "updated": "2025-05-27T17:45:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    40,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T12:40:32Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    40,
                    32,
                    0,
                    146,
                    0
                ],
                "title": "Evaluating AI cyber capabilities with crowdsourced elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI cyber capabilities with crowdsourced elicitation"
                },
                "summary": "As AI systems become increasingly capable, understanding their offensive\ncyber potential is critical for informed governance and responsible deployment.\nHowever, it's hard to accurately bound their capabilities, and some prior\nevaluations dramatically underestimated them. The art of extracting maximum\ntask-specific performance from AIs is called \"AI elicitation\", and today's\nsafety organizations typically conduct it in-house. In this paper, we explore\ncrowdsourcing elicitation efforts as an alternative to in-house elicitation\nwork.\n  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI\nvs. Humans (400 teams) and Cyber Apocalypse (8000 teams). The AI teams achieve\noutstanding performance at both events, ranking top-5% and top-10% respectively\nfor a total of \\$7500 in bounties. This impressive performance suggests that\nopen-market elicitation may offer an effective complement to in-house\nelicitation. We propose elicitation bounties as a practical mechanism for\nmaintaining timely, cost-effective situational awareness of emerging AI\ncapabilities.\n  Another advantage of open elicitations is the option to collect human\nperformance data at scale. Applying METR's methodology, we found that AI agents\ncan reliably solve cyber challenges requiring one hour or less of effort from a\nmedian human CTF participant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems become increasingly capable, understanding their offensive\ncyber potential is critical for informed governance and responsible deployment.\nHowever, it's hard to accurately bound their capabilities, and some prior\nevaluations dramatically underestimated them. The art of extracting maximum\ntask-specific performance from AIs is called \"AI elicitation\", and today's\nsafety organizations typically conduct it in-house. In this paper, we explore\ncrowdsourcing elicitation efforts as an alternative to in-house elicitation\nwork.\n  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI\nvs. Humans (400 teams) and Cyber Apocalypse (8000 teams). The AI teams achieve\noutstanding performance at both events, ranking top-5% and top-10% respectively\nfor a total of \\$7500 in bounties. This impressive performance suggests that\nopen-market elicitation may offer an effective complement to in-house\nelicitation. We propose elicitation bounties as a practical mechanism for\nmaintaining timely, cost-effective situational awareness of emerging AI\ncapabilities.\n  Another advantage of open elicitations is the option to collect human\nperformance data at scale. Applying METR's methodology, we found that AI agents\ncan reliably solve cyber challenges requiring one hour or less of effort from a\nmedian human CTF participant."
                },
                "authors": [
                    {
                        "name": "Artem Petrov"
                    },
                    {
                        "name": "Dmitrii Volkov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Volkov"
                },
                "author": "Dmitrii Volkov",
                "arxiv_comment": "Updated abstract to fix a typo; no changes to the content of the\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21471v1",
                "updated": "2025-05-27T17:45:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:45:04Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    45,
                    4,
                    1,
                    147,
                    0
                ],
                "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration"
                },
                "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications."
                },
                "authors": [
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Zhennan Wan"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "30 pages, 9 figures. Code and data are available at\n  https://github.com/THUNLP-MT/ExtAgents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00675v2",
                "updated": "2025-05-27T17:38:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    38,
                    40,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-01T17:31:33Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    31,
                    33,
                    3,
                    121,
                    0
                ],
                "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future\n  Directions"
                },
                "summary": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs)-based agents. While prior surveys have focused on memory\napplications with LLMs (e.g., enabling personalized memory in conversational\nagents), they often overlook the atomic operations that underlie memory\ndynamics. In this survey, we first categorize memory representations into\nparametric and contextual forms, and then introduce six fundamental memory\noperations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and\nCompression. We map these operations to the most relevant research topics\nacross long-term, long-context, parametric modification, and multi-source\nmemory. By reframing memory systems through the lens of atomic operations and\nrepresentation types, this survey provides a structured and dynamic perspective\non research, benchmark datasets, and tools related to memory in AI, clarifying\nthe functional interplay in LLMs based agents while outlining promising\ndirections for future research\\footnote{The paper list, datasets, methods and\ntools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs)-based agents. While prior surveys have focused on memory\napplications with LLMs (e.g., enabling personalized memory in conversational\nagents), they often overlook the atomic operations that underlie memory\ndynamics. In this survey, we first categorize memory representations into\nparametric and contextual forms, and then introduce six fundamental memory\noperations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and\nCompression. We map these operations to the most relevant research topics\nacross long-term, long-context, parametric modification, and multi-source\nmemory. By reframing memory systems through the lens of atomic operations and\nrepresentation types, this survey provides a structured and dynamic perspective\non research, benchmark datasets, and tools related to memory in AI, clarifying\nthe functional interplay in LLMs based agents while outlining promising\ndirections for future research\\footnote{The paper list, datasets, methods and\ntools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}."
                },
                "authors": [
                    {
                        "name": "Yiming Du"
                    },
                    {
                        "name": "Wenyu Huang"
                    },
                    {
                        "name": "Danna Zheng"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Sebastien Montella"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13398v2",
                "updated": "2025-05-27T17:37:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    37,
                    58,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-19T03:14:11Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    3,
                    14,
                    11,
                    2,
                    50,
                    0
                ],
                "title": "GeLLMO: Generalizing Large Language Models for Multi-property Molecule\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeLLMO: Generalizing Large Language Models for Multi-property Molecule\n  Optimization"
                },
                "summary": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce MuMOInstruct, the first high-quality\ninstruction-tuning dataset specifically focused on complex multi-property\nmolecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a\nseries of instruction-tuned LLMs for molecule optimization. Extensive\nevaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that\nGeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also\nexhibit outstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of GeLLMOs as foundational models for\nmolecule optimization, thereby tackling novel optimization tasks without\nresource-intensive retraining. MuMOInstruct, models, and code are accessible\nthrough https://github.com/ninglab/GeLLMO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce MuMOInstruct, the first high-quality\ninstruction-tuning dataset specifically focused on complex multi-property\nmolecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a\nseries of instruction-tuned LLMs for molecule optimization. Extensive\nevaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that\nGeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also\nexhibit outstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of GeLLMOs as foundational models for\nmolecule optimization, thereby tackling novel optimization tasks without\nresource-intensive retraining. MuMOInstruct, models, and code are accessible\nthrough https://github.com/ninglab/GeLLMO."
                },
                "authors": [
                    {
                        "name": "Vishal Dey"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "arxiv_comment": "Accepted to ACL Main 2025. Vishal Dey and Xiao Hu contributed equally\n  to this paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21462v1",
                "updated": "2025-05-27T17:34:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    34,
                    1,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:34:01Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    34,
                    1,
                    1,
                    147,
                    0
                ],
                "title": "M3S-UPD: Efficient Multi-Stage Self-Supervised Learning for Fine-Grained\n  Encrypted Traffic Classification with Unknown Pattern Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3S-UPD: Efficient Multi-Stage Self-Supervised Learning for Fine-Grained\n  Encrypted Traffic Classification with Unknown Pattern Discovery"
                },
                "summary": "The growing complexity of encrypted network traffic presents dual challenges\nfor modern network management: accurate multiclass classification of known\napplications and reliable detection of unknown traffic patterns. Although deep\nlearning models show promise in controlled environments, their real-world\ndeployment is hindered by data scarcity, concept drift, and operational\nconstraints. This paper proposes M3S-UPD, a novel Multi-Stage Self-Supervised\nUnknown-aware Packet Detection framework that synergistically integrates\nsemi-supervised learning with representation analysis. Our approach eliminates\nartificial segregation between classification and detection tasks through a\nfour-phase iterative process: 1) probabilistic embedding generation, 2)\nclustering-based structure discovery, 3) distribution-aligned outlier\nidentification, and 4) confidence-aware model updating. Key innovations include\na self-supervised unknown detection mechanism that requires neither synthetic\nsamples nor prior knowledge, and a continuous learning architecture that is\nresistant to performance degradation. Experimental results show that M3S-UPD\nnot only outperforms existing methods on the few-shot encrypted traffic\nclassification task, but also simultaneously achieves competitive performance\non the zero-shot unknown traffic discovery task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of encrypted network traffic presents dual challenges\nfor modern network management: accurate multiclass classification of known\napplications and reliable detection of unknown traffic patterns. Although deep\nlearning models show promise in controlled environments, their real-world\ndeployment is hindered by data scarcity, concept drift, and operational\nconstraints. This paper proposes M3S-UPD, a novel Multi-Stage Self-Supervised\nUnknown-aware Packet Detection framework that synergistically integrates\nsemi-supervised learning with representation analysis. Our approach eliminates\nartificial segregation between classification and detection tasks through a\nfour-phase iterative process: 1) probabilistic embedding generation, 2)\nclustering-based structure discovery, 3) distribution-aligned outlier\nidentification, and 4) confidence-aware model updating. Key innovations include\na self-supervised unknown detection mechanism that requires neither synthetic\nsamples nor prior knowledge, and a continuous learning architecture that is\nresistant to performance degradation. Experimental results show that M3S-UPD\nnot only outperforms existing methods on the few-shot encrypted traffic\nclassification task, but also simultaneously achieves competitive performance\non the zero-shot unknown traffic discovery task."
                },
                "authors": [
                    {
                        "name": "Yali Yuan"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Xingjian Zeng"
                    },
                    {
                        "name": "Hantao Mei"
                    },
                    {
                        "name": "Guang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Cheng"
                },
                "author": "Guang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21458v1",
                "updated": "2025-05-27T17:30:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    30,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:30:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    30,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "Do LLMs Need to Think in One Language? Correlation between Latent\n  Language and Task Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Need to Think in One Language? Correlation between Latent\n  Language and Task Performance"
                },
                "summary": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance."
                },
                "authors": [
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Tatsuya Hiraoka"
                    },
                    {
                        "name": "Hiroto Otake"
                    },
                    {
                        "name": "Hiroki Ouchi"
                    },
                    {
                        "name": "Masaru Isonuma"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Taro Watanabe"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Yu Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Yu Takagi"
                },
                "author": "Yu Takagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09192v2",
                "updated": "2025-05-27T17:24:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    24,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-13T11:32:09Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    32,
                    9,
                    3,
                    44,
                    0
                ],
                "title": "Thinking beyond the anthropomorphic paradigm benefits LLM research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking beyond the anthropomorphic paradigm benefits LLM research"
                },
                "summary": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof research articles to present empirical evidence of the prevalence and growth\nof anthropomorphic terminology in research on large language models (LLMs). We\nargue for challenging the deeper assumptions reflected in this terminology --\nwhich, though often useful, may inadvertently constrain LLM development -- and\nbroadening beyond them to open new pathways for understanding and improving\nLLMs. Specifically, we identify and examine five anthropomorphic assumptions\nthat shape research across the LLM development lifecycle. For each assumption\n(e.g., that LLMs must use natural language for reasoning, or that they should\nbe evaluated on benchmarks originally meant for humans), we demonstrate\nempirical, non-anthropomorphic alternatives that remain under-explored yet\noffer promising directions for LLM research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof research articles to present empirical evidence of the prevalence and growth\nof anthropomorphic terminology in research on large language models (LLMs). We\nargue for challenging the deeper assumptions reflected in this terminology --\nwhich, though often useful, may inadvertently constrain LLM development -- and\nbroadening beyond them to open new pathways for understanding and improving\nLLMs. Specifically, we identify and examine five anthropomorphic assumptions\nthat shape research across the LLM development lifecycle. For each assumption\n(e.g., that LLMs must use natural language for reasoning, or that they should\nbe evaluated on benchmarks originally meant for humans), we demonstrate\nempirical, non-anthropomorphic alternatives that remain under-explored yet\noffer promising directions for LLM research and development."
                },
                "authors": [
                    {
                        "name": "Lujain Ibrahim"
                    },
                    {
                        "name": "Myra Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Myra Cheng"
                },
                "author": "Myra Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21451v1",
                "updated": "2025-05-27T17:23:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    23,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:23:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    23,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of\n  Violent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words Like Knives: Backstory-Personalized Modeling and Detection of\n  Violent Communication"
                },
                "summary": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection."
                },
                "authors": [
                    {
                        "name": "Jocelyn Shen"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Cynthia Breazeal"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Hae Won Park"
                    }
                ],
                "author_detail": {
                    "name": "Hae Won Park"
                },
                "author": "Hae Won Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19184v2",
                "updated": "2025-05-27T17:17:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    17,
                    17,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-25T15:06:17Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    15,
                    6,
                    17,
                    6,
                    145,
                    0
                ],
                "title": "When Two LLMs Debate, Both Think They'll Win",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Two LLMs Debate, Both Think They'll Win"
                },
                "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."
                },
                "authors": [
                    {
                        "name": "Pradyumna Shyama Prasad"
                    },
                    {
                        "name": "Minh Nhat Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Nhat Nguyen"
                },
                "author": "Minh Nhat Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21444v1",
                "updated": "2025-05-27T17:16:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    16,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:16:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    16,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "Can Large Reasoning Models Self-Train?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Reasoning Models Self-Train?"
                },
                "summary": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Fahim Tajwar"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Jeff Schneider"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "arxiv_comment": "Project website: https://self-rewarding-llm-training.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01082v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01082v6",
                "updated": "2025-05-27T17:15:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    15,
                    3,
                    1,
                    147,
                    0
                ],
                "published": "2024-07-01T08:37:25Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    37,
                    25,
                    0,
                    183,
                    0
                ],
                "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs"
                },
                "summary": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its considerable impact on improving text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its considerable impact on improving text generation quality."
                },
                "authors": [
                    {
                        "name": "Minh Nhat Nguyen"
                    },
                    {
                        "name": "Andrew Baker"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Allen Roush"
                    },
                    {
                        "name": "Andreas Kirsch"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "arxiv_comment": "Oral presentation at ICLR 2025. Camera-ready version available at\n  https://iclr.cc/virtual/2025/poster/30358",
                "arxiv_journal_ref": "In Proceedings of the 2025 International Conference on Learning\n  Representations (ICLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01082v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01082v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13010v2",
                "updated": "2025-05-27T17:05:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    5,
                    15,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-18T16:29:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    29,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering:\n  Bridging the Gap Between LLMs and Evolving Medical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering:\n  Bridging the Gap Between LLMs and Evolving Medical Knowledge"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Rezaei"
                    },
                    {
                        "name": "Reza Saadati Fard"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Milad Lankarany"
                    }
                ],
                "author_detail": {
                    "name": "Milad Lankarany"
                },
                "author": "Milad Lankarany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v1",
                "updated": "2025-05-27T17:04:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21427v1",
                "updated": "2025-05-27T16:57:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    57,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:57:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    57,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Policy Induction: Predicting Startup Success via Explainable\n  Memory-Augmented In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Induction: Predicting Startup Success via Explainable\n  Memory-Augmented In-Context Learning"
                },
                "summary": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms."
                },
                "authors": [
                    {
                        "name": "Xianling Mu"
                    },
                    {
                        "name": "Joseph Ternasky"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05159v2",
                "updated": "2025-05-27T16:54:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    54,
                    17,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-07T18:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    41,
                    21,
                    4,
                    38,
                    0
                ],
                "title": "A Lightweight Method to Disrupt Memorized Sequences in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Method to Disrupt Memorized Sequences in LLM"
                },
                "summary": "As language models scale, their performance improves dramatically across a\nwide range of tasks, but so does their tendency to memorize and regurgitate\nparts of their training data verbatim. This tradeoff poses serious legal,\nethical, and safety concerns, especially in real-world deployments. Existing\nmitigation techniques, such as differential privacy or model unlearning, often\nrequire retraining or access to internal weights making them impractical for\nmost users. In this work, we introduce TokenSwap, a lightweight, post-hoc\ndefense designed for realistic settings where the user can only access\ntoken-level outputs. Our key insight is that while large models are necessary\nfor high task performance, small models (e.g., DistilGPT-2) are often\nsufficient to assign fluent, grammatically plausible probabilities to common\nfunction words - and crucially, they memorize far less. By selectively swapping\ntoken probabilities between models, TokenSwap preserves the capabilities of\nlarge models while reducing their propensity for verbatim reproduction.\nEvaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact\nmemorization with negligible task degradation. Our method offers a practical,\naccessible solution for mitigating memorized generation in deployed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models scale, their performance improves dramatically across a\nwide range of tasks, but so does their tendency to memorize and regurgitate\nparts of their training data verbatim. This tradeoff poses serious legal,\nethical, and safety concerns, especially in real-world deployments. Existing\nmitigation techniques, such as differential privacy or model unlearning, often\nrequire retraining or access to internal weights making them impractical for\nmost users. In this work, we introduce TokenSwap, a lightweight, post-hoc\ndefense designed for realistic settings where the user can only access\ntoken-level outputs. Our key insight is that while large models are necessary\nfor high task performance, small models (e.g., DistilGPT-2) are often\nsufficient to assign fluent, grammatically plausible probabilities to common\nfunction words - and crucially, they memorize far less. By selectively swapping\ntoken probabilities between models, TokenSwap preserves the capabilities of\nlarge models while reducing their propensity for verbatim reproduction.\nEvaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact\nmemorization with negligible task degradation. Our method offers a practical,\naccessible solution for mitigating memorized generation in deployed LLMs."
                },
                "authors": [
                    {
                        "name": "Parjanya Prajakta Prashant"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Babak Salimi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Salimi"
                },
                "author": "Babak Salimi",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08313v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08313v4",
                "updated": "2025-05-27T16:54:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    54,
                    13,
                    1,
                    147,
                    0
                ],
                "published": "2024-08-15T17:59:57Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Symbolic Graphics Programs?"
                },
                "summary": "Against the backdrop of enthusiasm for large language models (LLMs), there is\na growing need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer semantic\nquestions about the images or 3D geometries without a vision encoder. To\nsemantically understand the symbolic programs, LLMs would need to possess the\nability to \"imagine\" and reason how the corresponding graphics content would\nlook with only the symbolic description of the local curvatures and strokes. We\nuse this task to evaluate LLMs by creating a large benchmark for the semantic\nvisual understanding of symbolic graphics programs, built procedurally with\nminimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against the backdrop of enthusiasm for large language models (LLMs), there is\na growing need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer semantic\nquestions about the images or 3D geometries without a vision encoder. To\nsemantically understand the symbolic programs, LLMs would need to possess the\nability to \"imagine\" and reason how the corresponding graphics content would\nlook with only the symbolic description of the local curvatures and strokes. We\nuse this task to evaluate LLMs by creating a large benchmark for the semantic\nvisual understanding of symbolic graphics programs, built procedurally with\nminimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks."
                },
                "authors": [
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "ICLR 2025 Spotlight (v4: 47 pages, 26 figures, project page:\n  https://sgp-bench.github.io/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08313v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08313v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11618v2",
                "updated": "2025-05-27T16:52:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    52,
                    19,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-16T18:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    32,
                    35,
                    4,
                    136,
                    0
                ],
                "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models:\n  Capabilities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models:\n  Capabilities and Challenges"
                },
                "summary": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs."
                },
                "authors": [
                    {
                        "name": "Pengrui Quan"
                    },
                    {
                        "name": "Brian Wang"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Liying Han"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21419v2",
                "updated": "2025-05-28T02:17:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    17,
                    40,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T16:43:45Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    43,
                    45,
                    1,
                    147,
                    0
                ],
                "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG\n  LLMs"
                },
                "summary": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Kenneth P. Birman"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth P. Birman"
                },
                "author": "Kenneth P. Birman",
                "arxiv_doi": "10.1145/3721146.3721958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in EuroMLSys2025",
                "arxiv_journal_ref": "2025, Association for Computing Machinery, Proceedings of the 5th\n  Workshop on Machine Learning and Systems, 139-147, 9, series EuroMLSys '25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21418v1",
                "updated": "2025-05-27T16:43:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    43,
                    31,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:43:31Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    43,
                    31,
                    1,
                    147,
                    0
                ],
                "title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused\n  Ultrasound Ablation Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused\n  Ultrasound Ablation Surgery"
                },
                "summary": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains."
                },
                "authors": [
                    {
                        "name": "Lina Zhao"
                    },
                    {
                        "name": "Jiaxing Bai"
                    },
                    {
                        "name": "Zihao Bian"
                    },
                    {
                        "name": "Qingyue Chen"
                    },
                    {
                        "name": "Yafang Li"
                    },
                    {
                        "name": "Guangbo Li"
                    },
                    {
                        "name": "Min He"
                    },
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Zongjiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zongjiu Zhang"
                },
                "author": "Zongjiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20993v2",
                "updated": "2025-05-27T16:41:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-30T14:57:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "Efficiently Scaling LLM Reasoning with Certaindex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Scaling LLM Reasoning with Certaindex"
                },
                "summary": "Test-time reasoning algorithms such as chain-of-thought, self-consistency,\nand MCTS enhance LLM problem-solving but can wastefully generate many tokens\nwithout improving accuracy. At the same time, we observe that these algorithms\nexhibit answer stabilization: their intermediate solutions often cease to\nchange after a certain point, and further investment of compute does not change\ntheir final answer. To quantify this phenomenon, we introduce Certaindex, an\nalgorithm-agnostic metric measuring this evolving stability, signaling when\nfurther computation is unlikely to alter the final result. Certaindex is\nlightweight, can accelerate reasoning program inference via early exit, and\nfurther enables dynamic token allocation, gang scheduling, and many\nopportunities when integrated with real-world LLM serving systems. To quantify\nreal-world benefits, we built Certaindex as a scheduler into Dynasor, our\nreasoning-aware LLM serving system, and demonstrate up to 50% compute savings\nand 3.3x higher throughput in real workloads with no accuracy drop. Our code is\navailable at https://github.com/hao-ai-lab/Dynasor.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time reasoning algorithms such as chain-of-thought, self-consistency,\nand MCTS enhance LLM problem-solving but can wastefully generate many tokens\nwithout improving accuracy. At the same time, we observe that these algorithms\nexhibit answer stabilization: their intermediate solutions often cease to\nchange after a certain point, and further investment of compute does not change\ntheir final answer. To quantify this phenomenon, we introduce Certaindex, an\nalgorithm-agnostic metric measuring this evolving stability, signaling when\nfurther computation is unlikely to alter the final result. Certaindex is\nlightweight, can accelerate reasoning program inference via early exit, and\nfurther enables dynamic token allocation, gang scheduling, and many\nopportunities when integrated with real-world LLM serving systems. To quantify\nreal-world benefits, we built Certaindex as a scheduler into Dynasor, our\nreasoning-aware LLM serving system, and demonstrate up to 50% compute savings\nand 3.3x higher throughput in real workloads with no accuracy drop. Our code is\navailable at https://github.com/hao-ai-lab/Dynasor.git"
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Zheyu Fu"
                    },
                    {
                        "name": "Zhongdongming Dai"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Yian Ma"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21414v1",
                "updated": "2025-05-27T16:41:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    23,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:41:23Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    23,
                    1,
                    147,
                    0
                ],
                "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior\n  to Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Adversarial Analysis of Decision Support Systems Prior\n  to Deployment"
                },
                "summary": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments."
                },
                "authors": [
                    {
                        "name": "Brett Bissey"
                    },
                    {
                        "name": "Kyle Gatesman"
                    },
                    {
                        "name": "Walker Dimon"
                    },
                    {
                        "name": "Mohammad Alam"
                    },
                    {
                        "name": "Luis Robaina"
                    },
                    {
                        "name": "Joseph Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Weissman"
                },
                "author": "Joseph Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21413v1",
                "updated": "2025-05-27T16:41:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    19,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:41:19Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    41,
                    19,
                    1,
                    147,
                    0
                ],
                "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation"
                },
                "summary": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning."
                },
                "authors": [
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Zirui Wu"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "arxiv_comment": "Code is available at https://github.com/xxxiaol/RefTool",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09598v2",
                "updated": "2025-05-27T16:40:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    40,
                    26,
                    1,
                    147,
                    0
                ],
                "published": "2025-03-12T17:59:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation"
                },
                "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world interactions. We curated EchoMist, the\nfirst comprehensive benchmark for implicit misinformation, where false\nassumptions are embedded in the query to LLMs. EchoMist targets circulated,\nharmful, and ever-evolving implicit misinformation from diverse sources,\nincluding realistic human-AI conversations and social media interactions.\nThrough extensive empirical studies on 15 state-of-the-art LLMs, we find that\ncurrent models perform alarmingly poorly on this task, often failing to detect\nfalse premises and generating counterfactual explanations. We also investigate\ntwo mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability\nto counter implicit misinformation. Our findings indicate that EchoMist remains\na persistent challenge and underscore the critical need to safeguard against\nthe risk of implicit misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world interactions. We curated EchoMist, the\nfirst comprehensive benchmark for implicit misinformation, where false\nassumptions are embedded in the query to LLMs. EchoMist targets circulated,\nharmful, and ever-evolving implicit misinformation from diverse sources,\nincluding realistic human-AI conversations and social media interactions.\nThrough extensive empirical studies on 15 state-of-the-art LLMs, we find that\ncurrent models perform alarmingly poorly on this task, often failing to detect\nfalse premises and generating counterfactual explanations. We also investigate\ntwo mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability\nto counter implicit misinformation. Our findings indicate that EchoMist remains\na persistent challenge and underscore the critical need to safeguard against\nthe risk of implicit misinformation."
                },
                "authors": [
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21409v1",
                "updated": "2025-05-27T16:33:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    33,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:33:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    33,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from\n  Large Language Models"
                },
                "summary": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality."
                },
                "authors": [
                    {
                        "name": "Dario Satriani"
                    },
                    {
                        "name": "Enzo Veltri"
                    },
                    {
                        "name": "Donatello Santoro"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21408v1",
                "updated": "2025-05-27T16:33:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    33,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:33:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    33,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "WiCAL: Accurate Wi-Fi-Based 3D Localization Enabled by Collaborative\n  Antenna Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiCAL: Accurate Wi-Fi-Based 3D Localization Enabled by Collaborative\n  Antenna Arrays"
                },
                "summary": "Accurate 3D localization is essential for realizing advanced sensing\nfunctionalities in next-generation Wi-Fi communication systems. This study\ninvestigates the potential of multistatic localization in Wi-Fi networks\nthrough the deployment of multiple cooperative antenna arrays. The\ncollaborative gain offered by these arrays is twofold: (i) intra-array coherent\ngain at the wavelength scale among antenna elements, and (ii) inter-array\ncooperative gain across arrays. To evaluate the feasibility and performance of\nthis approach, we develop WiCAL (Wi-Fi Collaborative Antenna Localization), a\nsystem built upon commercial Wi-Fi infrastructure equipped with uniform\nrectangular arrays. These arrays are driven by multiplexing embedded radio\nfrequency chains available in standard access points or user devices, thereby\neliminating the need for sophisticated, costly, and power-hungry\nmulti-transceiver modules typically required in multiple-input and\nmultiple-output systems. To address phase offsets introduced by RF chain\nmultiplexing, we propose a three-stage, fine-grained phase alignment scheme to\nsynchronize signals across antenna elements within each array. A bidirectional\nspatial smoothing MUSIC algorithm is employed to estimate angles of arrival\n(AoAs) and mitigate performance degradation caused by correlated interference.\nTo further exploit inter-array cooperative gain, we elaborate on the\nsynchronization mechanism among distributed URAs, which enables direct position\ndetermination by bypassing intermediate angle estimation. Once synchronized,\nthe distributed URAs effectively form a virtual large-scale array,\nsignificantly enhancing spatial resolution and localization accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate 3D localization is essential for realizing advanced sensing\nfunctionalities in next-generation Wi-Fi communication systems. This study\ninvestigates the potential of multistatic localization in Wi-Fi networks\nthrough the deployment of multiple cooperative antenna arrays. The\ncollaborative gain offered by these arrays is twofold: (i) intra-array coherent\ngain at the wavelength scale among antenna elements, and (ii) inter-array\ncooperative gain across arrays. To evaluate the feasibility and performance of\nthis approach, we develop WiCAL (Wi-Fi Collaborative Antenna Localization), a\nsystem built upon commercial Wi-Fi infrastructure equipped with uniform\nrectangular arrays. These arrays are driven by multiplexing embedded radio\nfrequency chains available in standard access points or user devices, thereby\neliminating the need for sophisticated, costly, and power-hungry\nmulti-transceiver modules typically required in multiple-input and\nmultiple-output systems. To address phase offsets introduced by RF chain\nmultiplexing, we propose a three-stage, fine-grained phase alignment scheme to\nsynchronize signals across antenna elements within each array. A bidirectional\nspatial smoothing MUSIC algorithm is employed to estimate angles of arrival\n(AoAs) and mitigate performance degradation caused by correlated interference.\nTo further exploit inter-array cooperative gain, we elaborate on the\nsynchronization mechanism among distributed URAs, which enables direct position\ndetermination by bypassing intermediate angle estimation. Once synchronized,\nthe distributed URAs effectively form a virtual large-scale array,\nsignificantly enhancing spatial resolution and localization accuracy."
                },
                "authors": [
                    {
                        "name": "Fuhai Wang"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Rujing Xiong"
                    },
                    {
                        "name": "Tiebin Mi"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Robert Caiming Qiu"
                },
                "author": "Robert Caiming Qiu",
                "arxiv_comment": "14 page, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21400v1",
                "updated": "2025-05-27T16:24:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    20,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:24:20Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    20,
                    1,
                    147,
                    0
                ],
                "title": "A Convergence Theory for Diffusion Language Models: An\n  Information-Theoretic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Convergence Theory for Diffusion Language Models: An\n  Information-Theoretic Perspective"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Changxiao Cai"
                    }
                ],
                "author_detail": {
                    "name": "Changxiao Cai"
                },
                "author": "Changxiao Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21399v1",
                "updated": "2025-05-27T16:24:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    2,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:24:02Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    2,
                    1,
                    147,
                    0
                ],
                "title": "Factual Self-Awareness in Language Models: Representation, Robustness,\n  and Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factual Self-Awareness in Language Models: Representation, Robustness,\n  and Scaling"
                },
                "summary": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability."
                },
                "authors": [
                    {
                        "name": "Hovhannes Tamoyan"
                    },
                    {
                        "name": "Subhabrata Dutta"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21397v1",
                "updated": "2025-05-27T16:23:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:23:53Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    53,
                    1,
                    147,
                    0
                ],
                "title": "DecisionFlow: Advancing Large Language Model as Principled Decision\n  Maker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecisionFlow: Advancing Large Language Model as Principled Decision\n  Maker"
                },
                "summary": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow."
                },
                "authors": [
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Shanyong Wang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11189v3",
                "updated": "2025-05-27T16:23:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    46,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-15T13:48:39Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    13,
                    48,
                    39,
                    6,
                    350,
                    0
                ],
                "title": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters"
                },
                "summary": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions with active NPCs have been a focus, price negotiations\nbetween merchant NPCs and players remain underexplored. First, passive pricing\nrefers to the limited ability of merchants to modify predefined item prices.\nSecond, passive communication means that merchants can only interact with\nplayers in a scripted manner. To tackle these issues and create an active\nmerchant NPC, we propose a merchant framework based on large language models\n(LLMs), called MART, which consists of an appraiser module and a negotiator\nmodule. We conducted two experiments to explore various implementation options\nunder different training methods and LLM sizes, considering a range of possible\ngame environments. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions with active NPCs have been a focus, price negotiations\nbetween merchant NPCs and players remain underexplored. First, passive pricing\nrefers to the limited ability of merchants to modify predefined item prices.\nSecond, passive communication means that merchants can only interact with\nplayers in a scripted manner. To tackle these issues and create an active\nmerchant NPC, we propose a merchant framework based on large language models\n(LLMs), called MART, which consists of an appraiser module and a negotiator\nmodule. We conducted two experiments to explore various implementation options\nunder different training methods and LLM sizes, considering a range of possible\ngame environments. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Accepted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21396v1",
                "updated": "2025-05-27T16:23:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    42,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:23:42Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    23,
                    42,
                    1,
                    147,
                    0
                ],
                "title": "Improving Research Idea Generation Through Data: An Empirical\n  Investigation in Social Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Research Idea Generation Through Data: An Empirical\n  Investigation in Social Science"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings."
                },
                "authors": [
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xinyi Dong"
                    },
                    {
                        "name": "Xinyang Gao"
                    },
                    {
                        "name": "Yansong Feng"
                    },
                    {
                        "name": "Xun Pang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Pang"
                },
                "author": "Xun Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01834v2",
                "updated": "2025-05-27T16:17:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    17,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2024-11-04T06:07:53Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    7,
                    53,
                    0,
                    309,
                    0
                ],
                "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback"
                },
                "summary": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs."
                },
                "authors": [
                    {
                        "name": "Guan-Ting Lin"
                    },
                    {
                        "name": "Prashanth Gurunath Shivakumar"
                    },
                    {
                        "name": "Aditya Gourav"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Ankur Gandhe"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Ivan Bulyko"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Bulyko"
                },
                "author": "Ivan Bulyko",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21382v1",
                "updated": "2025-05-27T16:10:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    10,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:10:53Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    10,
                    53,
                    1,
                    147,
                    0
                ],
                "title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation\n  of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation\n  of Foundation Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions."
                },
                "authors": [
                    {
                        "name": "Nastaran Saadati"
                    },
                    {
                        "name": "Zhanhong Jiang"
                    },
                    {
                        "name": "Joshua R. Waite"
                    },
                    {
                        "name": "Shreyan Ganguly"
                    },
                    {
                        "name": "Aditya Balu"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Soumik Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Soumik Sarkar"
                },
                "author": "Soumik Sarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21378v1",
                "updated": "2025-05-27T16:07:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    7,
                    33,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:07:33Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    7,
                    33,
                    1,
                    147,
                    0
                ],
                "title": "Analyzing values about gendered language reform in LLMs' revisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing values about gendered language reform in LLMs' revisions"
                },
                "summary": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment."
                },
                "authors": [
                    {
                        "name": "Jules Watson"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Raymond Liu"
                    },
                    {
                        "name": "Suzanne Stevenson"
                    },
                    {
                        "name": "Barend Beekhuizen"
                    }
                ],
                "author_detail": {
                    "name": "Barend Beekhuizen"
                },
                "author": "Barend Beekhuizen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05203v2",
                "updated": "2025-05-27T16:06:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    6,
                    13,
                    1,
                    147,
                    0
                ],
                "published": "2025-03-07T07:48:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    48,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "Path Pooling: Training-Free Structure Enhancement for Efficient\n  Knowledge Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path Pooling: Training-Free Structure Enhancement for Efficient\n  Knowledge Graph Retrieval-Augmented Generation"
                },
                "summary": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, training-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, training-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost."
                },
                "authors": [
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09606v2",
                "updated": "2025-05-27T16:03:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    3,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-14T17:57:21Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    57,
                    21,
                    2,
                    134,
                    0
                ],
                "title": "Comparative Analysis of GFN Methods in Geometry Optimization of Small\n  Organic Semiconductor Molecules: A DFT Benchmarking Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of GFN Methods in Geometry Optimization of Small\n  Organic Semiconductor Molecules: A DFT Benchmarking Study"
                },
                "summary": "This study benchmarks the GFN family of semiempirical methods (GFN1-xTB,\nGFN2-xTB, GFN0-xTB, and GFN-FF) against density functional theory (DFT) for the\nevaluation of optimized molecular geometries and electronic properties of small\norganic semiconductor molecules. This work offers a systematic assessment of\nthese computationally efficient quantum chemical methods and their\naccuracy-cost profiles when applied to a challenging class of systems,\ncharacterized, for instance, by extended $\\pi$-conjugation, conformational\nflexibility, and sensitivity of properties to subtle structural changes. Two\ndatasets are evaluated: a QM9-derived subset of small organic molecules and the\nHarvard Clean Energy Project (CEP) database of extended $\\pi$-systems relevant\nto organic photovoltaics. Structural agreement is quantified using heavy-atom\nRMSD, equilibrium rotational constants, bond lengths, and angles, while\nelectronic property prediction is assessed via HOMO-LUMO energy gaps.\nComputational efficiency is assessed via CPU time and scaling behavior.\nGFN1-xTB and GFN2-xTB demonstrate the highest structural fidelity, while GFN-FF\noffers an optimal balance between accuracy and speed, particularly for larger\nsystems. The results indicate that GFN-based methods are suitable for\nhigh-throughput molecular screening of small organic semiconductors, with the\nchoice of method depending on accuracy-cost trade-offs. The findings support\nthe deployment of GFN approaches in computational pipelines for the discovery\nof organic electronics and materials, providing information on their strengths\nand limitations relative to established DFT methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study benchmarks the GFN family of semiempirical methods (GFN1-xTB,\nGFN2-xTB, GFN0-xTB, and GFN-FF) against density functional theory (DFT) for the\nevaluation of optimized molecular geometries and electronic properties of small\norganic semiconductor molecules. This work offers a systematic assessment of\nthese computationally efficient quantum chemical methods and their\naccuracy-cost profiles when applied to a challenging class of systems,\ncharacterized, for instance, by extended $\\pi$-conjugation, conformational\nflexibility, and sensitivity of properties to subtle structural changes. Two\ndatasets are evaluated: a QM9-derived subset of small organic molecules and the\nHarvard Clean Energy Project (CEP) database of extended $\\pi$-systems relevant\nto organic photovoltaics. Structural agreement is quantified using heavy-atom\nRMSD, equilibrium rotational constants, bond lengths, and angles, while\nelectronic property prediction is assessed via HOMO-LUMO energy gaps.\nComputational efficiency is assessed via CPU time and scaling behavior.\nGFN1-xTB and GFN2-xTB demonstrate the highest structural fidelity, while GFN-FF\noffers an optimal balance between accuracy and speed, particularly for larger\nsystems. The results indicate that GFN-based methods are suitable for\nhigh-throughput molecular screening of small organic semiconductors, with the\nchoice of method depending on accuracy-cost trade-offs. The findings support\nthe deployment of GFN approaches in computational pipelines for the discovery\nof organic electronics and materials, providing information on their strengths\nand limitations relative to established DFT methods."
                },
                "authors": [
                    {
                        "name": "Steve Cabrel Teguia Kouam"
                    },
                    {
                        "name": "Jean-Pierre Tchapet Njafa"
                    },
                    {
                        "name": "Raoult Dabou Teukam"
                    },
                    {
                        "name": "Patrick Mvoto Kongo"
                    },
                    {
                        "name": "Jean-Pierre Nguenang"
                    },
                    {
                        "name": "Serge Guy Nana Engo"
                    }
                ],
                "author_detail": {
                    "name": "Serge Guy Nana Engo"
                },
                "author": "Serge Guy Nana Engo",
                "arxiv_comment": "The PDF (version 2) contains the main text and the supplementary\n  materials, including 42 pages, 23 figures, and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21372v1",
                "updated": "2025-05-27T16:01:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    49,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:01:49Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    49,
                    1,
                    147,
                    0
                ],
                "title": "Improving LLM-based Global Optimization with Search Space Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM-based Global Optimization with Search Space Partitioning"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies."
                },
                "authors": [
                    {
                        "name": "Andrej Schwanke"
                    },
                    {
                        "name": "Lyubomir Ivanov"
                    },
                    {
                        "name": "David Salinas"
                    },
                    {
                        "name": "Fabio Ferreira"
                    },
                    {
                        "name": "Aaron Klein"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Arber Zela"
                    }
                ],
                "author_detail": {
                    "name": "Arber Zela"
                },
                "author": "Arber Zela",
                "arxiv_comment": "25 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21371v1",
                "updated": "2025-05-27T16:01:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T16:01:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    1,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "When Experimental Economics Meets Large Language Models: Tactics with\n  Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Experimental Economics Meets Large Language Models: Tactics with\n  Evidence"
                },
                "summary": "Advancements in large language models (LLMs) have sparked a growing interest\nin measuring and understanding their behavior through experimental economics.\nHowever, there is still a lack of established guidelines for designing economic\nexperiments for LLMs. By combining principles from experimental economics with\ninsights from LLM research in artificial intelligence, we outline and discuss\neight practical tactics for conducting experiments with LLMs. We further\nperform two sets of experiments to demonstrate the significance of these\ntactics. Our study enhances the design, replicability, and generalizability of\nLLM experiments, and broadens the scope of experimental economics in the\ndigital age.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have sparked a growing interest\nin measuring and understanding their behavior through experimental economics.\nHowever, there is still a lack of established guidelines for designing economic\nexperiments for LLMs. By combining principles from experimental economics with\ninsights from LLM research in artificial intelligence, we outline and discuss\neight practical tactics for conducting experiments with LLMs. We further\nperform two sets of experiments to demonstrate the significance of these\ntactics. Our study enhances the design, replicability, and generalizability of\nLLM experiments, and broadens the scope of experimental economics in the\ndigital age."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Shuhuai Zhang"
                    },
                    {
                        "name": "Jianuo Gai"
                    },
                    {
                        "name": "Tracy Xiao Liu"
                    },
                    {
                        "name": "Songfa Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Songfa Zhong"
                },
                "author": "Songfa Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21362v1",
                "updated": "2025-05-27T15:52:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    52,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:52:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    52,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs.\n  Dialogue History",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs.\n  Dialogue History"
                },
                "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation."
                },
                "authors": [
                    {
                        "name": "Qishuai Zhong"
                    },
                    {
                        "name": "Zongmin Li"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17266v2",
                "updated": "2025-05-27T15:50:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    50,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-22T20:24:08Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    24,
                    8,
                    3,
                    142,
                    0
                ],
                "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT\n  Reasoning"
                },
                "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost."
                },
                "authors": [
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21356v2",
                "updated": "2025-05-28T01:58:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    58,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T15:48:17Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    48,
                    17,
                    1,
                    147,
                    0
                ],
                "title": "Towards Robust Automated Perceptual Voice Quality Assessment with Speech\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Automated Perceptual Voice Quality Assessment with Speech\n  Foundation Models"
                },
                "summary": "Perceptual voice quality assessment is essential for diagnosing and\nmonitoring voice disorders. Traditionally, expert raters use scales such as the\nCAPE-V and GRBAS. However, these are subjective and prone to inter-rater\nvariability, motivating the need for automated, objective assessment methods.\nThis study proposes VOQANet, a deep learning framework with an attention\nmechanism that leverages a Speech Foundation Model (SFM) to extract high-level\nacoustic and prosodic information from raw speech. To improve robustness and\ninterpretability, we introduce VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings into a hybrid representation. Unlike prior work focusing only on\nvowel-based phonation (PVQD-A subset) from the Perceptual Voice Quality Dataset\n(PVQD), we evaluate our models on both vowel-based and sentence-level speech\n(PVQD-S subset) for better generalizability. Results show that sentence-based\ninput outperforms vowel-based input, particularly at the patient level,\nhighlighting the benefit of longer utterances for capturing voice attributes.\nVOQANet consistently surpasses baseline methods in root mean squared error and\nPearson correlation across CAPE-V and GRBAS dimensions, with VOQANet+ achieving\nfurther improvements. Additional tests under noisy conditions show that\nVOQANet+ maintains high prediction accuracy, supporting its use in real-world\nand telehealth settings. These findings demonstrate the value of combining SFM\nembeddings with domain-informed acoustic features for interpretable and robust\nvoice quality assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceptual voice quality assessment is essential for diagnosing and\nmonitoring voice disorders. Traditionally, expert raters use scales such as the\nCAPE-V and GRBAS. However, these are subjective and prone to inter-rater\nvariability, motivating the need for automated, objective assessment methods.\nThis study proposes VOQANet, a deep learning framework with an attention\nmechanism that leverages a Speech Foundation Model (SFM) to extract high-level\nacoustic and prosodic information from raw speech. To improve robustness and\ninterpretability, we introduce VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings into a hybrid representation. Unlike prior work focusing only on\nvowel-based phonation (PVQD-A subset) from the Perceptual Voice Quality Dataset\n(PVQD), we evaluate our models on both vowel-based and sentence-level speech\n(PVQD-S subset) for better generalizability. Results show that sentence-based\ninput outperforms vowel-based input, particularly at the patient level,\nhighlighting the benefit of longer utterances for capturing voice attributes.\nVOQANet consistently surpasses baseline methods in root mean squared error and\nPearson correlation across CAPE-V and GRBAS dimensions, with VOQANet+ achieving\nfurther improvements. Additional tests under noisy conditions show that\nVOQANet+ maintains high prediction accuracy, supporting its use in real-world\nand telehealth settings. These findings demonstrate the value of combining SFM\nembeddings with domain-informed acoustic features for interpretable and robust\nvoice quality assessment."
                },
                "authors": [
                    {
                        "name": "Whenty Ariyanti"
                    },
                    {
                        "name": "Kuan-Yu Chen"
                    },
                    {
                        "name": "Sabato Marco Siniscalchi"
                    },
                    {
                        "name": "Hsin-Min Wang"
                    },
                    {
                        "name": "Yu Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tsao"
                },
                "author": "Yu Tsao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21354v1",
                "updated": "2025-05-27T15:47:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    47,
                    10,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:47:10Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    47,
                    10,
                    1,
                    147,
                    0
                ],
                "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning"
                },
                "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies."
                },
                "authors": [
                    {
                        "name": "Bidyarthi Paul"
                    },
                    {
                        "name": "Jalisha Jashim Era"
                    },
                    {
                        "name": "Mirazur Rahman Zim"
                    },
                    {
                        "name": "Tahmid Sattar Aothoi"
                    },
                    {
                        "name": "Faisal Muhammad Shah"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Muhammad Shah"
                },
                "author": "Faisal Muhammad Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21342v2",
                "updated": "2025-05-28T10:15:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    10,
                    15,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T15:34:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    34,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims"
                },
                "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code."
                },
                "authors": [
                    {
                        "name": "Valentin Knappich"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    },
                    {
                        "name": "Anna Hätty"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11642v2",
                "updated": "2025-05-27T15:31:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    31,
                    58,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-16T19:08:29Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    19,
                    8,
                    29,
                    4,
                    136,
                    0
                ],
                "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks\n  Through Mutual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks\n  Through Mutual Reasoning"
                },
                "summary": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."
                },
                "authors": [
                    {
                        "name": "Falong Fan"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "arxiv_comment": "This paper has been accepted to IEEE IRI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06832v3",
                "updated": "2025-05-27T15:29:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    29,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-05T20:45:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    45,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach"
                },
                "summary": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods. The code is publicly\navailable at https://github.com/TIML-Group/Robust-MoE-Dual-Model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods. The code is publicly\navailable at https://github.com/TIML-Group/Robust-MoE-Dual-Model."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Ziqing Hu"
                    },
                    {
                        "name": "Ren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ren Wang"
                },
                "author": "Ren Wang",
                "arxiv_comment": "15 pages, 7 figures, accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21334v1",
                "updated": "2025-05-27T15:28:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    28,
                    45,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:28:45Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    28,
                    45,
                    1,
                    147,
                    0
                ],
                "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoliTom: Holistic Token Merging for Fast Video Large Language Models"
                },
                "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference."
                },
                "authors": [
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21333v1",
                "updated": "2025-05-27T15:27:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    27,
                    46,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:27:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    27,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios."
                },
                "authors": [
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Huanqian Wang"
                    },
                    {
                        "name": "Wulin Xie"
                    },
                    {
                        "name": "Huanyao Zhang"
                    },
                    {
                        "name": "Lijie Zhao"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Zhuoer Wen"
                    },
                    {
                        "name": "Wenting Liu"
                    },
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Bohan Zeng"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Yang"
                },
                "author": "Wenjing Yang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21324v1",
                "updated": "2025-05-27T15:22:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    22,
                    1,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:22:01Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    22,
                    1,
                    1,
                    147,
                    0
                ],
                "title": "Leveraging large language models and traditional machine learning\n  ensembles for ADHD detection from narrative transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models and traditional machine learning\n  ensembles for ADHD detection from narrative transcripts"
                },
                "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhu"
                    },
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Noah Marchuck"
                    },
                    {
                        "name": "Abeed Sarker"
                    },
                    {
                        "name": "Yun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Wang"
                },
                "author": "Yun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21322v1",
                "updated": "2025-05-27T15:21:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    21,
                    6,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:21:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    21,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Assured Autonomy with Neuro-Symbolic Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assured Autonomy with Neuro-Symbolic Perception"
                },
                "summary": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS."
                },
                "authors": [
                    {
                        "name": "R. Spencer Hallyburton"
                    },
                    {
                        "name": "Miroslav Pajic"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Pajic"
                },
                "author": "Miroslav Pajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21318v1",
                "updated": "2025-05-27T15:15:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    15,
                    44,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:15:44Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    15,
                    44,
                    1,
                    147,
                    0
                ],
                "title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular\n  Chemical Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular\n  Chemical Operations"
                },
                "summary": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "He Cao"
                    },
                    {
                        "name": "Bin Feng"
                    },
                    {
                        "name": "Yanjun Shao"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Yu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu Li"
                },
                "author": "Yu Li",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21315v1",
                "updated": "2025-05-27T15:13:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    13,
                    8,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:13:08Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    13,
                    8,
                    1,
                    147,
                    0
                ],
                "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead"
                },
                "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages."
                },
                "authors": [
                    {
                        "name": "Jesujoba O. Alabi"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "arxiv_comment": "Working paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18048v2",
                "updated": "2025-05-27T15:11:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    11,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-23T15:52:31Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    52,
                    31,
                    4,
                    143,
                    0
                ],
                "title": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded\n  Scenarios"
                },
                "summary": "Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS)."
                },
                "authors": [
                    {
                        "name": "Simon Malzard"
                    },
                    {
                        "name": "Nitish Mital"
                    },
                    {
                        "name": "Richard Walters"
                    },
                    {
                        "name": "Victoria Nockles"
                    },
                    {
                        "name": "Raghuveer Rao"
                    },
                    {
                        "name": "Celso M. De Melo"
                    }
                ],
                "author_detail": {
                    "name": "Celso M. De Melo"
                },
                "author": "Celso M. De Melo",
                "arxiv_comment": "19 pages, 2 images, updated acknowledgements versus previous versions\n  to be compliant with funders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21304v1",
                "updated": "2025-05-27T15:06:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    6,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:06:04Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    6,
                    4,
                    1,
                    147,
                    0
                ],
                "title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with\n  Limited Participants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with\n  Limited Participants"
                },
                "summary": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort."
                },
                "authors": [
                    {
                        "name": "Louis Jalouzot"
                    },
                    {
                        "name": "Alexis Thual"
                    },
                    {
                        "name": "Yair Lakretz"
                    },
                    {
                        "name": "Christophe Pallier"
                    },
                    {
                        "name": "Bertrand Thirion"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Thirion"
                },
                "author": "Bertrand Thirion",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21301v1",
                "updated": "2025-05-27T15:04:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    4,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:04:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    4,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate\n  Categories in Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate\n  Categories in Italian"
                },
                "summary": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research."
                },
                "authors": [
                    {
                        "name": "Andrea Pedrotti"
                    },
                    {
                        "name": "Giulia Rambelli"
                    },
                    {
                        "name": "Caterina Villani"
                    },
                    {
                        "name": "Marianna Bolognesi"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Bolognesi"
                },
                "author": "Marianna Bolognesi",
                "arxiv_comment": "Accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21298v1",
                "updated": "2025-05-27T15:01:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    1,
                    6,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    1,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Large Language Models Miss the Multi-Agent Mark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Miss the Multi-Agent Mark"
                },
                "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities."
                },
                "authors": [
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Gabriele La Malfa"
                    },
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Elizabeth Black"
                    },
                    {
                        "name": "Micheal Luck"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21297v1",
                "updated": "2025-05-27T15:00:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    0,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T15:00:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    0,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset"
                },
                "summary": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar."
                },
                "authors": [
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Bingcheng Dong"
                    },
                    {
                        "name": "Xudong Zhou"
                    },
                    {
                        "name": "Ning Shang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20947v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20947v4",
                "updated": "2025-05-27T14:59:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    59,
                    5,
                    1,
                    147,
                    0
                ],
                "published": "2024-05-31T15:44:33Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    44,
                    33,
                    4,
                    152,
                    0
                ],
                "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OR-Bench: An Over-Refusal Benchmark for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models."
                },
                "authors": [
                    {
                        "name": "Justin Cui"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "arxiv_comment": "Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20947v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20947v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00715v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00715v6",
                "updated": "2025-05-27T14:58:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    58,
                    40,
                    1,
                    147,
                    0
                ],
                "published": "2024-04-25T15:34:53Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    15,
                    34,
                    53,
                    3,
                    116,
                    0
                ],
                "title": "Towards Adapting Open-Source Large Language Models for Expert-Level\n  Clinical Note Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adapting Open-Source Large Language Models for Expert-Level\n  Clinical Note Generation"
                },
                "summary": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pretraining,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(92.8%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic matched physician-authored notes in real-world readiness\nscore. We highlight key considerations for future clinical note-generation\ntasks, emphasizing the importance of pre-defining a \"best practice\" note\nformat, rather than relying on LLMs to determine this for clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pretraining,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(92.8%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic matched physician-authored notes in real-world readiness\nscore. We highlight key considerations for future clinical note-generation\ntasks, emphasizing the importance of pre-defining a \"best practice\" note\nformat, rather than relying on LLMs to determine this for clinical practice."
                },
                "authors": [
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Chufan Gao"
                    },
                    {
                        "name": "Bolun Liu"
                    },
                    {
                        "name": "Qiping Xu"
                    },
                    {
                        "name": "Guleid Hussein"
                    },
                    {
                        "name": "Mohamad El Labban"
                    },
                    {
                        "name": "Kingsley Iheasirim"
                    },
                    {
                        "name": "Hariprasad Korsapati"
                    },
                    {
                        "name": "Chuck Outcalt"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00715v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00715v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12134v2",
                "updated": "2025-05-27T14:54:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    51,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-17T18:52:29Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    52,
                    29,
                    0,
                    48,
                    0
                ],
                "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nrequire full-model fine-tuning and suffer from catastrophic forgetting,\nlimiting their applicability to state-of-the-art LLMs that already perform well\nin zero-shot settings with a proper instruction. To address this challenge, we\npropose a novel approach for continuous-space reasoning that does not require\nmodifying the LLM. Specifically, we employ a lightweight fixed assistant model\nto speculatively generate instance-specific soft thought tokens as the initial\nchain of thoughts, which are then mapped into the LLM's representation space\nvia a trainable projection module. Experimental results on five reasoning\nbenchmarks demonstrate that our method enhances LLM reasoning performance\nthrough supervised, parameter-efficient fine-tuning. Source code is available\nat https://github.com/xuyige/SoftCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nrequire full-model fine-tuning and suffer from catastrophic forgetting,\nlimiting their applicability to state-of-the-art LLMs that already perform well\nin zero-shot settings with a proper instruction. To address this challenge, we\npropose a novel approach for continuous-space reasoning that does not require\nmodifying the LLM. Specifically, we employ a lightweight fixed assistant model\nto speculatively generate instance-specific soft thought tokens as the initial\nchain of thoughts, which are then mapped into the LLM's representation space\nvia a trainable projection module. Experimental results on five reasoning\nbenchmarks demonstrate that our method enhances LLM reasoning performance\nthrough supervised, parameter-efficient fine-tuning. Source code is available\nat https://github.com/xuyige/SoftCoT."
                },
                "authors": [
                    {
                        "name": "Yige Xu"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Zhiwei Zeng"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "arxiv_comment": "Camera-ready for ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21291v1",
                "updated": "2025-05-27T14:54:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    49,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:54:49Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    54,
                    49,
                    1,
                    147,
                    0
                ],
                "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large\n  Language Model-Enhanced Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large\n  Language Model-Enhanced Framework"
                },
                "summary": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics."
                },
                "authors": [
                    {
                        "name": "Saman Marandi"
                    },
                    {
                        "name": "Yu-Shu Hu"
                    },
                    {
                        "name": "Mohammad Modarres"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Modarres"
                },
                "author": "Mohammad Modarres",
                "arxiv_comment": "22 Pages, 11 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21286v1",
                "updated": "2025-05-27T14:53:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    53,
                    25,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:53:25Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    53,
                    25,
                    1,
                    147,
                    0
                ],
                "title": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services\n  Powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services\n  Powered by Large Language Models"
                },
                "summary": "Agentic AI, often powered by large language models (LLMs), is becoming\nincreasingly popular and adopted to support autonomous reasoning,\ndecision-making, and task execution across various domains. While agentic AI\nholds great promise, its deployment as services for easy access raises critical\nchallenges in pricing, due to high infrastructure and computation costs,\nmulti-dimensional and task-dependent Quality of Service (QoS), and growing\nconcerns around liability in high-stakes applications. In this work, we propose\nPACT, a Pricing framework for cloud-based Agentic AI services through a\nContract-Theoretic approach, which models QoS along both objective (e.g.,\nresponse time) and subjective (e.g., user satisfaction) dimensions. PACT\naccounts for computational, infrastructure, and potential liability costs for\nthe service provider, while ensuring incentive compatibility and individual\nrationality for the user under information asymmetry. Through contract-based\nselection, users receive tailored service offerings aligned with their needs.\nNumerical evaluations demonstrate that PACT improves QoS alignment between\nusers and providers and offers a scalable, liable approach to pricing agentic\nAI services in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI, often powered by large language models (LLMs), is becoming\nincreasingly popular and adopted to support autonomous reasoning,\ndecision-making, and task execution across various domains. While agentic AI\nholds great promise, its deployment as services for easy access raises critical\nchallenges in pricing, due to high infrastructure and computation costs,\nmulti-dimensional and task-dependent Quality of Service (QoS), and growing\nconcerns around liability in high-stakes applications. In this work, we propose\nPACT, a Pricing framework for cloud-based Agentic AI services through a\nContract-Theoretic approach, which models QoS along both objective (e.g.,\nresponse time) and subjective (e.g., user satisfaction) dimensions. PACT\naccounts for computational, infrastructure, and potential liability costs for\nthe service provider, while ensuring incentive compatibility and individual\nrationality for the user under information asymmetry. Through contract-based\nselection, users receive tailored service offerings aligned with their needs.\nNumerical evaluations demonstrate that PACT improves QoS alignment between\nusers and providers and offers a scalable, liable approach to pricing agentic\nAI services in the future."
                },
                "authors": [
                    {
                        "name": "Ya-Ting Yang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21277v1",
                "updated": "2025-05-27T14:48:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    48,
                    44,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:48:44Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    48,
                    44,
                    1,
                    147,
                    0
                ],
                "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space"
                },
                "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
                },
                "authors": [
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Shouwei Ruan"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Xingxing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wei"
                },
                "author": "Xingxing Wei",
                "arxiv_comment": "19 pages, 20 figures, accepted by ACL 2025, Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03181v2",
                "updated": "2025-05-27T14:46:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    46,
                    3,
                    1,
                    147,
                    0
                ],
                "published": "2024-07-03T15:01:18Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    15,
                    1,
                    18,
                    2,
                    185,
                    0
                ],
                "title": "Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT\n  Refinement in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT\n  Refinement in LLMs"
                },
                "summary": "Requiring a large language model (LLM) to generate intermediary reasoning\nsteps, known as Chain of Thought (CoT), has been shown to be an effective way\nof boosting performance. Previous approaches have focused on generating\nmultiple independent CoTs, combining them through ensembling or other post-hoc\nstrategies to enhance reasoning. In this work, we introduce a novel approach\nwhere LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought\n(DCoT) within a single inference step, which is fundamentally different from\nprior work that primarily operate on parallel CoT generations. DCoT allows LLMs\nto gain the ability to perform within-inference refinement of reasoning chains\nwithout requiring external feedback. Through a rigorous set of experiments\nspanning a wide range of tasks that require various reasoning types, we show\nthat fine-tuning on DCoT improves performance over the CoT baseline across\nmodel families and scales (1.3B to 70B). These improvements are particularly\nimpactful for tasks with a large result state space, such as those involving\nnumeric answers. Our work is also significant because both quantitative\nanalyses and manual evaluations reveal the observed gains stem from the models'\nability to refine an initial reasoning chain by generating a second, improved\nchain within the same inference step, demonstrating previously elusive\nself-improvement. Our code and data are publicly available at\nhttps://github.com/UKPLab/acl2025-diverse-cot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requiring a large language model (LLM) to generate intermediary reasoning\nsteps, known as Chain of Thought (CoT), has been shown to be an effective way\nof boosting performance. Previous approaches have focused on generating\nmultiple independent CoTs, combining them through ensembling or other post-hoc\nstrategies to enhance reasoning. In this work, we introduce a novel approach\nwhere LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought\n(DCoT) within a single inference step, which is fundamentally different from\nprior work that primarily operate on parallel CoT generations. DCoT allows LLMs\nto gain the ability to perform within-inference refinement of reasoning chains\nwithout requiring external feedback. Through a rigorous set of experiments\nspanning a wide range of tasks that require various reasoning types, we show\nthat fine-tuning on DCoT improves performance over the CoT baseline across\nmodel families and scales (1.3B to 70B). These improvements are particularly\nimpactful for tasks with a large result state space, such as those involving\nnumeric answers. Our work is also significant because both quantitative\nanalyses and manual evaluations reveal the observed gains stem from the models'\nability to refine an initial reasoning chain by generating a second, improved\nchain within the same inference step, demonstrating previously elusive\nself-improvement. Our code and data are publicly available at\nhttps://github.com/UKPLab/acl2025-diverse-cot."
                },
                "authors": [
                    {
                        "name": "Haritz Puerto"
                    },
                    {
                        "name": "Tilek Chubakov"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19388v2",
                "updated": "2025-05-27T14:43:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    43,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-01-31T18:43:29Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    43,
                    29,
                    4,
                    31,
                    0
                ],
                "title": "Online Decision-Making in Tree-Like Multi-Agent Games with Transfers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Decision-Making in Tree-Like Multi-Agent Games with Transfers"
                },
                "summary": "The widespread deployment of Machine Learning systems everywhere raises\nchallenges, such as dealing with interactions or competition between multiple\nlearners. In that goal, we study multi-agent sequential decision-making by\nconsidering principal-agent interactions in a tree structure. In this problem,\nthe reward of a player is influenced by the actions of her children, who are\nall self-interested and non-cooperative, hence the complexity of making good\ndecisions. Our main finding is that it is possible to steer all the players\ntowards the globally optimal set of actions by simply allowing single-step\ntransfers between them. A transfer is established between a principal and one\nof her agents: the principal actually offers the proposed payment if the agent\npicks the recommended action. The analysis poses specific challenges due to the\nintricate interactions between the nodes of the tree and the propagation of the\nregret within this tree. Considering a bandit setup, we propose algorithmic\nsolutions for the players to end up being no-regret with respect to the optimal\npair of actions and incentives. In the long run, allowing transfers between\nplayers makes them act as if they were collaborating together, although they\nremain self-interested non-cooperative: transfers restore efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Machine Learning systems everywhere raises\nchallenges, such as dealing with interactions or competition between multiple\nlearners. In that goal, we study multi-agent sequential decision-making by\nconsidering principal-agent interactions in a tree structure. In this problem,\nthe reward of a player is influenced by the actions of her children, who are\nall self-interested and non-cooperative, hence the complexity of making good\ndecisions. Our main finding is that it is possible to steer all the players\ntowards the globally optimal set of actions by simply allowing single-step\ntransfers between them. A transfer is established between a principal and one\nof her agents: the principal actually offers the proposed payment if the agent\npicks the recommended action. The analysis poses specific challenges due to the\nintricate interactions between the nodes of the tree and the propagation of the\nregret within this tree. Considering a bandit setup, we propose algorithmic\nsolutions for the players to end up being no-regret with respect to the optimal\npair of actions and incentives. In the long run, allowing transfers between\nplayers makes them act as if they were collaborating together, although they\nremain self-interested non-cooperative: transfers restore efficiency."
                },
                "authors": [
                    {
                        "name": "Antoine Scheid"
                    },
                    {
                        "name": "Etienne Boursier"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Eric Moulines"
                    },
                    {
                        "name": "Michael Jordan"
                    }
                ],
                "author_detail": {
                    "name": "Michael Jordan"
                },
                "author": "Michael Jordan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19311v2",
                "updated": "2025-05-27T14:41:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    41,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-26T17:08:07Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    7,
                    2,
                    57,
                    0
                ],
                "title": "Faithful Logic Embeddings in HOL -- Deep and Shallow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful Logic Embeddings in HOL -- Deep and Shallow"
                },
                "summary": "Deep and shallow embeddings of non-classical logics in classical higher-order\nlogic have been explored, implemented, and used in various reasoning tools in\nrecent years. This paper presents a method for the simultaneous deployment of\ndeep and shallow embeddings of various degrees in classical higher-order logic.\nThis enables flexible, interactive and automated theorem proving and\ncounterexample finding at meta and object level, as well as automated\nfaithfulness proofs between these logic embeddings. The method is beneficial\nfor logic education, research and application and is illustrated here using a\nsimple propositional modal logic. However, this approach is conceptual in\nnature and not limited to this simple logical context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep and shallow embeddings of non-classical logics in classical higher-order\nlogic have been explored, implemented, and used in various reasoning tools in\nrecent years. This paper presents a method for the simultaneous deployment of\ndeep and shallow embeddings of various degrees in classical higher-order logic.\nThis enables flexible, interactive and automated theorem proving and\ncounterexample finding at meta and object level, as well as automated\nfaithfulness proofs between these logic embeddings. The method is beneficial\nfor logic education, research and application and is illustrated here using a\nsimple propositional modal logic. However, this approach is conceptual in\nnature and not limited to this simple logical context."
                },
                "authors": [
                    {
                        "name": "Christoph Benzmüller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Benzmüller"
                },
                "author": "Christoph Benzmüller",
                "arxiv_comment": "Preprint of paper accepted for CADE 2025; 24 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03Axx, 03Bxx, 03B15, 68T15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4; I.2.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21263v1",
                "updated": "2025-05-27T14:40:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    40,
                    25,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:40:25Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    40,
                    25,
                    1,
                    147,
                    0
                ],
                "title": "JavaSith: A Client-Side Framework for Analyzing Potentially Malicious\n  Extensions in Browsers, VS Code, and NPM Packages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JavaSith: A Client-Side Framework for Analyzing Potentially Malicious\n  Extensions in Browsers, VS Code, and NPM Packages"
                },
                "summary": "Modern software supply chains face an increasing threat from malicious code\nhidden in trusted components such as browser extensions, IDE extensions, and\nopen-source packages. This paper introduces JavaSith, a novel client-side\nframework for analyzing potentially malicious extensions in web browsers,\nVisual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a\nruntime sandbox that emulates browser/Node.js extension APIs (with a ``time\nmachine'' to accelerate time-based triggers) with static analysis and a local\nlarge language model (LLM) to assess risk from code and metadata. We present\nthe design and architecture of JavaSith, including techniques for intercepting\nextension behavior over simulated time and extracting suspicious patterns.\nThrough case studies on real-world attacks (such as a supply-chain compromise\nof a Chrome extension and malicious VSCode extensions installing cryptominers),\nwe demonstrate how JavaSith can catch stealthy malicious behaviors that evade\ntraditional detection. We evaluate the framework's effectiveness and discuss\nits limitations and future enhancements. JavaSith's client-side approach\nempowers end-users/organizations to vet extensions and packages before\ntrustingly integrating them into their environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software supply chains face an increasing threat from malicious code\nhidden in trusted components such as browser extensions, IDE extensions, and\nopen-source packages. This paper introduces JavaSith, a novel client-side\nframework for analyzing potentially malicious extensions in web browsers,\nVisual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a\nruntime sandbox that emulates browser/Node.js extension APIs (with a ``time\nmachine'' to accelerate time-based triggers) with static analysis and a local\nlarge language model (LLM) to assess risk from code and metadata. We present\nthe design and architecture of JavaSith, including techniques for intercepting\nextension behavior over simulated time and extracting suspicious patterns.\nThrough case studies on real-world attacks (such as a supply-chain compromise\nof a Chrome extension and malicious VSCode extensions installing cryptominers),\nwe demonstrate how JavaSith can catch stealthy malicious behaviors that evade\ntraditional detection. We evaluate the framework's effectiveness and discuss\nits limitations and future enhancements. JavaSith's client-side approach\nempowers end-users/organizations to vet extensions and packages before\ntrustingly integrating them into their environments."
                },
                "authors": [
                    {
                        "name": "Avihay Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Avihay Cohen"
                },
                "author": "Avihay Cohen",
                "arxiv_comment": "28 pages , 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11484v2",
                "updated": "2025-05-27T14:35:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    35,
                    29,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-16T17:47:50Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    17,
                    47,
                    50,
                    4,
                    136,
                    0
                ],
                "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning"
                },
                "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT."
                },
                "authors": [
                    {
                        "name": "Yige Xu"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Zhiwei Zeng"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10479v2",
                "updated": "2025-05-27T14:29:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    29,
                    54,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-14T13:15:34Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    15,
                    34,
                    0,
                    288,
                    0
                ],
                "title": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs"
                },
                "summary": "The rapid advancement of large language models has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate the strategic reasoning capabilities of LLMs, game\ntheory, with its concise structure, has become the preferred approach for many\nresearchers. However, current research typically focuses on a limited selection\nof games, resulting in low coverage of game types. Additionally, classic game\nscenarios carry risks of data leakage, and the benchmarks used often lack\nextensibility, rendering them inadequate for evaluating state-of-the-art\nmodels. To address these challenges, we propose TMGBench, characterized by\ncomprehensive game type coverage, diverse scenarios and flexible game\norganization. Specifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games in our\nbenchmark; we also synthetize diverse, higher-quality game scenarios for each\nclassic game, which we refer to as story-based games. Lastly, to provide a\nsustainable evaluation framework adaptable to increasingly powerful LLMs, we\ntreat the aforementioned games as atomic units and organize them into more\ncomplex forms through sequential, parallel, and nested structures. We conducted\na comprehensive evaluation of mainstream LLMs, covering tests on rational\nreasoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in\ncomplex game forms. The results revealed LLMs still have flaws in the accuracy\nand consistency of strategic reasoning processes, and their levels of mastery\nover Theory-of-Mind also vary. Additionally, SOTA models like o3-mini, Qwen3\nand deepseek-reasoner, were also evaluated across the sequential, parallel, and\nnested game structures while the results highlighted the challenges posed by\nTMGBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate the strategic reasoning capabilities of LLMs, game\ntheory, with its concise structure, has become the preferred approach for many\nresearchers. However, current research typically focuses on a limited selection\nof games, resulting in low coverage of game types. Additionally, classic game\nscenarios carry risks of data leakage, and the benchmarks used often lack\nextensibility, rendering them inadequate for evaluating state-of-the-art\nmodels. To address these challenges, we propose TMGBench, characterized by\ncomprehensive game type coverage, diverse scenarios and flexible game\norganization. Specifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games in our\nbenchmark; we also synthetize diverse, higher-quality game scenarios for each\nclassic game, which we refer to as story-based games. Lastly, to provide a\nsustainable evaluation framework adaptable to increasingly powerful LLMs, we\ntreat the aforementioned games as atomic units and organize them into more\ncomplex forms through sequential, parallel, and nested structures. We conducted\na comprehensive evaluation of mainstream LLMs, covering tests on rational\nreasoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in\ncomplex game forms. The results revealed LLMs still have flaws in the accuracy\nand consistency of strategic reasoning processes, and their levels of mastery\nover Theory-of-Mind also vary. Additionally, SOTA models like o3-mini, Qwen3\nand deepseek-reasoner, were also evaluated across the sequential, parallel, and\nnested game structures while the results highlighted the challenges posed by\nTMGBench."
                },
                "authors": [
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yu Guo"
                    },
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21249v1",
                "updated": "2025-05-27T14:26:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    26,
                    59,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:26:59Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    26,
                    59,
                    1,
                    147,
                    0
                ],
                "title": "Data-Driven Cellular Mobility Management via Bayesian Optimization and\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Cellular Mobility Management via Bayesian Optimization and\n  Reinforcement Learning"
                },
                "summary": "Mobility management in cellular networks faces increasing complexity due to\nnetwork densification and heterogeneous user mobility characteristics.\nTraditional handover (HO) mechanisms, which rely on predefined parameters such\nas A3-offset and time-to-trigger (TTT), often fail to optimize mobility\nperformance across varying speeds and deployment conditions. Fixed A3-offset\nand TTT configurations either delay HOs, increasing radio link failures (RLFs),\nor accelerate them, leading to excessive ping-pong effects. To address these\nchallenges, we propose two data-driven mobility management approaches\nleveraging high-dimensional Bayesian optimization (HD-BO) and deep\nreinforcement learning (DRL). HD-BO optimizes HO parameters such as A3-offset\nand TTT, striking a desired trade-off between ping-pongs vs. RLF. DRL provides\na non-parameter-based approach, allowing an agent to select serving cells based\non real-time network conditions. We validate our approach using a real-world\ncellular deployment scenario, and employing Sionna ray tracing for\nsite-specific channel propagation modeling. Results show that both HD-BO and\nDRL outperform 3GPP set-1 (TTT of 480 ms and A3-offset of 3 dB) and set-5 (TTT\nof 40 ms and A3-offset of -1 dB) benchmarks. We augment HD-BO with transfer\nlearning so it can generalize across a range of user speeds. Applying the same\ntransfer-learning strategy to the DRL method reduces its training time by a\nfactor of 2.5 while preserving optimal HO performance, showing that it adapts\nefficiently to the mobility of aerial users such as UAVs. Simulations further\nreveal that HD-BO remains more sample-efficient than DRL, making it more\nsuitable for scenarios with limited training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility management in cellular networks faces increasing complexity due to\nnetwork densification and heterogeneous user mobility characteristics.\nTraditional handover (HO) mechanisms, which rely on predefined parameters such\nas A3-offset and time-to-trigger (TTT), often fail to optimize mobility\nperformance across varying speeds and deployment conditions. Fixed A3-offset\nand TTT configurations either delay HOs, increasing radio link failures (RLFs),\nor accelerate them, leading to excessive ping-pong effects. To address these\nchallenges, we propose two data-driven mobility management approaches\nleveraging high-dimensional Bayesian optimization (HD-BO) and deep\nreinforcement learning (DRL). HD-BO optimizes HO parameters such as A3-offset\nand TTT, striking a desired trade-off between ping-pongs vs. RLF. DRL provides\na non-parameter-based approach, allowing an agent to select serving cells based\non real-time network conditions. We validate our approach using a real-world\ncellular deployment scenario, and employing Sionna ray tracing for\nsite-specific channel propagation modeling. Results show that both HD-BO and\nDRL outperform 3GPP set-1 (TTT of 480 ms and A3-offset of 3 dB) and set-5 (TTT\nof 40 ms and A3-offset of -1 dB) benchmarks. We augment HD-BO with transfer\nlearning so it can generalize across a range of user speeds. Applying the same\ntransfer-learning strategy to the DRL method reduces its training time by a\nfactor of 2.5 while preserving optimal HO performance, showing that it adapts\nefficiently to the mobility of aerial users such as UAVs. Simulations further\nreveal that HD-BO remains more sample-efficient than DRL, making it more\nsuitable for scenarios with limited training data."
                },
                "authors": [
                    {
                        "name": "Mohamed Benzaghta"
                    },
                    {
                        "name": "Sahar Ammar"
                    },
                    {
                        "name": "David López-Pérez"
                    },
                    {
                        "name": "Basem Shihada"
                    },
                    {
                        "name": "Giovanni Geraci"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Geraci"
                },
                "author": "Giovanni Geraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21242v1",
                "updated": "2025-05-27T14:23:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    23,
                    3,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:23:03Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    23,
                    3,
                    1,
                    147,
                    0
                ],
                "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary\n  Adaptation in High OOV Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary\n  Adaptation in High OOV Settings"
                },
                "summary": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark."
                },
                "authors": [
                    {
                        "name": "Gunjan Balde"
                    },
                    {
                        "name": "Soumyadeep Roy"
                    },
                    {
                        "name": "Mainack Mondal"
                    },
                    {
                        "name": "Niloy Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Niloy Ganguly"
                },
                "author": "Niloy Ganguly",
                "arxiv_comment": "16 pages. Accepted for publication in the Findings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21239v1",
                "updated": "2025-05-27T14:19:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    35,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:19:35Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    35,
                    1,
                    147,
                    0
                ],
                "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners"
                },
                "summary": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD"
                },
                "authors": [
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Chentao Song"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Qing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Huang"
                },
                "author": "Qing Huang",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21233v1",
                "updated": "2025-05-27T14:16:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    16,
                    52,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:16:52Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    16,
                    52,
                    1,
                    147,
                    0
                ],
                "title": "CROP: Contextual Region-Oriented Visual Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROP: Contextual Region-Oriented Visual Token Pruning"
                },
                "summary": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Qianrun Wei"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18942v2",
                "updated": "2025-05-27T14:15:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    15,
                    31,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-25T02:28:40Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    2,
                    28,
                    40,
                    6,
                    145,
                    0
                ],
                "title": "Language Models Surface the Unwritten Code of Science and Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Surface the Unwritten Code of Science and Society"
                },
                "summary": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI."
                },
                "authors": [
                    {
                        "name": "Honglin Bao"
                    },
                    {
                        "name": "Siyang Wu"
                    },
                    {
                        "name": "Jiwoong Choi"
                    },
                    {
                        "name": "Yingrong Mao"
                    },
                    {
                        "name": "James A. Evans"
                    }
                ],
                "author_detail": {
                    "name": "James A. Evans"
                },
                "author": "James A. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04183v2",
                "updated": "2025-05-27T14:15:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    15,
                    14,
                    1,
                    147,
                    0
                ],
                "published": "2024-09-06T10:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding"
                },
                "summary": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "ACL 2025 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18389v2",
                "updated": "2025-05-27T14:13:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    13,
                    53,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-23T21:33:16Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    21,
                    33,
                    16,
                    4,
                    143,
                    0
                ],
                "title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN"
                },
                "summary": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types"
                },
                "authors": [
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Reshma Prasad"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Under submission to an IEEE journal, copyright may change without\n  notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21219v1",
                "updated": "2025-05-27T14:06:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    6,
                    51,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:06:51Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    6,
                    51,
                    1,
                    147,
                    0
                ],
                "title": "Addressing Data Quality Decompensation in Federated Learning via Dynamic\n  Client Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Data Quality Decompensation in Federated Learning via Dynamic\n  Client Selection"
                },
                "summary": "In cross-silo Federated Learning (FL), client selection is critical to ensure\nhigh model performance, yet it remains challenging due to data quality\ndecompensation, budget constraints, and incentive compatibility. As training\nprogresses, these factors exacerbate client heterogeneity and degrade global\nperformance. Most existing approaches treat these challenges in isolation,\nmaking jointly optimizing multiple factors difficult. To address this, we\npropose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a\nunified framework integrating dynamic bidding, reputation modeling, and\ncost-aware selection. Clients submit bids based on their perceived data\nquality, and their contributions are evaluated using Shapley values to quantify\ntheir marginal impact on the global model. A reputation system, inspired by\nprospect theory, captures historical performance while penalizing\ninconsistency. The client selection problem is formulated as a 0-1 integer\nprogram that maximizes reputation-weighted utility under budget constraints.\nExperiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that\nSBRO-FL improves accuracy, convergence speed, and robustness, even in\nadversarial and low-bid interference scenarios. Our results highlight the\nimportance of balancing data reliability, incentive compatibility, and cost\nefficiency to enable scalable and trustworthy FL deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cross-silo Federated Learning (FL), client selection is critical to ensure\nhigh model performance, yet it remains challenging due to data quality\ndecompensation, budget constraints, and incentive compatibility. As training\nprogresses, these factors exacerbate client heterogeneity and degrade global\nperformance. Most existing approaches treat these challenges in isolation,\nmaking jointly optimizing multiple factors difficult. To address this, we\npropose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a\nunified framework integrating dynamic bidding, reputation modeling, and\ncost-aware selection. Clients submit bids based on their perceived data\nquality, and their contributions are evaluated using Shapley values to quantify\ntheir marginal impact on the global model. A reputation system, inspired by\nprospect theory, captures historical performance while penalizing\ninconsistency. The client selection problem is formulated as a 0-1 integer\nprogram that maximizes reputation-weighted utility under budget constraints.\nExperiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that\nSBRO-FL improves accuracy, convergence speed, and robustness, even in\nadversarial and low-bid interference scenarios. Our results highlight the\nimportance of balancing data reliability, incentive compatibility, and cost\nefficiency to enable scalable and trustworthy FL deployments."
                },
                "authors": [
                    {
                        "name": "Qinjun Fei"
                    },
                    {
                        "name": "Nuria Rodríguez-Barroso"
                    },
                    {
                        "name": "María Victoria Luzón"
                    },
                    {
                        "name": "Zhongliang Zhang"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21218v1",
                "updated": "2025-05-27T14:06:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    6,
                    15,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:06:15Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    6,
                    15,
                    1,
                    147,
                    0
                ],
                "title": "Pretrained LLMs Learn Multiple Types of Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained LLMs Learn Multiple Types of Uncertainty"
                },
                "summary": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction."
                },
                "authors": [
                    {
                        "name": "Roi Cohen"
                    },
                    {
                        "name": "Omri Fahn"
                    },
                    {
                        "name": "Gerard de Melo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard de Melo"
                },
                "author": "Gerard de Melo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19075v2",
                "updated": "2025-05-27T13:53:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    53,
                    36,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-25T10:19:10Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    19,
                    10,
                    6,
                    145,
                    0
                ],
                "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"
                },
                "authors": [
                    {
                        "name": "Jaemin Kim"
                    },
                    {
                        "name": "Hangeol Chang"
                    },
                    {
                        "name": "Hyunmin Hwang"
                    },
                    {
                        "name": "Choonghan Kim"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "22 pages, typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21200v1",
                "updated": "2025-05-27T13:47:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    47,
                    18,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:47:18Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    47,
                    18,
                    1,
                    147,
                    0
                ],
                "title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for\n  Efficient Inference in Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for\n  Efficient Inference in Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Jialin Zheng"
                    },
                    {
                        "name": "Bizhe Bai"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Jia Hao"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21191v1",
                "updated": "2025-05-27T13:40:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    40,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:40:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    40,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities"
                },
                "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity."
                },
                "authors": [
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Yubo Gao"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Jungang Li"
                    },
                    {
                        "name": "Zhaorui Hou"
                    },
                    {
                        "name": "Sicheng Tao"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Song Dai"
                    },
                    {
                        "name": "Yonghua Hei"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21189v1",
                "updated": "2025-05-27T13:39:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    39,
                    24,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:39:24Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    39,
                    24,
                    1,
                    147,
                    0
                ],
                "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Latent Capacity of LLMs for One-Step Text Generation"
                },
                "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space."
                },
                "authors": [
                    {
                        "name": "Gleb Mezentsev"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21184v1",
                "updated": "2025-05-27T13:33:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    33,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:33:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    33,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing"
                },
                "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhifei Zheng"
                    },
                    {
                        "name": "Ziji Hao"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21178v1",
                "updated": "2025-05-27T13:29:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    29,
                    51,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:29:51Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    29,
                    51,
                    1,
                    147,
                    0
                ],
                "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning"
                },
                "summary": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Mao Zheng"
                },
                "author": "Mao Zheng",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18893v2",
                "updated": "2025-05-27T13:26:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    26,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-24T22:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    22,
                    35,
                    32,
                    5,
                    144,
                    0
                ],
                "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand\n  AI's Real World Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand\n  AI's Real World Effects"
                },
                "summary": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem."
                },
                "authors": [
                    {
                        "name": "Reva Schwartz"
                    },
                    {
                        "name": "Rumman Chowdhury"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Heather Frase"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Tom David"
                    },
                    {
                        "name": "Gabriella Waters"
                    },
                    {
                        "name": "Afaf Taik"
                    },
                    {
                        "name": "Morgan Briggs"
                    },
                    {
                        "name": "Patrick Hall"
                    },
                    {
                        "name": "Shomik Jain"
                    },
                    {
                        "name": "Kyra Yee"
                    },
                    {
                        "name": "Spencer Thomas"
                    },
                    {
                        "name": "Sundeep Bhandari"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Matthew Holmes"
                    },
                    {
                        "name": "Theodora Skeadas"
                    }
                ],
                "author_detail": {
                    "name": "Theodora Skeadas"
                },
                "author": "Theodora Skeadas",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21172v1",
                "updated": "2025-05-27T13:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    26,
                    2,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:26:02Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    26,
                    2,
                    1,
                    147,
                    0
                ],
                "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and\n  Word Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and\n  Word Alignment"
                },
                "summary": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Wenjie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Yang"
                },
                "author": "Wenjie Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21171v1",
                "updated": "2025-05-27T13:24:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    24,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T13:24:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    24,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs"
                },
                "summary": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning."
                },
                "authors": [
                    {
                        "name": "Rochelle Choenni"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14613v2",
                "updated": "2025-05-27T13:13:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    13,
                    38,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T14:52:23Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    23,
                    3,
                    51,
                    0
                ],
                "title": "Behavioral Analysis of Information Salience in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral Analysis of Information Salience in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience."
                },
                "authors": [
                    {
                        "name": "Jan Trienes"
                    },
                    {
                        "name": "Jörg Schlötterer"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_comment": "Accepted at ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21148v1",
                "updated": "2025-05-27T12:58:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    58,
                    21,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T12:58:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    58,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Assessment of L2 Oral Proficiency using Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment of L2 Oral Proficiency using Speech Large Language Models"
                },
                "summary": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training."
                },
                "authors": [
                    {
                        "name": "Rao Ma"
                    },
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Siyuan Tang"
                    },
                    {
                        "name": "Stefano Bannò"
                    },
                    {
                        "name": "Kate M. Knill"
                    },
                    {
                        "name": "Mark J. F. Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. F. Gales"
                },
                "author": "Mark J. F. Gales",
                "arxiv_comment": "submitted to Interspeech",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20101v2",
                "updated": "2025-05-27T12:54:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    54,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T15:08:51Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    8,
                    51,
                    0,
                    146,
                    0
                ],
                "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT. In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype. Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT. In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype. Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications."
                },
                "authors": [
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12187v2",
                "updated": "2025-05-27T12:53:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    53,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-15T07:28:40Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    7,
                    28,
                    40,
                    5,
                    46,
                    0
                ],
                "title": "Hallucinations are inevitable but can be made statistically negligible.\n  The \"innate\" inevitability of hallucinations cannot explain practical LLM\n  issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are inevitable but can be made statistically negligible.\n  The \"innate\" inevitability of hallucinations cannot explain practical LLM\n  issues"
                },
                "summary": "Hallucinations, a phenomenon where a language model (LM) generates nonfactual\ncontent, pose a significant challenge to the practical deployment of LMs. While\nmany empirical methods have been proposed to mitigate hallucinations, recent\nstudies established a computability-theoretic result showing that any LM will\ninevitably generate hallucinations on an infinite set of inputs, regardless of\nthe quality and quantity of training datasets and the choice of the language\nmodel architecture and training and inference algorithms. Although the\ncomputability-theoretic result may seem pessimistic, its significance in\npractical viewpoints has remained unclear. This paper claims that those\n\"innate\" inevitability results from computability theory and diagonal argument,\nin principle, cannot explain practical issues of LLMs. We demonstrate this\nclaim by presenting a positive theoretical result from a probabilistic\nperspective. Specifically, we prove that hallucinations can be made\nstatistically negligible, provided that the quality and quantity of the\ntraining data are sufficient. Interestingly, our positive result coexists with\nthe computability-theoretic result, implying that while hallucinations on an\ninfinite set of inputs cannot be entirely eliminated, their probability can\nalways be reduced by improving algorithms and training data. By evaluating the\ntwo seemingly contradictory results through the lens of information theory, we\nargue that our probability-theoretic positive result better reflects practical\nconsiderations than the computability-theoretic negative result.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations, a phenomenon where a language model (LM) generates nonfactual\ncontent, pose a significant challenge to the practical deployment of LMs. While\nmany empirical methods have been proposed to mitigate hallucinations, recent\nstudies established a computability-theoretic result showing that any LM will\ninevitably generate hallucinations on an infinite set of inputs, regardless of\nthe quality and quantity of training datasets and the choice of the language\nmodel architecture and training and inference algorithms. Although the\ncomputability-theoretic result may seem pessimistic, its significance in\npractical viewpoints has remained unclear. This paper claims that those\n\"innate\" inevitability results from computability theory and diagonal argument,\nin principle, cannot explain practical issues of LLMs. We demonstrate this\nclaim by presenting a positive theoretical result from a probabilistic\nperspective. Specifically, we prove that hallucinations can be made\nstatistically negligible, provided that the quality and quantity of the\ntraining data are sufficient. Interestingly, our positive result coexists with\nthe computability-theoretic result, implying that while hallucinations on an\ninfinite set of inputs cannot be entirely eliminated, their probability can\nalways be reduced by improving algorithms and training data. By evaluating the\ntwo seemingly contradictory results through the lens of information theory, we\nargue that our probability-theoretic positive result better reflects practical\nconsiderations than the computability-theoretic negative result."
                },
                "authors": [
                    {
                        "name": "Atsushi Suzuki"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyuan Wang"
                },
                "author": "Zhongyuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21138v1",
                "updated": "2025-05-27T12:50:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    50,
                    55,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T12:50:55Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    50,
                    55,
                    1,
                    147,
                    0
                ],
                "title": "Leveraging LLM and Self-Supervised Training Models for Speech\n  Recognition in Chinese Dialects: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM and Self-Supervised Training Models for Speech\n  Recognition in Chinese Dialects: A Comparative Analysis"
                },
                "summary": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch"
                },
                "authors": [
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Wang Qing"
                    },
                    {
                        "name": "Lv Hang"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Li Jie"
                    },
                    {
                        "name": "Zhennan Lin"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Xie Lei"
                    }
                ],
                "author_detail": {
                    "name": "Xie Lei"
                },
                "author": "Xie Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21116v1",
                "updated": "2025-05-27T12:36:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    36,
                    14,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T12:36:14Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    36,
                    14,
                    1,
                    147,
                    0
                ],
                "title": "Creativity in LLM-based Multi-Agent Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity in LLM-based Multi-Agent Systems: A Survey"
                },
                "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Kang-Chieh Chen"
                    },
                    {
                        "name": "Zhe-Yan Li"
                    },
                    {
                        "name": "Tzu-Heng Wu"
                    },
                    {
                        "name": "Tzu-Hsuan Wu"
                    },
                    {
                        "name": "Kuan-Yu Chen"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21115v1",
                "updated": "2025-05-27T12:35:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    35,
                    13,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T12:35:13Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    35,
                    13,
                    1,
                    147,
                    0
                ],
                "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA"
                },
                "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior."
                },
                "authors": [
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Viktor Moskvoretskii"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Moskvoretskii"
                },
                "author": "Viktor Moskvoretskii",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]