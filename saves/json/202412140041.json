[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v1",
                "updated": "2024-12-11T18:03:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08652v1",
                "updated": "2024-11-26T17:52:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    52,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:52:21Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    52,
                    21,
                    1,
                    331,
                    0
                ],
                "title": "Twenty-Year Review of Outdoor Air Quality in Utah, USA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twenty-Year Review of Outdoor Air Quality in Utah, USA"
                },
                "summary": "Air quality is a prevalent concern due to its imposing health risks. The\nstate of Utah, USA, has, at times over the last 20 years, experienced some of\nthe worst air quality in the nation. The propensity for Utah to experience\nelevated concentrations of particulate matter ($\\mathrm{PM_{2.5}}$) and ozone\n($\\mathrm{O_3}$) can, in part, be attributed to its unique geography, which\nfeatures dry, mountainous terrain. Valleys in Utah create ideal environments\nfor extended cold-pool events. In this review, we summarize air quality\nresearch conducted in Utah over the past 20 years (2002-2022) by dividing the\nstate into six regions: Utah Valley, Summit County, Southern Utah (regions\nsouth of Utah Valley), Cache Valley, Uinta Basin, and Salt Lake Valley. We\nreview the published literature chronologically and provide a summary for each\nregion, identifying areas where additional research is warranted. We found that\nresearch efforts are heavily weighted toward the Uinta Basin and Salt Lake\nValley, with the remaining regions collectively accounting for only 20% of\nstudies. We identified the need for more source apportionment studies,\nspeciated volatile organic compound (VOC) analyses, and ozone isopleths. Where\nozone isopleths cannot be created, measurements of glyoxal ($\\mathrm{CHOCHO}$)\nand formaldehyde ($\\mathrm{HCHO}$) concentrations could serve as cost-effective\nsurrogates to inform ozone mitigation policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air quality is a prevalent concern due to its imposing health risks. The\nstate of Utah, USA, has, at times over the last 20 years, experienced some of\nthe worst air quality in the nation. The propensity for Utah to experience\nelevated concentrations of particulate matter ($\\mathrm{PM_{2.5}}$) and ozone\n($\\mathrm{O_3}$) can, in part, be attributed to its unique geography, which\nfeatures dry, mountainous terrain. Valleys in Utah create ideal environments\nfor extended cold-pool events. In this review, we summarize air quality\nresearch conducted in Utah over the past 20 years (2002-2022) by dividing the\nstate into six regions: Utah Valley, Summit County, Southern Utah (regions\nsouth of Utah Valley), Cache Valley, Uinta Basin, and Salt Lake Valley. We\nreview the published literature chronologically and provide a summary for each\nregion, identifying areas where additional research is warranted. We found that\nresearch efforts are heavily weighted toward the Uinta Basin and Salt Lake\nValley, with the remaining regions collectively accounting for only 20% of\nstudies. We identified the need for more source apportionment studies,\nspeciated volatile organic compound (VOC) analyses, and ozone isopleths. Where\nozone isopleths cannot be created, measurements of glyoxal ($\\mathrm{CHOCHO}$)\nand formaldehyde ($\\mathrm{HCHO}$) concentrations could serve as cost-effective\nsurrogates to inform ozone mitigation policies."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14101496",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14101496",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1496",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.09626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09626v1",
                "updated": "2024-12-12T18:59:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    59,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:59Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    59,
                    3,
                    347,
                    0
                ],
                "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free\n  Scale Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free\n  Scale Fusion"
                },
                "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. To tackle this challenge, we propose FreeScale, a\ntuning-free inference paradigm to enable higher-resolution visual generation\nvia scale fusion. Specifically, FreeScale processes information from different\nreceptive scales and then fuses it by extracting desired frequency components.\nExtensive experiments validate the superiority of our paradigm in extending the\ncapabilities of higher-resolution visual generation for both image and video\nmodels. Notably, compared with the previous best-performing method, FreeScale\nunlocks the generation of 8k-resolution images for the first time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. To tackle this challenge, we propose FreeScale, a\ntuning-free inference paradigm to enable higher-resolution visual generation\nvia scale fusion. Specifically, FreeScale processes information from different\nreceptive scales and then fuses it by extracting desired frequency components.\nExtensive experiments validate the superiority of our paradigm in extending the\ncapabilities of higher-resolution visual generation for both image and video\nmodels. Notably, compared with the previous best-performing method, FreeScale\nunlocks the generation of 8k-resolution images for the first time."
                },
                "authors": [
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Project Page: http://haonanqiu.com/projects/FreeScale.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09618v1",
                "updated": "2024-12-12T18:59:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:48Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    48,
                    3,
                    347,
                    0
                ],
                "title": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via\n  Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via\n  Multimodal LLM"
                },
                "summary": "Significant achievements in personalization of diffusion models have been\nwitnessed. Conventional tuning-free methods mostly encode multiple reference\nimages by averaging their image embeddings as the injection condition, but such\nan image-independent operation cannot perform interaction among images to\ncapture consistent visual elements within multiple references. Although the\ntuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent\nelements within multiple images through the training process, it necessitates\nspecific finetuning for each distinct image group. This paper introduces\nEasyRef, a novel plug-and-play adaptation method that enables diffusion models\nto be conditioned on multiple reference images and the text prompt. To\neffectively exploit consistent visual elements within multiple images, we\nleverage the multi-image comprehension and instruction-following capabilities\nof the multimodal large language model (MLLM), prompting it to capture\nconsistent visual elements based on the instruction. Besides, injecting the\nMLLM's representations into the diffusion process through adapters can easily\ngeneralize to unseen domains, mining the consistent visual elements within\nunseen data. To mitigate computational costs and enhance fine-grained detail\npreservation, we introduce an efficient reference aggregation strategy and a\nprogressive training scheme. Finally, we introduce MRBench, a new\nmulti-reference image generation benchmark. Experimental results demonstrate\nEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based\nmethods like LoRA, achieving superior aesthetic quality and robust zero-shot\ngeneralization across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant achievements in personalization of diffusion models have been\nwitnessed. Conventional tuning-free methods mostly encode multiple reference\nimages by averaging their image embeddings as the injection condition, but such\nan image-independent operation cannot perform interaction among images to\ncapture consistent visual elements within multiple references. Although the\ntuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent\nelements within multiple images through the training process, it necessitates\nspecific finetuning for each distinct image group. This paper introduces\nEasyRef, a novel plug-and-play adaptation method that enables diffusion models\nto be conditioned on multiple reference images and the text prompt. To\neffectively exploit consistent visual elements within multiple images, we\nleverage the multi-image comprehension and instruction-following capabilities\nof the multimodal large language model (MLLM), prompting it to capture\nconsistent visual elements based on the instruction. Besides, injecting the\nMLLM's representations into the diffusion process through adapters can easily\ngeneralize to unseen domains, mining the consistent visual elements within\nunseen data. To mitigate computational costs and enhance fine-grained detail\npreservation, we introduce an efficient reference aggregation strategy and a\nprogressive training scheme. Finally, we introduce MRBench, a new\nmulti-reference image generation benchmark. Experimental results demonstrate\nEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based\nmethods like LoRA, achieving superior aesthetic quality and robust zero-shot\ngeneralization across diverse domains."
                },
                "authors": [
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09604v1",
                "updated": "2024-12-12T18:59:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with\n  Vision Experts and Token Folding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with\n  Vision Experts and Token Folding"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Xiaogang Wang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Jifeng Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Dai"
                },
                "author": "Jifeng Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09601v1",
                "updated": "2024-12-12T18:59:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "TimeRefine: Temporal Grounding with Time Refining Video LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeRefine: Temporal Grounding with Time Refining Video LLM"
                },
                "summary": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released."
                },
                "authors": [
                    {
                        "name": "Xizi Wang"
                    },
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "David Crandall"
                    }
                ],
                "author_detail": {
                    "name": "David Crandall"
                },
                "author": "David Crandall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09585v1",
                "updated": "2024-12-12T18:55:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    55,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:55:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    55,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\n  Embedding Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\n  Embedding Distillation"
                },
                "summary": "The standard practice for developing contemporary MLLMs is to feed features\nfrom vision encoder(s) into the LLM and train with natural language\nsupervision. In this work, we posit an overlooked opportunity to optimize the\nintermediate LLM representations through a vision perspective (objective),\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\ndistilling knowledge into the LLM's hidden representations from a set of target\nvisual representations. Firstly, we formulate the objective during the\npretraining stage in MLLMs as a coupled optimization of predictive visual\nembedding and next text-token prediction. Secondly, we investigate MLLMs\ntrained solely with natural language supervision and identify a positive\ncorrelation between the quality of visual representations within these models\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\nobserve improved representation quality owing to the embedding optimization.\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\nmulti-encoder baselines, proving our approach's superiority over explicitly\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\nperformance by an average margin of up to 2.5% on various benchmarks, with a\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard practice for developing contemporary MLLMs is to feed features\nfrom vision encoder(s) into the LLM and train with natural language\nsupervision. In this work, we posit an overlooked opportunity to optimize the\nintermediate LLM representations through a vision perspective (objective),\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\ndistilling knowledge into the LLM's hidden representations from a set of target\nvisual representations. Firstly, we formulate the objective during the\npretraining stage in MLLMs as a coupled optimization of predictive visual\nembedding and next text-token prediction. Secondly, we investigate MLLMs\ntrained solely with natural language supervision and identify a positive\ncorrelation between the quality of visual representations within these models\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\nobserve improved representation quality owing to the embedding optimization.\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\nmulti-encoder baselines, proving our approach's superiority over explicitly\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\nperformance by an average margin of up to 2.5% on various benchmarks, with a\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM ."
                },
                "authors": [
                    {
                        "name": "Jitesh Jain"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "arxiv_comment": "Project Page: https://praeclarumjj3.github.io/ola_vlm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09582v1",
                "updated": "2024-12-12T18:54:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    54,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:54:48Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    54,
                    48,
                    3,
                    347,
                    0
                ],
                "title": "Neptune: The Long Orbit to Benchmarking Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neptune: The Long Orbit to Benchmarking Long Video Understanding"
                },
                "summary": "This paper describes a semi-automatic pipeline to generate challenging\nquestion-answer-decoy sets for understanding long videos. Many existing video\ndatasets and models are focused on short clips (10s-30s). While some long video\ndatasets do exist, they can often be solved by powerful image models applied\nper frame (and often to very few frames) in a video, and are usually manually\nannotated at high cost. In order to mitigate both these problems, we propose a\nscalable dataset creation pipeline which leverages large models (VLMs and\nLLMs), to automatically generate dense, time-aligned video captions, as well as\ntough question answer decoy sets for video segments (up to 15 minutes in\nlength). Our dataset Neptune covers a broad range of long video reasoning\nabilities and consists of a subset that emphasizes multimodal reasoning. Since\nexisting metrics for open-ended question answering are either rule-based or may\nrely on proprietary models, we provide a new open source model-based metric GEM\nto score open-ended responses on Neptune. Benchmark evaluations reveal that\nmost current open-source long video models perform poorly on Neptune,\nparticularly on questions testing temporal ordering, counting and state\nchanges. Through Neptune, we aim to spur the development of more advanced\nmodels capable of understanding long videos. The dataset is available at\nhttps://github.com/google-deepmind/neptune",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a semi-automatic pipeline to generate challenging\nquestion-answer-decoy sets for understanding long videos. Many existing video\ndatasets and models are focused on short clips (10s-30s). While some long video\ndatasets do exist, they can often be solved by powerful image models applied\nper frame (and often to very few frames) in a video, and are usually manually\nannotated at high cost. In order to mitigate both these problems, we propose a\nscalable dataset creation pipeline which leverages large models (VLMs and\nLLMs), to automatically generate dense, time-aligned video captions, as well as\ntough question answer decoy sets for video segments (up to 15 minutes in\nlength). Our dataset Neptune covers a broad range of long video reasoning\nabilities and consists of a subset that emphasizes multimodal reasoning. Since\nexisting metrics for open-ended question answering are either rule-based or may\nrely on proprietary models, we provide a new open source model-based metric GEM\nto score open-ended responses on Neptune. Benchmark evaluations reveal that\nmost current open-source long video models perform poorly on Neptune,\nparticularly on questions testing temporal ordering, counting and state\nchanges. Through Neptune, we aim to spur the development of more advanced\nmodels capable of understanding long videos. The dataset is available at\nhttps://github.com/google-deepmind/neptune"
                },
                "authors": [
                    {
                        "name": "Arsha Nagrani"
                    },
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Ramin Mehran"
                    },
                    {
                        "name": "Rachel Hornung"
                    },
                    {
                        "name": "Nitesh Bharadwaj Gundavarapu"
                    },
                    {
                        "name": "Nilpa Jha"
                    },
                    {
                        "name": "Austin Myers"
                    },
                    {
                        "name": "Xingyi Zhou"
                    },
                    {
                        "name": "Boqing Gong"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Mikhail Sirotenko"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Tobias Weyand"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weyand"
                },
                "author": "Tobias Weyand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09572v1",
                "updated": "2024-12-12T18:52:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    52,
                    40,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:52:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    52,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through\n  Diverse Perspectives and Multi-Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through\n  Diverse Perspectives and Multi-Agent Interaction"
                },
                "summary": "Quantifying the uncertainty in the factual parametric knowledge of Large\nLanguage Models (LLMs), especially in a black-box setting, poses a significant\nchallenge. Existing methods, which gauge a model's uncertainty through\nevaluating self-consistency in responses to the original query, do not always\ncapture true uncertainty. Models might respond consistently to the origin query\nwith a wrong answer, yet respond correctly to varied questions from different\nperspectives about the same query, and vice versa. In this paper, we propose a\nnovel method, DiverseAgentEntropy, for evaluating a model's uncertainty using\nmulti-agent interaction under the assumption that if a model is certain, it\nshould consistently recall the answer to the original query across a diverse\ncollection of questions about the same original query. We further implement an\nabstention policy to withhold responses when uncertainty is high. Our method\noffers a more accurate prediction of the model's reliability and further\ndetects hallucinations, outperforming other self-consistency-based methods.\nAdditionally, it demonstrates that existing models often fail to consistently\nretrieve the correct answer to the same query under diverse varied questions\neven when knowing the correct answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the uncertainty in the factual parametric knowledge of Large\nLanguage Models (LLMs), especially in a black-box setting, poses a significant\nchallenge. Existing methods, which gauge a model's uncertainty through\nevaluating self-consistency in responses to the original query, do not always\ncapture true uncertainty. Models might respond consistently to the origin query\nwith a wrong answer, yet respond correctly to varied questions from different\nperspectives about the same query, and vice versa. In this paper, we propose a\nnovel method, DiverseAgentEntropy, for evaluating a model's uncertainty using\nmulti-agent interaction under the assumption that if a model is certain, it\nshould consistently recall the answer to the original query across a diverse\ncollection of questions about the same original query. We further implement an\nabstention policy to withhold responses when uncertainty is high. Our method\noffers a more accurate prediction of the model's reliability and further\ndetects hallucinations, outperforming other self-consistency-based methods.\nAdditionally, it demonstrates that existing models often fail to consistently\nretrieve the correct answer to the same query under diverse varied questions\neven when knowing the correct answer."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Phu Mon Htut"
                    },
                    {
                        "name": "Zheng Qi"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Manuel Mager"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Kishaloy Halder"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09569v1",
                "updated": "2024-12-12T18:51:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    51,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:51:13Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    51,
                    13,
                    3,
                    347,
                    0
                ],
                "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuStRank: Benchmarking LLM Judges for System Ranking"
                },
                "summary": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias."
                },
                "authors": [
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Odellia Boni"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Lilach Eden"
                    },
                    {
                        "name": "Asaf Yehudai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Yehudai"
                },
                "author": "Asaf Yehudai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09565v1",
                "updated": "2024-12-12T18:49:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    53,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:49:53Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    53,
                    3,
                    347,
                    0
                ],
                "title": "Obfuscated Activations Bypass LLM Latent-Space Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obfuscated Activations Bypass LLM Latent-Space Defenses"
                },
                "summary": "Recent latent-space monitoring techniques have shown promise as defenses\nagainst LLM attacks. These defenses act as scanners that seek to detect harmful\nactivations before they lead to undesirable actions. This prompts the question:\nCan models execute harmful behavior via inconspicuous latent states? Here, we\nstudy such obfuscated activations. We show that state-of-the-art latent-space\ndefenses -- including sparse autoencoders, representation probing, and latent\nOOD detection -- are all vulnerable to obfuscated activations. For example,\nagainst probes trained to classify harmfulness, our attacks can often reduce\nrecall from 100% to 0% while retaining a 90% jailbreaking rate. However,\nobfuscation has limits: we find that on a complex task (writing SQL code),\nobfuscation reduces model performance. Together, our results demonstrate that\nneural activations are highly malleable: we can reshape activation patterns in\na variety of ways, often while preserving a network's behavior. This poses a\nfundamental challenge to latent-space defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent latent-space monitoring techniques have shown promise as defenses\nagainst LLM attacks. These defenses act as scanners that seek to detect harmful\nactivations before they lead to undesirable actions. This prompts the question:\nCan models execute harmful behavior via inconspicuous latent states? Here, we\nstudy such obfuscated activations. We show that state-of-the-art latent-space\ndefenses -- including sparse autoencoders, representation probing, and latent\nOOD detection -- are all vulnerable to obfuscated activations. For example,\nagainst probes trained to classify harmfulness, our attacks can often reduce\nrecall from 100% to 0% while retaining a 90% jailbreaking rate. However,\nobfuscation has limits: we find that on a complex task (writing SQL code),\nobfuscation reduces model performance. Together, our results demonstrate that\nneural activations are highly malleable: we can reshape activation patterns in\na variety of ways, often while preserving a network's behavior. This poses a\nfundamental challenge to latent-space defenses."
                },
                "authors": [
                    {
                        "name": "Luke Bailey"
                    },
                    {
                        "name": "Alex Serrano"
                    },
                    {
                        "name": "Abhay Sheshadri"
                    },
                    {
                        "name": "Mikhail Seleznyov"
                    },
                    {
                        "name": "Jordan Taylor"
                    },
                    {
                        "name": "Erik Jenner"
                    },
                    {
                        "name": "Jacob Hilton"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Carlos Guestrin"
                    },
                    {
                        "name": "Scott Emmons"
                    }
                ],
                "author_detail": {
                    "name": "Scott Emmons"
                },
                "author": "Scott Emmons",
                "arxiv_comment": "Project page: https://obfuscated-activations.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09564v1",
                "updated": "2024-12-12T18:49:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:49:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "Improving the Reliability of Cable Broadband Networks via Proactive\n  Network Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Reliability of Cable Broadband Networks via Proactive\n  Network Maintenance"
                },
                "summary": "Cable broadband networks are one of the few \"last-mile\" broadband\ntechnologies widely available in the U.S. Unfortunately, they have poor\nreliability after decades of deployment. The cable industry proposed a\nframework called Proactive Network Maintenance (PNM) to diagnose the cable\nnetworks. However, there is little public knowledge or systematic study on how\nto use these data to detect and localize cable network problems. Existing tools\nin the public domain have prohibitive high false-positive rates. In this paper,\nwe propose CableMon, the first public-domain system that applies machine\nlearning techniques to PNM data to improve the reliability of cable broadband\nnetworks. CableMon tackles two key challenges faced by cable ISPs: accurately\ndetecting failures, and distinguishing whether a failure occurs within a\nnetwork or at a subscriber's premise. CableMon uses statistical models to\ngenerate features from time series data and uses customer trouble tickets as\nhints to infer abnormal/failure thresholds for these generated features.\nFurther, CableMon employs an unsupervised learning model to group cable devices\nsharing similar anomalous patterns and effectively identify impairments that\noccur inside a cable network and impairments occur at a subscriber's premise,\nas these two different faults require different types of technical personnel to\nrepair them. We use eight months of PNM data and customer trouble tickets from\nan ISP and experimental deployment to evaluate CableMon's performance. Our\nevaluation results show that CableMon can effectively detect and distinguish\nfailures from PNM data and outperforms existing public-domain tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cable broadband networks are one of the few \"last-mile\" broadband\ntechnologies widely available in the U.S. Unfortunately, they have poor\nreliability after decades of deployment. The cable industry proposed a\nframework called Proactive Network Maintenance (PNM) to diagnose the cable\nnetworks. However, there is little public knowledge or systematic study on how\nto use these data to detect and localize cable network problems. Existing tools\nin the public domain have prohibitive high false-positive rates. In this paper,\nwe propose CableMon, the first public-domain system that applies machine\nlearning techniques to PNM data to improve the reliability of cable broadband\nnetworks. CableMon tackles two key challenges faced by cable ISPs: accurately\ndetecting failures, and distinguishing whether a failure occurs within a\nnetwork or at a subscriber's premise. CableMon uses statistical models to\ngenerate features from time series data and uses customer trouble tickets as\nhints to infer abnormal/failure thresholds for these generated features.\nFurther, CableMon employs an unsupervised learning model to group cable devices\nsharing similar anomalous patterns and effectively identify impairments that\noccur inside a cable network and impairments occur at a subscriber's premise,\nas these two different faults require different types of technical personnel to\nrepair them. We use eight months of PNM data and customer trouble tickets from\nan ISP and experimental deployment to evaluate CableMon's performance. Our\nevaluation results show that CableMon can effectively detect and distinguish\nfailures from PNM data and outperforms existing public-domain tools."
                },
                "authors": [
                    {
                        "name": "Jiyao Hu"
                    },
                    {
                        "name": "Zhenyu Zhou"
                    },
                    {
                        "name": "Xiaowei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Yang"
                },
                "author": "Xiaowei Yang",
                "arxiv_comment": "15 pages including reference. Submitted to IEEE/ACM Transactions on\n  Networking. Partly published in NSDI'20, this is the extended version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09563v1",
                "updated": "2024-12-12T18:48:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    48,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:48:51Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    48,
                    51,
                    3,
                    347,
                    0
                ],
                "title": "Does Representation Matter? Exploring Intermediate Layers in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Representation Matter? Exploring Intermediate Layers in Large\n  Language Models"
                },
                "summary": "Understanding what defines a good representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical\napplications. In this paper, we investigate the quality of intermediate\nrepresentations in various LLM architectures, including Transformers and State\nSpace Models (SSMs). We find that intermediate layers often yield more\ninformative representations for downstream tasks than the final layers. To\nmeasure the representation quality, we adapt and apply a suite of metrics -\nsuch as prompt entropy, curvature, and augmentation-invariance - originally\nproposed in other contexts. Our empirical study reveals significant\narchitectural differences, how representations evolve throughout training, and\nhow factors like input randomness and prompt length affect each layer. Notably,\nwe observe a bimodal pattern in the entropy of some intermediate layers and\nconsider potential explanations tied to training data. Overall, our results\nilluminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what defines a good representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical\napplications. In this paper, we investigate the quality of intermediate\nrepresentations in various LLM architectures, including Transformers and State\nSpace Models (SSMs). We find that intermediate layers often yield more\ninformative representations for downstream tasks than the final layers. To\nmeasure the representation quality, we adapt and apply a suite of metrics -\nsuch as prompt entropy, curvature, and augmentation-invariance - originally\nproposed in other contexts. Our empirical study reveals significant\narchitectural differences, how representations evolve throughout training, and\nhow factors like input randomness and prompt length affect each layer. Notably,\nwe observe a bimodal pattern in the entropy of some intermediate layers and\nconsider potential explanations tied to training data. Overall, our results\nilluminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training."
                },
                "authors": [
                    {
                        "name": "Oscar Skean"
                    },
                    {
                        "name": "Md Rifat Arefin"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "arxiv_comment": "Accepted to 2024 NeurIPs Workshop on Machine Learning and Compression",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09560v1",
                "updated": "2024-12-12T18:46:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    46,
                    38,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:46:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    46,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "Foundational Large Language Models for Materials Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models for Materials Research"
                },
                "summary": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems."
                },
                "authors": [
                    {
                        "name": "Vaibhav Mishra"
                    },
                    {
                        "name": "Somaditya Singh"
                    },
                    {
                        "name": "Dhruv Ahlawat"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Vaibhav Bihani"
                    },
                    {
                        "name": "Hargun Singh Grover"
                    },
                    {
                        "name": "Biswajit Mishra"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Mausam"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "author": "N. M. Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20535v2",
                "updated": "2024-12-12T18:45:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    45,
                    33,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-30T23:20:25Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    23,
                    20,
                    25,
                    3,
                    151,
                    0
                ],
                "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning"
                },
                "summary": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent."
                },
                "authors": [
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Zhiyu Zoey Chen"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Linda Ruth Petzold"
                    }
                ],
                "author_detail": {
                    "name": "Linda Ruth Petzold"
                },
                "author": "Linda Ruth Petzold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08525v2",
                "updated": "2024-12-12T18:40:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    40,
                    37,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-14T12:02:28Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    12,
                    2,
                    28,
                    1,
                    135,
                    0
                ],
                "title": "Doubly-robust inference and optimality in structure-agnostic models with\n  smoothness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly-robust inference and optimality in structure-agnostic models with\n  smoothness"
                },
                "summary": "We study the problem of constructing an estimator of the average treatment\neffect (ATE) with observational data. The celebrated doubly-robust,\naugmented-IPW (AIPW) estimator generally requires consistent estimation of both\nnuisance functions for standard root-n inference, and moreover that the product\nof the errors of the nuisances should shrink at a rate faster than $n^{-1/2}$.\nA recent strand of research has aimed to understand the extent to which the\nAIPW estimator can be improved upon (in a minimax sense). Under structural\nassumptions on the nuisance functions, the AIPW estimator is typically not\nminimax-optimal, and improvements can be made using higher-order influence\nfunctions (Robins et al, 2017). Conversely, without any assumptions on the\nnuisances beyond the mean-square-error rates at which they can be estimated,\nthe rate achieved by the AIPW estimator is already optimal (Balakrishnan et al,\n2023; Jin and Syrgkanis, 2024).\n  We make three main contributions. First, we propose a new hybrid class of\ndistributions that combine structural agnosticism regarding the nuisance\nfunction space with additional smoothness constraints. Second, we calculate\nminimax lower bounds for estimating the ATE in the new class, as well as in the\npure structure-agnostic one. Third, we propose a new estimator of the ATE that\nenjoys doubly-robust asymptotic linearity; it can yield asymptotically valid\nWald-type confidence intervals even when the propensity score or the outcome\nmodel is inconsistently estimated, or estimated at a slow rate. Under certain\nconditions, we show that its rate of convergence in the new class can be much\nfaster than that achieved by the AIPW estimator and, in particular, matches the\nminimax lower bound rate, thereby establishing its optimality. Finally, we\ncomplement our theoretical findings with simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of constructing an estimator of the average treatment\neffect (ATE) with observational data. The celebrated doubly-robust,\naugmented-IPW (AIPW) estimator generally requires consistent estimation of both\nnuisance functions for standard root-n inference, and moreover that the product\nof the errors of the nuisances should shrink at a rate faster than $n^{-1/2}$.\nA recent strand of research has aimed to understand the extent to which the\nAIPW estimator can be improved upon (in a minimax sense). Under structural\nassumptions on the nuisance functions, the AIPW estimator is typically not\nminimax-optimal, and improvements can be made using higher-order influence\nfunctions (Robins et al, 2017). Conversely, without any assumptions on the\nnuisances beyond the mean-square-error rates at which they can be estimated,\nthe rate achieved by the AIPW estimator is already optimal (Balakrishnan et al,\n2023; Jin and Syrgkanis, 2024).\n  We make three main contributions. First, we propose a new hybrid class of\ndistributions that combine structural agnosticism regarding the nuisance\nfunction space with additional smoothness constraints. Second, we calculate\nminimax lower bounds for estimating the ATE in the new class, as well as in the\npure structure-agnostic one. Third, we propose a new estimator of the ATE that\nenjoys doubly-robust asymptotic linearity; it can yield asymptotically valid\nWald-type confidence intervals even when the propensity score or the outcome\nmodel is inconsistently estimated, or estimated at a slow rate. Under certain\nconditions, we show that its rate of convergence in the new class can be much\nfaster than that achieved by the AIPW estimator and, in particular, matches the\nminimax lower bound rate, thereby establishing its optimality. Finally, we\ncomplement our theoretical findings with simulations."
                },
                "authors": [
                    {
                        "name": "Matteo Bonvini"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    },
                    {
                        "name": "Oliver Dukes"
                    },
                    {
                        "name": "Sivaraman Balakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Sivaraman Balakrishnan"
                },
                "author": "Sivaraman Balakrishnan",
                "arxiv_comment": "68 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09548v1",
                "updated": "2024-12-12T18:38:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    38,
                    42,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:38:42Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    38,
                    42,
                    3,
                    347,
                    0
                ],
                "title": "Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale"
                },
                "summary": "Meshes are fundamental representations of 3D surfaces. However, creating\nhigh-quality meshes is a labor-intensive task that requires significant time\nand expertise in 3D modeling. While a delicate object often requires over\n$10^4$ faces to be accurately modeled, recent attempts at generating\nartist-like meshes are limited to $1.6$K faces and heavy discretization of\nvertex coordinates. Hence, scaling both the maximum face count and vertex\ncoordinate resolution is crucial to producing high-quality meshes of realistic,\ncomplex 3D objects. We present Meshtron, a novel autoregressive mesh generation\nmodel able to generate meshes with up to 64K faces at 1024-level coordinate\nresolution --over an order of magnitude higher face count and $8{\\times}$\nhigher coordinate resolution than current state-of-the-art methods. Meshtron's\nscalability is driven by four key components: (1) an hourglass neural\narchitecture, (2) truncated sequence training, (3) sliding window inference,\n(4) a robust sampling strategy that enforces the order of mesh sequences. This\nresults in over $50{\\%}$ less training memory, $2.5{\\times}$ faster throughput,\nand better consistency than existing works. Meshtron generates meshes of\ndetailed, complex 3D objects at unprecedented levels of resolution and\nfidelity, closely resembling those created by professional artists, and opening\nthe door to more realistic generation of detailed 3D assets for animation,\ngaming, and virtual environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meshes are fundamental representations of 3D surfaces. However, creating\nhigh-quality meshes is a labor-intensive task that requires significant time\nand expertise in 3D modeling. While a delicate object often requires over\n$10^4$ faces to be accurately modeled, recent attempts at generating\nartist-like meshes are limited to $1.6$K faces and heavy discretization of\nvertex coordinates. Hence, scaling both the maximum face count and vertex\ncoordinate resolution is crucial to producing high-quality meshes of realistic,\ncomplex 3D objects. We present Meshtron, a novel autoregressive mesh generation\nmodel able to generate meshes with up to 64K faces at 1024-level coordinate\nresolution --over an order of magnitude higher face count and $8{\\times}$\nhigher coordinate resolution than current state-of-the-art methods. Meshtron's\nscalability is driven by four key components: (1) an hourglass neural\narchitecture, (2) truncated sequence training, (3) sliding window inference,\n(4) a robust sampling strategy that enforces the order of mesh sequences. This\nresults in over $50{\\%}$ less training memory, $2.5{\\times}$ faster throughput,\nand better consistency than existing works. Meshtron generates meshes of\ndetailed, complex 3D objects at unprecedented levels of resolution and\nfidelity, closely resembling those created by professional artists, and opening\nthe door to more realistic generation of detailed 3D assets for animation,\ngaming, and virtual environments."
                },
                "authors": [
                    {
                        "name": "Zekun Hao"
                    },
                    {
                        "name": "David W. Romero"
                    },
                    {
                        "name": "Tsung-Yi Lin"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Yu Liu"
                },
                "author": "Ming-Yu Liu",
                "arxiv_comment": "Project page: https://research.nvidia.com/labs/dir/meshtron/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09529v1",
                "updated": "2024-12-12T18:20:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    20,
                    16,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:20:16Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    20,
                    16,
                    3,
                    347,
                    0
                ],
                "title": "Can Modern LLMs Act as Agent Cores in Radiology~Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Modern LLMs Act as Agent Cores in Radiology~Environments?"
                },
                "summary": "Advancements in large language models (LLMs) have paved the way for LLM-based\nagent systems that offer enhanced accuracy and interpretability across various\ndomains. Radiology, with its complex analytical requirements, is an ideal field\nfor the application of these agents. This paper aims to investigate the\npre-requisite question for building concrete radiology agents which is, `Can\nmodern LLMs act as agent cores in radiology environments?' To investigate it,\nwe introduce RadABench with three-fold contributions: First, we present\nRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based\nagents, generated from an extensive taxonomy encompassing 6 anatomies, 5\nimaging modalities, 10 tool categories, and 11 radiology tasks. Second, we\npropose RadABench-EvalPlat, a novel evaluation platform for agents featuring a\nprompt-driven workflow and the capability to simulate a wide range of radiology\ntoolsets. Third, we assess the performance of 7 leading LLMs on our benchmark\nfrom 5 perspectives with multiple metrics. Our findings indicate that while\ncurrent LLMs demonstrate strong capabilities in many areas, they are still not\nsufficiently advanced to serve as the central agent core in a fully operational\nradiology agent system. Additionally, we identify key factors influencing the\nperformance of LLM-based agent cores, offering insights for clinicians on how\nto apply agent systems in real-world radiology practices effectively. All of\nour code and data are open-sourced in\nhttps://github.com/MAGIC-AI4Med/RadABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have paved the way for LLM-based\nagent systems that offer enhanced accuracy and interpretability across various\ndomains. Radiology, with its complex analytical requirements, is an ideal field\nfor the application of these agents. This paper aims to investigate the\npre-requisite question for building concrete radiology agents which is, `Can\nmodern LLMs act as agent cores in radiology environments?' To investigate it,\nwe introduce RadABench with three-fold contributions: First, we present\nRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based\nagents, generated from an extensive taxonomy encompassing 6 anatomies, 5\nimaging modalities, 10 tool categories, and 11 radiology tasks. Second, we\npropose RadABench-EvalPlat, a novel evaluation platform for agents featuring a\nprompt-driven workflow and the capability to simulate a wide range of radiology\ntoolsets. Third, we assess the performance of 7 leading LLMs on our benchmark\nfrom 5 perspectives with multiple metrics. Our findings indicate that while\ncurrent LLMs demonstrate strong capabilities in many areas, they are still not\nsufficiently advanced to serve as the central agent core in a fully operational\nradiology agent system. Additionally, we identify key factors influencing the\nperformance of LLM-based agent cores, offering insights for clinicians on how\nto apply agent systems in real-world radiology practices effectively. All of\nour code and data are open-sourced in\nhttps://github.com/MAGIC-AI4Med/RadABench."
                },
                "authors": [
                    {
                        "name": "Qiaoyu Zheng"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Lisong Dai"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_comment": "22 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19401v2",
                "updated": "2024-12-12T18:10:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    10,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-28T05:09:17Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    5,
                    9,
                    17,
                    6,
                    210,
                    0
                ],
                "title": "Towards Secure and Private AI: A Framework for Decentralized Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Secure and Private AI: A Framework for Decentralized Inference"
                },
                "summary": "The rapid advancement of ML models in critical sectors such as healthcare,\nfinance, and security has intensified the need for robust data security, model\nintegrity, and reliable outputs. Large multimodal foundational models, while\ncrucial for complex tasks, present challenges in scalability, reliability, and\npotential misuse. Decentralized systems offer a solution by distributing\nworkload and mitigating central points of failure, but they introduce risks of\nunauthorized access to sensitive data across nodes. We address these challenges\nwith a comprehensive framework designed for responsible AI development. Our\napproach incorporates: 1) Zero-knowledge proofs for secure model verification,\nenhancing trust without compromising privacy. 2) Consensus-based verification\nchecks to ensure consistent outputs across nodes, mitigating hallucinations and\nmaintaining model integrity. 3) Split Learning techniques that segment models\nacross different nodes, preserving data privacy by preventing full data access\nat any point. 4) Hardware-based security through trusted execution environments\n(TEEs) to protect data and computations. This framework aims to enhance\nsecurity and privacy and improve the reliability and fairness of multimodal AI\nsystems. Promoting efficient resource utilization contributes to more\nsustainable AI development. Our state-of-the-art proofs and principles\ndemonstrate the framework's effectiveness in responsibly democratizing\nartificial intelligence, offering a promising approach for building secure and\nprivate foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of ML models in critical sectors such as healthcare,\nfinance, and security has intensified the need for robust data security, model\nintegrity, and reliable outputs. Large multimodal foundational models, while\ncrucial for complex tasks, present challenges in scalability, reliability, and\npotential misuse. Decentralized systems offer a solution by distributing\nworkload and mitigating central points of failure, but they introduce risks of\nunauthorized access to sensitive data across nodes. We address these challenges\nwith a comprehensive framework designed for responsible AI development. Our\napproach incorporates: 1) Zero-knowledge proofs for secure model verification,\nenhancing trust without compromising privacy. 2) Consensus-based verification\nchecks to ensure consistent outputs across nodes, mitigating hallucinations and\nmaintaining model integrity. 3) Split Learning techniques that segment models\nacross different nodes, preserving data privacy by preventing full data access\nat any point. 4) Hardware-based security through trusted execution environments\n(TEEs) to protect data and computations. This framework aims to enhance\nsecurity and privacy and improve the reliability and fairness of multimodal AI\nsystems. Promoting efficient resource utilization contributes to more\nsustainable AI development. Our state-of-the-art proofs and principles\ndemonstrate the framework's effectiveness in responsibly democratizing\nartificial intelligence, offering a promising approach for building secure and\nprivate foundational models."
                },
                "authors": [
                    {
                        "name": "Hongyang Zhang"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Claudio Angione"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "James Buban"
                    },
                    {
                        "name": "Ahmad Farhan"
                    },
                    {
                        "name": "Fielding Johnston"
                    },
                    {
                        "name": "Patrick Colangelo"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Colangelo"
                },
                "author": "Patrick Colangelo",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04332v2",
                "updated": "2024-12-12T18:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    8,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-05T16:48:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Liquid: Language Models are Scalable Multi-modal Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid: Language Models are Scalable Multi-modal Generators"
                },
                "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid."
                },
                "authors": [
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Chuofan Ma"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Song Bai"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Technical report. Project page:\n  https://github.com/FoundationVision/Liquid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09521v1",
                "updated": "2024-12-12T18:07:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    7,
                    23,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:07:23Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    7,
                    23,
                    3,
                    347,
                    0
                ],
                "title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language\n  Model for Clinical Pathology Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Comprehensive Feature Extraction in Large Vision-Language\n  Model for Clinical Pathology Analysis"
                },
                "summary": "Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\ntraditional pure vision models face challenges of redundant feature extraction,\nwhereas existing large vision-language models (LVLMs) are limited by input\nresolution constraints, hindering their efficiency and accuracy. To overcome\nthese issues, we propose two innovative strategies: the mixed task-guided\nfeature enhancement, which directs feature extraction toward lesion-related\ndetails across scales, and the prompt-guided detail feature completion, which\nintegrates coarse- and fine-grained features from WSI based on specific prompts\nwithout compromising inference speed. Leveraging a comprehensive dataset of\n490,000 samples from diverse pathology tasks-including cancer detection,\ngrading, vascular and neural invasion identification, and so on-we trained the\npathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that\nthis model significantly outperforms existing methods in diagnostic accuracy\nand efficiency, offering an interactive, clinically aligned approach for\nauxiliary diagnosis in a wide range of pathology applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\ntraditional pure vision models face challenges of redundant feature extraction,\nwhereas existing large vision-language models (LVLMs) are limited by input\nresolution constraints, hindering their efficiency and accuracy. To overcome\nthese issues, we propose two innovative strategies: the mixed task-guided\nfeature enhancement, which directs feature extraction toward lesion-related\ndetails across scales, and the prompt-guided detail feature completion, which\nintegrates coarse- and fine-grained features from WSI based on specific prompts\nwithout compromising inference speed. Leveraging a comprehensive dataset of\n490,000 samples from diverse pathology tasks-including cancer detection,\ngrading, vascular and neural invasion identification, and so on-we trained the\npathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that\nthis model significantly outperforms existing methods in diagnostic accuracy\nand efficiency, offering an interactive, clinically aligned approach for\nauxiliary diagnosis in a wide range of pathology applications."
                },
                "authors": [
                    {
                        "name": "Shengxuming Zhang"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Tianhong Gao"
                    },
                    {
                        "name": "Jiacong Hu"
                    },
                    {
                        "name": "Haoming Luo"
                    },
                    {
                        "name": "Mingli Song"
                    },
                    {
                        "name": "Xiuming Zhang"
                    },
                    {
                        "name": "Zunlei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zunlei Feng"
                },
                "author": "Zunlei Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09498v1",
                "updated": "2024-12-12T17:47:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    47,
                    8,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:47:08Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    47,
                    8,
                    3,
                    347,
                    0
                ],
                "title": "Gradient descent inference in empirical risk minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient descent inference in empirical risk minimization"
                },
                "summary": "Gradient descent is one of the most widely used iterative algorithms in\nmodern statistical learning. However, its precise algorithmic dynamics in\nhigh-dimensional settings remain only partially understood, which has therefore\nlimited its broader potential for statistical inference applications.\n  This paper provides a precise, non-asymptotic distributional characterization\nof gradient descent iterates in a broad class of empirical risk minimization\nproblems, in the so-called mean-field regime where the sample size is\nproportional to the signal dimension. Our non-asymptotic state evolution theory\nholds for both general non-convex loss functions and non-Gaussian data, and\nreveals the central role of two Onsager correction matrices that precisely\ncharacterize the non-trivial dependence among all gradient descent iterates in\nthe mean-field regime.\n  Although the Onsager correction matrices are typically analytically\nintractable, our state evolution theory facilitates a generic gradient descent\ninference algorithm that consistently estimates these matrices across a broad\nclass of models. Leveraging this algorithm, we show that the state evolution\ncan be inverted to construct (i) data-driven estimators for the generalization\nerror of gradient descent iterates and (ii) debiased gradient descent iterates\nfor inference of the unknown signal. Detailed applications to two canonical\nmodels--linear regression and (generalized) logistic regression--are worked out\nto illustrate model-specific features of our general theory and inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient descent is one of the most widely used iterative algorithms in\nmodern statistical learning. However, its precise algorithmic dynamics in\nhigh-dimensional settings remain only partially understood, which has therefore\nlimited its broader potential for statistical inference applications.\n  This paper provides a precise, non-asymptotic distributional characterization\nof gradient descent iterates in a broad class of empirical risk minimization\nproblems, in the so-called mean-field regime where the sample size is\nproportional to the signal dimension. Our non-asymptotic state evolution theory\nholds for both general non-convex loss functions and non-Gaussian data, and\nreveals the central role of two Onsager correction matrices that precisely\ncharacterize the non-trivial dependence among all gradient descent iterates in\nthe mean-field regime.\n  Although the Onsager correction matrices are typically analytically\nintractable, our state evolution theory facilitates a generic gradient descent\ninference algorithm that consistently estimates these matrices across a broad\nclass of models. Leveraging this algorithm, we show that the state evolution\ncan be inverted to construct (i) data-driven estimators for the generalization\nerror of gradient descent iterates and (ii) debiased gradient descent iterates\nfor inference of the unknown signal. Detailed applications to two canonical\nmodels--linear regression and (generalized) logistic regression--are worked out\nto illustrate model-specific features of our general theory and inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Qiyang Han"
                    },
                    {
                        "name": "Xiaocong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaocong Xu"
                },
                "author": "Xiaocong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09482v1",
                "updated": "2024-12-12T17:32:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    32,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:32:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    32,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Inference under Staggered Adoption: Case Study of the Affordable Care\n  Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference under Staggered Adoption: Case Study of the Affordable Care\n  Act"
                },
                "summary": "Panel data consists of a collection of $N$ units that are observed over $T$\nunits of time. A policy or treatment is subject to staggered adoption if\ndifferent units take on treatment at different times and remains treated (or\nnever at all). Assessing the effectiveness of such a policy requires estimating\nthe treatment effect, corresponding to the difference between outcomes for\ntreated versus untreated units. We develop inference procedures that build upon\na computationally efficient matrix estimator for treatment effects in panel\ndata. Our routines return confidence intervals (CIs) both for individual\ntreatment effects, as well as for more general bilinear functionals of\ntreatment effects, with prescribed coverage guarantees. We apply these\ninferential methods to analyze the effectiveness of Medicaid expansion portion\nof the Affordable Care Act. Based on our analysis, Medicaid expansion has led\nto substantial reductions in uninsurance rates, has reduced infant mortality\nrates, and has had no significant effects on healthcare expenditures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panel data consists of a collection of $N$ units that are observed over $T$\nunits of time. A policy or treatment is subject to staggered adoption if\ndifferent units take on treatment at different times and remains treated (or\nnever at all). Assessing the effectiveness of such a policy requires estimating\nthe treatment effect, corresponding to the difference between outcomes for\ntreated versus untreated units. We develop inference procedures that build upon\na computationally efficient matrix estimator for treatment effects in panel\ndata. Our routines return confidence intervals (CIs) both for individual\ntreatment effects, as well as for more general bilinear functionals of\ntreatment effects, with prescribed coverage guarantees. We apply these\ninferential methods to analyze the effectiveness of Medicaid expansion portion\nof the Affordable Care Act. Based on our analysis, Medicaid expansion has led\nto substantial reductions in uninsurance rates, has reduced infant mortality\nrates, and has had no significant effects on healthcare expenditures."
                },
                "authors": [
                    {
                        "name": "Eric Xia"
                    },
                    {
                        "name": "Yuling Yan"
                    },
                    {
                        "name": "Martin J. Wainwright"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Wainwright"
                },
                "author": "Martin J. Wainwright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08268v2",
                "updated": "2024-12-12T17:32:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    32,
                    23,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-11T10:35:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCFO: Long Context and Long Form Output Dataset and Benchmarking"
                },
                "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-jussà"
                    },
                    {
                        "name": "Pierre Andrews"
                    },
                    {
                        "name": "Mariano Coria Meglioli"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "David Dale"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Alexandre Mourachko"
                    },
                    {
                        "name": "Eduardo Sánchez"
                    },
                    {
                        "name": "Holger Schwenk"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Carleigh Wood"
                    }
                ],
                "author_detail": {
                    "name": "Carleigh Wood"
                },
                "author": "Carleigh Wood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01304v2",
                "updated": "2024-12-12T17:26:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    26,
                    4,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-02T20:25:50Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    20,
                    25,
                    50,
                    5,
                    62,
                    0
                ],
                "title": "Improving the Validity of Automatically Generated Feedback via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Validity of Automatically Generated Feedback via\n  Reinforcement Learning"
                },
                "summary": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work."
                },
                "authors": [
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Digory Smith"
                    },
                    {
                        "name": "Simon Woodhead"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_doi": "10.1007/978-3-031-64302-6_20",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-64302-6_20",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.01304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Best student paper award, Published in AIED 2024: The 25th\n  International Conference on Artificial Intelligence in Education",
                "arxiv_journal_ref": "In International Conference on Artificial Intelligence in\n  Education (pp. 280-294). Cham: Springer Nature Switzerland (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v1",
                "updated": "2024-12-12T17:11:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training generative language models\nraises critical legal and ethical questions. This paper presents a framework\nfor and the results of empirically assessing the impact of copyrighted\nmaterials on the performance of large language models (LLMs) for Norwegian. We\nfound that both books and newspapers contribute positively when the models are\nevaluated on a diverse set of Norwegian benchmarks, while fiction works\npossibly lead to decreased performance. Our experiments could inform the\ncreation of a compensation scheme for authors whose works contribute to AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training generative language models\nraises critical legal and ethical questions. This paper presents a framework\nfor and the results of empirically assessing the impact of copyrighted\nmaterials on the performance of large language models (LLMs) for Norwegian. We\nfound that both books and newspapers contribute positively when the models are\nevaluated on a diverse set of Norwegian benchmarks, while fiction works\npossibly lead to decreased performance. Our experiments could inform the\ncreation of a compensation scheme for authors whose works contribute to AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "pre-print, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v4",
                "updated": "2024-12-12T16:59:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    59,
                    55,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09441v1",
                "updated": "2024-12-12T16:57:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    57,
                    20,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:57:20Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    57,
                    20,
                    3,
                    347,
                    0
                ],
                "title": "MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental\n  Learning"
                },
                "summary": "Class-Incremental Learning (CIL) requires models to continually acquire\nknowledge of new classes without forgetting old ones. Despite Pre-trained\nModels (PTMs) have shown excellent performance in CIL, catastrophic forgetting\nstill occurs as the model learns new concepts. Existing work seeks to utilize\nlightweight components to adjust the PTM, while the forgetting phenomenon still\ncomes from {\\em parameter and retrieval} levels. Specifically, iterative\nupdates of the model result in parameter drift, while mistakenly retrieving\nirrelevant modules leads to the mismatch during inference. To this end, we\npropose MOdel Surgery (MOS) to rescue the model from forgetting previous\nknowledge. By training task-specific adapters, we continually adjust the PTM to\ndownstream tasks. To mitigate parameter-level forgetting, we present an adapter\nmerging approach to learn task-specific adapters, which aims to bridge the gap\nbetween different components while reserve task-specific information. Besides,\nto address retrieval-level forgetting, we introduce a training-free\nself-refined adapter retrieval mechanism during inference, which leverages the\nmodel's inherent ability for better adapter retrieval. By jointly rectifying\nthe model with those steps, MOS can robustly resist catastrophic forgetting in\nthe learning process. Extensive experiments on seven benchmark datasets\nvalidate MOS's state-of-the-art performance. Code is available at:\nhttps://github.com/sun-hailong/AAAI25-MOS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) requires models to continually acquire\nknowledge of new classes without forgetting old ones. Despite Pre-trained\nModels (PTMs) have shown excellent performance in CIL, catastrophic forgetting\nstill occurs as the model learns new concepts. Existing work seeks to utilize\nlightweight components to adjust the PTM, while the forgetting phenomenon still\ncomes from {\\em parameter and retrieval} levels. Specifically, iterative\nupdates of the model result in parameter drift, while mistakenly retrieving\nirrelevant modules leads to the mismatch during inference. To this end, we\npropose MOdel Surgery (MOS) to rescue the model from forgetting previous\nknowledge. By training task-specific adapters, we continually adjust the PTM to\ndownstream tasks. To mitigate parameter-level forgetting, we present an adapter\nmerging approach to learn task-specific adapters, which aims to bridge the gap\nbetween different components while reserve task-specific information. Besides,\nto address retrieval-level forgetting, we introduce a training-free\nself-refined adapter retrieval mechanism during inference, which leverages the\nmodel's inherent ability for better adapter retrieval. By jointly rectifying\nthe model with those steps, MOS can robustly resist catastrophic forgetting in\nthe learning process. Extensive experiments on seven benchmark datasets\nvalidate MOS's state-of-the-art performance. Code is available at:\nhttps://github.com/sun-hailong/AAAI25-MOS"
                },
                "authors": [
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Da-Wei Zhou"
                    },
                    {
                        "name": "Hanbin Zhao"
                    },
                    {
                        "name": "Le Gan"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "Accepted to AAAI 2025. Code is available at:\n  https://github.com/sun-hailong/AAAI25-MOS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05565v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05565v4",
                "updated": "2024-12-12T16:41:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    41,
                    43,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-10T14:21:45Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    14,
                    21,
                    45,
                    5,
                    223,
                    0
                ],
                "title": "Dynamic Resource Allocation with Quantum Error Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Resource Allocation with Quantum Error Detection"
                },
                "summary": "Quantum processing units (QPUs) are highly heterogeneous in terms of physical\nqubit performance. To add even more complexity, drift in quantum noise\nlandscapes has been well-documented. This makes resource allocation a\nchallenging problem whenever a quantum program must be mapped to hardware. As a\nsolution, we propose a novel resource allocation framework that applies Pauli\nchecks. Pauli checks have demonstrated their efficacy at error mitigation in\nprior work, and in this paper, we highlight their potential to infer the noise\ncharacteristics of a quantum system. Circuits with embedded Pauli checks can be\nexecuted on different regions of qubits, and the syndrome data created by\nerror-detecting Pauli checks can be leveraged to guide quantum program outcomes\ntoward regions that produce higher-fidelity final distributions. Using noisy\nsimulation and a real QPU testbed, we show that dynamic quantum resource\nallocation with Pauli checks can outperform state-of-art mapping techniques,\nsuch as those that are noise-aware. Further, when applied toward the Quantum\nApproximate Optimization Algorithm, techniques guided by Pauli checks\ndemonstrate the ability to increase circuit fidelity 11% on average, and up to\n33%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum processing units (QPUs) are highly heterogeneous in terms of physical\nqubit performance. To add even more complexity, drift in quantum noise\nlandscapes has been well-documented. This makes resource allocation a\nchallenging problem whenever a quantum program must be mapped to hardware. As a\nsolution, we propose a novel resource allocation framework that applies Pauli\nchecks. Pauli checks have demonstrated their efficacy at error mitigation in\nprior work, and in this paper, we highlight their potential to infer the noise\ncharacteristics of a quantum system. Circuits with embedded Pauli checks can be\nexecuted on different regions of qubits, and the syndrome data created by\nerror-detecting Pauli checks can be leveraged to guide quantum program outcomes\ntoward regions that produce higher-fidelity final distributions. Using noisy\nsimulation and a real QPU testbed, we show that dynamic quantum resource\nallocation with Pauli checks can outperform state-of-art mapping techniques,\nsuch as those that are noise-aware. Further, when applied toward the Quantum\nApproximate Optimization Algorithm, techniques guided by Pauli checks\ndemonstrate the ability to increase circuit fidelity 11% on average, and up to\n33%."
                },
                "authors": [
                    {
                        "name": "Quinn Langfitt"
                    },
                    {
                        "name": "Alvin Gonzales"
                    },
                    {
                        "name": "Joshua Gao"
                    },
                    {
                        "name": "Ji Liu"
                    },
                    {
                        "name": "Zain H. Saleem"
                    },
                    {
                        "name": "Nikos Hardavellas"
                    },
                    {
                        "name": "Kaitlin N. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kaitlin N. Smith"
                },
                "author": "Kaitlin N. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05565v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05565v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09429v1",
                "updated": "2024-12-12T16:35:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    35,
                    5,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:35:05Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    35,
                    5,
                    3,
                    347,
                    0
                ],
                "title": "From Intention To Implementation: Automating Biomedical Research via\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Intention To Implementation: Automating Biomedical Research via\n  LLMs"
                },
                "summary": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Linghang Shi"
                    },
                    {
                        "name": "Yihao Li"
                    },
                    {
                        "name": "Aobo Zhuang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Lin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lin Chen"
                },
                "author": "Lin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09421v1",
                "updated": "2024-12-12T16:27:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    27,
                    21,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:27:21Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    27,
                    21,
                    3,
                    347,
                    0
                ],
                "title": "HfO$_2$-based platform for high-index-contrast visible/UV integrated\n  photonics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HfO$_2$-based platform for high-index-contrast visible/UV integrated\n  photonics"
                },
                "summary": "Ultraviolet and visible integrated photonics are enabling for applications in\nquantum information, sensing, and spectroscopy, among others. Few materials\nsupport low-loss photonics into the UV, and the relatively low refractive index\nof known depositable materials limits the achievable functionality. Here we\npresent a high-index integrated photonics platform based on HfO$_2$ and\nAl$_2$O$_3$ composites deposited via Atomic Layer Deposition (ALD) with low\nloss in the visible and near-UV. We show that Al$_2$O$_3$ incorporation\ndramatically decreases bulk loss compared to pure HfO$_2$, consistent with\ninhibited crystallization due to the admixture of Al$_2$O$_3$. Composites\nexhibit refractive index $n$ following the average of that of HfO$_2$ and\nAl$_2$O$_3$, weighted by the HfO$_2$ fractional composition $x$. At\n$\\lambda=375$ nm, composites with $x=0.67$ exhibit $n=2.08$ preserving most of\nHfO$_2$'s significantly higher index, and $3.8(7) $ dB/cm material loss. We\nfurther present fully etched and cladded waveguides, grating couplers, and ring\nresonators, realizing single-mode waveguide loss of $0.25(2)$ dB/cm inferred\nfrom resonators of 2.6 million intrinsic quality factor at $\\lambda=729$ nm,\n$2.6(2)$ dB/cm at $\\lambda=405$ nm, and $7.7(6)$ dB/cm at $\\lambda=375$ nm. We\nmeasure the composite's thermo-optic coefficient (TOC) to be $2.44(3) \\times\n10^{-5}$ RIU/$^\\circ$C near $\\lambda=397$ nm. This work establishes\n(HfO$_2$)$_x$(Al$_2$O$_3$)$_{1-x}$ composites as a platform amenable to\nintegration for low-loss, high-index photonics spanning the UV to NIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultraviolet and visible integrated photonics are enabling for applications in\nquantum information, sensing, and spectroscopy, among others. Few materials\nsupport low-loss photonics into the UV, and the relatively low refractive index\nof known depositable materials limits the achievable functionality. Here we\npresent a high-index integrated photonics platform based on HfO$_2$ and\nAl$_2$O$_3$ composites deposited via Atomic Layer Deposition (ALD) with low\nloss in the visible and near-UV. We show that Al$_2$O$_3$ incorporation\ndramatically decreases bulk loss compared to pure HfO$_2$, consistent with\ninhibited crystallization due to the admixture of Al$_2$O$_3$. Composites\nexhibit refractive index $n$ following the average of that of HfO$_2$ and\nAl$_2$O$_3$, weighted by the HfO$_2$ fractional composition $x$. At\n$\\lambda=375$ nm, composites with $x=0.67$ exhibit $n=2.08$ preserving most of\nHfO$_2$'s significantly higher index, and $3.8(7) $ dB/cm material loss. We\nfurther present fully etched and cladded waveguides, grating couplers, and ring\nresonators, realizing single-mode waveguide loss of $0.25(2)$ dB/cm inferred\nfrom resonators of 2.6 million intrinsic quality factor at $\\lambda=729$ nm,\n$2.6(2)$ dB/cm at $\\lambda=405$ nm, and $7.7(6)$ dB/cm at $\\lambda=375$ nm. We\nmeasure the composite's thermo-optic coefficient (TOC) to be $2.44(3) \\times\n10^{-5}$ RIU/$^\\circ$C near $\\lambda=397$ nm. This work establishes\n(HfO$_2$)$_x$(Al$_2$O$_3$)$_{1-x}$ composites as a platform amenable to\nintegration for low-loss, high-index photonics spanning the UV to NIR."
                },
                "authors": [
                    {
                        "name": "Oscar Jaramillo"
                    },
                    {
                        "name": "Vighnesh Natarajan"
                    },
                    {
                        "name": "Hamim Mahmud Rivy"
                    },
                    {
                        "name": "Joshua Tensuan"
                    },
                    {
                        "name": "Leonardo Massai"
                    },
                    {
                        "name": "Karan K. Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Karan K. Mehta"
                },
                "author": "Karan K. Mehta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09420v1",
                "updated": "2024-12-12T16:26:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    26,
                    38,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:26:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    26,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "Mixture of neural fields for heterogeneous reconstruction in cryo-EM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of neural fields for heterogeneous reconstruction in cryo-EM"
                },
                "summary": "Cryo-electron microscopy (cryo-EM) is an experimental technique for protein\nstructure determination that images an ensemble of macromolecules in\nnear-physiological contexts. While recent advances enable the reconstruction of\ndynamic conformations of a single biomolecular complex, current methods do not\nadequately model samples with mixed conformational and compositional\nheterogeneity. In particular, datasets containing mixtures of multiple proteins\nrequire the joint inference of structure, pose, compositional class, and\nconformational states for 3D reconstruction. Here, we present Hydra, an\napproach that models both conformational and compositional heterogeneity fully\nab initio by parameterizing structures as arising from one of K neural fields.\nWe employ a new likelihood-based loss function and demonstrate the\neffectiveness of our approach on synthetic datasets composed of mixtures of\nproteins with large degrees of conformational variability. We additionally\ndemonstrate Hydra on an experimental dataset of a cellular lysate containing a\nmixture of different protein complexes. Hydra expands the expressivity of\nheterogeneous reconstruction methods and thus broadens the scope of cryo-EM to\nincreasingly complex samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryo-electron microscopy (cryo-EM) is an experimental technique for protein\nstructure determination that images an ensemble of macromolecules in\nnear-physiological contexts. While recent advances enable the reconstruction of\ndynamic conformations of a single biomolecular complex, current methods do not\nadequately model samples with mixed conformational and compositional\nheterogeneity. In particular, datasets containing mixtures of multiple proteins\nrequire the joint inference of structure, pose, compositional class, and\nconformational states for 3D reconstruction. Here, we present Hydra, an\napproach that models both conformational and compositional heterogeneity fully\nab initio by parameterizing structures as arising from one of K neural fields.\nWe employ a new likelihood-based loss function and demonstrate the\neffectiveness of our approach on synthetic datasets composed of mixtures of\nproteins with large degrees of conformational variability. We additionally\ndemonstrate Hydra on an experimental dataset of a cellular lysate containing a\nmixture of different protein complexes. Hydra expands the expressivity of\nheterogeneous reconstruction methods and thus broadens the scope of cryo-EM to\nincreasingly complex samples."
                },
                "authors": [
                    {
                        "name": "Axel Levy"
                    },
                    {
                        "name": "Rishwanth Raghu"
                    },
                    {
                        "name": "David Shustin"
                    },
                    {
                        "name": "Adele Rui-Yang Peng"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Oliver Biggs Clarke"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Ellen D. Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Ellen D. Zhong"
                },
                "author": "Ellen D. Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09413v1",
                "updated": "2024-12-12T16:20:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    20,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:20:36Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    20,
                    36,
                    3,
                    347,
                    0
                ],
                "title": "Imitate, Explore, and Self-Improve: A Reproduction Report on\n  Slow-thinking Reasoning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitate, Explore, and Self-Improve: A Reproduction Report on\n  Slow-thinking Reasoning Systems"
                },
                "summary": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an \"imitate, explore, and self-improve\" framework as our\nprimary technical approach to train the reasoning model. In the initial phase,\nwe use distilled long-form thought data to fine-tune the reasoning model,\nenabling it to invoke a slow-thinking mode. The model is then encouraged to\nexplore challenging problems by generating multiple rollouts, which can result\nin increasingly more high-quality trajectories that lead to correct answers.\nFurthermore, the model undergoes self-improvement by iteratively refining its\ntraining dataset. To verify the effectiveness of this approach, we conduct\nextensive experiments on three challenging benchmarks. The experimental results\ndemonstrate that our approach achieves competitive performance compared to\nindustry-level reasoning systems on these benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an \"imitate, explore, and self-improve\" framework as our\nprimary technical approach to train the reasoning model. In the initial phase,\nwe use distilled long-form thought data to fine-tune the reasoning model,\nenabling it to invoke a slow-thinking mode. The model is then encouraged to\nexplore challenging problems by generating multiple rollouts, which can result\nin increasingly more high-quality trajectories that lead to correct answers.\nFurthermore, the model undergoes self-improvement by iteratively refining its\ntraining dataset. To verify the effectiveness of this approach, we conduct\nextensive experiments on three challenging benchmarks. The experimental results\ndemonstrate that our approach achieves competitive performance compared to\nindustry-level reasoning systems on these benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Part II",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18564v2",
                "updated": "2024-12-12T16:03:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    3,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-27T18:04:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    4,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems."
                },
                "authors": [
                    {
                        "name": "Rong Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Jonas Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kuhn"
                },
                "author": "Jonas Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09395v1",
                "updated": "2024-12-12T16:00:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    0,
                    12,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:00:12Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    0,
                    12,
                    3,
                    347,
                    0
                ],
                "title": "Radial evolution of a density structure within a solar wind magnetic\n  sector boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial evolution of a density structure within a solar wind magnetic\n  sector boundary"
                },
                "summary": "This study focuses on a radial alignment between Parker Solar Probe (PSP) and\nSolar Orbiter (SolO) on the 29$^{\\text{th}}$ of April 2021 (during a solar\nminimum), when the two spacecraft were respectively located at $\\sim 0.075$ and\n$\\sim 0.9$~au from the Sun. A previous study of this alignment allowed the\nidentification of the same density enhancement (with a time scale of\n$\\sim$1.5~h), and substructures ($\\sim$20-30~min timescale), passing first by\nPSP, and then SolO after a $\\sim 138$~h propagation time in the inner\nheliosphere. We show here that this structure belongs to the large scale\nheliospheric magnetic sector boundary. In this region, the density is dominated\nby radial gradients, whereas the magnetic field reversal is consistent with\nlongitudinal gradients in the Carrington reference frame. We estimate the\ndensity structure radial size to remain of the order L$_R \\sim 10^6$~km, while\nits longitudinal and latitudinal sizes, are estimated to expand from\nL$_{\\varphi, \\theta} \\sim 10^4$-$10^5$~km in the high solar corona, to\nL$_{\\varphi, \\theta} \\sim 10^5$-$10^6$~km at PSP, and L$_{\\varphi, \\theta} \\sim\n10^6$-$10^7$~km at SolO. This implies a strong evolution of the structure's\naspect ratio during the propagation, due to the plasma's nearly spherical\nexpansion. The structure's shape is therefore inferred to evolve from elongated\nin the radial direction at $\\sim$2-3 solar radii (high corona), to sizes of\nnearly the same order in all directions at PSP, and then becoming elongated in\nthe directions transverse to the radial at SolO. Measurements are not\nconcordant with local reconnection of open solar wind field lines, so we\npropose that the structure has been generated through interchange reconnection\nnear the tip of a coronal streamer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study focuses on a radial alignment between Parker Solar Probe (PSP) and\nSolar Orbiter (SolO) on the 29$^{\\text{th}}$ of April 2021 (during a solar\nminimum), when the two spacecraft were respectively located at $\\sim 0.075$ and\n$\\sim 0.9$~au from the Sun. A previous study of this alignment allowed the\nidentification of the same density enhancement (with a time scale of\n$\\sim$1.5~h), and substructures ($\\sim$20-30~min timescale), passing first by\nPSP, and then SolO after a $\\sim 138$~h propagation time in the inner\nheliosphere. We show here that this structure belongs to the large scale\nheliospheric magnetic sector boundary. In this region, the density is dominated\nby radial gradients, whereas the magnetic field reversal is consistent with\nlongitudinal gradients in the Carrington reference frame. We estimate the\ndensity structure radial size to remain of the order L$_R \\sim 10^6$~km, while\nits longitudinal and latitudinal sizes, are estimated to expand from\nL$_{\\varphi, \\theta} \\sim 10^4$-$10^5$~km in the high solar corona, to\nL$_{\\varphi, \\theta} \\sim 10^5$-$10^6$~km at PSP, and L$_{\\varphi, \\theta} \\sim\n10^6$-$10^7$~km at SolO. This implies a strong evolution of the structure's\naspect ratio during the propagation, due to the plasma's nearly spherical\nexpansion. The structure's shape is therefore inferred to evolve from elongated\nin the radial direction at $\\sim$2-3 solar radii (high corona), to sizes of\nnearly the same order in all directions at PSP, and then becoming elongated in\nthe directions transverse to the radial at SolO. Measurements are not\nconcordant with local reconnection of open solar wind field lines, so we\npropose that the structure has been generated through interchange reconnection\nnear the tip of a coronal streamer."
                },
                "authors": [
                    {
                        "name": "Etienne Berriot"
                    },
                    {
                        "name": "Pascal Démoulin"
                    },
                    {
                        "name": "Olga Alexandrova"
                    },
                    {
                        "name": "Arnaud Zaslavsky"
                    },
                    {
                        "name": "Milan Maksimovic"
                    },
                    {
                        "name": "Georgios Nicolaou"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Nicolaou"
                },
                "author": "Georgios Nicolaou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09394v1",
                "updated": "2024-12-12T15:59:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    59,
                    58,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:59:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    59,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage"
                },
                "summary": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities."
                },
                "authors": [
                    {
                        "name": "Sebastien Valeyre"
                    },
                    {
                        "name": "Sofiane Aboura"
                    }
                ],
                "author_detail": {
                    "name": "Sofiane Aboura"
                },
                "author": "Sofiane Aboura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09385v1",
                "updated": "2024-12-12T15:52:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    52,
                    41,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    52,
                    41,
                    3,
                    347,
                    0
                ],
                "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities"
                },
                "summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Fabrizio Davide"
                    },
                    {
                        "name": "Pietro Torre"
                    },
                    {
                        "name": "Andrea Gaggioli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Gaggioli"
                },
                "author": "Andrea Gaggioli",
                "arxiv_comment": "47 pages, 8 figures, 17 tables, appendix with data and code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22296v3",
                "updated": "2024-12-12T15:48:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    48,
                    47,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-29T17:45:57Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "title": "LLMs are Highly-Constrained Biophysical Sequence Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Highly-Constrained Biophysical Sequence Optimizers"
                },
                "summary": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Samuel D. Stanton"
                    },
                    {
                        "name": "Robert G. Alberstein"
                    },
                    {
                        "name": "Andrew M. Watkins"
                    },
                    {
                        "name": "Richard Bonneau"
                    },
                    {
                        "name": "Vladimir Gligorijević"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Nathan C. Frey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan C. Frey"
                },
                "author": "Nathan C. Frey",
                "arxiv_comment": "Supercedes arXiv:2407.00236v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16747v2",
                "updated": "2024-12-12T15:43:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    43,
                    15,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-29T17:40:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    40,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "Ringdown of a postinnermost stable circular orbit of a rapidly spinning\n  black hole: Mass ratio dependence of higher harmonic quasinormal mode\n  excitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ringdown of a postinnermost stable circular orbit of a rapidly spinning\n  black hole: Mass ratio dependence of higher harmonic quasinormal mode\n  excitation"
                },
                "summary": "In a binary merger with a small mass ratio, as the secondary body approaches\nthe innermost stable circular orbit (ISCO) of the primary black hole, the\nmotion transitions from the adiabatic inspiral to the plunge governed by the\ngeodesic equation. The plunge orbit is expected to excite the ringdown\ngravitational wave, which encodes information about the primary black hole's\ngeometry. The details of the transition regime depend on the binary's mass\nratio through radiation fluxes, which in turn influence the initial conditions\nfor the plunge. As such, the mass ratio affects the post-ISCO ringdown\ngravitational wave excitation. In this study, we numerically investigate the\nmass ratio dependence of higher harmonic quasi-normal mode excitations in the\npost-ISCO gravitational waves of rapidly spinning black holes, based on the\nTeukolsky-Sasaki-Nakamura formalism. We consider the effect of mass ratio on\nthe gravitational waves by accounting for the energy and angular momentum\nlosses during the transition regime following the Ori-Thorne procedure. We\nexamine two mass ratio scenarios: the intermediate mass ratio (IMR) and the\nextreme mass ratio (EMR). Our main finding is that higher harmonic quasi-normal\nmodes are significantly excited in an IMR merger involving a highly spinning\nprimary black hole. This implies that detecting an IMR merger involving such a\nprimary black hole with space-based gravitational wave interferometers can\nprovide valuable opportunities to infer black hole properties or test general\nrelativity with excellent precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a binary merger with a small mass ratio, as the secondary body approaches\nthe innermost stable circular orbit (ISCO) of the primary black hole, the\nmotion transitions from the adiabatic inspiral to the plunge governed by the\ngeodesic equation. The plunge orbit is expected to excite the ringdown\ngravitational wave, which encodes information about the primary black hole's\ngeometry. The details of the transition regime depend on the binary's mass\nratio through radiation fluxes, which in turn influence the initial conditions\nfor the plunge. As such, the mass ratio affects the post-ISCO ringdown\ngravitational wave excitation. In this study, we numerically investigate the\nmass ratio dependence of higher harmonic quasi-normal mode excitations in the\npost-ISCO gravitational waves of rapidly spinning black holes, based on the\nTeukolsky-Sasaki-Nakamura formalism. We consider the effect of mass ratio on\nthe gravitational waves by accounting for the energy and angular momentum\nlosses during the transition regime following the Ori-Thorne procedure. We\nexamine two mass ratio scenarios: the intermediate mass ratio (IMR) and the\nextreme mass ratio (EMR). Our main finding is that higher harmonic quasi-normal\nmodes are significantly excited in an IMR merger involving a highly spinning\nprimary black hole. This implies that detecting an IMR merger involving such a\nprimary black hole with space-based gravitational wave interferometers can\nprovide valuable opportunities to infer black hole properties or test general\nrelativity with excellent precision."
                },
                "authors": [
                    {
                        "name": "Daiki Watarai"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Watarai"
                },
                "author": "Daiki Watarai",
                "arxiv_doi": "10.1103/PhysRevD.110.124029",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.124029",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 12 figures, v2: matches the published version",
                "arxiv_journal_ref": "Phys. Rev. D 110, 124029 (2024)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03318v2",
                "updated": "2024-12-12T15:26:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    26,
                    8,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-18T18:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    18,
                    37,
                    43,
                    4,
                    292,
                    0
                ],
                "title": "FUsion-based ConstitutivE model (FuCe): Towards model-data augmentation\n  in constitutive modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUsion-based ConstitutivE model (FuCe): Towards model-data augmentation\n  in constitutive modelling"
                },
                "summary": "Constitutive modelling is crucial for engineering design and simulations to\naccurately describe material behavior. However, traditional phenomenological\nmodels often struggle to capture the complexities of real materials under\nvarying stress conditions due to their fixed forms and limited parameters.\nWhile recent advances in deep learning have addressed some limitations of\nclassical models, purely data-driven methods tend to require large datasets,\nlack interpretability, and struggle to generalize beyond their training data.\nTo tackle these issues, we introduce \"Fusion-based Constitutive model (FuCe):\nTowards model-data augmentation in constitutive modelling\". This approach\ncombines established phenomenological models with an ICNN architecture,\ndesigned to train on the limited and noisy force-displacement data typically\navailable in practical applications. The hybrid model inherently adheres to\nnecessary constitutive conditions. During inference, Monte Carlo dropout is\nemployed to generate Bayesian predictions, providing mean values and confidence\nintervals that quantify uncertainty. We demonstrate the model's effectiveness\nby learning two isotropic constitutive models and one anisotropic model with a\nsingle fibre direction, across six different stress states. The framework's\napplicability is also showcased in finite element simulations across three\ngeometries of varying complexities. Our results highlight the framework's\nsuperior extrapolation capabilities, even when trained on limited and noisy\ndata, delivering accurate and physically meaningful predictions across all\nnumerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constitutive modelling is crucial for engineering design and simulations to\naccurately describe material behavior. However, traditional phenomenological\nmodels often struggle to capture the complexities of real materials under\nvarying stress conditions due to their fixed forms and limited parameters.\nWhile recent advances in deep learning have addressed some limitations of\nclassical models, purely data-driven methods tend to require large datasets,\nlack interpretability, and struggle to generalize beyond their training data.\nTo tackle these issues, we introduce \"Fusion-based Constitutive model (FuCe):\nTowards model-data augmentation in constitutive modelling\". This approach\ncombines established phenomenological models with an ICNN architecture,\ndesigned to train on the limited and noisy force-displacement data typically\navailable in practical applications. The hybrid model inherently adheres to\nnecessary constitutive conditions. During inference, Monte Carlo dropout is\nemployed to generate Bayesian predictions, providing mean values and confidence\nintervals that quantify uncertainty. We demonstrate the model's effectiveness\nby learning two isotropic constitutive models and one anisotropic model with a\nsingle fibre direction, across six different stress states. The framework's\napplicability is also showcased in finite element simulations across three\ngeometries of varying complexities. Our results highlight the framework's\nsuperior extrapolation capabilities, even when trained on limited and noisy\ndata, delivering accurate and physically meaningful predictions across all\nnumerical examples."
                },
                "authors": [
                    {
                        "name": "Tushar"
                    },
                    {
                        "name": "Sawan Kumar"
                    },
                    {
                        "name": "Souvik Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Souvik Chakraborty"
                },
                "author": "Souvik Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09352v1",
                "updated": "2024-12-12T15:19:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    19,
                    46,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:19:46Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    19,
                    46,
                    3,
                    347,
                    0
                ],
                "title": "Using nebular near-IR spectroscopy to measure asymmetric chemical\n  distributions in 2003fg-like thermonuclear supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using nebular near-IR spectroscopy to measure asymmetric chemical\n  distributions in 2003fg-like thermonuclear supernovae"
                },
                "summary": "We present an analysis of three near-infrared (NIR; 1.0-2.4 $\\mu$m) spectra\nof the SN 2003fg-like/\"super-Chandrasekhar\" type Ia supernovae (SNe Ia) SN\n2009dc, SN 2020hvf, and SN 2022pul at respective phases +372, +296, and +294~d\nrelative to the epoch of $B$-band maximum. We find that all objects in our\nsample have asymmetric, or \"tilted\", [Fe~II] 1.257 and 1.644 $\\mu$m profiles.\nWe quantify the asymmetry of these features using five methods: velocity at\npeak flux, profile tilts, residual testing, velocity fitting, and comparison to\ndeflagration-detonation transition models. Our results demonstrate that, while\nthe profiles of the [Fe II] 1.257 and 1.644 $\\mu$m features are widely varied\nbetween 2003fg-likes, these features are correlated in shape within the same\nSN. This implies that line blending is most likely not the dominant cause of\nthe asymmetries inferred from these profiles. Instead, it is more plausible\nthat 2003fg-like SNe have aspherical chemical distributions in their inner\nregions. These distributions may come from aspherical progenitor systems, such\nas double white dwarf mergers, or off-center delayed-detonation explosions of\nChandrasekhar-mass Carbon-Oxygen white dwarfs. Additional late-phase NIR\nobservation of 2003fg-like SNe and detailed 3-D NLTE modeling of these two\nexplosion scenarios are encouraged.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of three near-infrared (NIR; 1.0-2.4 $\\mu$m) spectra\nof the SN 2003fg-like/\"super-Chandrasekhar\" type Ia supernovae (SNe Ia) SN\n2009dc, SN 2020hvf, and SN 2022pul at respective phases +372, +296, and +294~d\nrelative to the epoch of $B$-band maximum. We find that all objects in our\nsample have asymmetric, or \"tilted\", [Fe~II] 1.257 and 1.644 $\\mu$m profiles.\nWe quantify the asymmetry of these features using five methods: velocity at\npeak flux, profile tilts, residual testing, velocity fitting, and comparison to\ndeflagration-detonation transition models. Our results demonstrate that, while\nthe profiles of the [Fe II] 1.257 and 1.644 $\\mu$m features are widely varied\nbetween 2003fg-likes, these features are correlated in shape within the same\nSN. This implies that line blending is most likely not the dominant cause of\nthe asymmetries inferred from these profiles. Instead, it is more plausible\nthat 2003fg-like SNe have aspherical chemical distributions in their inner\nregions. These distributions may come from aspherical progenitor systems, such\nas double white dwarf mergers, or off-center delayed-detonation explosions of\nChandrasekhar-mass Carbon-Oxygen white dwarfs. Additional late-phase NIR\nobservation of 2003fg-like SNe and detailed 3-D NLTE modeling of these two\nexplosion scenarios are encouraged."
                },
                "authors": [
                    {
                        "name": "J. O'Hora"
                    },
                    {
                        "name": "C. Ashall"
                    },
                    {
                        "name": "M. Shahbandeh"
                    },
                    {
                        "name": "E. Hsiao"
                    },
                    {
                        "name": "P. Hoeflich"
                    },
                    {
                        "name": "M. D. Stritzinger"
                    },
                    {
                        "name": "L. Galbany"
                    },
                    {
                        "name": "E. Baron"
                    },
                    {
                        "name": "J. DerKacy"
                    },
                    {
                        "name": "S. Kumar"
                    },
                    {
                        "name": "J. Lu"
                    },
                    {
                        "name": "K. Medler"
                    },
                    {
                        "name": "B. Shappee"
                    }
                ],
                "author_detail": {
                    "name": "B. Shappee"
                },
                "author": "B. Shappee",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09345v1",
                "updated": "2024-12-12T15:12:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    12,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:12:44Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    12,
                    44,
                    3,
                    347,
                    0
                ],
                "title": "Delving into Youth Perspectives on In-game Gambling-like Elements: A\n  Proof-of-Concept Study Utilising Large Language Models for Analysing\n  User-Generated Text Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into Youth Perspectives on In-game Gambling-like Elements: A\n  Proof-of-Concept Study Utilising Large Language Models for Analysing\n  User-Generated Text Data"
                },
                "summary": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content."
                },
                "authors": [
                    {
                        "name": "Thomas Krause"
                    },
                    {
                        "name": "Steffen Otterbach"
                    },
                    {
                        "name": "Johannes Singer"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Singer"
                },
                "author": "Johannes Singer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09849v2",
                "updated": "2024-12-12T14:56:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    56,
                    20,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-19T09:51:02Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    51,
                    2,
                    0,
                    232,
                    0
                ],
                "title": "Importance Weighting Can Help Large Language Models Self-Improve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance Weighting Can Help Large Language Models Self-Improve"
                },
                "summary": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models."
                },
                "authors": [
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Chi-min Chan"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15889v2",
                "updated": "2024-12-12T14:51:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    51,
                    7,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-28T16:02:00Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    2,
                    0,
                    2,
                    241,
                    0
                ],
                "title": "Strongly Interacting Quark Matter in Massive Quark Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strongly Interacting Quark Matter in Massive Quark Stars"
                },
                "summary": "This paper investigates the properties of strongly coupled matter at high\nbaryon densities ($\\rho_B$) in a quark star (QS). The QS is built from the\ndensity-dependent quark mass model (DDQM model), modified (MDDQM model) to\nobtain a higher maximum gravitational mass ($\\rm M_{max}$) of the QS, using the\ndata from observed pulsars: HESS J1731$-$347, PSR J0030$+$0451, PSR\nJ0740$+$6620, and PSR J0952$-$0607 as constraints in Bayesian inference to\ndetermine the model parameters. The parameters yielding a quark matter (QM)\nequation of state that generates $\\rm M_{max} > 2M_\\odot$ violate the\nnear-conformality conditions analyzed at high $\\rho_B$. This behavior is\ninterpreted as a consequence of the increasing quark population with $\\rho_B$,\nalong with the simultaneous formation of colored quark and gluon condensates,\nboth of which are influenced by the pressure build-up in the stellar core as\n$\\rho_B$ rises. This is reflected in the MDDQM model employed, which introduces\nan additional term that becomes significant at high densities. On the other\nhand, parameters that yield $\\rm M_{max} < 2M_\\odot$ conform to the expected\nnear-conformal behavior at higher densities, as analyzed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the properties of strongly coupled matter at high\nbaryon densities ($\\rho_B$) in a quark star (QS). The QS is built from the\ndensity-dependent quark mass model (DDQM model), modified (MDDQM model) to\nobtain a higher maximum gravitational mass ($\\rm M_{max}$) of the QS, using the\ndata from observed pulsars: HESS J1731$-$347, PSR J0030$+$0451, PSR\nJ0740$+$6620, and PSR J0952$-$0607 as constraints in Bayesian inference to\ndetermine the model parameters. The parameters yielding a quark matter (QM)\nequation of state that generates $\\rm M_{max} > 2M_\\odot$ violate the\nnear-conformality conditions analyzed at high $\\rho_B$. This behavior is\ninterpreted as a consequence of the increasing quark population with $\\rho_B$,\nalong with the simultaneous formation of colored quark and gluon condensates,\nboth of which are influenced by the pressure build-up in the stellar core as\n$\\rho_B$ rises. This is reflected in the MDDQM model employed, which introduces\nan additional term that becomes significant at high densities. On the other\nhand, parameters that yield $\\rm M_{max} < 2M_\\odot$ conform to the expected\nnear-conformal behavior at higher densities, as analyzed."
                },
                "authors": [
                    {
                        "name": "Adamu Issifu"
                    },
                    {
                        "name": "Franciele M. da Silva"
                    },
                    {
                        "name": "Luis C. N. Santos"
                    },
                    {
                        "name": "Débora P. Menezes"
                    },
                    {
                        "name": "Tobias Frederico"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Frederico"
                },
                "author": "Tobias Frederico",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09318v1",
                "updated": "2024-12-12T14:43:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    3,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T14:43:03Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    3,
                    3,
                    347,
                    0
                ],
                "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction"
                },
                "summary": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Abdellah Fourtassi"
                    }
                ],
                "author_detail": {
                    "name": "Abdellah Fourtassi"
                },
                "author": "Abdellah Fourtassi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09304v1",
                "updated": "2024-12-12T14:21:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    21,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T14:21:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    21,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "Nonparametric estimation of the total treatment effect with multiple\n  outcomes in the presence of terminal events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric estimation of the total treatment effect with multiple\n  outcomes in the presence of terminal events"
                },
                "summary": "As standards of care advance, patients are living longer and once-fatal\ndiseases are becoming manageable. Clinical trials increasingly focus on\nreducing disease burden, which can be quantified by the timing and occurrence\nof multiple non-fatal clinical events. Most existing methods for the analysis\nof multiple event-time data require stringent modeling assumptions that can be\ndifficult to verify empirically, leading to treatment efficacy estimates that\nforego interpretability when the underlying assumptions are not met. Moreover,\nmost existing methods do not appropriately account for informative terminal\nevents, such as premature treatment discontinuation or death, which prevent the\noccurrence of subsequent events. To address these limitations, we derive and\nvalidate estimation and inference procedures for the area under the mean\ncumulative function (AUMCF), an extension of the restricted mean survival time\nto the multiple event-time setting. The AUMCF is nonparametric, clinically\ninterpretable, and properly accounts for terminal competing risks. To enable\ncovariate adjustment, we also develop an augmentation estimator that provides\nefficiency at least equaling, and often exceeding, the unadjusted estimator.\nThe utility and interpretability of the AUMCF are illustrated with extensive\nsimulation studies and through an analysis of multiple heart-failure-related\nendpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST)\nclinical trial. Our open-source R package MCC makes conducting AUMCF analyses\nstraightforward and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As standards of care advance, patients are living longer and once-fatal\ndiseases are becoming manageable. Clinical trials increasingly focus on\nreducing disease burden, which can be quantified by the timing and occurrence\nof multiple non-fatal clinical events. Most existing methods for the analysis\nof multiple event-time data require stringent modeling assumptions that can be\ndifficult to verify empirically, leading to treatment efficacy estimates that\nforego interpretability when the underlying assumptions are not met. Moreover,\nmost existing methods do not appropriately account for informative terminal\nevents, such as premature treatment discontinuation or death, which prevent the\noccurrence of subsequent events. To address these limitations, we derive and\nvalidate estimation and inference procedures for the area under the mean\ncumulative function (AUMCF), an extension of the restricted mean survival time\nto the multiple event-time setting. The AUMCF is nonparametric, clinically\ninterpretable, and properly accounts for terminal competing risks. To enable\ncovariate adjustment, we also develop an augmentation estimator that provides\nefficiency at least equaling, and often exceeding, the unadjusted estimator.\nThe utility and interpretability of the AUMCF are illustrated with extensive\nsimulation studies and through an analysis of multiple heart-failure-related\nendpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST)\nclinical trial. Our open-source R package MCC makes conducting AUMCF analyses\nstraightforward and accessible."
                },
                "authors": [
                    {
                        "name": "Jessica Gronsbell"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    },
                    {
                        "name": "Isabella-Emmanuella Nogues"
                    },
                    {
                        "name": "Xiangshan Kong"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "LJ Wei"
                    }
                ],
                "author_detail": {
                    "name": "LJ Wei"
                },
                "author": "LJ Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v3",
                "updated": "2024-12-12T14:07:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    7,
                    42,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts."
                },
                "authors": [
                    {
                        "name": "Rainer Muehlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "33 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18446v2",
                "updated": "2024-12-12T13:48:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    48,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-09-27T05:06:43Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    6,
                    43,
                    4,
                    271,
                    0
                ],
                "title": "Exploring Language Model Generalization in Low-Resource Extractive QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Language Model Generalization in Low-Resource Extractive QA"
                },
                "summary": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize to domains\nthat require specific knowledge such as medicine and law in a zero-shot fashion\nwithout additional in-domain training? To this end, we devise a series of\nexperiments to explain the performance gap empirically. Our findings suggest\nthat: (a) LLMs struggle with dataset demands of closed domains such as\nretrieving long answer spans; (b) Certain LLMs, despite showing strong overall\nperformance, display weaknesses in meeting basic requirements as discriminating\nbetween domain-specific senses of words which we link to pre-processing\ndecisions; (c) Scaling model parameters is not always effective for cross\ndomain generalization; and (d) Closed-domain datasets are quantitatively much\ndifferent than open-domain EQA datasets and current LLMs struggle to deal with\nthem. Our findings point out important directions for improving existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize to domains\nthat require specific knowledge such as medicine and law in a zero-shot fashion\nwithout additional in-domain training? To this end, we devise a series of\nexperiments to explain the performance gap empirically. Our findings suggest\nthat: (a) LLMs struggle with dataset demands of closed domains such as\nretrieving long answer spans; (b) Certain LLMs, despite showing strong overall\nperformance, display weaknesses in meeting basic requirements as discriminating\nbetween domain-specific senses of words which we link to pre-processing\ndecisions; (c) Scaling model parameters is not always effective for cross\ndomain generalization; and (d) Closed-domain datasets are quantitatively much\ndifferent than open-domain EQA datasets and current LLMs struggle to deal with\nthem. Our findings point out important directions for improving existing LLMs."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09283v1",
                "updated": "2024-12-12T13:48:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    48,
                    40,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:48:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    48,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware\n  Structured Caption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstanceCap: Improving Text-to-Video Generation via Instance-aware\n  Structured Caption"
                },
                "summary": "Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations."
                },
                "authors": [
                    {
                        "name": "Tiehan Fan"
                    },
                    {
                        "name": "Kepan Nan"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Penghao Zhou"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ying Tai"
                    }
                ],
                "author_detail": {
                    "name": "Ying Tai"
                },
                "author": "Ying Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09282v1",
                "updated": "2024-12-12T13:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    45,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:45:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    45,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of\n  LLMs"
                },
                "summary": "Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging multiple codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging multiple codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09280v1",
                "updated": "2024-12-12T13:42:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    42,
                    58,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:42:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    42,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "Learning to Solve Domain-Specific Calculation Problems with\n  Knowledge-Intensive Programs Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Solve Domain-Specific Calculation Problems with\n  Knowledge-Intensive Programs Generator"
                },
                "summary": "Domain Large Language Models (LLMs) are developed for domain-specific tasks\nbased on general LLMs. But it still requires professional knowledge to\nfacilitate the expertise for some domain-specific tasks. In this paper, we\ninvestigate into knowledge-intensive calculation problems. We find that the\nmath problems to be challenging for LLMs, when involving complex\ndomain-specific rules and knowledge documents, rather than simple formulations\nof terminologies. Therefore, we propose a pipeline to solve the domain-specific\ncalculation problems with Knowledge-Intensive Programs Generator more\neffectively, named as KIPG. It generates knowledge-intensive programs according\nto the domain-specific documents. For each query, key variables are extracted,\nthen outcomes which are dependent on domain knowledge are calculated with the\nprograms. By iterative preference alignment, the code generator learns to\nimprove the logic consistency with the domain knowledge. Taking legal domain as\nan example, we have conducted experiments to prove the effectiveness of our\npipeline, and extensive analysis on the modules. We also find that the code\ngenerator is also adaptable to other domains, without training on the new\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Large Language Models (LLMs) are developed for domain-specific tasks\nbased on general LLMs. But it still requires professional knowledge to\nfacilitate the expertise for some domain-specific tasks. In this paper, we\ninvestigate into knowledge-intensive calculation problems. We find that the\nmath problems to be challenging for LLMs, when involving complex\ndomain-specific rules and knowledge documents, rather than simple formulations\nof terminologies. Therefore, we propose a pipeline to solve the domain-specific\ncalculation problems with Knowledge-Intensive Programs Generator more\neffectively, named as KIPG. It generates knowledge-intensive programs according\nto the domain-specific documents. For each query, key variables are extracted,\nthen outcomes which are dependent on domain knowledge are calculated with the\nprograms. By iterative preference alignment, the code generator learns to\nimprove the logic consistency with the domain knowledge. Taking legal domain as\nan example, we have conducted experiments to prove the effectiveness of our\npipeline, and extensive analysis on the modules. We also find that the code\ngenerator is also adaptable to other domains, without training on the new\nknowledge."
                },
                "authors": [
                    {
                        "name": "Chengyuan Liu"
                    },
                    {
                        "name": "Shihang Wang"
                    },
                    {
                        "name": "Lizhi Qing"
                    },
                    {
                        "name": "Jun Lin"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Kun Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Kuang"
                },
                "author": "Kun Kuang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09278v1",
                "updated": "2024-12-12T13:41:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    41,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:41:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    41,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine"
                },
                "summary": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB."
                },
                "authors": [
                    {
                        "name": "Xiaoshuang Huang"
                    },
                    {
                        "name": "Lingdong Shen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Fangxin Shang"
                    },
                    {
                        "name": "Hongxiang Li"
                    },
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Yehui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yehui Yang"
                },
                "author": "Yehui Yang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16995v2",
                "updated": "2024-12-12T13:33:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    33,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2023-10-25T20:48:16Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    20,
                    48,
                    16,
                    2,
                    298,
                    0
                ],
                "title": "TOP-Training: Target-Oriented Pretraining for Medical Extractive\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOP-Training: Target-Oriented Pretraining for Medical Extractive\n  Question Answering"
                },
                "summary": "We study extractive question-answering in the medical domain (Medical-EQA).\nThis problem has two main challenges: (i) domain specificity, as most AI models\nlack necessary domain knowledge, and (ii) extraction-based answering style,\nwhich restricts most autoregressive LLMs due to potential hallucinations. To\nhandle those challenges, we propose TOP-Training, a target-oriented\npre-training paradigm that stands out among all domain adaptation techniques\nwith two desirable features: (i) TOP-Training moves one step further than\npopular domain-oriented fine-tuning since it not only moves closer to the\ntarget domain, but also familiarizes itself with the target dataset, and (ii)\nit does not assume the existence of a large set of unlabeled instances from the\ntarget domain. Specifically, for a target Medical-EQA dataset, we extract its\nentities and leverage large language models (LLMs) to generate synthetic texts\ncontaining those entities; we then demonstrate that pretraining on this\nsynthetic text data yields better performance on the target Medical-EQA\nbenchmarks. Overall, our contributions are threefold: (i) TOP-Training, a new\npretraining technique to effectively adapt LLMs to better solve a target\nproblem, (ii) TOP-Training has a wide application scope because it does not\nrequire the target problem to have a large set of unlabeled data, and (iii) our\nexperiments highlight the limitations of autoregressive LLMs, emphasizing\nTOP-Training as a means to unlock the true potential of bidirectional LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study extractive question-answering in the medical domain (Medical-EQA).\nThis problem has two main challenges: (i) domain specificity, as most AI models\nlack necessary domain knowledge, and (ii) extraction-based answering style,\nwhich restricts most autoregressive LLMs due to potential hallucinations. To\nhandle those challenges, we propose TOP-Training, a target-oriented\npre-training paradigm that stands out among all domain adaptation techniques\nwith two desirable features: (i) TOP-Training moves one step further than\npopular domain-oriented fine-tuning since it not only moves closer to the\ntarget domain, but also familiarizes itself with the target dataset, and (ii)\nit does not assume the existence of a large set of unlabeled instances from the\ntarget domain. Specifically, for a target Medical-EQA dataset, we extract its\nentities and leverage large language models (LLMs) to generate synthetic texts\ncontaining those entities; we then demonstrate that pretraining on this\nsynthetic text data yields better performance on the target Medical-EQA\nbenchmarks. Overall, our contributions are threefold: (i) TOP-Training, a new\npretraining technique to effectively adapt LLMs to better solve a target\nproblem, (ii) TOP-Training has a wide application scope because it does not\nrequire the target problem to have a large set of unlabeled data, and (iii) our\nexperiments highlight the limitations of autoregressive LLMs, emphasizing\nTOP-Training as a means to unlock the true potential of bidirectional LLMs."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Connor Heaton"
                    },
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09269v1",
                "updated": "2024-12-12T13:31:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    31,
                    58,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:31:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    31,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "Towards Understanding the Robustness of LLM-based Evaluations under\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding the Robustness of LLM-based Evaluations under\n  Perturbations"
                },
                "summary": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics."
                },
                "authors": [
                    {
                        "name": "Manav Chaudhary"
                    },
                    {
                        "name": "Harshit Gupta"
                    },
                    {
                        "name": "Savita Bhat"
                    },
                    {
                        "name": "Vasudeva Varma"
                    }
                ],
                "author_detail": {
                    "name": "Vasudeva Varma"
                },
                "author": "Vasudeva Varma",
                "arxiv_comment": "Accepted at ICON 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09265v1",
                "updated": "2024-12-12T13:22:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    22,
                    2,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:22:02Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    22,
                    2,
                    3,
                    347,
                    0
                ],
                "title": "Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation"
                },
                "summary": "Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks."
                },
                "authors": [
                    {
                        "name": "Bofang Jia"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Pengfang Qian"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09263v1",
                "updated": "2024-12-12T13:21:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    21,
                    9,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:21:09Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    21,
                    9,
                    3,
                    347,
                    0
                ],
                "title": "First Train to Generate, then Generate to Train: UnitedSynT5 for\n  Few-Shot NLI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Train to Generate, then Generate to Train: UnitedSynT5 for\n  Few-Shot NLI"
                },
                "summary": "Natural Language Inference (NLI) tasks require identifying the relationship\nbetween sentence pairs, typically classified as entailment, contradiction, or\nneutrality. While the current state-of-the-art (SOTA) model, Entailment\nFew-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural\nLanguage Inference (SNLI) dataset, further advancements are constrained by the\ndataset's limitations. To address this, we propose a novel approach leveraging\nsynthetic data augmentation to enhance dataset diversity and complexity. We\npresent UnitedSynT5, an advanced extension of EFL that leverages a T5-based\ngenerator to synthesize additional premise-hypothesis pairs, which are\nrigorously cleaned and integrated into the training data. These augmented\nexamples are processed within the EFL framework, embedding labels directly into\nhypotheses for consistency. We train a GTR-T5-XL model on this expanded\ndataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset,\n94.01% accuracy on the E-SNLI dataset, and 92.57% accuracy on the MultiNLI\ndataset, surpassing the previous SOTA models. This research demonstrates the\npotential of synthetic data augmentation in improving NLI models, offering a\npath forward for further advancements in natural language understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference (NLI) tasks require identifying the relationship\nbetween sentence pairs, typically classified as entailment, contradiction, or\nneutrality. While the current state-of-the-art (SOTA) model, Entailment\nFew-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural\nLanguage Inference (SNLI) dataset, further advancements are constrained by the\ndataset's limitations. To address this, we propose a novel approach leveraging\nsynthetic data augmentation to enhance dataset diversity and complexity. We\npresent UnitedSynT5, an advanced extension of EFL that leverages a T5-based\ngenerator to synthesize additional premise-hypothesis pairs, which are\nrigorously cleaned and integrated into the training data. These augmented\nexamples are processed within the EFL framework, embedding labels directly into\nhypotheses for consistency. We train a GTR-T5-XL model on this expanded\ndataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset,\n94.01% accuracy on the E-SNLI dataset, and 92.57% accuracy on the MultiNLI\ndataset, surpassing the previous SOTA models. This research demonstrates the\npotential of synthetic data augmentation in improving NLI models, offering a\npath forward for further advancements in natural language understanding tasks."
                },
                "authors": [
                    {
                        "name": "Sourav Banerjee"
                    },
                    {
                        "name": "Anush Mahajan"
                    },
                    {
                        "name": "Ayushi Agarwal"
                    },
                    {
                        "name": "Eishkaran Singh"
                    }
                ],
                "author_detail": {
                    "name": "Eishkaran Singh"
                },
                "author": "Eishkaran Singh",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09261v1",
                "updated": "2024-12-12T13:20:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    20,
                    23,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:20:23Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    20,
                    23,
                    3,
                    347,
                    0
                ],
                "title": "Single-View Graph Contrastive Learning with Soft Neighborhood Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-View Graph Contrastive Learning with Soft Neighborhood Awareness"
                },
                "summary": "Most graph contrastive learning (GCL) methods heavily rely on cross-view\ncontrast, thus facing several concomitant challenges, such as the complexity of\ndesigning effective augmentations, the potential for information loss between\nviews, and increased computational costs. To mitigate reliance on cross-view\ncontrasts, we propose \\ttt{SIGNA}, a novel single-view graph contrastive\nlearning framework. Regarding the inconsistency between structural connection\nand semantic similarity of neighborhoods, we resort to soft neighborhood\nawareness for GCL. Specifically, we leverage dropout to obtain\nstructurally-related yet randomly-noised embedding pairs for neighbors, which\nserve as potential positive samples. At each epoch, the role of partial\nneighbors is switched from positive to negative, leading to probabilistic\nneighborhood contrastive learning effect. Furthermore, we propose a normalized\nJensen-Shannon divergence estimator for a better effect of contrastive\nlearning. Surprisingly, experiments on diverse node-level tasks demonstrate\nthat our simple single-view GCL framework consistently outperforms existing\nmethods by margins of up to 21.74% (PPI). In particular, with soft neighborhood\nawareness, SIGNA can adopt MLPs instead of complicated GCNs as the encoder to\ngenerate representations in transductive learning tasks, thus speeding up its\ninference process by 109 times to 331 times. The source code is available at\nhttps://github.com/sunisfighting/SIGNA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most graph contrastive learning (GCL) methods heavily rely on cross-view\ncontrast, thus facing several concomitant challenges, such as the complexity of\ndesigning effective augmentations, the potential for information loss between\nviews, and increased computational costs. To mitigate reliance on cross-view\ncontrasts, we propose \\ttt{SIGNA}, a novel single-view graph contrastive\nlearning framework. Regarding the inconsistency between structural connection\nand semantic similarity of neighborhoods, we resort to soft neighborhood\nawareness for GCL. Specifically, we leverage dropout to obtain\nstructurally-related yet randomly-noised embedding pairs for neighbors, which\nserve as potential positive samples. At each epoch, the role of partial\nneighbors is switched from positive to negative, leading to probabilistic\nneighborhood contrastive learning effect. Furthermore, we propose a normalized\nJensen-Shannon divergence estimator for a better effect of contrastive\nlearning. Surprisingly, experiments on diverse node-level tasks demonstrate\nthat our simple single-view GCL framework consistently outperforms existing\nmethods by margins of up to 21.74% (PPI). In particular, with soft neighborhood\nawareness, SIGNA can adopt MLPs instead of complicated GCNs as the encoder to\ngenerate representations in transductive learning tasks, thus speeding up its\ninference process by 109 times to 331 times. The source code is available at\nhttps://github.com/sunisfighting/SIGNA."
                },
                "authors": [
                    {
                        "name": "Qingqiang Sun"
                    },
                    {
                        "name": "Chaoqi Chen"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Xubin Zheng"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "arxiv_comment": "Accepted by AAAI2025; full version including appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03108v2",
                "updated": "2024-12-12T13:17:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    17,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T13:56:42Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    56,
                    42,
                    1,
                    310,
                    0
                ],
                "title": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning"
                },
                "summary": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations"
                },
                "authors": [
                    {
                        "name": "Veronika Krauß"
                    },
                    {
                        "name": "Mark McGill"
                    },
                    {
                        "name": "Thomas Kosch"
                    },
                    {
                        "name": "Yolanda Thiel"
                    },
                    {
                        "name": "Dominik Schön"
                    },
                    {
                        "name": "Jan Gugenheimer"
                    }
                ],
                "author_detail": {
                    "name": "Jan Gugenheimer"
                },
                "author": "Jan Gugenheimer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.14112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.14112v2",
                "updated": "2024-12-12T13:15:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    15,
                    42,
                    3,
                    347,
                    0
                ],
                "published": "2023-02-27T19:51:42Z",
                "published_parsed": [
                    2023,
                    2,
                    27,
                    19,
                    51,
                    42,
                    0,
                    58,
                    0
                ],
                "title": "Injectivity of ReLU networks: perspectives from statistical physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injectivity of ReLU networks: perspectives from statistical physics"
                },
                "summary": "When can the input of a ReLU neural network be inferred from its output? In\nother words, when is the network injective? We consider a single layer, $x\n\\mapsto \\mathrm{ReLU}(Wx)$, with a random Gaussian $m \\times n$ matrix $W$, in\na high-dimensional setting where $n, m \\to \\infty$. Recent work connects this\nproblem to spherical integral geometry giving rise to a conjectured sharp\ninjectivity threshold for $\\alpha = \\frac{m}{n}$ by studying the expected Euler\ncharacteristic of a certain random set. We adopt a different perspective and\nshow that injectivity is equivalent to a property of the ground state of the\nspherical perceptron, an important spin glass model in statistical physics. By\nleveraging the (non-rigorous) replica symmetry-breaking theory, we derive\nanalytical equations for the threshold whose solution is at odds with that from\nthe Euler characteristic. Furthermore, we use Gordon's min--max theorem to\nprove that a replica-symmetric upper bound refutes the Euler characteristic\nprediction. Along the way we aim to give a tutorial-style introduction to key\nideas from statistical physics in an effort to make the exposition accessible\nto a broad audience. Our analysis establishes a connection between spin glasses\nand integral geometry but leaves open the problem of explaining the\ndiscrepancies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When can the input of a ReLU neural network be inferred from its output? In\nother words, when is the network injective? We consider a single layer, $x\n\\mapsto \\mathrm{ReLU}(Wx)$, with a random Gaussian $m \\times n$ matrix $W$, in\na high-dimensional setting where $n, m \\to \\infty$. Recent work connects this\nproblem to spherical integral geometry giving rise to a conjectured sharp\ninjectivity threshold for $\\alpha = \\frac{m}{n}$ by studying the expected Euler\ncharacteristic of a certain random set. We adopt a different perspective and\nshow that injectivity is equivalent to a property of the ground state of the\nspherical perceptron, an important spin glass model in statistical physics. By\nleveraging the (non-rigorous) replica symmetry-breaking theory, we derive\nanalytical equations for the threshold whose solution is at odds with that from\nthe Euler characteristic. Furthermore, we use Gordon's min--max theorem to\nprove that a replica-symmetric upper bound refutes the Euler characteristic\nprediction. Along the way we aim to give a tutorial-style introduction to key\nideas from statistical physics in an effort to make the exposition accessible\nto a broad audience. Our analysis establishes a connection between spin glasses\nand integral geometry but leaves open the problem of explaining the\ndiscrepancies."
                },
                "authors": [
                    {
                        "name": "Antoine Maillard"
                    },
                    {
                        "name": "Afonso S. Bandeira"
                    },
                    {
                        "name": "David Belius"
                    },
                    {
                        "name": "Ivan Dokmanić"
                    },
                    {
                        "name": "Shuta Nakajima"
                    }
                ],
                "author_detail": {
                    "name": "Shuta Nakajima"
                },
                "author": "Shuta Nakajima",
                "arxiv_comment": "62 pages ; Changes to match the published version (v2), in particular\n  Appendix A.7 was added, and Appendix G was re-worked as an alternative proof\n  of Theorem 1.8",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.14112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.14112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09250v1",
                "updated": "2024-12-12T13:04:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    4,
                    54,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:04:54Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    4,
                    54,
                    3,
                    347,
                    0
                ],
                "title": "GeLoRA: Geometric Adaptive Ranks For Efficient LoRA Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeLoRA: Geometric Adaptive Ranks For Efficient LoRA Fine-tuning"
                },
                "summary": "Fine-tuning large language models (LLMs) is computationally intensive because\nit requires updating all parameters. Low-Rank Adaptation (LoRA) improves\nefficiency by modifying only a subset of weights but introduces a trade-off\nbetween expressivity and computational cost: lower ranks reduce resources but\nlimit expressiveness, while higher ranks enhance expressivity at increased\ncost. Despite recent advances in adaptive LoRA techniques, existing methods\nfail to provide a theoretical basis for optimizing the trade-off between model\nperformance and efficiency. We propose Geometric Low-Rank Adaptation (GeLoRA),\na novel framework that computes the intrinsic dimensionality of hidden state\nrepresentations to adaptively select LoRA ranks. We demonstrate that the\nintrinsic dimension provides a lower bound for the optimal rank of LoRA\nmatrices, allowing for a principled selection that balances efficiency and\nexpressivity. GeLoRA dynamically adjusts the rank for each layer based on the\nintrinsic dimensionality of its input and output representations, recognizing\nthat not all model parameters equally impact fine-tuning. Empirical validation\non multiple tasks shows that GeLoRA consistently outperforms recent baselines\nwithin the same parameter budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is computationally intensive because\nit requires updating all parameters. Low-Rank Adaptation (LoRA) improves\nefficiency by modifying only a subset of weights but introduces a trade-off\nbetween expressivity and computational cost: lower ranks reduce resources but\nlimit expressiveness, while higher ranks enhance expressivity at increased\ncost. Despite recent advances in adaptive LoRA techniques, existing methods\nfail to provide a theoretical basis for optimizing the trade-off between model\nperformance and efficiency. We propose Geometric Low-Rank Adaptation (GeLoRA),\na novel framework that computes the intrinsic dimensionality of hidden state\nrepresentations to adaptively select LoRA ranks. We demonstrate that the\nintrinsic dimension provides a lower bound for the optimal rank of LoRA\nmatrices, allowing for a principled selection that balances efficiency and\nexpressivity. GeLoRA dynamically adjusts the rank for each layer based on the\nintrinsic dimensionality of its input and output representations, recognizing\nthat not all model parameters equally impact fine-tuning. Empirical validation\non multiple tasks shows that GeLoRA consistently outperforms recent baselines\nwithin the same parameter budget."
                },
                "authors": [
                    {
                        "name": "Abdessalam Ed-dib"
                    },
                    {
                        "name": "Zhanibek Datbayev"
                    },
                    {
                        "name": "Amine Mohamed Aboussalah"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mohamed Aboussalah"
                },
                "author": "Amine Mohamed Aboussalah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09248v1",
                "updated": "2024-12-12T13:00:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    0,
                    50,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:00:50Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    0,
                    50,
                    3,
                    347,
                    0
                ],
                "title": "A Systematic Review of Knowledge Tracing and Large Language Models in\n  Education: Opportunities, Issues, and Future Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Review of Knowledge Tracing and Large Language Models in\n  Education: Opportunities, Issues, and Future Research"
                },
                "summary": "Knowledge Tracing (KT) is a research field that aims to estimate a student's\nknowledge state through learning interactions-a crucial component of\nIntelligent Tutoring Systems (ITSs). Despite significant advancements, no\ncurrent KT models excel in both predictive accuracy and interpretability.\nMeanwhile, Large Language Models (LLMs), pre-trained on vast natural language\ndatasets, have emerged as powerful tools with immense potential in various\neducational applications. This systematic review explores the intersections,\nopportunities, and challenges of combining KT models and LLMs in educational\ncontexts. The review first investigates LLM applications in education,\nincluding their adaptability to domain-specific content and ability to support\npersonalized learning. It then examines the development and current state of KT\nmodels, from traditional to advanced approaches, aiming to uncover potential\nchallenges that LLMs could mitigate. The core of this review focuses on\nintegrating LLMs with KT, exploring three primary functions: addressing general\nconcerns in KT fields, overcoming specific KT model limitations, and performing\nas KT models themselves. Our findings reveal that LLMs can be customized for\nspecific educational tasks through tailor-making techniques such as in-context\nlearning and agent-based approaches, effectively managing complex and\nunbalanced educational data. These models can enhance existing KT models'\nperformance and solve cold-start problems by generating relevant features from\nquestion data. However, both current models depend heavily on structured,\nlimited datasets, missing opportunities to use diverse educational data that\ncould offer deeper insights into individual learners and support various\neducational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tracing (KT) is a research field that aims to estimate a student's\nknowledge state through learning interactions-a crucial component of\nIntelligent Tutoring Systems (ITSs). Despite significant advancements, no\ncurrent KT models excel in both predictive accuracy and interpretability.\nMeanwhile, Large Language Models (LLMs), pre-trained on vast natural language\ndatasets, have emerged as powerful tools with immense potential in various\neducational applications. This systematic review explores the intersections,\nopportunities, and challenges of combining KT models and LLMs in educational\ncontexts. The review first investigates LLM applications in education,\nincluding their adaptability to domain-specific content and ability to support\npersonalized learning. It then examines the development and current state of KT\nmodels, from traditional to advanced approaches, aiming to uncover potential\nchallenges that LLMs could mitigate. The core of this review focuses on\nintegrating LLMs with KT, exploring three primary functions: addressing general\nconcerns in KT fields, overcoming specific KT model limitations, and performing\nas KT models themselves. Our findings reveal that LLMs can be customized for\nspecific educational tasks through tailor-making techniques such as in-context\nlearning and agent-based approaches, effectively managing complex and\nunbalanced educational data. These models can enhance existing KT models'\nperformance and solve cold-start problems by generating relevant features from\nquestion data. However, both current models depend heavily on structured,\nlimited datasets, missing opportunities to use diverse educational data that\ncould offer deeper insights into individual learners and support various\neducational settings."
                },
                "authors": [
                    {
                        "name": "Yongwan Cho"
                    },
                    {
                        "name": "Rabia Emhamed AlMamlook"
                    },
                    {
                        "name": "Tasnim Gharaibeh"
                    }
                ],
                "author_detail": {
                    "name": "Tasnim Gharaibeh"
                },
                "author": "Tasnim Gharaibeh",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04448v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04448v4",
                "updated": "2024-12-12T12:57:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    57,
                    59,
                    3,
                    347,
                    0
                ],
                "published": "2023-11-08T04:19:28Z",
                "published_parsed": [
                    2023,
                    11,
                    8,
                    4,
                    19,
                    28,
                    2,
                    312,
                    0
                ],
                "title": "Boosting Static Resource Leak Detection via LLM-based Resource-Oriented\n  Intention Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Static Resource Leak Detection via LLM-based Resource-Oriented\n  Intention Inference"
                },
                "summary": "Resource leaks, caused by resources not being released after acquisition,\noften lead to performance issues and system crashes. Existing static detection\ntechniques rely on mechanical matching of predefined resource\nacquisition/release APIs and null-checking conditions to find unreleased\nresources, suffering from both (1) false negatives caused by the incompleteness\nof predefined resource acquisition/release APIs and (2) false positives caused\nby the incompleteness of resource reachability validation identification. To\novercome these challenges, we propose InferROI, a novel approach that leverages\nthe exceptional code comprehension capability of large language models (LLMs)\nto directly infer resource-oriented intentions (acquisition, release, and\nreachability validation) in code. InferROI first prompts the LLM to infer\ninvolved intentions for a given code snippet, and then incorporates a two-stage\nstatic analysis approach to check control-flow paths for resource leak\ndetection based on the inferred intentions.\n  We evaluate the effectiveness of InferROI in both resource-oriented intention\ninference and resource leak detection. Experimental results on the DroidLeaks\nand JLeaks datasets demonstrate InferROI achieves promising bug detection rate\n(59.3% and 62.5%) and false alarm rate (18.6% and 19.5%). Compared to three\nindustrial static detectors, InferROI detects 14~45 and 149~485 more bugs in\nDroidLeaks and JLeaks, respectively. When applied to real-world open-source\nprojects, InferROI identifies 29 unknown resource leak bugs (verified by\nauthors), with 7 of them being confirmed by developers. In addition, the\nresults of an ablation study underscores the importance of combining LLM-based\ninference with static analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource leaks, caused by resources not being released after acquisition,\noften lead to performance issues and system crashes. Existing static detection\ntechniques rely on mechanical matching of predefined resource\nacquisition/release APIs and null-checking conditions to find unreleased\nresources, suffering from both (1) false negatives caused by the incompleteness\nof predefined resource acquisition/release APIs and (2) false positives caused\nby the incompleteness of resource reachability validation identification. To\novercome these challenges, we propose InferROI, a novel approach that leverages\nthe exceptional code comprehension capability of large language models (LLMs)\nto directly infer resource-oriented intentions (acquisition, release, and\nreachability validation) in code. InferROI first prompts the LLM to infer\ninvolved intentions for a given code snippet, and then incorporates a two-stage\nstatic analysis approach to check control-flow paths for resource leak\ndetection based on the inferred intentions.\n  We evaluate the effectiveness of InferROI in both resource-oriented intention\ninference and resource leak detection. Experimental results on the DroidLeaks\nand JLeaks datasets demonstrate InferROI achieves promising bug detection rate\n(59.3% and 62.5%) and false alarm rate (18.6% and 19.5%). Compared to three\nindustrial static detectors, InferROI detects 14~45 and 149~485 more bugs in\nDroidLeaks and JLeaks, respectively. When applied to real-world open-source\nprojects, InferROI identifies 29 unknown resource leak bugs (verified by\nauthors), with 7 of them being confirmed by developers. In addition, the\nresults of an ablation study underscores the importance of combining LLM-based\ninference with static analysis."
                },
                "authors": [
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jianan Liu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04448v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04448v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09247v1",
                "updated": "2024-12-12T12:57:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    57,
                    55,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:57:55Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    57,
                    55,
                    3,
                    347,
                    0
                ],
                "title": "Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by\n  Utilizing Generative LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by\n  Utilizing Generative LLMs"
                },
                "summary": "Satire detection is essential for accurately extracting opinions from textual\ndata and combating misinformation online. However, the lack of diverse corpora\nfor satire leads to the problem of stylistic bias which impacts the models'\ndetection performances. This study proposes a debiasing approach for satire\ndetection, focusing on reducing biases in training data by utilizing generative\nlarge language models. The approach is evaluated in both cross-domain (irony\ndetection) and cross-lingual (English) settings. Results show that the\ndebiasing method enhances the robustness and generalizability of the models for\nsatire and irony detection tasks in Turkish and English. However, its impact on\ncausal language models, such as Llama-3.1, is limited. Additionally, this work\ncurates and presents the Turkish Satirical News Dataset with detailed human\nannotations, with case studies on classification, debiasing, and\nexplainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satire detection is essential for accurately extracting opinions from textual\ndata and combating misinformation online. However, the lack of diverse corpora\nfor satire leads to the problem of stylistic bias which impacts the models'\ndetection performances. This study proposes a debiasing approach for satire\ndetection, focusing on reducing biases in training data by utilizing generative\nlarge language models. The approach is evaluated in both cross-domain (irony\ndetection) and cross-lingual (English) settings. Results show that the\ndebiasing method enhances the robustness and generalizability of the models for\nsatire and irony detection tasks in Turkish and English. However, its impact on\ncausal language models, such as Llama-3.1, is limited. Additionally, this work\ncurates and presents the Turkish Satirical News Dataset with detailed human\nannotations, with case studies on classification, debiasing, and\nexplainability."
                },
                "authors": [
                    {
                        "name": "Asli Umay Ozturk"
                    },
                    {
                        "name": "Recep Firat Cekinel"
                    },
                    {
                        "name": "Pinar Karagoz"
                    }
                ],
                "author_detail": {
                    "name": "Pinar Karagoz"
                },
                "author": "Pinar Karagoz",
                "arxiv_comment": "Accepted to BUCC2025 Workshop @COLING2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09243v1",
                "updated": "2024-12-12T12:53:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:53:30Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "title": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns."
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09237v1",
                "updated": "2024-12-12T12:47:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    47,
                    9,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:47:09Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    47,
                    9,
                    3,
                    347,
                    0
                ],
                "title": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user\n  Simulation"
                },
                "summary": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Wu Liu"
                    },
                    {
                        "name": "Xiaoyan Gu"
                    },
                    {
                        "name": "Yong Rui"
                    },
                    {
                        "name": "Xiaodong He"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09232v1",
                "updated": "2024-12-12T12:43:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    43,
                    42,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:43:42Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    43,
                    42,
                    3,
                    347,
                    0
                ],
                "title": "Uplift modeling with continuous treatments: A predict-then-optimize\n  approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uplift modeling with continuous treatments: A predict-then-optimize\n  approach"
                },
                "summary": "The goal of uplift modeling is to recommend actions that optimize specific\noutcomes by determining which entities should receive treatment. One common\napproach involves two steps: first, an inference step that estimates\nconditional average treatment effects (CATEs), and second, an optimization step\nthat ranks entities based on their CATE values and assigns treatment to the top\nk within a given budget. While uplift modeling typically focuses on binary\ntreatments, many real-world applications are characterized by continuous-valued\ntreatments, i.e., a treatment dose. This paper presents a predict-then-optimize\nframework to allow for continuous treatments in uplift modeling. First, in the\ninference step, conditional average dose responses (CADRs) are estimated from\ndata using causal machine learning techniques. Second, in the optimization\nstep, we frame the assignment task of continuous treatments as a\ndose-allocation problem and solve it using integer linear programming (ILP).\nThis approach allows decision-makers to efficiently and effectively allocate\ntreatment doses while balancing resource availability, with the possibility of\nadding extra constraints like fairness considerations or adapting the objective\nfunction to take into account instance-dependent costs and benefits to maximize\nutility. The experiments compare several CADR estimators and illustrate the\ntrade-offs between policy value and fairness, as well as the impact of an\nadapted objective function. This showcases the framework's advantages and\nflexibility across diverse applications in healthcare, lending, and human\nresource management. All code is available on github.com/SimonDeVos/UMCT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of uplift modeling is to recommend actions that optimize specific\noutcomes by determining which entities should receive treatment. One common\napproach involves two steps: first, an inference step that estimates\nconditional average treatment effects (CATEs), and second, an optimization step\nthat ranks entities based on their CATE values and assigns treatment to the top\nk within a given budget. While uplift modeling typically focuses on binary\ntreatments, many real-world applications are characterized by continuous-valued\ntreatments, i.e., a treatment dose. This paper presents a predict-then-optimize\nframework to allow for continuous treatments in uplift modeling. First, in the\ninference step, conditional average dose responses (CADRs) are estimated from\ndata using causal machine learning techniques. Second, in the optimization\nstep, we frame the assignment task of continuous treatments as a\ndose-allocation problem and solve it using integer linear programming (ILP).\nThis approach allows decision-makers to efficiently and effectively allocate\ntreatment doses while balancing resource availability, with the possibility of\nadding extra constraints like fairness considerations or adapting the objective\nfunction to take into account instance-dependent costs and benefits to maximize\nutility. The experiments compare several CADR estimators and illustrate the\ntrade-offs between policy value and fairness, as well as the impact of an\nadapted objective function. This showcases the framework's advantages and\nflexibility across diverse applications in healthcare, lending, and human\nresource management. All code is available on github.com/SimonDeVos/UMCT."
                },
                "authors": [
                    {
                        "name": "Simon De Vos"
                    },
                    {
                        "name": "Christopher Bockel-Rickermann"
                    },
                    {
                        "name": "Stefan Lessmann"
                    },
                    {
                        "name": "Wouter Verbeke"
                    }
                ],
                "author_detail": {
                    "name": "Wouter Verbeke"
                },
                "author": "Wouter Verbeke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11954v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11954v3",
                "updated": "2024-12-12T12:43:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    43,
                    21,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-18T16:51:56Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    16,
                    51,
                    56,
                    0,
                    78,
                    0
                ],
                "title": "Robust Estimation and Inference for Categorical Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Estimation and Inference for Categorical Data"
                },
                "summary": "While there is a rich literature on robust methodologies for contamination in\ncontinuously distributed data, contamination in categorical data is largely\noverlooked. This is regrettable because many datasets are categorical and\noftentimes suffer from contamination. Examples include inattentive responding\nand bot responses in questionnaires or zero-inflated count data. We propose a\nnovel class of contamination-robust estimators of models for categorical data,\ncoined $C$-estimators (``$C$'' for categorical). We show that the countable and\npossibly finite sample space of categorical data results in non-standard\ntheoretical properties. Notably, in contrast to classic robustness theory,\n$C$-estimators can be simultaneously robust \\textit{and} fully efficient at the\npostulated model. In addition, a certain particularly robust specification\nfails to be asymptotically Gaussian at the postulated model, but is\nasymptotically Gaussian in the presence of contamination. We furthermore\npropose a diagnostic test to identify categorical outliers and demonstrate the\nenhanced robustness of $C$-estimators in a simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there is a rich literature on robust methodologies for contamination in\ncontinuously distributed data, contamination in categorical data is largely\noverlooked. This is regrettable because many datasets are categorical and\noftentimes suffer from contamination. Examples include inattentive responding\nand bot responses in questionnaires or zero-inflated count data. We propose a\nnovel class of contamination-robust estimators of models for categorical data,\ncoined $C$-estimators (``$C$'' for categorical). We show that the countable and\npossibly finite sample space of categorical data results in non-standard\ntheoretical properties. Notably, in contrast to classic robustness theory,\n$C$-estimators can be simultaneously robust \\textit{and} fully efficient at the\npostulated model. In addition, a certain particularly robust specification\nfails to be asymptotically Gaussian at the postulated model, but is\nasymptotically Gaussian in the presence of contamination. We furthermore\npropose a diagnostic test to identify categorical outliers and demonstrate the\nenhanced robustness of $C$-estimators in a simulation study."
                },
                "authors": [
                    {
                        "name": "Max Welz"
                    }
                ],
                "author_detail": {
                    "name": "Max Welz"
                },
                "author": "Max Welz",
                "arxiv_comment": "47 pages, 3 figures, 1 table (corrected properties of the matrix V)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11954v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11954v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12456v2",
                "updated": "2024-12-12T12:38:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    38,
                    12,
                    3,
                    347,
                    0
                ],
                "published": "2023-12-16T02:27:00Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    2,
                    27,
                    0,
                    5,
                    350,
                    0
                ],
                "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"
                },
                "summary": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key principle underlying the design of PowerInfer is\nexploiting the high locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution indicates that a\nsmall subset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. The evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance\ncomparable to that of a high-end server-grade A100 GPU, reaching 82% of its\ntoken generation rate on a single consumer-grade RTX 4090 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key principle underlying the design of PowerInfer is\nexploiting the high locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution indicates that a\nsmall subset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. The evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance\ncomparable to that of a high-end server-grade A100 GPU, reaching 82% of its\ntoken generation rate on a single consumer-grade RTX 4090 GPU."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haotong Xie"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00574v2",
                "updated": "2024-12-12T12:37:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    37,
                    32,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-30T03:31:21Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    3,
                    31,
                    21,
                    6,
                    182,
                    0
                ],
                "title": "Humans as Checkerboards: Calibrating Camera Motion Scale for\n  World-Coordinate Human Mesh Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans as Checkerboards: Calibrating Camera Motion Scale for\n  World-Coordinate Human Mesh Recovery"
                },
                "summary": "Accurate camera motion estimation is essential for recovering global human\nmotion in world coordinates from RGB video inputs. SLAM is widely used for\nestimating camera trajectory and point cloud, but monocular SLAM does so only\nup to an unknown scale factor. Previous works estimate the scale factor through\noptimization, but this is unreliable and time-consuming. This paper presents an\noptimization-free scale calibration framework, Human as Checkerboard (HAC). HAC\ninnovatively leverages the human body predicted by human mesh recovery model as\na calibration reference. Specifically, it uses the absolute depth of\nhuman-scene contact joints as references to calibrate the corresponding\nrelative scene depth from SLAM. HAC benefits from geometric priors encoded in\nhuman mesh recovery models to estimate the SLAM scale and achieves precise\nglobal human motion estimation. Simple yet powerful, our method sets a new\nstate-of-the-art performance for global human mesh estimation tasks, reducing\nmotion errors by 50% over prior local-to-global methods while using 100$\\times$\nless inference time than optimization-based methods. Project page:\nhttps://martayang.github.io/HAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate camera motion estimation is essential for recovering global human\nmotion in world coordinates from RGB video inputs. SLAM is widely used for\nestimating camera trajectory and point cloud, but monocular SLAM does so only\nup to an unknown scale factor. Previous works estimate the scale factor through\noptimization, but this is unreliable and time-consuming. This paper presents an\noptimization-free scale calibration framework, Human as Checkerboard (HAC). HAC\ninnovatively leverages the human body predicted by human mesh recovery model as\na calibration reference. Specifically, it uses the absolute depth of\nhuman-scene contact joints as references to calibrate the corresponding\nrelative scene depth from SLAM. HAC benefits from geometric priors encoded in\nhuman mesh recovery models to estimate the SLAM scale and achieves precise\nglobal human motion estimation. Simple yet powerful, our method sets a new\nstate-of-the-art performance for global human mesh estimation tasks, reducing\nmotion errors by 50% over prior local-to-global methods while using 100$\\times$\nless inference time than optimization-based methods. Project page:\nhttps://martayang.github.io/HAC."
                },
                "authors": [
                    {
                        "name": "Fengyuan Yang"
                    },
                    {
                        "name": "Kerui Gu"
                    },
                    {
                        "name": "Ha Linh Nguyen"
                    },
                    {
                        "name": "Tze Ho Elden Tse"
                    },
                    {
                        "name": "Angela Yao"
                    }
                ],
                "author_detail": {
                    "name": "Angela Yao"
                },
                "author": "Angela Yao",
                "arxiv_comment": "13 pages, 11 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14285v2",
                "updated": "2024-12-12T12:12:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    12,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-04-22T15:35:33Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    15,
                    35,
                    33,
                    0,
                    113,
                    0
                ],
                "title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots"
                },
                "summary": "Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/."
                },
                "authors": [
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Trevor McInroe"
                    },
                    {
                        "name": "Adam Jelley"
                    },
                    {
                        "name": "Stefano V. Albrecht"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Amos Storkey"
                    }
                ],
                "author_detail": {
                    "name": "Amos Storkey"
                },
                "author": "Amos Storkey",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16048v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16048v3",
                "updated": "2024-12-12T12:01:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    1,
                    43,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-25T10:13:04Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    10,
                    13,
                    4,
                    6,
                    56,
                    0
                ],
                "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Likely Do LLMs with CoT Mimic Human Reasoning?"
                },
                "summary": "Chain-of-thought emerges as a promising technique for eliciting reasoning\ncapabilities from Large Language Models (LLMs). However, it does not always\nimprove task performance or accurately represent reasoning processes, leaving\nunresolved questions about its usage. In this paper, we diagnose the underlying\nmechanism by comparing the reasoning process of LLMs with humans, using causal\nanalysis to understand the relationships between the problem instruction,\nreasoning, and the answer in LLMs. Our empirical study reveals that LLMs often\ndeviate from the ideal causal chain, resulting in spurious correlations and\npotential consistency errors (inconsistent reasoning and answers). We also\nexamine various factors influencing the causal structure, finding that\nin-context learning with examples strengthens it, while post-training\ntechniques like supervised fine-tuning and reinforcement learning on human\nfeedback weaken it. To our surprise, the causal structure cannot be\nstrengthened by enlarging the model size only, urging research on new\ntechniques. We hope that this preliminary study will shed light on\nunderstanding and improving the reasoning process in LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought emerges as a promising technique for eliciting reasoning\ncapabilities from Large Language Models (LLMs). However, it does not always\nimprove task performance or accurately represent reasoning processes, leaving\nunresolved questions about its usage. In this paper, we diagnose the underlying\nmechanism by comparing the reasoning process of LLMs with humans, using causal\nanalysis to understand the relationships between the problem instruction,\nreasoning, and the answer in LLMs. Our empirical study reveals that LLMs often\ndeviate from the ideal causal chain, resulting in spurious correlations and\npotential consistency errors (inconsistent reasoning and answers). We also\nexamine various factors influencing the causal structure, finding that\nin-context learning with examples strengthens it, while post-training\ntechniques like supervised fine-tuning and reinforcement learning on human\nfeedback weaken it. To our surprise, the causal structure cannot be\nstrengthened by enlarging the model size only, urging research on new\ntechniques. We hope that this preliminary study will shed light on\nunderstanding and improving the reasoning process in LLM."
                },
                "authors": [
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "COLING 2025 Camera Version (8 pages, 3 figures, 18 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16048v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16048v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09191v1",
                "updated": "2024-12-12T11:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    38,
                    46,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T11:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    38,
                    46,
                    3,
                    347,
                    0
                ],
                "title": "RAD: Region-Aware Diffusion Models for Image Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAD: Region-Aware Diffusion Models for Image Inpainting"
                },
                "summary": "Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets."
                },
                "authors": [
                    {
                        "name": "Sora Kim"
                    },
                    {
                        "name": "Sungho Suh"
                    },
                    {
                        "name": "Minsik Lee"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Lee"
                },
                "author": "Minsik Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13516v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13516v5",
                "updated": "2024-12-12T11:29:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    29,
                    32,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-21T03:58:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    58,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models"
                },
                "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xiyu Shi"
                    },
                    {
                        "name": "Kuai Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13516v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13516v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01606v2",
                "updated": "2024-12-12T11:27:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    27,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-03T15:25:47Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    15,
                    25,
                    47,
                    6,
                    308,
                    0
                ],
                "title": "DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with\n  Large Language Models"
                },
                "summary": "The rise of Large Language Models (LLMs) has streamlined frontend interface\ncreation through tools like Vercel's V0, yet surfaced challenges in design\nquality (e.g., accessibility, and usability). Current solutions, often limited\nby their focus, generalisability, or data dependency, fall short in addressing\nthese complexities. Moreover, none of them examine the quality of LLM-generated\nUI design. In this work, we introduce DesignRepair, a novel dual-stream design\nguideline-aware system to examine and repair the UI design quality issues from\nboth code aspect and rendered page aspect. We utilised the mature and popular\nMaterial Design as our knowledge base to guide this process. Specifically, we\nfirst constructed a comprehensive knowledge base encoding Google's Material\nDesign principles into low-level component knowledge base and high-level system\ndesign knowledge base. After that, DesignRepair employs a LLM for the\nextraction of key components and utilizes the Playwright tool for precise page\nanalysis, aligning these with the established knowledge bases. Finally, we\nintegrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4\nto holistically refine and repair frontend code through a strategic divide and\nconquer approach. Our extensive evaluations validated the efficacy and utility\nof our approach, demonstrating significant enhancements in adherence to design\nguidelines, accessibility, and user experience metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has streamlined frontend interface\ncreation through tools like Vercel's V0, yet surfaced challenges in design\nquality (e.g., accessibility, and usability). Current solutions, often limited\nby their focus, generalisability, or data dependency, fall short in addressing\nthese complexities. Moreover, none of them examine the quality of LLM-generated\nUI design. In this work, we introduce DesignRepair, a novel dual-stream design\nguideline-aware system to examine and repair the UI design quality issues from\nboth code aspect and rendered page aspect. We utilised the mature and popular\nMaterial Design as our knowledge base to guide this process. Specifically, we\nfirst constructed a comprehensive knowledge base encoding Google's Material\nDesign principles into low-level component knowledge base and high-level system\ndesign knowledge base. After that, DesignRepair employs a LLM for the\nextraction of key components and utilizes the Playwright tool for precise page\nanalysis, aligning these with the established knowledge bases. Finally, we\nintegrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4\nto holistically refine and repair frontend code through a strategic divide and\nconquer approach. Our extensive evaluations validated the efficacy and utility\nof our approach, demonstrating significant enhancements in adherence to design\nguidelines, accessibility, and user experience metrics."
                },
                "authors": [
                    {
                        "name": "Mingyue Yuan"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Aaron Quigley"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Tianqi Luo"
                    },
                    {
                        "name": "Gelareh Mohammadi"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "2025 IEEE/ACM 47th International Conference on Software Engineering\n  (ICSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09176v1",
                "updated": "2024-12-12T11:06:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    6,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T11:06:36Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    6,
                    36,
                    3,
                    347,
                    0
                ],
                "title": "LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting"
                },
                "summary": "Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has\nshown immense potential in VR content creation due to its high-quality\nrendering and efficient production process. However, existing physics-based\ninteraction systems for 3DGS can only perform simple and non-realistic\nsimulations or demand extensive user input for complex scenes, primarily due to\nthe absence of scene understanding. In this paper, we propose LIVE-GS, a highly\nrealistic interactive VR system powered by LLM. After object-aware GS\nreconstruction, we prompt GPT-4o to analyze the physical properties of objects\nin the scene, which are used to guide physical simulations consistent with real\nphenomena. We also design a GPT-assisted GS inpainting module to fill the\nunseen area covered by manipulative objects. To perform a precise segmentation\nof Gaussian kernels, we propose a feature-mask segmentation strategy. To enable\nrich interaction, we further propose a computationally efficient physical\nsimulation framework through an PBD-based unified interpolation method,\nsupporting various physical forms such as rigid body, soft body, and granular\nmaterials. Our experimental results show that with the help of LLM's\nunderstanding and enhancement of scenes, our VR system can support complex and\nrealistic interactions without additional manual design and annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has\nshown immense potential in VR content creation due to its high-quality\nrendering and efficient production process. However, existing physics-based\ninteraction systems for 3DGS can only perform simple and non-realistic\nsimulations or demand extensive user input for complex scenes, primarily due to\nthe absence of scene understanding. In this paper, we propose LIVE-GS, a highly\nrealistic interactive VR system powered by LLM. After object-aware GS\nreconstruction, we prompt GPT-4o to analyze the physical properties of objects\nin the scene, which are used to guide physical simulations consistent with real\nphenomena. We also design a GPT-assisted GS inpainting module to fill the\nunseen area covered by manipulative objects. To perform a precise segmentation\nof Gaussian kernels, we propose a feature-mask segmentation strategy. To enable\nrich interaction, we further propose a computationally efficient physical\nsimulation framework through an PBD-based unified interpolation method,\nsupporting various physical forms such as rigid body, soft body, and granular\nmaterials. Our experimental results show that with the help of LLM's\nunderstanding and enhancement of scenes, our VR system can support complex and\nrealistic interactions without additional manual design and annotation."
                },
                "authors": [
                    {
                        "name": "Haotian Mao"
                    },
                    {
                        "name": "Zhuoxiong Xu"
                    },
                    {
                        "name": "Siyue Wei"
                    },
                    {
                        "name": "Yule Quan"
                    },
                    {
                        "name": "Nianchen Deng"
                    },
                    {
                        "name": "Xubo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xubo Yang"
                },
                "author": "Xubo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09173v1",
                "updated": "2024-12-12T11:03:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    3,
                    25,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T11:03:25Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    3,
                    25,
                    3,
                    347,
                    0
                ],
                "title": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied\n  Tasks"
                },
                "summary": "Following formatting instructions to generate well-structured content is a\nfundamental yet often unmet capability for large language models (LLMs). To\nstudy this capability, which we refer to as format faithfulness, we present\nFormatBench, a comprehensive format-related benchmark. Compared to previous\nformat-related benchmarks, FormatBench involves a greater variety of tasks in\nterms of application scenes (traditional NLP tasks, creative works, autonomous\nagency tasks), human-LLM interaction styles (single-turn instruction,\nmulti-turn chat), and format types (inclusion, wrapping, length, coding).\nMoreover, each task in FormatBench is attached with a format checker program.\nExtensive experiments on the benchmark reveal that state-of-the-art open- and\nclosed-source LLMs still suffer from severe deficiency in format faithfulness.\nBy virtue of the decidable nature of formats, we propose to Reinforce Format\nFaithfulness (ReFF) to help LLMs generate formatted output as instructed\nwithout compromising general quality. Without any annotated data, ReFF can\nsubstantially improve the format faithfulness rate (e.g., from 21.6% in\noriginal LLaMA3 to 95.0% on caption segmentation task), while keep the general\nquality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with\nlabeled training data, ReFF can simultaneously improve both format faithfulness\n(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from\n47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to\nexplain how ReFF improves both format faithfulness and general quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following formatting instructions to generate well-structured content is a\nfundamental yet often unmet capability for large language models (LLMs). To\nstudy this capability, which we refer to as format faithfulness, we present\nFormatBench, a comprehensive format-related benchmark. Compared to previous\nformat-related benchmarks, FormatBench involves a greater variety of tasks in\nterms of application scenes (traditional NLP tasks, creative works, autonomous\nagency tasks), human-LLM interaction styles (single-turn instruction,\nmulti-turn chat), and format types (inclusion, wrapping, length, coding).\nMoreover, each task in FormatBench is attached with a format checker program.\nExtensive experiments on the benchmark reveal that state-of-the-art open- and\nclosed-source LLMs still suffer from severe deficiency in format faithfulness.\nBy virtue of the decidable nature of formats, we propose to Reinforce Format\nFaithfulness (ReFF) to help LLMs generate formatted output as instructed\nwithout compromising general quality. Without any annotated data, ReFF can\nsubstantially improve the format faithfulness rate (e.g., from 21.6% in\noriginal LLaMA3 to 95.0% on caption segmentation task), while keep the general\nquality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with\nlabeled training data, ReFF can simultaneously improve both format faithfulness\n(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from\n47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to\nexplain how ReFF improves both format faithfulness and general quality."
                },
                "authors": [
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Haoyu Wen"
                    },
                    {
                        "name": "Wei Su"
                    },
                    {
                        "name": "Boao Qian"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09169v1",
                "updated": "2024-12-12T10:59:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    59,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T10:59:44Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    59,
                    44,
                    3,
                    347,
                    0
                ],
                "title": "DECOR:Decomposition and Projection of Text Embeddings for Text-to-Image\n  Customization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DECOR:Decomposition and Projection of Text Embeddings for Text-to-Image\n  Customization"
                },
                "summary": "Text-to-image (T2I) models can effectively capture the content or style of\nreference images to perform high-quality customization. A representative\ntechnique for this is fine-tuning using low-rank adaptations (LoRA), which\nenables efficient model customization with reference images. However,\nfine-tuning with a limited number of reference images often leads to\noverfitting, resulting in issues such as prompt misalignment or content\nleakage. These issues prevent the model from accurately following the input\nprompt or generating undesired objects during inference. To address this\nproblem, we examine the text embeddings that guide the diffusion model during\ninference. This study decomposes the text embedding matrix and conducts a\ncomponent analysis to understand the embedding space geometry and identify the\ncause of overfitting. Based on this, we propose DECOR, which projects text\nembeddings onto a vector space orthogonal to undesired token vectors, thereby\nreducing the influence of unwanted semantics in the text embeddings.\nExperimental results demonstrate that DECOR outperforms state-of-the-art\ncustomization models and achieves Pareto frontier performance across text and\nvisual alignment evaluation metrics. Furthermore, it generates images more\nfaithful to the input prompts, showcasing its effectiveness in addressing\noverfitting and enhancing text-to-image customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models can effectively capture the content or style of\nreference images to perform high-quality customization. A representative\ntechnique for this is fine-tuning using low-rank adaptations (LoRA), which\nenables efficient model customization with reference images. However,\nfine-tuning with a limited number of reference images often leads to\noverfitting, resulting in issues such as prompt misalignment or content\nleakage. These issues prevent the model from accurately following the input\nprompt or generating undesired objects during inference. To address this\nproblem, we examine the text embeddings that guide the diffusion model during\ninference. This study decomposes the text embedding matrix and conducts a\ncomponent analysis to understand the embedding space geometry and identify the\ncause of overfitting. Based on this, we propose DECOR, which projects text\nembeddings onto a vector space orthogonal to undesired token vectors, thereby\nreducing the influence of unwanted semantics in the text embeddings.\nExperimental results demonstrate that DECOR outperforms state-of-the-art\ncustomization models and achieves Pareto frontier performance across text and\nvisual alignment evaluation metrics. Furthermore, it generates images more\nfaithful to the input prompts, showcasing its effectiveness in addressing\noverfitting and enhancing text-to-image customization."
                },
                "authors": [
                    {
                        "name": "Geonhui Jang"
                    },
                    {
                        "name": "Jin-Hwa Kim"
                    },
                    {
                        "name": "Yong-Hyun Park"
                    },
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Gayoung Lee"
                    },
                    {
                        "name": "Yonghyun Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Yonghyun Jeong"
                },
                "author": "Yonghyun Jeong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04108v2",
                "updated": "2024-12-12T10:56:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    56,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-04-05T14:04:07Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    14,
                    4,
                    7,
                    4,
                    96,
                    0
                ],
                "title": "Large language models as oracles for instantiating ontologies with\n  domain-specific knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models as oracles for instantiating ontologies with\n  domain-specific knowledge"
                },
                "summary": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes and properties and (ii) a set of query\ntemplates, our method queries the LLM multiple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nautomatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nExperimentally, our approach achieves a quality metric that is up to five times\nhigher than the state-of-the-art, while reducing erroneous entities and\nrelations by up to ten times. Finally, we provide a SWOT analysis of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes and properties and (ii) a set of query\ntemplates, our method queries the LLM multiple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nautomatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nExperimentally, our approach achieves a quality metric that is up to five times\nhigher than the state-of-the-art, while reducing erroneous entities and\nrelations by up to ten times. Finally, we provide a SWOT analysis of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Giovanni Ciatto"
                    },
                    {
                        "name": "Andrea Agiollo"
                    },
                    {
                        "name": "Matteo Magnini"
                    },
                    {
                        "name": "Andrea Omicini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Omicini"
                },
                "author": "Andrea Omicini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00686v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00686v4",
                "updated": "2024-12-12T10:54:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    54,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-01T15:48:40Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    48,
                    40,
                    3,
                    32,
                    0
                ],
                "title": "Maximum a posteriori testing in statistical inverse problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum a posteriori testing in statistical inverse problems"
                },
                "summary": "This paper is concerned with a Bayesian approach to testing hypotheses in\nstatistical inverse problems. Based on the posterior distribution $\\Pi\n\\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\langle\\varphi,\nu^\\dagger\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.\nThis can be done by the so-called maximum a posteriori test. We provide a\nfrequentistic analysis of this test's properties such as level and power, and\nprove that it is a regularized test in the sense of Kretschmann et al. (2024).\nFurthermore we provide lower bounds for its power under classical spectral\nsource conditions in case of Gaussian priors. Numerical simulations illustrate\nits superior performance both in moderately and severely ill-posed situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is concerned with a Bayesian approach to testing hypotheses in\nstatistical inverse problems. Based on the posterior distribution $\\Pi\n\\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\langle\\varphi,\nu^\\dagger\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.\nThis can be done by the so-called maximum a posteriori test. We provide a\nfrequentistic analysis of this test's properties such as level and power, and\nprove that it is a regularized test in the sense of Kretschmann et al. (2024).\nFurthermore we provide lower bounds for its power under classical spectral\nsource conditions in case of Gaussian priors. Numerical simulations illustrate\nits superior performance both in moderately and severely ill-posed situations."
                },
                "authors": [
                    {
                        "name": "Remo Kretschmann"
                    },
                    {
                        "name": "Frank Werner"
                    }
                ],
                "author_detail": {
                    "name": "Frank Werner"
                },
                "author": "Frank Werner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00686v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00686v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "47A52, 62F15, 62G10, 62G20, 65J20, 65J22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09649v3",
                "updated": "2024-12-12T10:51:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    51,
                    53,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-12T19:09:26Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    19,
                    9,
                    26,
                    4,
                    194,
                    0
                ],
                "title": "VDB-GPDF: Online Gaussian Process Distance Field with VDB Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VDB-GPDF: Online Gaussian Process Distance Field with VDB Structure"
                },
                "summary": "Robots reason about the environment through dedicated representations.\nPopular choices for dense representations exploit Truncated Signed Distance\nFunctions (TSDF) and Octree data structures. However, TSDF provides a\nprojective or non-projective signed distance obtained directly from depth\nmeasurements that overestimate the Euclidean distance. Octrees, despite being\nmemory efficient, require tree traversal and can lead to increased runtime in\nlarge scenarios. Other representations based on the Gaussian Process (GP)\ndistance fields are appealing due to their probabilistic and continuous nature,\nbut the computational complexity is a concern. In this paper, we present an\nonline efficient mapping framework that seamlessly couples GP distance fields\nand the fast-access OpenVDB data structure. The key aspect is a latent Local GP\nSigned Distance Field (L-GPDF) contained in a local VDB structure that allows\nfast queries of the Euclidean distance, surface properties and their\nuncertainties for arbitrary points in the field of view. Probabilistic fusion\nis then performed by merging the inferred values of these points into a global\nVDB structure that is efficiently maintained over time. After fusion, the\nsurface mesh is recovered, and a global GP Signed Distance Field (G-GPDF) is\ngenerated and made available for downstream applications to query accurate\ndistance and gradients. A comparison with the state-of-the-art frameworks shows\nsuperior efficiency and accuracy of the inferred distance field and comparable\nreconstruction performance. https://github.com/UTS-RI/VDB_GPDF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots reason about the environment through dedicated representations.\nPopular choices for dense representations exploit Truncated Signed Distance\nFunctions (TSDF) and Octree data structures. However, TSDF provides a\nprojective or non-projective signed distance obtained directly from depth\nmeasurements that overestimate the Euclidean distance. Octrees, despite being\nmemory efficient, require tree traversal and can lead to increased runtime in\nlarge scenarios. Other representations based on the Gaussian Process (GP)\ndistance fields are appealing due to their probabilistic and continuous nature,\nbut the computational complexity is a concern. In this paper, we present an\nonline efficient mapping framework that seamlessly couples GP distance fields\nand the fast-access OpenVDB data structure. The key aspect is a latent Local GP\nSigned Distance Field (L-GPDF) contained in a local VDB structure that allows\nfast queries of the Euclidean distance, surface properties and their\nuncertainties for arbitrary points in the field of view. Probabilistic fusion\nis then performed by merging the inferred values of these points into a global\nVDB structure that is efficiently maintained over time. After fusion, the\nsurface mesh is recovered, and a global GP Signed Distance Field (G-GPDF) is\ngenerated and made available for downstream applications to query accurate\ndistance and gradients. A comparison with the state-of-the-art frameworks shows\nsuperior efficiency and accuracy of the inferred distance field and comparable\nreconstruction performance. https://github.com/UTS-RI/VDB_GPDF"
                },
                "authors": [
                    {
                        "name": "Lan Wu"
                    },
                    {
                        "name": "Cedric Le Gentil"
                    },
                    {
                        "name": "Teresa Vidal-Calleja"
                    }
                ],
                "author_detail": {
                    "name": "Teresa Vidal-Calleja"
                },
                "author": "Teresa Vidal-Calleja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v1",
                "updated": "2024-12-12T10:50:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications, such as semantic matching, clustering,\nand information retrieval, continue to rely on text embeddings for their\nefficiency and effectiveness. In this survey, we categorize the interplay\nbetween LLMs and text embeddings into three overarching themes: (1)\nLLM-augmented text embedding, enhancing traditional embedding methods with\nLLMs; (2) LLMs as text embedders, utilizing their innate capabilities for\nembedding generation; and (3) Text embedding understanding with LLMs,\nleveraging LLMs to analyze and interpret embeddings. By organizing these\nefforts based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications, such as semantic matching, clustering,\nand information retrieval, continue to rely on text embeddings for their\nefficiency and effectiveness. In this survey, we categorize the interplay\nbetween LLMs and text embeddings into three overarching themes: (1)\nLLM-augmented text embedding, enhancing traditional embedding methods with\nLLMs; (2) LLMs as text embedders, utilizing their innate capabilities for\nembedding generation; and (3) Text embedding understanding with LLMs,\nleveraging LLMs to analyze and interpret embeddings. By organizing these\nefforts based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07604v2",
                "updated": "2024-12-12T10:49:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    49,
                    38,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-10T15:45:22Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    45,
                    22,
                    1,
                    345,
                    0
                ],
                "title": "Nested exemplar latent space models for dimension reduction in dynamic\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested exemplar latent space models for dimension reduction in dynamic\n  networks"
                },
                "summary": "Dynamic latent space models are widely used for characterizing changes in\nnetworks and relational data over time. These models assign to each node latent\nattributes that characterize connectivity with other nodes, with these latent\nattributes dynamically changing over time. Node attributes can be organized as\na three-way tensor with modes corresponding to nodes, latent space dimension,\nand time. Unfortunately, as the number of nodes and time points increases, the\nnumber of elements of this tensor becomes enormous, leading to computational\nand statistical challenges, particularly when data are sparse. We propose a new\napproach for massively reducing dimensionality by expressing the latent node\nattribute tensor as low rank. This leads to an interesting new nested exemplar\nlatent space model, which characterizes the node attribute tensor as dependent\non low-dimensional exemplar traits for each node, weights for each latent space\ndimension, and exemplar curves characterizing time variation. We study\nproperties of this framework, including expressivity, and develop efficient\nBayesian inference algorithms. The approach leads to substantial advantages in\nsimulations and applications to ecological networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic latent space models are widely used for characterizing changes in\nnetworks and relational data over time. These models assign to each node latent\nattributes that characterize connectivity with other nodes, with these latent\nattributes dynamically changing over time. Node attributes can be organized as\na three-way tensor with modes corresponding to nodes, latent space dimension,\nand time. Unfortunately, as the number of nodes and time points increases, the\nnumber of elements of this tensor becomes enormous, leading to computational\nand statistical challenges, particularly when data are sparse. We propose a new\napproach for massively reducing dimensionality by expressing the latent node\nattribute tensor as low rank. This leads to an interesting new nested exemplar\nlatent space model, which characterizes the node attribute tensor as dependent\non low-dimensional exemplar traits for each node, weights for each latent space\ndimension, and exemplar curves characterizing time variation. We study\nproperties of this framework, including expressivity, and develop efficient\nBayesian inference algorithms. The approach leads to substantial advantages in\nsimulations and applications to ecological networks."
                },
                "authors": [
                    {
                        "name": "Jennifer Noelle Kampe"
                    },
                    {
                        "name": "Luca Alessandro Silva"
                    },
                    {
                        "name": "Tomas Roslin"
                    },
                    {
                        "name": "David Brian Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David Brian Dunson"
                },
                "author": "David Brian Dunson",
                "arxiv_comment": "Main 17 pages (including bibliography), 5 figures. With Appendix, 33\n  Pages and 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20612v2",
                "updated": "2024-12-12T10:46:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    46,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-31T03:59:15Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    3,
                    59,
                    15,
                    4,
                    152,
                    0
                ],
                "title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention\n  and FFN Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention\n  and FFN Manipulation"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs."
                },
                "authors": [
                    {
                        "name": "Hanzhang Zhou"
                    },
                    {
                        "name": "Zijian Feng"
                    },
                    {
                        "name": "Zixiao Zhu"
                    },
                    {
                        "name": "Junlang Qian"
                    },
                    {
                        "name": "Kezhi Mao"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Mao"
                },
                "author": "Kezhi Mao",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10510v2",
                "updated": "2024-12-12T10:40:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    40,
                    22,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-15T08:06:37Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    8,
                    6,
                    37,
                    0,
                    197,
                    0
                ],
                "title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription\n  Prediction"
                },
                "summary": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis."
                },
                "authors": [
                    {
                        "name": "Xingzhi Zhou"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Chunhao Li"
                    },
                    {
                        "name": "Yuning Bai"
                    },
                    {
                        "name": "Yulong Xu"
                    },
                    {
                        "name": "Ka Chun Cheung"
                    },
                    {
                        "name": "Simon See"
                    },
                    {
                        "name": "Xinpeng Song"
                    },
                    {
                        "name": "Runshun Zhang"
                    },
                    {
                        "name": "Xuezhong Zhou"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Camera-ready version to be published in BIBM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09502v2",
                "updated": "2024-12-12T10:39:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    39,
                    16,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-14T15:13:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    13,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Golden Noise for Diffusion Models: A Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Golden Noise for Diffusion Models: A Learning Framework"
                },
                "summary": "Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline."
                },
                "authors": [
                    {
                        "name": "Zikai Zhou"
                    },
                    {
                        "name": "Shitong Shao"
                    },
                    {
                        "name": "Lichen Bai"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Zeke Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Xie"
                },
                "author": "Zeke Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13173v2",
                "updated": "2024-12-12T10:22:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    22,
                    37,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-20T10:17:09Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    17,
                    9,
                    2,
                    325,
                    0
                ],
                "title": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems"
                },
                "summary": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models."
                },
                "authors": [
                    {
                        "name": "Hongliu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Hongliu Cao"
                },
                "author": "Hongliu Cao",
                "arxiv_doi": "10.1145/3701551.3703514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03551v2",
                "updated": "2024-12-12T10:15:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    15,
                    41,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-06T08:51:09Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    8,
                    51,
                    9,
                    2,
                    66,
                    0
                ],
                "title": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers"
                },
                "summary": "Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge."
                },
                "authors": [
                    {
                        "name": "Tim Selig"
                    },
                    {
                        "name": "Thomas März"
                    },
                    {
                        "name": "Martin Storath"
                    },
                    {
                        "name": "Andreas Weinmann"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Weinmann"
                },
                "author": "Andreas Weinmann",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09125v1",
                "updated": "2024-12-12T10:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    2,
                    16,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T10:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    2,
                    16,
                    3,
                    347,
                    0
                ],
                "title": "Goal-Driven Query Answering over First- and Second-Order Dependencies\n  with Equality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-Driven Query Answering over First- and Second-Order Dependencies\n  with Equality"
                },
                "summary": "Query answering over data with dependencies plays a central role in most\napplications of dependencies. The problem is commonly solved by using a\nsuitable variant of the chase algorithm to compute a universal model of the\ndependencies and the data and thus explicate all knowledge implicit in the\ndependencies. After this preprocessing step, an arbitrary conjunctive query\nover the dependencies and the data can be answered by evaluating it the\ncomputed universal model. If, however, the query to be answered is fixed and\nknown in advance, computing the universal model is often inefficient as many\ninferences made during this process can be irrelevant to a given query. In such\ncases, a goal-driven approach, which avoids drawing unnecessary inferences,\npromises to be more efficient and thus preferable in practice.\n  In this paper we present what we believe to be the first technique for\ngoal-driven query answering over first- and second-order dependencies with\nequality reasoning. Our technique transforms the input dependencies so that\napplying the chase to the output avoids many inferences that are irrelevant to\nthe query. The transformation proceeds in several steps, which comprise the\nfollowing three novel techniques. First, we present a variant of the\nsingularisation technique by Marnette [60] that is applicable to second-order\ndependencies and that corrects an incompleteness of a related formulation by\nten Cate et al. [74]. Second, we present a relevance analysis technique that\ncan eliminate from the input dependencies that provably do not contribute to\nquery answers. Third, we present a variant of the magic sets algorithm [19]\nthat can handle second-order dependencies with equality reasoning. We also\npresent the results of an extensive empirical evaluation, which show that\ngoal-driven query answering can be orders of magnitude faster than computing\nthe full universal model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query answering over data with dependencies plays a central role in most\napplications of dependencies. The problem is commonly solved by using a\nsuitable variant of the chase algorithm to compute a universal model of the\ndependencies and the data and thus explicate all knowledge implicit in the\ndependencies. After this preprocessing step, an arbitrary conjunctive query\nover the dependencies and the data can be answered by evaluating it the\ncomputed universal model. If, however, the query to be answered is fixed and\nknown in advance, computing the universal model is often inefficient as many\ninferences made during this process can be irrelevant to a given query. In such\ncases, a goal-driven approach, which avoids drawing unnecessary inferences,\npromises to be more efficient and thus preferable in practice.\n  In this paper we present what we believe to be the first technique for\ngoal-driven query answering over first- and second-order dependencies with\nequality reasoning. Our technique transforms the input dependencies so that\napplying the chase to the output avoids many inferences that are irrelevant to\nthe query. The transformation proceeds in several steps, which comprise the\nfollowing three novel techniques. First, we present a variant of the\nsingularisation technique by Marnette [60] that is applicable to second-order\ndependencies and that corrects an incompleteness of a related formulation by\nten Cate et al. [74]. Second, we present a relevance analysis technique that\ncan eliminate from the input dependencies that provably do not contribute to\nquery answers. Third, we present a variant of the magic sets algorithm [19]\nthat can handle second-order dependencies with equality reasoning. We also\npresent the results of an extensive empirical evaluation, which show that\ngoal-driven query answering can be orders of magnitude faster than computing\nthe full universal model."
                },
                "authors": [
                    {
                        "name": "Efthymia Tsamoura"
                    },
                    {
                        "name": "Boris Motik"
                    }
                ],
                "author_detail": {
                    "name": "Boris Motik"
                },
                "author": "Boris Motik",
                "arxiv_comment": "47 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08237v2",
                "updated": "2024-12-12T10:01:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    1,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-11T09:38:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    38,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"
                },
                "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data."
                },
                "authors": [
                    {
                        "name": "Xingchen Song"
                    },
                    {
                        "name": "Mengtao Xing"
                    },
                    {
                        "name": "Changwei Ma"
                    },
                    {
                        "name": "Shengqiang Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Fuping Pan"
                    },
                    {
                        "name": "Dinghao Zhou"
                    },
                    {
                        "name": "Yuekai Zhang"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Zhendong Peng"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09110v1",
                "updated": "2024-12-12T09:42:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    42,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:42:51Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    42,
                    51,
                    3,
                    347,
                    0
                ],
                "title": "OriginPruner: Leveraging Method Origins for Guided Call Graph Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OriginPruner: Leveraging Method Origins for Guided Call Graph Pruning"
                },
                "summary": "Most static program analyses depend on Call Graphs (CGs), including\nreachability of security vulnerabilities. Static CGs ensure soundness through\nover-approximation, which results in inflated sizes and imprecision. Recent\nresearch has employed machine learning (ML) models to prune false edges and\nenhance CG precision. However, these models require real-world programs with\nhigh test coverage to generalize effectively and the inference is expensive. In\nthis paper, we present OriginPruner, a novel call graph pruning technique that\nleverages the method origin, which is where a method signature is first\nintroduced within a class hierarchy. By incorporating insights from a localness\nanalysis that investigated the scope of method interactions into our approach,\nOriginPruner confidently identifies and prunes edges related to these origin\nmethods. Our key findings reveal that (1) dominant origin methods, such as\nIterator.next, significantly impact CG sizes; (2) derivatives of these origin\nmethods are primarily local, enabling safe pruning without affecting downstream\ninter-procedural analyses; (3) OriginPruner achieves a significant reduction in\nCG size while maintaining the soundness of CGs for security applications like\nvulnerability propagation analysis; and (4) OriginPruner introduces minimal\ncomputational overhead. These findings underscore the potential of leveraging\ndomain knowledge about the type system for more effective CG pruning, offering\na promising direction for future work in static program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most static program analyses depend on Call Graphs (CGs), including\nreachability of security vulnerabilities. Static CGs ensure soundness through\nover-approximation, which results in inflated sizes and imprecision. Recent\nresearch has employed machine learning (ML) models to prune false edges and\nenhance CG precision. However, these models require real-world programs with\nhigh test coverage to generalize effectively and the inference is expensive. In\nthis paper, we present OriginPruner, a novel call graph pruning technique that\nleverages the method origin, which is where a method signature is first\nintroduced within a class hierarchy. By incorporating insights from a localness\nanalysis that investigated the scope of method interactions into our approach,\nOriginPruner confidently identifies and prunes edges related to these origin\nmethods. Our key findings reveal that (1) dominant origin methods, such as\nIterator.next, significantly impact CG sizes; (2) derivatives of these origin\nmethods are primarily local, enabling safe pruning without affecting downstream\ninter-procedural analyses; (3) OriginPruner achieves a significant reduction in\nCG size while maintaining the soundness of CGs for security applications like\nvulnerability propagation analysis; and (4) OriginPruner introduces minimal\ncomputational overhead. These findings underscore the potential of leveraging\ndomain knowledge about the type system for more effective CG pruning, offering\na promising direction for future work in static program analysis."
                },
                "authors": [
                    {
                        "name": "Amir M. Mir"
                    },
                    {
                        "name": "Mehdi Keshani"
                    },
                    {
                        "name": "Sebastian Proksch"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Proksch"
                },
                "author": "Sebastian Proksch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17786v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17786v3",
                "updated": "2024-12-12T09:36:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    36,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-01-31T12:31:42Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    12,
                    31,
                    42,
                    2,
                    31,
                    0
                ],
                "title": "A Modular Graph-Native Query Optimization Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular Graph-Native Query Optimization Framework"
                },
                "summary": "Complex Graph Patterns (CGPs), which combine pattern matching with relational\noperations, are widely used in real-world applications. Existing systems rely\non monolithic architectures for CGPs, which restrict their ability to integrate\nmultiple query languages and lack certain advanced optimization techniques.\nTherefore, to address these issues, we introduce GOpt, a modular graph-native\nquery optimization framework with the following features: (1) support for\nqueries in multiple query languages, (2) decoupling execution from specific\ngraph systems, and (3) integration of advanced optimization techniques.\nSpecifically, GOpt offers a high-level interface, GraphIrBuilder, for\nconverting queries from various graph query languages into a unified\nintermediate representation (GIR), thereby streamlining the optimization\nprocess. It also provides a low-level interface, PhysicalSpec, enabling\nbackends to register backend-specific physical operators and cost models.\nMoreover, GOpt employs a graph-native optimizer that encompasses extensive\nheuristic rules, an automatic type inference approach, and cost-based\noptimization techniques tailored for CGPs. Comprehensive experiments show that\nintegrating GOpt significantly boosts performance, with Neo4j achieving an\naverage speedup of 9.2 times (up to 48.6 times), and GraphsScope achieving an\naverage speedup of 33.4 times (up to 78.7 times), on real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex Graph Patterns (CGPs), which combine pattern matching with relational\noperations, are widely used in real-world applications. Existing systems rely\non monolithic architectures for CGPs, which restrict their ability to integrate\nmultiple query languages and lack certain advanced optimization techniques.\nTherefore, to address these issues, we introduce GOpt, a modular graph-native\nquery optimization framework with the following features: (1) support for\nqueries in multiple query languages, (2) decoupling execution from specific\ngraph systems, and (3) integration of advanced optimization techniques.\nSpecifically, GOpt offers a high-level interface, GraphIrBuilder, for\nconverting queries from various graph query languages into a unified\nintermediate representation (GIR), thereby streamlining the optimization\nprocess. It also provides a low-level interface, PhysicalSpec, enabling\nbackends to register backend-specific physical operators and cost models.\nMoreover, GOpt employs a graph-native optimizer that encompasses extensive\nheuristic rules, an automatic type inference approach, and cost-based\noptimization techniques tailored for CGPs. Comprehensive experiments show that\nintegrating GOpt significantly boosts performance, with Neo4j achieving an\naverage speedup of 9.2 times (up to 48.6 times), and GraphsScope achieving an\naverage speedup of 33.4 times (up to 78.7 times), on real-world datasets."
                },
                "authors": [
                    {
                        "name": "Bingqing Lyu"
                    },
                    {
                        "name": "Xiaoli Zhou"
                    },
                    {
                        "name": "Longbin Lai"
                    },
                    {
                        "name": "Yufan Yang"
                    },
                    {
                        "name": "Yunkai Lou"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17786v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17786v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09105v1",
                "updated": "2024-12-12T09:35:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    35,
                    47,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:35:47Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    35,
                    47,
                    3,
                    347,
                    0
                ],
                "title": "ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal\n  Resolution Motion Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal\n  Resolution Motion Estimation"
                },
                "summary": "Event cameras hold significant promise for high-temporal-resolution (HTR)\nmotion estimation. However, estimating event-based HTR optical flow faces two\nkey challenges: the absence of HTR ground-truth data and the intrinsic sparsity\nof event data. Most existing approaches rely on the flow accumulation paradigms\nto indirectly supervise intermediate flows, often resulting in accumulation\nerrors and optimization difficulties. To address these challenges, we propose a\nresidual-based paradigm for estimating HTR optical flow with event data. Our\napproach separates HTR flow estimation into two stages: global linear motion\nestimation and HTR residual flow refinement. The residual paradigm effectively\nmitigates the impacts of event sparsity on optimization and is compatible with\nany LTR algorithm. Next, to address the challenge posed by the absence of HTR\nground truth, we incorporate novel learning strategies. Specifically, we\ninitially employ a shared refiner to estimate the residual flows, enabling both\nLTR supervision and HTR inference. Subsequently, we introduce regional noise to\nsimulate the residual patterns of intermediate flows, facilitating the\nadaptation from LTR supervision to HTR inference. Additionally, we show that\nthe noise-based strategy supports in-domain self-supervised training.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art accuracy in both LTR and HTR metrics, highlighting its\neffectiveness and superiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras hold significant promise for high-temporal-resolution (HTR)\nmotion estimation. However, estimating event-based HTR optical flow faces two\nkey challenges: the absence of HTR ground-truth data and the intrinsic sparsity\nof event data. Most existing approaches rely on the flow accumulation paradigms\nto indirectly supervise intermediate flows, often resulting in accumulation\nerrors and optimization difficulties. To address these challenges, we propose a\nresidual-based paradigm for estimating HTR optical flow with event data. Our\napproach separates HTR flow estimation into two stages: global linear motion\nestimation and HTR residual flow refinement. The residual paradigm effectively\nmitigates the impacts of event sparsity on optimization and is compatible with\nany LTR algorithm. Next, to address the challenge posed by the absence of HTR\nground truth, we incorporate novel learning strategies. Specifically, we\ninitially employ a shared refiner to estimate the residual flows, enabling both\nLTR supervision and HTR inference. Subsequently, we introduce regional noise to\nsimulate the residual patterns of intermediate flows, facilitating the\nadaptation from LTR supervision to HTR inference. Additionally, we show that\nthe noise-based strategy supports in-domain self-supervised training.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art accuracy in both LTR and HTR metrics, highlighting its\neffectiveness and superiority."
                },
                "authors": [
                    {
                        "name": "Qianang Zhou"
                    },
                    {
                        "name": "Zhiyu Zhu"
                    },
                    {
                        "name": "Junhui Hou"
                    },
                    {
                        "name": "Yongjian Deng"
                    },
                    {
                        "name": "Youfu Li"
                    },
                    {
                        "name": "Junlin Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Junlin Xiong"
                },
                "author": "Junlin Xiong",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10865v3",
                "updated": "2024-12-12T09:24:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    24,
                    21,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-16T09:18:36Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    9,
                    18,
                    36,
                    5,
                    76,
                    0
                ],
                "title": "On the Hubble expansion in a Big Bang quantum cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Hubble expansion in a Big Bang quantum cosmology"
                },
                "summary": "The Hubble expansion of the Universe is considered in the classical limit of\na Big Bang quantum cosmology. In an IR-consistent coupling to the the bare\ncosmological constant, we infer a dark energy as a relic of the Big Bang by\nloss of time-translation invariance on a Hubble time-scale. This dark energy is\nidentified with the trace $J$ of the Schouten tensor permitting an analytic\nsolution $H(z)$. Anchored by the {\\em Baryonic Accoustic Oscillations}, $J$CDM\npredicts a Hubble constant $H_0=\\sqrt{6/5}\\,H_0^\\Lambda$ alleviating\n$H_0$-tension between the Local Distance Ladder and $H_0^\\Lambda$ in\n$\\Lambda$CDM, whose dark energy $\\Lambda$ is a constant. Emulated by\n$w(a)\\Lambda$CDM, a CAMB analysis shows a $J$CDM fit to the {\\em Planck} 2018\n$C_l^{TT}$ power spectrum on par with $\\Lambda$CDM with small positive\ncurvature consistent with {\\em Planck}-$\\Lambda$CDM with no extra relativistic\ndegrees of freedom. In late-time cosmology, $J$CDM is also consistent with the\nBAO recently measured by DESI. { $J$CDM offers a novel framework to address\n$H_0$-tension, predicting background quantities consistent with the\nuncertainties in BAO measurements and early-Universe observations.} It predicts\na deceleration parameter $q_0\\simeq-1$, that may be tested with upcoming\nlow-redshift galaxy surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hubble expansion of the Universe is considered in the classical limit of\na Big Bang quantum cosmology. In an IR-consistent coupling to the the bare\ncosmological constant, we infer a dark energy as a relic of the Big Bang by\nloss of time-translation invariance on a Hubble time-scale. This dark energy is\nidentified with the trace $J$ of the Schouten tensor permitting an analytic\nsolution $H(z)$. Anchored by the {\\em Baryonic Accoustic Oscillations}, $J$CDM\npredicts a Hubble constant $H_0=\\sqrt{6/5}\\,H_0^\\Lambda$ alleviating\n$H_0$-tension between the Local Distance Ladder and $H_0^\\Lambda$ in\n$\\Lambda$CDM, whose dark energy $\\Lambda$ is a constant. Emulated by\n$w(a)\\Lambda$CDM, a CAMB analysis shows a $J$CDM fit to the {\\em Planck} 2018\n$C_l^{TT}$ power spectrum on par with $\\Lambda$CDM with small positive\ncurvature consistent with {\\em Planck}-$\\Lambda$CDM with no extra relativistic\ndegrees of freedom. In late-time cosmology, $J$CDM is also consistent with the\nBAO recently measured by DESI. { $J$CDM offers a novel framework to address\n$H_0$-tension, predicting background quantities consistent with the\nuncertainties in BAO measurements and early-Universe observations.} It predicts\na deceleration parameter $q_0\\simeq-1$, that may be tested with upcoming\nlow-redshift galaxy surveys."
                },
                "authors": [
                    {
                        "name": "Maurice H. P. M. van Putten"
                    }
                ],
                "author_detail": {
                    "name": "Maurice H. P. M. van Putten"
                },
                "arxiv_affiliation": "INAF-OAS, Sejong University",
                "author": "Maurice H. P. M. van Putten",
                "arxiv_doi": "10.1016/j.jheap.2024.12.002",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jheap.2024.12.002",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.10865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version of talk at 32nd Texas Symp. Rel. Astroph., Shanghai,\n  Session 7 (2023); updated with a fit to the Planck 2018 power spectrum of the\n  CMB; published in JHEAP, 45, 194-199 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09131v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09131v5",
                "updated": "2024-12-12T09:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    14,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-14T06:49:16Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    6,
                    49,
                    16,
                    3,
                    74,
                    0
                ],
                "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Responses"
                },
                "summary": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including question answering and controlled text generation.\nHowever, studies into their ability to switch between opposite styles of\nresponses in professional domains remain underexplored. This study introduces a\nnovel approach, named ProSwitch, which enables a language model to switch\nbetween professional and non-professional answers, by tuning and evaluating\nthrough the guidance of domain and style knowledge. ProSwitch unfolds in three\nphases: LLM-augmented preparation to collect domain knowledge and QA pairs,\ninstruction tuning to optimize LLMs with multiple levels of knowledge, and\ncomprehensive evaluation to assess both style discrimination and\nreference-based quality of the generated text. Comparative analysis of\nProSwitch against general and specialized LLMs reveals that our approach\noutperforms baselines in switching between professional and non-professional\nresponses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including question answering and controlled text generation.\nHowever, studies into their ability to switch between opposite styles of\nresponses in professional domains remain underexplored. This study introduces a\nnovel approach, named ProSwitch, which enables a language model to switch\nbetween professional and non-professional answers, by tuning and evaluating\nthrough the guidance of domain and style knowledge. ProSwitch unfolds in three\nphases: LLM-augmented preparation to collect domain knowledge and QA pairs,\ninstruction tuning to optimize LLMs with multiple levels of knowledge, and\ncomprehensive evaluation to assess both style discrimination and\nreference-based quality of the generated text. Comparative analysis of\nProSwitch against general and specialized LLMs reveals that our approach\noutperforms baselines in switching between professional and non-professional\nresponses."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Yuyan Chen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Heng Chang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "8 pages main body, 16 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09131v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09131v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09094v1",
                "updated": "2024-12-12T09:22:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    4,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:22:04Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    4,
                    3,
                    347,
                    0
                ],
                "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion"
                },
                "summary": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihai Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Min Peng"
                    }
                ],
                "author_detail": {
                    "name": "Min Peng"
                },
                "author": "Min Peng",
                "arxiv_comment": "COLING 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00062v2",
                "updated": "2024-12-12T09:11:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    11,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-31T08:15:32Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    15,
                    32,
                    3,
                    305,
                    0
                ],
                "title": "Evolving Alignment via Asymmetric Self-Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Alignment via Asymmetric Self-Play"
                },
                "summary": "Current RLHF frameworks for aligning large language models (LLMs) typically\nassume a fixed prompt distribution, which is sub-optimal and limits the\nscalability of alignment and generalizability of models. To address this, we\nintroduce a general open-ended RLHF framework that casts alignment as an\nasymmetric game between two players: (i) a creator that generates increasingly\ninformative prompt distributions using reward signals, and (ii) a solver that\nlearns to produce more preferred responses on prompts produced by the creator.\nThis framework of Evolving Alignment via Asymmetric Self-Play (eva), results in\na simple and efficient approach that can utilize any existing RLHF algorithm\nfor scalable alignment. eva outperforms state-of-the-art methods on widely-used\nbenchmarks, without the need of any additional human crafted prompts.\nSpecifically, eva improves the win rate of Gemma-2-9B-it on Arena-Hard from\n51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7%\nwith SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and\nmatching claude-3-opus. This improvement is persistent even when new human\ncrafted prompts are introduced. Finally, we show eva is effective and robust\nunder various ablation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current RLHF frameworks for aligning large language models (LLMs) typically\nassume a fixed prompt distribution, which is sub-optimal and limits the\nscalability of alignment and generalizability of models. To address this, we\nintroduce a general open-ended RLHF framework that casts alignment as an\nasymmetric game between two players: (i) a creator that generates increasingly\ninformative prompt distributions using reward signals, and (ii) a solver that\nlearns to produce more preferred responses on prompts produced by the creator.\nThis framework of Evolving Alignment via Asymmetric Self-Play (eva), results in\na simple and efficient approach that can utilize any existing RLHF algorithm\nfor scalable alignment. eva outperforms state-of-the-art methods on widely-used\nbenchmarks, without the need of any additional human crafted prompts.\nSpecifically, eva improves the win rate of Gemma-2-9B-it on Arena-Hard from\n51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7%\nwith SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and\nmatching claude-3-opus. This improvement is persistent even when new human\ncrafted prompts are introduced. Finally, we show eva is effective and robust\nunder various ablation settings."
                },
                "authors": [
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Rishabh Joshi"
                    },
                    {
                        "name": "Sarmishta Velury"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Qijun Tan"
                    },
                    {
                        "name": "Yuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Liu"
                },
                "author": "Yuan Liu",
                "arxiv_comment": "35 pages, spotlight @ neurips language gamification workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05958v2",
                "updated": "2024-12-12T09:10:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    10,
                    32,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-08T14:34:30Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    14,
                    34,
                    30,
                    6,
                    343,
                    0
                ],
                "title": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension"
                },
                "summary": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub."
                },
                "authors": [
                    {
                        "name": "Adem Ait"
                    },
                    {
                        "name": "Javier Luis Cánovas Izquierdo"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.15361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.15361v2",
                "updated": "2024-12-12T09:06:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    6,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2023-03-27T16:32:21Z",
                "published_parsed": [
                    2023,
                    3,
                    27,
                    16,
                    32,
                    21,
                    0,
                    86,
                    0
                ],
                "title": "A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts"
                },
                "summary": "Machine learning methods strive to acquire a robust model during the training\nprocess that can effectively generalize to test samples, even in the presence\nof distribution shifts. However, these methods often suffer from performance\ndegradation due to unknown test distributions. Test-time adaptation (TTA), an\nemerging paradigm, has the potential to adapt a pre-trained model to unlabeled\ndata during testing, before making predictions. Recent progress in this\nparadigm has highlighted the significant benefits of using unlabeled data to\ntrain self-adapted models prior to inference. In this survey, we categorize TTA\ninto several distinct groups based on the form of test data, namely, test-time\ndomain adaptation, test-time batch adaptation, and online test-time adaptation.\nFor each category, we provide a comprehensive taxonomy of advanced algorithms\nand discuss various learning scenarios. Furthermore, we analyze relevant\napplications of TTA and discuss open challenges and promising areas for future\nresearch. For a comprehensive list of TTA methods, kindly refer to\n\\url{https://github.com/tim-learn/awesome-test-time-adaptation}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning methods strive to acquire a robust model during the training\nprocess that can effectively generalize to test samples, even in the presence\nof distribution shifts. However, these methods often suffer from performance\ndegradation due to unknown test distributions. Test-time adaptation (TTA), an\nemerging paradigm, has the potential to adapt a pre-trained model to unlabeled\ndata during testing, before making predictions. Recent progress in this\nparadigm has highlighted the significant benefits of using unlabeled data to\ntrain self-adapted models prior to inference. In this survey, we categorize TTA\ninto several distinct groups based on the form of test data, namely, test-time\ndomain adaptation, test-time batch adaptation, and online test-time adaptation.\nFor each category, we provide a comprehensive taxonomy of advanced algorithms\nand discuss various learning scenarios. Furthermore, we analyze relevant\napplications of TTA and discuss open challenges and promising areas for future\nresearch. For a comprehensive list of TTA methods, kindly refer to\n\\url{https://github.com/tim-learn/awesome-test-time-adaptation}."
                },
                "authors": [
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_doi": "10.1007/s11263-024-02181-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11263-024-02181-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.15361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.15361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Discussions, comments, and questions are all welcomed in\n  \\url{https://github.com/tim-learn/awesome-test-time-adaptation}",
                "arxiv_journal_ref": "International Journal of Computer Vision (2024)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v1",
                "updated": "2024-12-12T09:01:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable abilities across various\nlanguage tasks, but solving complex reasoning problems remains a challenge.\nWhile existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)\nenhance reasoning by decomposing problems or structuring prompts, they\ntypically perform a single pass of reasoning and may fail to revisit flawed\npaths, compromising accuracy. To address this, we propose a novel reasoning\nframework called Forest-of-Thought (FoT), which integrates multiple reasoning\ntrees to leverage collective decision-making for solving complex logical\nproblems. FoT utilizes sparse activation strategies to select the most relevant\nreasoning paths, improving both efficiency and accuracy. Additionally, we\nintroduce a dynamic self-correction strategy that enables real-time error\ncorrection and learning from past mistakes, as well as consensus-guided\ndecision making strategies to optimize correctness and computational resources.\nExperimental results demonstrate that the FoT framework, combined with these\nstrategies, significantly enhances the reasoning capabilities of LLMs, enabling\nthem to solve complex tasks with greater precision and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable abilities across various\nlanguage tasks, but solving complex reasoning problems remains a challenge.\nWhile existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)\nenhance reasoning by decomposing problems or structuring prompts, they\ntypically perform a single pass of reasoning and may fail to revisit flawed\npaths, compromising accuracy. To address this, we propose a novel reasoning\nframework called Forest-of-Thought (FoT), which integrates multiple reasoning\ntrees to leverage collective decision-making for solving complex logical\nproblems. FoT utilizes sparse activation strategies to select the most relevant\nreasoning paths, improving both efficiency and accuracy. Additionally, we\nintroduce a dynamic self-correction strategy that enables real-time error\ncorrection and learning from past mistakes, as well as consensus-guided\ndecision making strategies to optimize correctness and computational resources.\nExperimental results demonstrate that the FoT framework, combined with these\nstrategies, significantly enhances the reasoning capabilities of LLMs, enabling\nthem to solve complex tasks with greater precision and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13399v2",
                "updated": "2024-12-12T08:59:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    59,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-22T07:18:27Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    7,
                    18,
                    27,
                    2,
                    143,
                    0
                ],
                "title": "Scalable Bayesian Inference for Bradley--Terry Models with Ties: An\n  Application to Honour Based Abuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Inference for Bradley--Terry Models with Ties: An\n  Application to Honour Based Abuse"
                },
                "summary": "Honour based abuse covers a wide range of family abuse including female\ngenital mutilation and forced marriage. Safeguarding professionals need to\nidentify where abuses are happening in their local community to best support\nthose at risk of these crimes and take preventative action. However, there is\nlittle local data about these kinds of crime. To tackle this problem, we ran\ncomparative judgement surveys to map abuses at local level, where participants\nwhere shown pairs of wards and asked which had a higher rate of honour based\nabuse. In previous comparative judgement studies, participants reported fatigue\nassociated with comparisons between areas with similar levels of abuse.\nAllowing for tied comparisons reduces fatigue, but increase the computational\ncomplexity when fitting the model. We designed an efficient Markov Chain Monte\nCarlo algorithm to fit a model with ties, allowing for a wide range of prior\ndistributions on the model parameters. Working with South Yorkshire Police and\nOxford Against Cutting, we mapped the risk of honour based abuse at community\nlevel in two counties in the UK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honour based abuse covers a wide range of family abuse including female\ngenital mutilation and forced marriage. Safeguarding professionals need to\nidentify where abuses are happening in their local community to best support\nthose at risk of these crimes and take preventative action. However, there is\nlittle local data about these kinds of crime. To tackle this problem, we ran\ncomparative judgement surveys to map abuses at local level, where participants\nwhere shown pairs of wards and asked which had a higher rate of honour based\nabuse. In previous comparative judgement studies, participants reported fatigue\nassociated with comparisons between areas with similar levels of abuse.\nAllowing for tied comparisons reduces fatigue, but increase the computational\ncomplexity when fitting the model. We designed an efficient Markov Chain Monte\nCarlo algorithm to fit a model with ties, allowing for a wide range of prior\ndistributions on the model parameters. Working with South Yorkshire Police and\nOxford Against Cutting, we mapped the risk of honour based abuse at community\nlevel in two counties in the UK."
                },
                "authors": [
                    {
                        "name": "Rowland G Seymour"
                    },
                    {
                        "name": "Fabian Hernandez"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Hernandez"
                },
                "author": "Fabian Hernandez",
                "arxiv_doi": "10.1080/02664763.2024.2436608",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/02664763.2024.2436608",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.13399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Journal of Applied Statistics",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.09618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09618v1",
                "updated": "2024-12-12T18:59:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:48Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    48,
                    3,
                    347,
                    0
                ],
                "title": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via\n  Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via\n  Multimodal LLM"
                },
                "summary": "Significant achievements in personalization of diffusion models have been\nwitnessed. Conventional tuning-free methods mostly encode multiple reference\nimages by averaging their image embeddings as the injection condition, but such\nan image-independent operation cannot perform interaction among images to\ncapture consistent visual elements within multiple references. Although the\ntuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent\nelements within multiple images through the training process, it necessitates\nspecific finetuning for each distinct image group. This paper introduces\nEasyRef, a novel plug-and-play adaptation method that enables diffusion models\nto be conditioned on multiple reference images and the text prompt. To\neffectively exploit consistent visual elements within multiple images, we\nleverage the multi-image comprehension and instruction-following capabilities\nof the multimodal large language model (MLLM), prompting it to capture\nconsistent visual elements based on the instruction. Besides, injecting the\nMLLM's representations into the diffusion process through adapters can easily\ngeneralize to unseen domains, mining the consistent visual elements within\nunseen data. To mitigate computational costs and enhance fine-grained detail\npreservation, we introduce an efficient reference aggregation strategy and a\nprogressive training scheme. Finally, we introduce MRBench, a new\nmulti-reference image generation benchmark. Experimental results demonstrate\nEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based\nmethods like LoRA, achieving superior aesthetic quality and robust zero-shot\ngeneralization across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant achievements in personalization of diffusion models have been\nwitnessed. Conventional tuning-free methods mostly encode multiple reference\nimages by averaging their image embeddings as the injection condition, but such\nan image-independent operation cannot perform interaction among images to\ncapture consistent visual elements within multiple references. Although the\ntuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent\nelements within multiple images through the training process, it necessitates\nspecific finetuning for each distinct image group. This paper introduces\nEasyRef, a novel plug-and-play adaptation method that enables diffusion models\nto be conditioned on multiple reference images and the text prompt. To\neffectively exploit consistent visual elements within multiple images, we\nleverage the multi-image comprehension and instruction-following capabilities\nof the multimodal large language model (MLLM), prompting it to capture\nconsistent visual elements based on the instruction. Besides, injecting the\nMLLM's representations into the diffusion process through adapters can easily\ngeneralize to unseen domains, mining the consistent visual elements within\nunseen data. To mitigate computational costs and enhance fine-grained detail\npreservation, we introduce an efficient reference aggregation strategy and a\nprogressive training scheme. Finally, we introduce MRBench, a new\nmulti-reference image generation benchmark. Experimental results demonstrate\nEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based\nmethods like LoRA, achieving superior aesthetic quality and robust zero-shot\ngeneralization across diverse domains."
                },
                "authors": [
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09604v1",
                "updated": "2024-12-12T18:59:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with\n  Vision Experts and Token Folding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with\n  Vision Experts and Token Folding"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Xiaogang Wang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Jifeng Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Dai"
                },
                "author": "Jifeng Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09601v1",
                "updated": "2024-12-12T18:59:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:59:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "TimeRefine: Temporal Grounding with Time Refining Video LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeRefine: Temporal Grounding with Time Refining Video LLM"
                },
                "summary": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released."
                },
                "authors": [
                    {
                        "name": "Xizi Wang"
                    },
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "David Crandall"
                    }
                ],
                "author_detail": {
                    "name": "David Crandall"
                },
                "author": "David Crandall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09585v1",
                "updated": "2024-12-12T18:55:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    55,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:55:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    55,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\n  Embedding Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\n  Embedding Distillation"
                },
                "summary": "The standard practice for developing contemporary MLLMs is to feed features\nfrom vision encoder(s) into the LLM and train with natural language\nsupervision. In this work, we posit an overlooked opportunity to optimize the\nintermediate LLM representations through a vision perspective (objective),\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\ndistilling knowledge into the LLM's hidden representations from a set of target\nvisual representations. Firstly, we formulate the objective during the\npretraining stage in MLLMs as a coupled optimization of predictive visual\nembedding and next text-token prediction. Secondly, we investigate MLLMs\ntrained solely with natural language supervision and identify a positive\ncorrelation between the quality of visual representations within these models\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\nobserve improved representation quality owing to the embedding optimization.\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\nmulti-encoder baselines, proving our approach's superiority over explicitly\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\nperformance by an average margin of up to 2.5% on various benchmarks, with a\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard practice for developing contemporary MLLMs is to feed features\nfrom vision encoder(s) into the LLM and train with natural language\nsupervision. In this work, we posit an overlooked opportunity to optimize the\nintermediate LLM representations through a vision perspective (objective),\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\ndistilling knowledge into the LLM's hidden representations from a set of target\nvisual representations. Firstly, we formulate the objective during the\npretraining stage in MLLMs as a coupled optimization of predictive visual\nembedding and next text-token prediction. Secondly, we investigate MLLMs\ntrained solely with natural language supervision and identify a positive\ncorrelation between the quality of visual representations within these models\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\nobserve improved representation quality owing to the embedding optimization.\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\nmulti-encoder baselines, proving our approach's superiority over explicitly\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\nperformance by an average margin of up to 2.5% on various benchmarks, with a\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM ."
                },
                "authors": [
                    {
                        "name": "Jitesh Jain"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "arxiv_comment": "Project Page: https://praeclarumjj3.github.io/ola_vlm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09582v1",
                "updated": "2024-12-12T18:54:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    54,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:54:48Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    54,
                    48,
                    3,
                    347,
                    0
                ],
                "title": "Neptune: The Long Orbit to Benchmarking Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neptune: The Long Orbit to Benchmarking Long Video Understanding"
                },
                "summary": "This paper describes a semi-automatic pipeline to generate challenging\nquestion-answer-decoy sets for understanding long videos. Many existing video\ndatasets and models are focused on short clips (10s-30s). While some long video\ndatasets do exist, they can often be solved by powerful image models applied\nper frame (and often to very few frames) in a video, and are usually manually\nannotated at high cost. In order to mitigate both these problems, we propose a\nscalable dataset creation pipeline which leverages large models (VLMs and\nLLMs), to automatically generate dense, time-aligned video captions, as well as\ntough question answer decoy sets for video segments (up to 15 minutes in\nlength). Our dataset Neptune covers a broad range of long video reasoning\nabilities and consists of a subset that emphasizes multimodal reasoning. Since\nexisting metrics for open-ended question answering are either rule-based or may\nrely on proprietary models, we provide a new open source model-based metric GEM\nto score open-ended responses on Neptune. Benchmark evaluations reveal that\nmost current open-source long video models perform poorly on Neptune,\nparticularly on questions testing temporal ordering, counting and state\nchanges. Through Neptune, we aim to spur the development of more advanced\nmodels capable of understanding long videos. The dataset is available at\nhttps://github.com/google-deepmind/neptune",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a semi-automatic pipeline to generate challenging\nquestion-answer-decoy sets for understanding long videos. Many existing video\ndatasets and models are focused on short clips (10s-30s). While some long video\ndatasets do exist, they can often be solved by powerful image models applied\nper frame (and often to very few frames) in a video, and are usually manually\nannotated at high cost. In order to mitigate both these problems, we propose a\nscalable dataset creation pipeline which leverages large models (VLMs and\nLLMs), to automatically generate dense, time-aligned video captions, as well as\ntough question answer decoy sets for video segments (up to 15 minutes in\nlength). Our dataset Neptune covers a broad range of long video reasoning\nabilities and consists of a subset that emphasizes multimodal reasoning. Since\nexisting metrics for open-ended question answering are either rule-based or may\nrely on proprietary models, we provide a new open source model-based metric GEM\nto score open-ended responses on Neptune. Benchmark evaluations reveal that\nmost current open-source long video models perform poorly on Neptune,\nparticularly on questions testing temporal ordering, counting and state\nchanges. Through Neptune, we aim to spur the development of more advanced\nmodels capable of understanding long videos. The dataset is available at\nhttps://github.com/google-deepmind/neptune"
                },
                "authors": [
                    {
                        "name": "Arsha Nagrani"
                    },
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Ramin Mehran"
                    },
                    {
                        "name": "Rachel Hornung"
                    },
                    {
                        "name": "Nitesh Bharadwaj Gundavarapu"
                    },
                    {
                        "name": "Nilpa Jha"
                    },
                    {
                        "name": "Austin Myers"
                    },
                    {
                        "name": "Xingyi Zhou"
                    },
                    {
                        "name": "Boqing Gong"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Mikhail Sirotenko"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Tobias Weyand"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weyand"
                },
                "author": "Tobias Weyand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09572v1",
                "updated": "2024-12-12T18:52:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    52,
                    40,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:52:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    52,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through\n  Diverse Perspectives and Multi-Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through\n  Diverse Perspectives and Multi-Agent Interaction"
                },
                "summary": "Quantifying the uncertainty in the factual parametric knowledge of Large\nLanguage Models (LLMs), especially in a black-box setting, poses a significant\nchallenge. Existing methods, which gauge a model's uncertainty through\nevaluating self-consistency in responses to the original query, do not always\ncapture true uncertainty. Models might respond consistently to the origin query\nwith a wrong answer, yet respond correctly to varied questions from different\nperspectives about the same query, and vice versa. In this paper, we propose a\nnovel method, DiverseAgentEntropy, for evaluating a model's uncertainty using\nmulti-agent interaction under the assumption that if a model is certain, it\nshould consistently recall the answer to the original query across a diverse\ncollection of questions about the same original query. We further implement an\nabstention policy to withhold responses when uncertainty is high. Our method\noffers a more accurate prediction of the model's reliability and further\ndetects hallucinations, outperforming other self-consistency-based methods.\nAdditionally, it demonstrates that existing models often fail to consistently\nretrieve the correct answer to the same query under diverse varied questions\neven when knowing the correct answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the uncertainty in the factual parametric knowledge of Large\nLanguage Models (LLMs), especially in a black-box setting, poses a significant\nchallenge. Existing methods, which gauge a model's uncertainty through\nevaluating self-consistency in responses to the original query, do not always\ncapture true uncertainty. Models might respond consistently to the origin query\nwith a wrong answer, yet respond correctly to varied questions from different\nperspectives about the same query, and vice versa. In this paper, we propose a\nnovel method, DiverseAgentEntropy, for evaluating a model's uncertainty using\nmulti-agent interaction under the assumption that if a model is certain, it\nshould consistently recall the answer to the original query across a diverse\ncollection of questions about the same original query. We further implement an\nabstention policy to withhold responses when uncertainty is high. Our method\noffers a more accurate prediction of the model's reliability and further\ndetects hallucinations, outperforming other self-consistency-based methods.\nAdditionally, it demonstrates that existing models often fail to consistently\nretrieve the correct answer to the same query under diverse varied questions\neven when knowing the correct answer."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Phu Mon Htut"
                    },
                    {
                        "name": "Zheng Qi"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Manuel Mager"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Kishaloy Halder"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09569v1",
                "updated": "2024-12-12T18:51:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    51,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:51:13Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    51,
                    13,
                    3,
                    347,
                    0
                ],
                "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuStRank: Benchmarking LLM Judges for System Ranking"
                },
                "summary": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias."
                },
                "authors": [
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Odellia Boni"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Lilach Eden"
                    },
                    {
                        "name": "Asaf Yehudai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Yehudai"
                },
                "author": "Asaf Yehudai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09565v1",
                "updated": "2024-12-12T18:49:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    53,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:49:53Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    53,
                    3,
                    347,
                    0
                ],
                "title": "Obfuscated Activations Bypass LLM Latent-Space Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obfuscated Activations Bypass LLM Latent-Space Defenses"
                },
                "summary": "Recent latent-space monitoring techniques have shown promise as defenses\nagainst LLM attacks. These defenses act as scanners that seek to detect harmful\nactivations before they lead to undesirable actions. This prompts the question:\nCan models execute harmful behavior via inconspicuous latent states? Here, we\nstudy such obfuscated activations. We show that state-of-the-art latent-space\ndefenses -- including sparse autoencoders, representation probing, and latent\nOOD detection -- are all vulnerable to obfuscated activations. For example,\nagainst probes trained to classify harmfulness, our attacks can often reduce\nrecall from 100% to 0% while retaining a 90% jailbreaking rate. However,\nobfuscation has limits: we find that on a complex task (writing SQL code),\nobfuscation reduces model performance. Together, our results demonstrate that\nneural activations are highly malleable: we can reshape activation patterns in\na variety of ways, often while preserving a network's behavior. This poses a\nfundamental challenge to latent-space defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent latent-space monitoring techniques have shown promise as defenses\nagainst LLM attacks. These defenses act as scanners that seek to detect harmful\nactivations before they lead to undesirable actions. This prompts the question:\nCan models execute harmful behavior via inconspicuous latent states? Here, we\nstudy such obfuscated activations. We show that state-of-the-art latent-space\ndefenses -- including sparse autoencoders, representation probing, and latent\nOOD detection -- are all vulnerable to obfuscated activations. For example,\nagainst probes trained to classify harmfulness, our attacks can often reduce\nrecall from 100% to 0% while retaining a 90% jailbreaking rate. However,\nobfuscation has limits: we find that on a complex task (writing SQL code),\nobfuscation reduces model performance. Together, our results demonstrate that\nneural activations are highly malleable: we can reshape activation patterns in\na variety of ways, often while preserving a network's behavior. This poses a\nfundamental challenge to latent-space defenses."
                },
                "authors": [
                    {
                        "name": "Luke Bailey"
                    },
                    {
                        "name": "Alex Serrano"
                    },
                    {
                        "name": "Abhay Sheshadri"
                    },
                    {
                        "name": "Mikhail Seleznyov"
                    },
                    {
                        "name": "Jordan Taylor"
                    },
                    {
                        "name": "Erik Jenner"
                    },
                    {
                        "name": "Jacob Hilton"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Carlos Guestrin"
                    },
                    {
                        "name": "Scott Emmons"
                    }
                ],
                "author_detail": {
                    "name": "Scott Emmons"
                },
                "author": "Scott Emmons",
                "arxiv_comment": "Project page: https://obfuscated-activations.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09564v1",
                "updated": "2024-12-12T18:49:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:49:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    49,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "Improving the Reliability of Cable Broadband Networks via Proactive\n  Network Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Reliability of Cable Broadband Networks via Proactive\n  Network Maintenance"
                },
                "summary": "Cable broadband networks are one of the few \"last-mile\" broadband\ntechnologies widely available in the U.S. Unfortunately, they have poor\nreliability after decades of deployment. The cable industry proposed a\nframework called Proactive Network Maintenance (PNM) to diagnose the cable\nnetworks. However, there is little public knowledge or systematic study on how\nto use these data to detect and localize cable network problems. Existing tools\nin the public domain have prohibitive high false-positive rates. In this paper,\nwe propose CableMon, the first public-domain system that applies machine\nlearning techniques to PNM data to improve the reliability of cable broadband\nnetworks. CableMon tackles two key challenges faced by cable ISPs: accurately\ndetecting failures, and distinguishing whether a failure occurs within a\nnetwork or at a subscriber's premise. CableMon uses statistical models to\ngenerate features from time series data and uses customer trouble tickets as\nhints to infer abnormal/failure thresholds for these generated features.\nFurther, CableMon employs an unsupervised learning model to group cable devices\nsharing similar anomalous patterns and effectively identify impairments that\noccur inside a cable network and impairments occur at a subscriber's premise,\nas these two different faults require different types of technical personnel to\nrepair them. We use eight months of PNM data and customer trouble tickets from\nan ISP and experimental deployment to evaluate CableMon's performance. Our\nevaluation results show that CableMon can effectively detect and distinguish\nfailures from PNM data and outperforms existing public-domain tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cable broadband networks are one of the few \"last-mile\" broadband\ntechnologies widely available in the U.S. Unfortunately, they have poor\nreliability after decades of deployment. The cable industry proposed a\nframework called Proactive Network Maintenance (PNM) to diagnose the cable\nnetworks. However, there is little public knowledge or systematic study on how\nto use these data to detect and localize cable network problems. Existing tools\nin the public domain have prohibitive high false-positive rates. In this paper,\nwe propose CableMon, the first public-domain system that applies machine\nlearning techniques to PNM data to improve the reliability of cable broadband\nnetworks. CableMon tackles two key challenges faced by cable ISPs: accurately\ndetecting failures, and distinguishing whether a failure occurs within a\nnetwork or at a subscriber's premise. CableMon uses statistical models to\ngenerate features from time series data and uses customer trouble tickets as\nhints to infer abnormal/failure thresholds for these generated features.\nFurther, CableMon employs an unsupervised learning model to group cable devices\nsharing similar anomalous patterns and effectively identify impairments that\noccur inside a cable network and impairments occur at a subscriber's premise,\nas these two different faults require different types of technical personnel to\nrepair them. We use eight months of PNM data and customer trouble tickets from\nan ISP and experimental deployment to evaluate CableMon's performance. Our\nevaluation results show that CableMon can effectively detect and distinguish\nfailures from PNM data and outperforms existing public-domain tools."
                },
                "authors": [
                    {
                        "name": "Jiyao Hu"
                    },
                    {
                        "name": "Zhenyu Zhou"
                    },
                    {
                        "name": "Xiaowei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Yang"
                },
                "author": "Xiaowei Yang",
                "arxiv_comment": "15 pages including reference. Submitted to IEEE/ACM Transactions on\n  Networking. Partly published in NSDI'20, this is the extended version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09563v1",
                "updated": "2024-12-12T18:48:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    48,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:48:51Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    48,
                    51,
                    3,
                    347,
                    0
                ],
                "title": "Does Representation Matter? Exploring Intermediate Layers in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Representation Matter? Exploring Intermediate Layers in Large\n  Language Models"
                },
                "summary": "Understanding what defines a good representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical\napplications. In this paper, we investigate the quality of intermediate\nrepresentations in various LLM architectures, including Transformers and State\nSpace Models (SSMs). We find that intermediate layers often yield more\ninformative representations for downstream tasks than the final layers. To\nmeasure the representation quality, we adapt and apply a suite of metrics -\nsuch as prompt entropy, curvature, and augmentation-invariance - originally\nproposed in other contexts. Our empirical study reveals significant\narchitectural differences, how representations evolve throughout training, and\nhow factors like input randomness and prompt length affect each layer. Notably,\nwe observe a bimodal pattern in the entropy of some intermediate layers and\nconsider potential explanations tied to training data. Overall, our results\nilluminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what defines a good representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical\napplications. In this paper, we investigate the quality of intermediate\nrepresentations in various LLM architectures, including Transformers and State\nSpace Models (SSMs). We find that intermediate layers often yield more\ninformative representations for downstream tasks than the final layers. To\nmeasure the representation quality, we adapt and apply a suite of metrics -\nsuch as prompt entropy, curvature, and augmentation-invariance - originally\nproposed in other contexts. Our empirical study reveals significant\narchitectural differences, how representations evolve throughout training, and\nhow factors like input randomness and prompt length affect each layer. Notably,\nwe observe a bimodal pattern in the entropy of some intermediate layers and\nconsider potential explanations tied to training data. Overall, our results\nilluminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training."
                },
                "authors": [
                    {
                        "name": "Oscar Skean"
                    },
                    {
                        "name": "Md Rifat Arefin"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "arxiv_comment": "Accepted to 2024 NeurIPs Workshop on Machine Learning and Compression",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09560v1",
                "updated": "2024-12-12T18:46:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    46,
                    38,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:46:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    46,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "Foundational Large Language Models for Materials Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models for Materials Research"
                },
                "summary": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems."
                },
                "authors": [
                    {
                        "name": "Vaibhav Mishra"
                    },
                    {
                        "name": "Somaditya Singh"
                    },
                    {
                        "name": "Dhruv Ahlawat"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Vaibhav Bihani"
                    },
                    {
                        "name": "Hargun Singh Grover"
                    },
                    {
                        "name": "Biswajit Mishra"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Mausam"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "author": "N. M. Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20535v2",
                "updated": "2024-12-12T18:45:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    45,
                    33,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-30T23:20:25Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    23,
                    20,
                    25,
                    3,
                    151,
                    0
                ],
                "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning"
                },
                "summary": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent."
                },
                "authors": [
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Zhiyu Zoey Chen"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Linda Ruth Petzold"
                    }
                ],
                "author_detail": {
                    "name": "Linda Ruth Petzold"
                },
                "author": "Linda Ruth Petzold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09529v1",
                "updated": "2024-12-12T18:20:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    20,
                    16,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T18:20:16Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    20,
                    16,
                    3,
                    347,
                    0
                ],
                "title": "Can Modern LLMs Act as Agent Cores in Radiology~Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Modern LLMs Act as Agent Cores in Radiology~Environments?"
                },
                "summary": "Advancements in large language models (LLMs) have paved the way for LLM-based\nagent systems that offer enhanced accuracy and interpretability across various\ndomains. Radiology, with its complex analytical requirements, is an ideal field\nfor the application of these agents. This paper aims to investigate the\npre-requisite question for building concrete radiology agents which is, `Can\nmodern LLMs act as agent cores in radiology environments?' To investigate it,\nwe introduce RadABench with three-fold contributions: First, we present\nRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based\nagents, generated from an extensive taxonomy encompassing 6 anatomies, 5\nimaging modalities, 10 tool categories, and 11 radiology tasks. Second, we\npropose RadABench-EvalPlat, a novel evaluation platform for agents featuring a\nprompt-driven workflow and the capability to simulate a wide range of radiology\ntoolsets. Third, we assess the performance of 7 leading LLMs on our benchmark\nfrom 5 perspectives with multiple metrics. Our findings indicate that while\ncurrent LLMs demonstrate strong capabilities in many areas, they are still not\nsufficiently advanced to serve as the central agent core in a fully operational\nradiology agent system. Additionally, we identify key factors influencing the\nperformance of LLM-based agent cores, offering insights for clinicians on how\nto apply agent systems in real-world radiology practices effectively. All of\nour code and data are open-sourced in\nhttps://github.com/MAGIC-AI4Med/RadABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have paved the way for LLM-based\nagent systems that offer enhanced accuracy and interpretability across various\ndomains. Radiology, with its complex analytical requirements, is an ideal field\nfor the application of these agents. This paper aims to investigate the\npre-requisite question for building concrete radiology agents which is, `Can\nmodern LLMs act as agent cores in radiology environments?' To investigate it,\nwe introduce RadABench with three-fold contributions: First, we present\nRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based\nagents, generated from an extensive taxonomy encompassing 6 anatomies, 5\nimaging modalities, 10 tool categories, and 11 radiology tasks. Second, we\npropose RadABench-EvalPlat, a novel evaluation platform for agents featuring a\nprompt-driven workflow and the capability to simulate a wide range of radiology\ntoolsets. Third, we assess the performance of 7 leading LLMs on our benchmark\nfrom 5 perspectives with multiple metrics. Our findings indicate that while\ncurrent LLMs demonstrate strong capabilities in many areas, they are still not\nsufficiently advanced to serve as the central agent core in a fully operational\nradiology agent system. Additionally, we identify key factors influencing the\nperformance of LLM-based agent cores, offering insights for clinicians on how\nto apply agent systems in real-world radiology practices effectively. All of\nour code and data are open-sourced in\nhttps://github.com/MAGIC-AI4Med/RadABench."
                },
                "authors": [
                    {
                        "name": "Qiaoyu Zheng"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Lisong Dai"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_comment": "22 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04332v2",
                "updated": "2024-12-12T18:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    8,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-05T16:48:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Liquid: Language Models are Scalable Multi-modal Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid: Language Models are Scalable Multi-modal Generators"
                },
                "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid."
                },
                "authors": [
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Chuofan Ma"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Song Bai"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Technical report. Project page:\n  https://github.com/FoundationVision/Liquid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09486v1",
                "updated": "2024-12-12T17:35:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    35,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:35:36Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    35,
                    36,
                    3,
                    347,
                    0
                ],
                "title": "Regression and Classification with Single-Qubit Quantum Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression and Classification with Single-Qubit Quantum Neural Networks"
                },
                "summary": "Since classical machine learning has become a powerful tool for developing\ndata-driven algorithms, quantum machine learning is expected to similarly\nimpact the development of quantum algorithms. The literature reflects a\nmutually beneficial relationship between machine learning and quantum\ncomputing, where progress in one field frequently drives improvements in the\nother. Motivated by the fertile connection between machine learning and quantum\ncomputing enabled by parameterized quantum circuits, we use a\nresource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for\nboth regression and classification tasks. The SQQNN leverages parameterized\nsingle-qubit unitary operators and quantum measurements to achieve efficient\nlearning. To train the model, we use gradient descent for regression tasks. For\nclassification, we introduce a novel training method inspired by the Taylor\nseries, which can efficiently find a global minimum in a single step. This\napproach significantly accelerates training compared to iterative methods.\nEvaluated across various applications, the SQQNN exhibits virtually error-free\nand strong performance in regression and classification tasks, including the\nMNIST dataset. These results demonstrate the versatility, scalability, and\nsuitability of the SQQNN for deployment on near-term quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since classical machine learning has become a powerful tool for developing\ndata-driven algorithms, quantum machine learning is expected to similarly\nimpact the development of quantum algorithms. The literature reflects a\nmutually beneficial relationship between machine learning and quantum\ncomputing, where progress in one field frequently drives improvements in the\nother. Motivated by the fertile connection between machine learning and quantum\ncomputing enabled by parameterized quantum circuits, we use a\nresource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for\nboth regression and classification tasks. The SQQNN leverages parameterized\nsingle-qubit unitary operators and quantum measurements to achieve efficient\nlearning. To train the model, we use gradient descent for regression tasks. For\nclassification, we introduce a novel training method inspired by the Taylor\nseries, which can efficiently find a global minimum in a single step. This\napproach significantly accelerates training compared to iterative methods.\nEvaluated across various applications, the SQQNN exhibits virtually error-free\nand strong performance in regression and classification tasks, including the\nMNIST dataset. These results demonstrate the versatility, scalability, and\nsuitability of the SQQNN for deployment on near-term quantum devices."
                },
                "authors": [
                    {
                        "name": "Leandro C. Souza"
                    },
                    {
                        "name": "Bruno C. Guingo"
                    },
                    {
                        "name": "Gilson Giraldi"
                    },
                    {
                        "name": "Renato Portugal"
                    }
                ],
                "author_detail": {
                    "name": "Renato Portugal"
                },
                "author": "Renato Portugal",
                "arxiv_comment": "21 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08268v2",
                "updated": "2024-12-12T17:32:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    32,
                    23,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-11T10:35:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCFO: Long Context and Long Form Output Dataset and Benchmarking"
                },
                "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-jussà"
                    },
                    {
                        "name": "Pierre Andrews"
                    },
                    {
                        "name": "Mariano Coria Meglioli"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "David Dale"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Alexandre Mourachko"
                    },
                    {
                        "name": "Eduardo Sánchez"
                    },
                    {
                        "name": "Holger Schwenk"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Carleigh Wood"
                    }
                ],
                "author_detail": {
                    "name": "Carleigh Wood"
                },
                "author": "Carleigh Wood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01304v2",
                "updated": "2024-12-12T17:26:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    26,
                    4,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-02T20:25:50Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    20,
                    25,
                    50,
                    5,
                    62,
                    0
                ],
                "title": "Improving the Validity of Automatically Generated Feedback via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Validity of Automatically Generated Feedback via\n  Reinforcement Learning"
                },
                "summary": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work."
                },
                "authors": [
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Digory Smith"
                    },
                    {
                        "name": "Simon Woodhead"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_doi": "10.1007/978-3-031-64302-6_20",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-64302-6_20",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.01304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Best student paper award, Published in AIED 2024: The 25th\n  International Conference on Artificial Intelligence in Education",
                "arxiv_journal_ref": "In International Conference on Artificial Intelligence in\n  Education (pp. 280-294). Cham: Springer Nature Switzerland (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v1",
                "updated": "2024-12-12T17:11:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training generative language models\nraises critical legal and ethical questions. This paper presents a framework\nfor and the results of empirically assessing the impact of copyrighted\nmaterials on the performance of large language models (LLMs) for Norwegian. We\nfound that both books and newspapers contribute positively when the models are\nevaluated on a diverse set of Norwegian benchmarks, while fiction works\npossibly lead to decreased performance. Our experiments could inform the\ncreation of a compensation scheme for authors whose works contribute to AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training generative language models\nraises critical legal and ethical questions. This paper presents a framework\nfor and the results of empirically assessing the impact of copyrighted\nmaterials on the performance of large language models (LLMs) for Norwegian. We\nfound that both books and newspapers contribute positively when the models are\nevaluated on a diverse set of Norwegian benchmarks, while fiction works\npossibly lead to decreased performance. Our experiments could inform the\ncreation of a compensation scheme for authors whose works contribute to AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "pre-print, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v4",
                "updated": "2024-12-12T16:59:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    59,
                    55,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09440v1",
                "updated": "2024-12-12T16:56:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    56,
                    1,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:56:01Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    56,
                    1,
                    3,
                    347,
                    0
                ],
                "title": "Learning to Adapt: Bio-Inspired Gait Strategies for Versatile Quadruped\n  Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Adapt: Bio-Inspired Gait Strategies for Versatile Quadruped\n  Locomotion"
                },
                "summary": "Deep reinforcement learning (DRL) has revolutionised quadruped robot\nlocomotion, but existing control frameworks struggle to generalise beyond their\ntraining-induced observational scope, resulting in limited adaptability. In\ncontrast, animals achieve exceptional adaptability through gait transition\nstrategies, diverse gait utilisation, and seamless adjustment to immediate\nenvironmental demands. Inspired by these capabilities, we present a novel DRL\nframework that incorporates key attributes of animal locomotion: gait\ntransition strategies, pseudo gait procedural memory, and adaptive motion\nadjustments. This approach enables our framework to achieve unparalleled\nadaptability, demonstrated through blind zero-shot deployment on complex\nterrains and recovery from critically unstable states. Our findings offer\nvaluable insights into the biomechanics of animal locomotion, paving the way\nfor robust, adaptable robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) has revolutionised quadruped robot\nlocomotion, but existing control frameworks struggle to generalise beyond their\ntraining-induced observational scope, resulting in limited adaptability. In\ncontrast, animals achieve exceptional adaptability through gait transition\nstrategies, diverse gait utilisation, and seamless adjustment to immediate\nenvironmental demands. Inspired by these capabilities, we present a novel DRL\nframework that incorporates key attributes of animal locomotion: gait\ntransition strategies, pseudo gait procedural memory, and adaptive motion\nadjustments. This approach enables our framework to achieve unparalleled\nadaptability, demonstrated through blind zero-shot deployment on complex\nterrains and recovery from critically unstable states. Our findings offer\nvaluable insights into the biomechanics of animal locomotion, paving the way\nfor robust, adaptable robotic systems."
                },
                "authors": [
                    {
                        "name": "Joseph Humphreys"
                    },
                    {
                        "name": "Chengxu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chengxu Zhou"
                },
                "author": "Chengxu Zhou",
                "arxiv_comment": "15 pages, 8 figures, journal paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09429v1",
                "updated": "2024-12-12T16:35:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    35,
                    5,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:35:05Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    35,
                    5,
                    3,
                    347,
                    0
                ],
                "title": "From Intention To Implementation: Automating Biomedical Research via\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Intention To Implementation: Automating Biomedical Research via\n  LLMs"
                },
                "summary": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Linghang Shi"
                    },
                    {
                        "name": "Yihao Li"
                    },
                    {
                        "name": "Aobo Zhuang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Lin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lin Chen"
                },
                "author": "Lin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09413v1",
                "updated": "2024-12-12T16:20:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    20,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:20:36Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    20,
                    36,
                    3,
                    347,
                    0
                ],
                "title": "Imitate, Explore, and Self-Improve: A Reproduction Report on\n  Slow-thinking Reasoning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitate, Explore, and Self-Improve: A Reproduction Report on\n  Slow-thinking Reasoning Systems"
                },
                "summary": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an \"imitate, explore, and self-improve\" framework as our\nprimary technical approach to train the reasoning model. In the initial phase,\nwe use distilled long-form thought data to fine-tune the reasoning model,\nenabling it to invoke a slow-thinking mode. The model is then encouraged to\nexplore challenging problems by generating multiple rollouts, which can result\nin increasingly more high-quality trajectories that lead to correct answers.\nFurthermore, the model undergoes self-improvement by iteratively refining its\ntraining dataset. To verify the effectiveness of this approach, we conduct\nextensive experiments on three challenging benchmarks. The experimental results\ndemonstrate that our approach achieves competitive performance compared to\nindustry-level reasoning systems on these benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an \"imitate, explore, and self-improve\" framework as our\nprimary technical approach to train the reasoning model. In the initial phase,\nwe use distilled long-form thought data to fine-tune the reasoning model,\nenabling it to invoke a slow-thinking mode. The model is then encouraged to\nexplore challenging problems by generating multiple rollouts, which can result\nin increasingly more high-quality trajectories that lead to correct answers.\nFurthermore, the model undergoes self-improvement by iteratively refining its\ntraining dataset. To verify the effectiveness of this approach, we conduct\nextensive experiments on three challenging benchmarks. The experimental results\ndemonstrate that our approach achieves competitive performance compared to\nindustry-level reasoning systems on these benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Part II",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18564v2",
                "updated": "2024-12-12T16:03:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    3,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-27T18:04:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    4,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems."
                },
                "authors": [
                    {
                        "name": "Rong Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Jonas Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kuhn"
                },
                "author": "Jonas Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09394v1",
                "updated": "2024-12-12T15:59:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    59,
                    58,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:59:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    59,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage"
                },
                "summary": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities."
                },
                "authors": [
                    {
                        "name": "Sebastien Valeyre"
                    },
                    {
                        "name": "Sofiane Aboura"
                    }
                ],
                "author_detail": {
                    "name": "Sofiane Aboura"
                },
                "author": "Sofiane Aboura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09385v1",
                "updated": "2024-12-12T15:52:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    52,
                    41,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    52,
                    41,
                    3,
                    347,
                    0
                ],
                "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore\n  LLMs' Complex Reasoning Capabilities"
                },
                "summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Fabrizio Davide"
                    },
                    {
                        "name": "Pietro Torre"
                    },
                    {
                        "name": "Andrea Gaggioli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Gaggioli"
                },
                "author": "Andrea Gaggioli",
                "arxiv_comment": "47 pages, 8 figures, 17 tables, appendix with data and code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22296v3",
                "updated": "2024-12-12T15:48:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    48,
                    47,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-29T17:45:57Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "title": "LLMs are Highly-Constrained Biophysical Sequence Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Highly-Constrained Biophysical Sequence Optimizers"
                },
                "summary": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Samuel D. Stanton"
                    },
                    {
                        "name": "Robert G. Alberstein"
                    },
                    {
                        "name": "Andrew M. Watkins"
                    },
                    {
                        "name": "Richard Bonneau"
                    },
                    {
                        "name": "Vladimir Gligorijević"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Nathan C. Frey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan C. Frey"
                },
                "author": "Nathan C. Frey",
                "arxiv_comment": "Supercedes arXiv:2407.00236v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09345v1",
                "updated": "2024-12-12T15:12:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    12,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T15:12:44Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    12,
                    44,
                    3,
                    347,
                    0
                ],
                "title": "Delving into Youth Perspectives on In-game Gambling-like Elements: A\n  Proof-of-Concept Study Utilising Large Language Models for Analysing\n  User-Generated Text Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into Youth Perspectives on In-game Gambling-like Elements: A\n  Proof-of-Concept Study Utilising Large Language Models for Analysing\n  User-Generated Text Data"
                },
                "summary": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content."
                },
                "authors": [
                    {
                        "name": "Thomas Krause"
                    },
                    {
                        "name": "Steffen Otterbach"
                    },
                    {
                        "name": "Johannes Singer"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Singer"
                },
                "author": "Johannes Singer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09849v2",
                "updated": "2024-12-12T14:56:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    56,
                    20,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-19T09:51:02Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    51,
                    2,
                    0,
                    232,
                    0
                ],
                "title": "Importance Weighting Can Help Large Language Models Self-Improve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance Weighting Can Help Large Language Models Self-Improve"
                },
                "summary": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models."
                },
                "authors": [
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Chi-min Chan"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09318v1",
                "updated": "2024-12-12T14:43:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    3,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T14:43:03Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    3,
                    3,
                    347,
                    0
                ],
                "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction"
                },
                "summary": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Abdellah Fourtassi"
                    }
                ],
                "author_detail": {
                    "name": "Abdellah Fourtassi"
                },
                "author": "Abdellah Fourtassi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00727v2",
                "updated": "2024-12-12T14:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    28,
                    42,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-01T08:39:12Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    8,
                    39,
                    12,
                    6,
                    336,
                    0
                ],
                "title": "Perturb and Recover: Fine-tuning for Effective Backdoor Removal from\n  CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturb and Recover: Fine-tuning for Effective Backdoor Removal from\n  CLIP"
                },
                "summary": "Vision-Language models like CLIP have been shown to be highly effective at\nlinking visual perception and natural language understanding, enabling\nsophisticated image-text capabilities, including strong retrieval and zero-shot\nclassification performance. Their widespread use, as well as the fact that CLIP\nmodels are trained on image-text pairs from the web, make them both a\nworthwhile and relatively easy target for backdoor attacks. As training\nfoundational models, such as CLIP, from scratch is very expensive, this paper\nfocuses on cleaning potentially poisoned models via fine-tuning. We first show\nthat existing cleaning techniques are not effective against simple structured\ntriggers used in Blended or BadNet backdoor attacks, exposing a critical\nvulnerability for potential real-world deployment of these models. Then, we\nintroduce PAR, Perturb and Recover, a surprisingly simple yet effective\nmechanism to remove backdoors from CLIP models. Through extensive experiments\nacross different encoders and types of backdoor attacks, we show that PAR\nachieves high backdoor removal rate while preserving good standard performance.\nFinally, we illustrate that our approach is effective even only with synthetic\ntext-image pairs, i.e. without access to real training data. The code and\nmodels are available at https://github.com/nmndeep/PerturbAndRecover.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language models like CLIP have been shown to be highly effective at\nlinking visual perception and natural language understanding, enabling\nsophisticated image-text capabilities, including strong retrieval and zero-shot\nclassification performance. Their widespread use, as well as the fact that CLIP\nmodels are trained on image-text pairs from the web, make them both a\nworthwhile and relatively easy target for backdoor attacks. As training\nfoundational models, such as CLIP, from scratch is very expensive, this paper\nfocuses on cleaning potentially poisoned models via fine-tuning. We first show\nthat existing cleaning techniques are not effective against simple structured\ntriggers used in Blended or BadNet backdoor attacks, exposing a critical\nvulnerability for potential real-world deployment of these models. Then, we\nintroduce PAR, Perturb and Recover, a surprisingly simple yet effective\nmechanism to remove backdoors from CLIP models. Through extensive experiments\nacross different encoders and types of backdoor attacks, we show that PAR\nachieves high backdoor removal rate while preserving good standard performance.\nFinally, we illustrate that our approach is effective even only with synthetic\ntext-image pairs, i.e. without access to real training data. The code and\nmodels are available at https://github.com/nmndeep/PerturbAndRecover."
                },
                "authors": [
                    {
                        "name": "Naman Deep Singh"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Matthias Hein"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hein"
                },
                "author": "Matthias Hein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v3",
                "updated": "2024-12-12T14:07:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    7,
                    42,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts."
                },
                "authors": [
                    {
                        "name": "Rainer Muehlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "33 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18446v2",
                "updated": "2024-12-12T13:48:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    48,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-09-27T05:06:43Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    6,
                    43,
                    4,
                    271,
                    0
                ],
                "title": "Exploring Language Model Generalization in Low-Resource Extractive QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Language Model Generalization in Low-Resource Extractive QA"
                },
                "summary": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize to domains\nthat require specific knowledge such as medicine and law in a zero-shot fashion\nwithout additional in-domain training? To this end, we devise a series of\nexperiments to explain the performance gap empirically. Our findings suggest\nthat: (a) LLMs struggle with dataset demands of closed domains such as\nretrieving long answer spans; (b) Certain LLMs, despite showing strong overall\nperformance, display weaknesses in meeting basic requirements as discriminating\nbetween domain-specific senses of words which we link to pre-processing\ndecisions; (c) Scaling model parameters is not always effective for cross\ndomain generalization; and (d) Closed-domain datasets are quantitatively much\ndifferent than open-domain EQA datasets and current LLMs struggle to deal with\nthem. Our findings point out important directions for improving existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize to domains\nthat require specific knowledge such as medicine and law in a zero-shot fashion\nwithout additional in-domain training? To this end, we devise a series of\nexperiments to explain the performance gap empirically. Our findings suggest\nthat: (a) LLMs struggle with dataset demands of closed domains such as\nretrieving long answer spans; (b) Certain LLMs, despite showing strong overall\nperformance, display weaknesses in meeting basic requirements as discriminating\nbetween domain-specific senses of words which we link to pre-processing\ndecisions; (c) Scaling model parameters is not always effective for cross\ndomain generalization; and (d) Closed-domain datasets are quantitatively much\ndifferent than open-domain EQA datasets and current LLMs struggle to deal with\nthem. Our findings point out important directions for improving existing LLMs."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09282v1",
                "updated": "2024-12-12T13:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    45,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:45:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    45,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of\n  LLMs"
                },
                "summary": "Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging multiple codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging multiple codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09280v1",
                "updated": "2024-12-12T13:42:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    42,
                    58,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:42:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    42,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "Learning to Solve Domain-Specific Calculation Problems with\n  Knowledge-Intensive Programs Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Solve Domain-Specific Calculation Problems with\n  Knowledge-Intensive Programs Generator"
                },
                "summary": "Domain Large Language Models (LLMs) are developed for domain-specific tasks\nbased on general LLMs. But it still requires professional knowledge to\nfacilitate the expertise for some domain-specific tasks. In this paper, we\ninvestigate into knowledge-intensive calculation problems. We find that the\nmath problems to be challenging for LLMs, when involving complex\ndomain-specific rules and knowledge documents, rather than simple formulations\nof terminologies. Therefore, we propose a pipeline to solve the domain-specific\ncalculation problems with Knowledge-Intensive Programs Generator more\neffectively, named as KIPG. It generates knowledge-intensive programs according\nto the domain-specific documents. For each query, key variables are extracted,\nthen outcomes which are dependent on domain knowledge are calculated with the\nprograms. By iterative preference alignment, the code generator learns to\nimprove the logic consistency with the domain knowledge. Taking legal domain as\nan example, we have conducted experiments to prove the effectiveness of our\npipeline, and extensive analysis on the modules. We also find that the code\ngenerator is also adaptable to other domains, without training on the new\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Large Language Models (LLMs) are developed for domain-specific tasks\nbased on general LLMs. But it still requires professional knowledge to\nfacilitate the expertise for some domain-specific tasks. In this paper, we\ninvestigate into knowledge-intensive calculation problems. We find that the\nmath problems to be challenging for LLMs, when involving complex\ndomain-specific rules and knowledge documents, rather than simple formulations\nof terminologies. Therefore, we propose a pipeline to solve the domain-specific\ncalculation problems with Knowledge-Intensive Programs Generator more\neffectively, named as KIPG. It generates knowledge-intensive programs according\nto the domain-specific documents. For each query, key variables are extracted,\nthen outcomes which are dependent on domain knowledge are calculated with the\nprograms. By iterative preference alignment, the code generator learns to\nimprove the logic consistency with the domain knowledge. Taking legal domain as\nan example, we have conducted experiments to prove the effectiveness of our\npipeline, and extensive analysis on the modules. We also find that the code\ngenerator is also adaptable to other domains, without training on the new\nknowledge."
                },
                "authors": [
                    {
                        "name": "Chengyuan Liu"
                    },
                    {
                        "name": "Shihang Wang"
                    },
                    {
                        "name": "Lizhi Qing"
                    },
                    {
                        "name": "Jun Lin"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Kun Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Kuang"
                },
                "author": "Kun Kuang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16995v2",
                "updated": "2024-12-12T13:33:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    33,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2023-10-25T20:48:16Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    20,
                    48,
                    16,
                    2,
                    298,
                    0
                ],
                "title": "TOP-Training: Target-Oriented Pretraining for Medical Extractive\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOP-Training: Target-Oriented Pretraining for Medical Extractive\n  Question Answering"
                },
                "summary": "We study extractive question-answering in the medical domain (Medical-EQA).\nThis problem has two main challenges: (i) domain specificity, as most AI models\nlack necessary domain knowledge, and (ii) extraction-based answering style,\nwhich restricts most autoregressive LLMs due to potential hallucinations. To\nhandle those challenges, we propose TOP-Training, a target-oriented\npre-training paradigm that stands out among all domain adaptation techniques\nwith two desirable features: (i) TOP-Training moves one step further than\npopular domain-oriented fine-tuning since it not only moves closer to the\ntarget domain, but also familiarizes itself with the target dataset, and (ii)\nit does not assume the existence of a large set of unlabeled instances from the\ntarget domain. Specifically, for a target Medical-EQA dataset, we extract its\nentities and leverage large language models (LLMs) to generate synthetic texts\ncontaining those entities; we then demonstrate that pretraining on this\nsynthetic text data yields better performance on the target Medical-EQA\nbenchmarks. Overall, our contributions are threefold: (i) TOP-Training, a new\npretraining technique to effectively adapt LLMs to better solve a target\nproblem, (ii) TOP-Training has a wide application scope because it does not\nrequire the target problem to have a large set of unlabeled data, and (iii) our\nexperiments highlight the limitations of autoregressive LLMs, emphasizing\nTOP-Training as a means to unlock the true potential of bidirectional LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study extractive question-answering in the medical domain (Medical-EQA).\nThis problem has two main challenges: (i) domain specificity, as most AI models\nlack necessary domain knowledge, and (ii) extraction-based answering style,\nwhich restricts most autoregressive LLMs due to potential hallucinations. To\nhandle those challenges, we propose TOP-Training, a target-oriented\npre-training paradigm that stands out among all domain adaptation techniques\nwith two desirable features: (i) TOP-Training moves one step further than\npopular domain-oriented fine-tuning since it not only moves closer to the\ntarget domain, but also familiarizes itself with the target dataset, and (ii)\nit does not assume the existence of a large set of unlabeled instances from the\ntarget domain. Specifically, for a target Medical-EQA dataset, we extract its\nentities and leverage large language models (LLMs) to generate synthetic texts\ncontaining those entities; we then demonstrate that pretraining on this\nsynthetic text data yields better performance on the target Medical-EQA\nbenchmarks. Overall, our contributions are threefold: (i) TOP-Training, a new\npretraining technique to effectively adapt LLMs to better solve a target\nproblem, (ii) TOP-Training has a wide application scope because it does not\nrequire the target problem to have a large set of unlabeled data, and (iii) our\nexperiments highlight the limitations of autoregressive LLMs, emphasizing\nTOP-Training as a means to unlock the true potential of bidirectional LLMs."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Connor Heaton"
                    },
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09269v1",
                "updated": "2024-12-12T13:31:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    31,
                    58,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:31:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    31,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "Towards Understanding the Robustness of LLM-based Evaluations under\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding the Robustness of LLM-based Evaluations under\n  Perturbations"
                },
                "summary": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics."
                },
                "authors": [
                    {
                        "name": "Manav Chaudhary"
                    },
                    {
                        "name": "Harshit Gupta"
                    },
                    {
                        "name": "Savita Bhat"
                    },
                    {
                        "name": "Vasudeva Varma"
                    }
                ],
                "author_detail": {
                    "name": "Vasudeva Varma"
                },
                "author": "Vasudeva Varma",
                "arxiv_comment": "Accepted at ICON 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03108v2",
                "updated": "2024-12-12T13:17:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    17,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T13:56:42Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    56,
                    42,
                    1,
                    310,
                    0
                ],
                "title": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning"
                },
                "summary": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations"
                },
                "authors": [
                    {
                        "name": "Veronika Krauß"
                    },
                    {
                        "name": "Mark McGill"
                    },
                    {
                        "name": "Thomas Kosch"
                    },
                    {
                        "name": "Yolanda Thiel"
                    },
                    {
                        "name": "Dominik Schön"
                    },
                    {
                        "name": "Jan Gugenheimer"
                    }
                ],
                "author_detail": {
                    "name": "Jan Gugenheimer"
                },
                "author": "Jan Gugenheimer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09250v1",
                "updated": "2024-12-12T13:04:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    4,
                    54,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:04:54Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    4,
                    54,
                    3,
                    347,
                    0
                ],
                "title": "GeLoRA: Geometric Adaptive Ranks For Efficient LoRA Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeLoRA: Geometric Adaptive Ranks For Efficient LoRA Fine-tuning"
                },
                "summary": "Fine-tuning large language models (LLMs) is computationally intensive because\nit requires updating all parameters. Low-Rank Adaptation (LoRA) improves\nefficiency by modifying only a subset of weights but introduces a trade-off\nbetween expressivity and computational cost: lower ranks reduce resources but\nlimit expressiveness, while higher ranks enhance expressivity at increased\ncost. Despite recent advances in adaptive LoRA techniques, existing methods\nfail to provide a theoretical basis for optimizing the trade-off between model\nperformance and efficiency. We propose Geometric Low-Rank Adaptation (GeLoRA),\na novel framework that computes the intrinsic dimensionality of hidden state\nrepresentations to adaptively select LoRA ranks. We demonstrate that the\nintrinsic dimension provides a lower bound for the optimal rank of LoRA\nmatrices, allowing for a principled selection that balances efficiency and\nexpressivity. GeLoRA dynamically adjusts the rank for each layer based on the\nintrinsic dimensionality of its input and output representations, recognizing\nthat not all model parameters equally impact fine-tuning. Empirical validation\non multiple tasks shows that GeLoRA consistently outperforms recent baselines\nwithin the same parameter budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is computationally intensive because\nit requires updating all parameters. Low-Rank Adaptation (LoRA) improves\nefficiency by modifying only a subset of weights but introduces a trade-off\nbetween expressivity and computational cost: lower ranks reduce resources but\nlimit expressiveness, while higher ranks enhance expressivity at increased\ncost. Despite recent advances in adaptive LoRA techniques, existing methods\nfail to provide a theoretical basis for optimizing the trade-off between model\nperformance and efficiency. We propose Geometric Low-Rank Adaptation (GeLoRA),\na novel framework that computes the intrinsic dimensionality of hidden state\nrepresentations to adaptively select LoRA ranks. We demonstrate that the\nintrinsic dimension provides a lower bound for the optimal rank of LoRA\nmatrices, allowing for a principled selection that balances efficiency and\nexpressivity. GeLoRA dynamically adjusts the rank for each layer based on the\nintrinsic dimensionality of its input and output representations, recognizing\nthat not all model parameters equally impact fine-tuning. Empirical validation\non multiple tasks shows that GeLoRA consistently outperforms recent baselines\nwithin the same parameter budget."
                },
                "authors": [
                    {
                        "name": "Abdessalam Ed-dib"
                    },
                    {
                        "name": "Zhanibek Datbayev"
                    },
                    {
                        "name": "Amine Mohamed Aboussalah"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mohamed Aboussalah"
                },
                "author": "Amine Mohamed Aboussalah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09248v1",
                "updated": "2024-12-12T13:00:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    0,
                    50,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T13:00:50Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    0,
                    50,
                    3,
                    347,
                    0
                ],
                "title": "A Systematic Review of Knowledge Tracing and Large Language Models in\n  Education: Opportunities, Issues, and Future Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Review of Knowledge Tracing and Large Language Models in\n  Education: Opportunities, Issues, and Future Research"
                },
                "summary": "Knowledge Tracing (KT) is a research field that aims to estimate a student's\nknowledge state through learning interactions-a crucial component of\nIntelligent Tutoring Systems (ITSs). Despite significant advancements, no\ncurrent KT models excel in both predictive accuracy and interpretability.\nMeanwhile, Large Language Models (LLMs), pre-trained on vast natural language\ndatasets, have emerged as powerful tools with immense potential in various\neducational applications. This systematic review explores the intersections,\nopportunities, and challenges of combining KT models and LLMs in educational\ncontexts. The review first investigates LLM applications in education,\nincluding their adaptability to domain-specific content and ability to support\npersonalized learning. It then examines the development and current state of KT\nmodels, from traditional to advanced approaches, aiming to uncover potential\nchallenges that LLMs could mitigate. The core of this review focuses on\nintegrating LLMs with KT, exploring three primary functions: addressing general\nconcerns in KT fields, overcoming specific KT model limitations, and performing\nas KT models themselves. Our findings reveal that LLMs can be customized for\nspecific educational tasks through tailor-making techniques such as in-context\nlearning and agent-based approaches, effectively managing complex and\nunbalanced educational data. These models can enhance existing KT models'\nperformance and solve cold-start problems by generating relevant features from\nquestion data. However, both current models depend heavily on structured,\nlimited datasets, missing opportunities to use diverse educational data that\ncould offer deeper insights into individual learners and support various\neducational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tracing (KT) is a research field that aims to estimate a student's\nknowledge state through learning interactions-a crucial component of\nIntelligent Tutoring Systems (ITSs). Despite significant advancements, no\ncurrent KT models excel in both predictive accuracy and interpretability.\nMeanwhile, Large Language Models (LLMs), pre-trained on vast natural language\ndatasets, have emerged as powerful tools with immense potential in various\neducational applications. This systematic review explores the intersections,\nopportunities, and challenges of combining KT models and LLMs in educational\ncontexts. The review first investigates LLM applications in education,\nincluding their adaptability to domain-specific content and ability to support\npersonalized learning. It then examines the development and current state of KT\nmodels, from traditional to advanced approaches, aiming to uncover potential\nchallenges that LLMs could mitigate. The core of this review focuses on\nintegrating LLMs with KT, exploring three primary functions: addressing general\nconcerns in KT fields, overcoming specific KT model limitations, and performing\nas KT models themselves. Our findings reveal that LLMs can be customized for\nspecific educational tasks through tailor-making techniques such as in-context\nlearning and agent-based approaches, effectively managing complex and\nunbalanced educational data. These models can enhance existing KT models'\nperformance and solve cold-start problems by generating relevant features from\nquestion data. However, both current models depend heavily on structured,\nlimited datasets, missing opportunities to use diverse educational data that\ncould offer deeper insights into individual learners and support various\neducational settings."
                },
                "authors": [
                    {
                        "name": "Yongwan Cho"
                    },
                    {
                        "name": "Rabia Emhamed AlMamlook"
                    },
                    {
                        "name": "Tasnim Gharaibeh"
                    }
                ],
                "author_detail": {
                    "name": "Tasnim Gharaibeh"
                },
                "author": "Tasnim Gharaibeh",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04448v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04448v4",
                "updated": "2024-12-12T12:57:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    57,
                    59,
                    3,
                    347,
                    0
                ],
                "published": "2023-11-08T04:19:28Z",
                "published_parsed": [
                    2023,
                    11,
                    8,
                    4,
                    19,
                    28,
                    2,
                    312,
                    0
                ],
                "title": "Boosting Static Resource Leak Detection via LLM-based Resource-Oriented\n  Intention Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Static Resource Leak Detection via LLM-based Resource-Oriented\n  Intention Inference"
                },
                "summary": "Resource leaks, caused by resources not being released after acquisition,\noften lead to performance issues and system crashes. Existing static detection\ntechniques rely on mechanical matching of predefined resource\nacquisition/release APIs and null-checking conditions to find unreleased\nresources, suffering from both (1) false negatives caused by the incompleteness\nof predefined resource acquisition/release APIs and (2) false positives caused\nby the incompleteness of resource reachability validation identification. To\novercome these challenges, we propose InferROI, a novel approach that leverages\nthe exceptional code comprehension capability of large language models (LLMs)\nto directly infer resource-oriented intentions (acquisition, release, and\nreachability validation) in code. InferROI first prompts the LLM to infer\ninvolved intentions for a given code snippet, and then incorporates a two-stage\nstatic analysis approach to check control-flow paths for resource leak\ndetection based on the inferred intentions.\n  We evaluate the effectiveness of InferROI in both resource-oriented intention\ninference and resource leak detection. Experimental results on the DroidLeaks\nand JLeaks datasets demonstrate InferROI achieves promising bug detection rate\n(59.3% and 62.5%) and false alarm rate (18.6% and 19.5%). Compared to three\nindustrial static detectors, InferROI detects 14~45 and 149~485 more bugs in\nDroidLeaks and JLeaks, respectively. When applied to real-world open-source\nprojects, InferROI identifies 29 unknown resource leak bugs (verified by\nauthors), with 7 of them being confirmed by developers. In addition, the\nresults of an ablation study underscores the importance of combining LLM-based\ninference with static analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource leaks, caused by resources not being released after acquisition,\noften lead to performance issues and system crashes. Existing static detection\ntechniques rely on mechanical matching of predefined resource\nacquisition/release APIs and null-checking conditions to find unreleased\nresources, suffering from both (1) false negatives caused by the incompleteness\nof predefined resource acquisition/release APIs and (2) false positives caused\nby the incompleteness of resource reachability validation identification. To\novercome these challenges, we propose InferROI, a novel approach that leverages\nthe exceptional code comprehension capability of large language models (LLMs)\nto directly infer resource-oriented intentions (acquisition, release, and\nreachability validation) in code. InferROI first prompts the LLM to infer\ninvolved intentions for a given code snippet, and then incorporates a two-stage\nstatic analysis approach to check control-flow paths for resource leak\ndetection based on the inferred intentions.\n  We evaluate the effectiveness of InferROI in both resource-oriented intention\ninference and resource leak detection. Experimental results on the DroidLeaks\nand JLeaks datasets demonstrate InferROI achieves promising bug detection rate\n(59.3% and 62.5%) and false alarm rate (18.6% and 19.5%). Compared to three\nindustrial static detectors, InferROI detects 14~45 and 149~485 more bugs in\nDroidLeaks and JLeaks, respectively. When applied to real-world open-source\nprojects, InferROI identifies 29 unknown resource leak bugs (verified by\nauthors), with 7 of them being confirmed by developers. In addition, the\nresults of an ablation study underscores the importance of combining LLM-based\ninference with static analysis."
                },
                "authors": [
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jianan Liu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04448v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04448v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09247v1",
                "updated": "2024-12-12T12:57:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    57,
                    55,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:57:55Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    57,
                    55,
                    3,
                    347,
                    0
                ],
                "title": "Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by\n  Utilizing Generative LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by\n  Utilizing Generative LLMs"
                },
                "summary": "Satire detection is essential for accurately extracting opinions from textual\ndata and combating misinformation online. However, the lack of diverse corpora\nfor satire leads to the problem of stylistic bias which impacts the models'\ndetection performances. This study proposes a debiasing approach for satire\ndetection, focusing on reducing biases in training data by utilizing generative\nlarge language models. The approach is evaluated in both cross-domain (irony\ndetection) and cross-lingual (English) settings. Results show that the\ndebiasing method enhances the robustness and generalizability of the models for\nsatire and irony detection tasks in Turkish and English. However, its impact on\ncausal language models, such as Llama-3.1, is limited. Additionally, this work\ncurates and presents the Turkish Satirical News Dataset with detailed human\nannotations, with case studies on classification, debiasing, and\nexplainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satire detection is essential for accurately extracting opinions from textual\ndata and combating misinformation online. However, the lack of diverse corpora\nfor satire leads to the problem of stylistic bias which impacts the models'\ndetection performances. This study proposes a debiasing approach for satire\ndetection, focusing on reducing biases in training data by utilizing generative\nlarge language models. The approach is evaluated in both cross-domain (irony\ndetection) and cross-lingual (English) settings. Results show that the\ndebiasing method enhances the robustness and generalizability of the models for\nsatire and irony detection tasks in Turkish and English. However, its impact on\ncausal language models, such as Llama-3.1, is limited. Additionally, this work\ncurates and presents the Turkish Satirical News Dataset with detailed human\nannotations, with case studies on classification, debiasing, and\nexplainability."
                },
                "authors": [
                    {
                        "name": "Asli Umay Ozturk"
                    },
                    {
                        "name": "Recep Firat Cekinel"
                    },
                    {
                        "name": "Pinar Karagoz"
                    }
                ],
                "author_detail": {
                    "name": "Pinar Karagoz"
                },
                "author": "Pinar Karagoz",
                "arxiv_comment": "Accepted to BUCC2025 Workshop @COLING2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09243v1",
                "updated": "2024-12-12T12:53:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:53:30Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "title": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns."
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09237v1",
                "updated": "2024-12-12T12:47:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    47,
                    9,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T12:47:09Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    47,
                    9,
                    3,
                    347,
                    0
                ],
                "title": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user\n  Simulation"
                },
                "summary": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Wu Liu"
                    },
                    {
                        "name": "Xiaoyan Gu"
                    },
                    {
                        "name": "Yong Rui"
                    },
                    {
                        "name": "Xiaodong He"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12456v2",
                "updated": "2024-12-12T12:38:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    38,
                    12,
                    3,
                    347,
                    0
                ],
                "published": "2023-12-16T02:27:00Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    2,
                    27,
                    0,
                    5,
                    350,
                    0
                ],
                "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"
                },
                "summary": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key principle underlying the design of PowerInfer is\nexploiting the high locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution indicates that a\nsmall subset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. The evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance\ncomparable to that of a high-end server-grade A100 GPU, reaching 82% of its\ntoken generation rate on a single consumer-grade RTX 4090 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key principle underlying the design of PowerInfer is\nexploiting the high locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution indicates that a\nsmall subset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. The evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance\ncomparable to that of a high-end server-grade A100 GPU, reaching 82% of its\ntoken generation rate on a single consumer-grade RTX 4090 GPU."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haotong Xie"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08926v2",
                "updated": "2024-12-12T12:18:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    18,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T15:50:53Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    53,
                    4,
                    285,
                    0
                ],
                "title": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images"
                },
                "summary": "We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research."
                },
                "authors": [
                    {
                        "name": "Virmarie Maquiling"
                    },
                    {
                        "name": "Sean Anthony Byrne"
                    },
                    {
                        "name": "Diederick C. Niehorster"
                    },
                    {
                        "name": "Marco Carminati"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Virmarie Maquiling and Sean Anthony Byrne contributed equally to this\n  paper, 8 pages, 3 figures, CHI Case Study, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14285v2",
                "updated": "2024-12-12T12:12:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    12,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-04-22T15:35:33Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    15,
                    35,
                    33,
                    0,
                    113,
                    0
                ],
                "title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots"
                },
                "summary": "Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/."
                },
                "authors": [
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Trevor McInroe"
                    },
                    {
                        "name": "Adam Jelley"
                    },
                    {
                        "name": "Stefano V. Albrecht"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Amos Storkey"
                    }
                ],
                "author_detail": {
                    "name": "Amos Storkey"
                },
                "author": "Amos Storkey",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16048v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16048v3",
                "updated": "2024-12-12T12:01:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    1,
                    43,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-25T10:13:04Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    10,
                    13,
                    4,
                    6,
                    56,
                    0
                ],
                "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Likely Do LLMs with CoT Mimic Human Reasoning?"
                },
                "summary": "Chain-of-thought emerges as a promising technique for eliciting reasoning\ncapabilities from Large Language Models (LLMs). However, it does not always\nimprove task performance or accurately represent reasoning processes, leaving\nunresolved questions about its usage. In this paper, we diagnose the underlying\nmechanism by comparing the reasoning process of LLMs with humans, using causal\nanalysis to understand the relationships between the problem instruction,\nreasoning, and the answer in LLMs. Our empirical study reveals that LLMs often\ndeviate from the ideal causal chain, resulting in spurious correlations and\npotential consistency errors (inconsistent reasoning and answers). We also\nexamine various factors influencing the causal structure, finding that\nin-context learning with examples strengthens it, while post-training\ntechniques like supervised fine-tuning and reinforcement learning on human\nfeedback weaken it. To our surprise, the causal structure cannot be\nstrengthened by enlarging the model size only, urging research on new\ntechniques. We hope that this preliminary study will shed light on\nunderstanding and improving the reasoning process in LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought emerges as a promising technique for eliciting reasoning\ncapabilities from Large Language Models (LLMs). However, it does not always\nimprove task performance or accurately represent reasoning processes, leaving\nunresolved questions about its usage. In this paper, we diagnose the underlying\nmechanism by comparing the reasoning process of LLMs with humans, using causal\nanalysis to understand the relationships between the problem instruction,\nreasoning, and the answer in LLMs. Our empirical study reveals that LLMs often\ndeviate from the ideal causal chain, resulting in spurious correlations and\npotential consistency errors (inconsistent reasoning and answers). We also\nexamine various factors influencing the causal structure, finding that\nin-context learning with examples strengthens it, while post-training\ntechniques like supervised fine-tuning and reinforcement learning on human\nfeedback weaken it. To our surprise, the causal structure cannot be\nstrengthened by enlarging the model size only, urging research on new\ntechniques. We hope that this preliminary study will shed light on\nunderstanding and improving the reasoning process in LLM."
                },
                "authors": [
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "COLING 2025 Camera Version (8 pages, 3 figures, 18 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16048v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16048v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13516v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13516v5",
                "updated": "2024-12-12T11:29:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    29,
                    32,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-21T03:58:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    58,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models"
                },
                "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xiyu Shi"
                    },
                    {
                        "name": "Kuai Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13516v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13516v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01606v2",
                "updated": "2024-12-12T11:27:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    27,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-03T15:25:47Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    15,
                    25,
                    47,
                    6,
                    308,
                    0
                ],
                "title": "DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with\n  Large Language Models"
                },
                "summary": "The rise of Large Language Models (LLMs) has streamlined frontend interface\ncreation through tools like Vercel's V0, yet surfaced challenges in design\nquality (e.g., accessibility, and usability). Current solutions, often limited\nby their focus, generalisability, or data dependency, fall short in addressing\nthese complexities. Moreover, none of them examine the quality of LLM-generated\nUI design. In this work, we introduce DesignRepair, a novel dual-stream design\nguideline-aware system to examine and repair the UI design quality issues from\nboth code aspect and rendered page aspect. We utilised the mature and popular\nMaterial Design as our knowledge base to guide this process. Specifically, we\nfirst constructed a comprehensive knowledge base encoding Google's Material\nDesign principles into low-level component knowledge base and high-level system\ndesign knowledge base. After that, DesignRepair employs a LLM for the\nextraction of key components and utilizes the Playwright tool for precise page\nanalysis, aligning these with the established knowledge bases. Finally, we\nintegrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4\nto holistically refine and repair frontend code through a strategic divide and\nconquer approach. Our extensive evaluations validated the efficacy and utility\nof our approach, demonstrating significant enhancements in adherence to design\nguidelines, accessibility, and user experience metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has streamlined frontend interface\ncreation through tools like Vercel's V0, yet surfaced challenges in design\nquality (e.g., accessibility, and usability). Current solutions, often limited\nby their focus, generalisability, or data dependency, fall short in addressing\nthese complexities. Moreover, none of them examine the quality of LLM-generated\nUI design. In this work, we introduce DesignRepair, a novel dual-stream design\nguideline-aware system to examine and repair the UI design quality issues from\nboth code aspect and rendered page aspect. We utilised the mature and popular\nMaterial Design as our knowledge base to guide this process. Specifically, we\nfirst constructed a comprehensive knowledge base encoding Google's Material\nDesign principles into low-level component knowledge base and high-level system\ndesign knowledge base. After that, DesignRepair employs a LLM for the\nextraction of key components and utilizes the Playwright tool for precise page\nanalysis, aligning these with the established knowledge bases. Finally, we\nintegrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4\nto holistically refine and repair frontend code through a strategic divide and\nconquer approach. Our extensive evaluations validated the efficacy and utility\nof our approach, demonstrating significant enhancements in adherence to design\nguidelines, accessibility, and user experience metrics."
                },
                "authors": [
                    {
                        "name": "Mingyue Yuan"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Aaron Quigley"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Tianqi Luo"
                    },
                    {
                        "name": "Gelareh Mohammadi"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "2025 IEEE/ACM 47th International Conference on Software Engineering\n  (ICSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09128v2",
                "updated": "2024-12-12T11:16:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    16,
                    1,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-14T02:01:39Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    1,
                    39,
                    3,
                    319,
                    0
                ],
                "title": "Performance Analysis of uRLLC in scalable Cell-free Radio Access Network\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of uRLLC in scalable Cell-free Radio Access Network\n  System"
                },
                "summary": "As a critical component of beyond fifth-generation (B5G) and sixth-generation\n(6G) mobile communication systems, ultra-reliable low-latency communication\n(uRLLC) imposes stringent requirements on latency and reliability. In recent\nyears, with the improvement of mobile communication network, centralized and\ndistributed processing schemes for cellfree massive multiple-input\nmultiple-output (CF-mMIMO) have attracted significant research attention. This\npaper investigates the performance of a novel scalable cell-free radio access\nnetwork (CF-RAN) architecture featuring multiple edge distributed units (EDUs)\nunder the finite block length regime. Closed expressions for the upper and\nlower bounds of its expected spectral efficiency (SE) performance are derived,\nwhere centralized and fully distributed deployment can be treated as two\nspecial cases, respectively. Furthermore, the spatial distribution of user\nequipments (UEs) and remote radio units (RRUs) is examined and the analysis\nreveals that the interleaving RRUs deployment associated with the EDU can\nenhance SE performance under finite block length constraints with specific\ntransmission error probability. The paper also compares Monte Carlo simulation\nresults with multi-RRU clustering-based collaborative processing, validating\nthe accuracy of the space-time exchange theory in the scalable CF-RAN scenario.\nBy deploying scalable EDUs, a practical trade-off between latency and\nreliability can be achieved through spatial degree-of-freedom (DoF), offering a\ndistributed and scalable realization of the space-time exchange theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a critical component of beyond fifth-generation (B5G) and sixth-generation\n(6G) mobile communication systems, ultra-reliable low-latency communication\n(uRLLC) imposes stringent requirements on latency and reliability. In recent\nyears, with the improvement of mobile communication network, centralized and\ndistributed processing schemes for cellfree massive multiple-input\nmultiple-output (CF-mMIMO) have attracted significant research attention. This\npaper investigates the performance of a novel scalable cell-free radio access\nnetwork (CF-RAN) architecture featuring multiple edge distributed units (EDUs)\nunder the finite block length regime. Closed expressions for the upper and\nlower bounds of its expected spectral efficiency (SE) performance are derived,\nwhere centralized and fully distributed deployment can be treated as two\nspecial cases, respectively. Furthermore, the spatial distribution of user\nequipments (UEs) and remote radio units (RRUs) is examined and the analysis\nreveals that the interleaving RRUs deployment associated with the EDU can\nenhance SE performance under finite block length constraints with specific\ntransmission error probability. The paper also compares Monte Carlo simulation\nresults with multi-RRU clustering-based collaborative processing, validating\nthe accuracy of the space-time exchange theory in the scalable CF-RAN scenario.\nBy deploying scalable EDUs, a practical trade-off between latency and\nreliability can be achieved through spatial degree-of-freedom (DoF), offering a\ndistributed and scalable realization of the space-time exchange theory."
                },
                "authors": [
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Dongming Wang"
                    },
                    {
                        "name": "Yunxiang Guo"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Xiaohu You"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu You"
                },
                "author": "Xiaohu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09176v1",
                "updated": "2024-12-12T11:06:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    6,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T11:06:36Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    6,
                    36,
                    3,
                    347,
                    0
                ],
                "title": "LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting"
                },
                "summary": "Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has\nshown immense potential in VR content creation due to its high-quality\nrendering and efficient production process. However, existing physics-based\ninteraction systems for 3DGS can only perform simple and non-realistic\nsimulations or demand extensive user input for complex scenes, primarily due to\nthe absence of scene understanding. In this paper, we propose LIVE-GS, a highly\nrealistic interactive VR system powered by LLM. After object-aware GS\nreconstruction, we prompt GPT-4o to analyze the physical properties of objects\nin the scene, which are used to guide physical simulations consistent with real\nphenomena. We also design a GPT-assisted GS inpainting module to fill the\nunseen area covered by manipulative objects. To perform a precise segmentation\nof Gaussian kernels, we propose a feature-mask segmentation strategy. To enable\nrich interaction, we further propose a computationally efficient physical\nsimulation framework through an PBD-based unified interpolation method,\nsupporting various physical forms such as rigid body, soft body, and granular\nmaterials. Our experimental results show that with the help of LLM's\nunderstanding and enhancement of scenes, our VR system can support complex and\nrealistic interactions without additional manual design and annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has\nshown immense potential in VR content creation due to its high-quality\nrendering and efficient production process. However, existing physics-based\ninteraction systems for 3DGS can only perform simple and non-realistic\nsimulations or demand extensive user input for complex scenes, primarily due to\nthe absence of scene understanding. In this paper, we propose LIVE-GS, a highly\nrealistic interactive VR system powered by LLM. After object-aware GS\nreconstruction, we prompt GPT-4o to analyze the physical properties of objects\nin the scene, which are used to guide physical simulations consistent with real\nphenomena. We also design a GPT-assisted GS inpainting module to fill the\nunseen area covered by manipulative objects. To perform a precise segmentation\nof Gaussian kernels, we propose a feature-mask segmentation strategy. To enable\nrich interaction, we further propose a computationally efficient physical\nsimulation framework through an PBD-based unified interpolation method,\nsupporting various physical forms such as rigid body, soft body, and granular\nmaterials. Our experimental results show that with the help of LLM's\nunderstanding and enhancement of scenes, our VR system can support complex and\nrealistic interactions without additional manual design and annotation."
                },
                "authors": [
                    {
                        "name": "Haotian Mao"
                    },
                    {
                        "name": "Zhuoxiong Xu"
                    },
                    {
                        "name": "Siyue Wei"
                    },
                    {
                        "name": "Yule Quan"
                    },
                    {
                        "name": "Nianchen Deng"
                    },
                    {
                        "name": "Xubo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xubo Yang"
                },
                "author": "Xubo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09173v1",
                "updated": "2024-12-12T11:03:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    3,
                    25,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T11:03:25Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    11,
                    3,
                    25,
                    3,
                    347,
                    0
                ],
                "title": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied\n  Tasks"
                },
                "summary": "Following formatting instructions to generate well-structured content is a\nfundamental yet often unmet capability for large language models (LLMs). To\nstudy this capability, which we refer to as format faithfulness, we present\nFormatBench, a comprehensive format-related benchmark. Compared to previous\nformat-related benchmarks, FormatBench involves a greater variety of tasks in\nterms of application scenes (traditional NLP tasks, creative works, autonomous\nagency tasks), human-LLM interaction styles (single-turn instruction,\nmulti-turn chat), and format types (inclusion, wrapping, length, coding).\nMoreover, each task in FormatBench is attached with a format checker program.\nExtensive experiments on the benchmark reveal that state-of-the-art open- and\nclosed-source LLMs still suffer from severe deficiency in format faithfulness.\nBy virtue of the decidable nature of formats, we propose to Reinforce Format\nFaithfulness (ReFF) to help LLMs generate formatted output as instructed\nwithout compromising general quality. Without any annotated data, ReFF can\nsubstantially improve the format faithfulness rate (e.g., from 21.6% in\noriginal LLaMA3 to 95.0% on caption segmentation task), while keep the general\nquality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with\nlabeled training data, ReFF can simultaneously improve both format faithfulness\n(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from\n47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to\nexplain how ReFF improves both format faithfulness and general quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following formatting instructions to generate well-structured content is a\nfundamental yet often unmet capability for large language models (LLMs). To\nstudy this capability, which we refer to as format faithfulness, we present\nFormatBench, a comprehensive format-related benchmark. Compared to previous\nformat-related benchmarks, FormatBench involves a greater variety of tasks in\nterms of application scenes (traditional NLP tasks, creative works, autonomous\nagency tasks), human-LLM interaction styles (single-turn instruction,\nmulti-turn chat), and format types (inclusion, wrapping, length, coding).\nMoreover, each task in FormatBench is attached with a format checker program.\nExtensive experiments on the benchmark reveal that state-of-the-art open- and\nclosed-source LLMs still suffer from severe deficiency in format faithfulness.\nBy virtue of the decidable nature of formats, we propose to Reinforce Format\nFaithfulness (ReFF) to help LLMs generate formatted output as instructed\nwithout compromising general quality. Without any annotated data, ReFF can\nsubstantially improve the format faithfulness rate (e.g., from 21.6% in\noriginal LLaMA3 to 95.0% on caption segmentation task), while keep the general\nquality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with\nlabeled training data, ReFF can simultaneously improve both format faithfulness\n(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from\n47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to\nexplain how ReFF improves both format faithfulness and general quality."
                },
                "authors": [
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Haoyu Wen"
                    },
                    {
                        "name": "Wei Su"
                    },
                    {
                        "name": "Boao Qian"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04108v2",
                "updated": "2024-12-12T10:56:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    56,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-04-05T14:04:07Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    14,
                    4,
                    7,
                    4,
                    96,
                    0
                ],
                "title": "Large language models as oracles for instantiating ontologies with\n  domain-specific knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models as oracles for instantiating ontologies with\n  domain-specific knowledge"
                },
                "summary": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes and properties and (ii) a set of query\ntemplates, our method queries the LLM multiple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nautomatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nExperimentally, our approach achieves a quality metric that is up to five times\nhigher than the state-of-the-art, while reducing erroneous entities and\nrelations by up to ten times. Finally, we provide a SWOT analysis of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes and properties and (ii) a set of query\ntemplates, our method queries the LLM multiple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nautomatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nExperimentally, our approach achieves a quality metric that is up to five times\nhigher than the state-of-the-art, while reducing erroneous entities and\nrelations by up to ten times. Finally, we provide a SWOT analysis of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Giovanni Ciatto"
                    },
                    {
                        "name": "Andrea Agiollo"
                    },
                    {
                        "name": "Matteo Magnini"
                    },
                    {
                        "name": "Andrea Omicini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Omicini"
                },
                "author": "Andrea Omicini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v1",
                "updated": "2024-12-12T10:50:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications, such as semantic matching, clustering,\nand information retrieval, continue to rely on text embeddings for their\nefficiency and effectiveness. In this survey, we categorize the interplay\nbetween LLMs and text embeddings into three overarching themes: (1)\nLLM-augmented text embedding, enhancing traditional embedding methods with\nLLMs; (2) LLMs as text embedders, utilizing their innate capabilities for\nembedding generation; and (3) Text embedding understanding with LLMs,\nleveraging LLMs to analyze and interpret embeddings. By organizing these\nefforts based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications, such as semantic matching, clustering,\nand information retrieval, continue to rely on text embeddings for their\nefficiency and effectiveness. In this survey, we categorize the interplay\nbetween LLMs and text embeddings into three overarching themes: (1)\nLLM-augmented text embedding, enhancing traditional embedding methods with\nLLMs; (2) LLMs as text embedders, utilizing their innate capabilities for\nembedding generation; and (3) Text embedding understanding with LLMs,\nleveraging LLMs to analyze and interpret embeddings. By organizing these\nefforts based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20612v2",
                "updated": "2024-12-12T10:46:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    46,
                    44,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-31T03:59:15Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    3,
                    59,
                    15,
                    4,
                    152,
                    0
                ],
                "title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention\n  and FFN Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention\n  and FFN Manipulation"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs."
                },
                "authors": [
                    {
                        "name": "Hanzhang Zhou"
                    },
                    {
                        "name": "Zijian Feng"
                    },
                    {
                        "name": "Zixiao Zhu"
                    },
                    {
                        "name": "Junlang Qian"
                    },
                    {
                        "name": "Kezhi Mao"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Mao"
                },
                "author": "Kezhi Mao",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10510v2",
                "updated": "2024-12-12T10:40:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    40,
                    22,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-15T08:06:37Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    8,
                    6,
                    37,
                    0,
                    197,
                    0
                ],
                "title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription\n  Prediction"
                },
                "summary": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis."
                },
                "authors": [
                    {
                        "name": "Xingzhi Zhou"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Chunhao Li"
                    },
                    {
                        "name": "Yuning Bai"
                    },
                    {
                        "name": "Yulong Xu"
                    },
                    {
                        "name": "Ka Chun Cheung"
                    },
                    {
                        "name": "Simon See"
                    },
                    {
                        "name": "Xinpeng Song"
                    },
                    {
                        "name": "Runshun Zhang"
                    },
                    {
                        "name": "Xuezhong Zhou"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Camera-ready version to be published in BIBM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13173v2",
                "updated": "2024-12-12T10:22:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    22,
                    37,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-20T10:17:09Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    17,
                    9,
                    2,
                    325,
                    0
                ],
                "title": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems"
                },
                "summary": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models."
                },
                "authors": [
                    {
                        "name": "Hongliu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Hongliu Cao"
                },
                "author": "Hongliu Cao",
                "arxiv_doi": "10.1145/3701551.3703514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08237v2",
                "updated": "2024-12-12T10:01:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    1,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-11T09:38:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    38,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"
                },
                "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data."
                },
                "authors": [
                    {
                        "name": "Xingchen Song"
                    },
                    {
                        "name": "Mengtao Xing"
                    },
                    {
                        "name": "Changwei Ma"
                    },
                    {
                        "name": "Shengqiang Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Fuping Pan"
                    },
                    {
                        "name": "Dinghao Zhou"
                    },
                    {
                        "name": "Yuekai Zhang"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Zhendong Peng"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09119v1",
                "updated": "2024-12-12T09:54:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    54,
                    38,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:54:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    54,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "The Utility and Complexity of In- and Out-of-Distribution Machine\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Utility and Complexity of In- and Out-of-Distribution Machine\n  Unlearning"
                },
                "summary": "Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility."
                },
                "authors": [
                    {
                        "name": "Youssef Allouah"
                    },
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09131v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09131v5",
                "updated": "2024-12-12T09:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    14,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-14T06:49:16Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    6,
                    49,
                    16,
                    3,
                    74,
                    0
                ],
                "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Responses"
                },
                "summary": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including question answering and controlled text generation.\nHowever, studies into their ability to switch between opposite styles of\nresponses in professional domains remain underexplored. This study introduces a\nnovel approach, named ProSwitch, which enables a language model to switch\nbetween professional and non-professional answers, by tuning and evaluating\nthrough the guidance of domain and style knowledge. ProSwitch unfolds in three\nphases: LLM-augmented preparation to collect domain knowledge and QA pairs,\ninstruction tuning to optimize LLMs with multiple levels of knowledge, and\ncomprehensive evaluation to assess both style discrimination and\nreference-based quality of the generated text. Comparative analysis of\nProSwitch against general and specialized LLMs reveals that our approach\noutperforms baselines in switching between professional and non-professional\nresponses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including question answering and controlled text generation.\nHowever, studies into their ability to switch between opposite styles of\nresponses in professional domains remain underexplored. This study introduces a\nnovel approach, named ProSwitch, which enables a language model to switch\nbetween professional and non-professional answers, by tuning and evaluating\nthrough the guidance of domain and style knowledge. ProSwitch unfolds in three\nphases: LLM-augmented preparation to collect domain knowledge and QA pairs,\ninstruction tuning to optimize LLMs with multiple levels of knowledge, and\ncomprehensive evaluation to assess both style discrimination and\nreference-based quality of the generated text. Comparative analysis of\nProSwitch against general and specialized LLMs reveals that our approach\noutperforms baselines in switching between professional and non-professional\nresponses."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Yuyan Chen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Heng Chang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "8 pages main body, 16 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09131v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09131v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09094v1",
                "updated": "2024-12-12T09:22:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    4,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:22:04Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    4,
                    3,
                    347,
                    0
                ],
                "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion"
                },
                "summary": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihai Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Min Peng"
                    }
                ],
                "author_detail": {
                    "name": "Min Peng"
                },
                "author": "Min Peng",
                "arxiv_comment": "COLING 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00062v2",
                "updated": "2024-12-12T09:11:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    11,
                    30,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-31T08:15:32Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    15,
                    32,
                    3,
                    305,
                    0
                ],
                "title": "Evolving Alignment via Asymmetric Self-Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Alignment via Asymmetric Self-Play"
                },
                "summary": "Current RLHF frameworks for aligning large language models (LLMs) typically\nassume a fixed prompt distribution, which is sub-optimal and limits the\nscalability of alignment and generalizability of models. To address this, we\nintroduce a general open-ended RLHF framework that casts alignment as an\nasymmetric game between two players: (i) a creator that generates increasingly\ninformative prompt distributions using reward signals, and (ii) a solver that\nlearns to produce more preferred responses on prompts produced by the creator.\nThis framework of Evolving Alignment via Asymmetric Self-Play (eva), results in\na simple and efficient approach that can utilize any existing RLHF algorithm\nfor scalable alignment. eva outperforms state-of-the-art methods on widely-used\nbenchmarks, without the need of any additional human crafted prompts.\nSpecifically, eva improves the win rate of Gemma-2-9B-it on Arena-Hard from\n51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7%\nwith SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and\nmatching claude-3-opus. This improvement is persistent even when new human\ncrafted prompts are introduced. Finally, we show eva is effective and robust\nunder various ablation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current RLHF frameworks for aligning large language models (LLMs) typically\nassume a fixed prompt distribution, which is sub-optimal and limits the\nscalability of alignment and generalizability of models. To address this, we\nintroduce a general open-ended RLHF framework that casts alignment as an\nasymmetric game between two players: (i) a creator that generates increasingly\ninformative prompt distributions using reward signals, and (ii) a solver that\nlearns to produce more preferred responses on prompts produced by the creator.\nThis framework of Evolving Alignment via Asymmetric Self-Play (eva), results in\na simple and efficient approach that can utilize any existing RLHF algorithm\nfor scalable alignment. eva outperforms state-of-the-art methods on widely-used\nbenchmarks, without the need of any additional human crafted prompts.\nSpecifically, eva improves the win rate of Gemma-2-9B-it on Arena-Hard from\n51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7%\nwith SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and\nmatching claude-3-opus. This improvement is persistent even when new human\ncrafted prompts are introduced. Finally, we show eva is effective and robust\nunder various ablation settings."
                },
                "authors": [
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Rishabh Joshi"
                    },
                    {
                        "name": "Sarmishta Velury"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Qijun Tan"
                    },
                    {
                        "name": "Yuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Liu"
                },
                "author": "Yuan Liu",
                "arxiv_comment": "35 pages, spotlight @ neurips language gamification workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05958v2",
                "updated": "2024-12-12T09:10:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    10,
                    32,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-08T14:34:30Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    14,
                    34,
                    30,
                    6,
                    343,
                    0
                ],
                "title": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension"
                },
                "summary": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub."
                },
                "authors": [
                    {
                        "name": "Adem Ait"
                    },
                    {
                        "name": "Javier Luis Cánovas Izquierdo"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v1",
                "updated": "2024-12-12T09:01:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable abilities across various\nlanguage tasks, but solving complex reasoning problems remains a challenge.\nWhile existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)\nenhance reasoning by decomposing problems or structuring prompts, they\ntypically perform a single pass of reasoning and may fail to revisit flawed\npaths, compromising accuracy. To address this, we propose a novel reasoning\nframework called Forest-of-Thought (FoT), which integrates multiple reasoning\ntrees to leverage collective decision-making for solving complex logical\nproblems. FoT utilizes sparse activation strategies to select the most relevant\nreasoning paths, improving both efficiency and accuracy. Additionally, we\nintroduce a dynamic self-correction strategy that enables real-time error\ncorrection and learning from past mistakes, as well as consensus-guided\ndecision making strategies to optimize correctness and computational resources.\nExperimental results demonstrate that the FoT framework, combined with these\nstrategies, significantly enhances the reasoning capabilities of LLMs, enabling\nthem to solve complex tasks with greater precision and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable abilities across various\nlanguage tasks, but solving complex reasoning problems remains a challenge.\nWhile existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)\nenhance reasoning by decomposing problems or structuring prompts, they\ntypically perform a single pass of reasoning and may fail to revisit flawed\npaths, compromising accuracy. To address this, we propose a novel reasoning\nframework called Forest-of-Thought (FoT), which integrates multiple reasoning\ntrees to leverage collective decision-making for solving complex logical\nproblems. FoT utilizes sparse activation strategies to select the most relevant\nreasoning paths, improving both efficiency and accuracy. Additionally, we\nintroduce a dynamic self-correction strategy that enables real-time error\ncorrection and learning from past mistakes, as well as consensus-guided\ndecision making strategies to optimize correctness and computational resources.\nExperimental results demonstrate that the FoT framework, combined with these\nstrategies, significantly enhances the reasoning capabilities of LLMs, enabling\nthem to solve complex tasks with greater precision and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09671v2",
                "updated": "2024-12-12T08:48:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    48,
                    27,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-19T03:13:20Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    13,
                    20,
                    0,
                    232,
                    0
                ],
                "title": "GANPrompt: Enhancing Robustness in LLM-Based Recommendations with\n  GAN-Enhanced Diversity Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANPrompt: Enhancing Robustness in LLM-Based Recommendations with\n  GAN-Enhanced Diversity Prompts"
                },
                "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nproficiency in comprehending and generating natural language, with a growing\nprevalence in the domain of recommendation systems. However, LLMs still face a\nsignificant challenge called prompt sensitivity, which refers to that it is\nhighly susceptible to the influence of prompt words. This inconsistency in\nresponse to minor alterations in prompt input may compromise the accuracy and\nresilience of recommendation models. To address this issue, this paper proposes\nGANPrompt, a multi-dimensional LLMs prompt diversity framework based on\nGenerative Adversarial Networks (GANs). The framework enhances the model's\nadaptability and stability to diverse prompts by integrating GANs generation\ntechniques with the deep semantic understanding capabilities of LLMs. GANPrompt\nfirst trains a generator capable of producing diverse prompts by analysing\nmultidimensional user behavioural data. These diverse prompts are then used to\ntrain the LLMs to improve its performance in the face of unseen prompts.\nFurthermore, to ensure a high degree of diversity and relevance of the prompts,\nthis study introduces a mathematical theory-based diversity constraint\nmechanism that optimises the generated prompts to ensure that they are not only\nsuperficially distinct, but also semantically cover a wide range of user\nintentions. Through extensive experiments on multiple datasets, we demonstrate\nthe effectiveness of the proposed framework, especially in improving the\nadaptability and robustness of recommendation systems in complex and dynamic\nenvironments. The experimental results demonstrate that GANPrompt yields\nsubstantial enhancements in accuracy and robustness relative to existing\nstate-of-the-art methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nproficiency in comprehending and generating natural language, with a growing\nprevalence in the domain of recommendation systems. However, LLMs still face a\nsignificant challenge called prompt sensitivity, which refers to that it is\nhighly susceptible to the influence of prompt words. This inconsistency in\nresponse to minor alterations in prompt input may compromise the accuracy and\nresilience of recommendation models. To address this issue, this paper proposes\nGANPrompt, a multi-dimensional LLMs prompt diversity framework based on\nGenerative Adversarial Networks (GANs). The framework enhances the model's\nadaptability and stability to diverse prompts by integrating GANs generation\ntechniques with the deep semantic understanding capabilities of LLMs. GANPrompt\nfirst trains a generator capable of producing diverse prompts by analysing\nmultidimensional user behavioural data. These diverse prompts are then used to\ntrain the LLMs to improve its performance in the face of unseen prompts.\nFurthermore, to ensure a high degree of diversity and relevance of the prompts,\nthis study introduces a mathematical theory-based diversity constraint\nmechanism that optimises the generated prompts to ensure that they are not only\nsuperficially distinct, but also semantically cover a wide range of user\nintentions. Through extensive experiments on multiple datasets, we demonstrate\nthe effectiveness of the proposed framework, especially in improving the\nadaptability and robustness of recommendation systems in complex and dynamic\nenvironments. The experimental results demonstrate that GANPrompt yields\nsubstantial enhancements in accuracy and robustness relative to existing\nstate-of-the-art methodologies."
                },
                "authors": [
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Chuang Zhao"
                    },
                    {
                        "name": "Hongke Zhao"
                    },
                    {
                        "name": "Likang Wu"
                    },
                    {
                        "name": "Ming HE"
                    }
                ],
                "author_detail": {
                    "name": "Ming HE"
                },
                "author": "Ming HE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09058v1",
                "updated": "2024-12-12T08:34:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    34,
                    12,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:34:12Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    34,
                    12,
                    3,
                    347,
                    0
                ],
                "title": "EmbedGenius: Towards Automated Software Development for Generic Embedded\n  IoT Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbedGenius: Towards Automated Software Development for Generic Embedded\n  IoT Systems"
                },
                "summary": "Embedded IoT system development is crucial for enabling seamless connectivity\nand functionality across a wide range of applications. However, such a complex\nprocess requires cross-domain knowledge of hardware and software and hence\noften necessitates direct developer involvement, making it labor-intensive,\ntime-consuming, and error-prone. To address this challenge, this paper\nintroduces EmbedGenius, the first fully automated software development platform\nfor general-purpose embedded IoT systems. The key idea is to leverage the\nreasoning ability of Large Language Models (LLMs) and embedded system expertise\nto automate the hardware-in-the-loop development process. The main methods\ninclude a component-aware library resolution method for addressing hardware\ndependencies, a library knowledge generation method that injects utility domain\nknowledge into LLMs, and an auto-programming method that ensures successful\ndeployment. We evaluate EmbedGenius's performance across 71 modules and four\nmainstream embedded development platforms with over 350 IoT tasks. Experimental\nresults show that EmbedGenius can generate codes with an accuracy of 95.7% and\ncomplete tasks with a success rate of 86.5%, surpassing human-in-the-loop\nbaselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show\nEmbedGenius's potential through case studies in environmental monitoring and\nremote control systems development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded IoT system development is crucial for enabling seamless connectivity\nand functionality across a wide range of applications. However, such a complex\nprocess requires cross-domain knowledge of hardware and software and hence\noften necessitates direct developer involvement, making it labor-intensive,\ntime-consuming, and error-prone. To address this challenge, this paper\nintroduces EmbedGenius, the first fully automated software development platform\nfor general-purpose embedded IoT systems. The key idea is to leverage the\nreasoning ability of Large Language Models (LLMs) and embedded system expertise\nto automate the hardware-in-the-loop development process. The main methods\ninclude a component-aware library resolution method for addressing hardware\ndependencies, a library knowledge generation method that injects utility domain\nknowledge into LLMs, and an auto-programming method that ensures successful\ndeployment. We evaluate EmbedGenius's performance across 71 modules and four\nmainstream embedded development platforms with over 350 IoT tasks. Experimental\nresults show that EmbedGenius can generate codes with an accuracy of 95.7% and\ncomplete tasks with a success rate of 86.5%, surpassing human-in-the-loop\nbaselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show\nEmbedGenius's potential through case studies in environmental monitoring and\nremote control systems development."
                },
                "authors": [
                    {
                        "name": "Huanqi Yang"
                    },
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Mingda Han"
                    },
                    {
                        "name": "Zhenjiang Li"
                    },
                    {
                        "name": "Weitao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weitao Xu"
                },
                "author": "Weitao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09049v1",
                "updated": "2024-12-12T08:19:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    19,
                    1,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:19:01Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    19,
                    1,
                    3,
                    347,
                    0
                ],
                "title": "Dial-In LLM: Human-Aligned Dialogue Intent Clustering with\n  LLM-in-the-loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dial-In LLM: Human-Aligned Dialogue Intent Clustering with\n  LLM-in-the-loop"
                },
                "summary": "The discovery of customer intention from dialogue plays an important role in\nautomated support system. However, traditional text clustering methods are\npoorly aligned with human perceptions due to the shift from embedding distance\nto semantic distance, and existing quantitative metrics for text clustering may\nnot accurately reflect the true quality of intent clusters. In this paper, we\nleverage the superior language understanding capabilities of Large Language\nModels (LLMs) for designing better-calibrated intent clustering algorithms. We\nfirst establish the foundation by verifying the robustness of fine-tuned LLM\nutility in semantic coherence evaluation and cluster naming, resulting in an\naccuracy of 97.50% and 94.40%, respectively, when compared to the human-labeled\nground truth. Then, we propose an iterative clustering algorithm that\nfacilitates cluster-level refinement and the continuous discovery of\nhigh-quality intent clusters. Furthermore, we present several LLM-in-the-loop\nsemi-supervised clustering techniques tailored for intent discovery from\ncustomer service dialogue. Experiments on a large-scale industrial dataset\ncomprising 1,507 intent clusters demonstrate the effectiveness of the proposed\ntechniques. The methods outperformed existing counterparts, achieving 6.25%\nimprovement in quantitative metrics and 12% enhancement in application-level\nperformance when constructing an intent classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of customer intention from dialogue plays an important role in\nautomated support system. However, traditional text clustering methods are\npoorly aligned with human perceptions due to the shift from embedding distance\nto semantic distance, and existing quantitative metrics for text clustering may\nnot accurately reflect the true quality of intent clusters. In this paper, we\nleverage the superior language understanding capabilities of Large Language\nModels (LLMs) for designing better-calibrated intent clustering algorithms. We\nfirst establish the foundation by verifying the robustness of fine-tuned LLM\nutility in semantic coherence evaluation and cluster naming, resulting in an\naccuracy of 97.50% and 94.40%, respectively, when compared to the human-labeled\nground truth. Then, we propose an iterative clustering algorithm that\nfacilitates cluster-level refinement and the continuous discovery of\nhigh-quality intent clusters. Furthermore, we present several LLM-in-the-loop\nsemi-supervised clustering techniques tailored for intent discovery from\ncustomer service dialogue. Experiments on a large-scale industrial dataset\ncomprising 1,507 intent clusters demonstrate the effectiveness of the proposed\ntechniques. The methods outperformed existing counterparts, achieving 6.25%\nimprovement in quantitative metrics and 12% enhancement in application-level\nperformance when constructing an intent classifier."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Yanjie Sun"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jason Zhang"
                },
                "author": "Chen Jason Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09048v1",
                "updated": "2024-12-12T08:17:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    17,
                    33,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:17:33Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    17,
                    33,
                    3,
                    347,
                    0
                ],
                "title": "Oversight in Action: Experiences with Instructor-Moderated LLM Responses\n  in an Online Discussion Forum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oversight in Action: Experiences with Instructor-Moderated LLM Responses\n  in an Online Discussion Forum"
                },
                "summary": "The integration of large language models (LLMs) into computing education\noffers many potential benefits to student learning, and several novel\npedagogical approaches have been reported in the literature. However LLMs also\npresent challenges, one of the most commonly cited being that of student\nover-reliance. This challenge is compounded by the fact that LLMs are always\navailable to provide instant help and solutions to students, which can\nundermine their ability to independently solve problems and diagnose and\nresolve errors. Providing instructor oversight of LLM-generated content can\nmitigate this problem, however it is often not practical in real-time learning\ncontexts. Online class discussion forums, which are widely used in computing\neducation, present an opportunity for exploring instructor oversight because\nthey operate asynchronously. Unlike real-time interactions, the discussion\nforum format aligns with the expectation that responses may take time, making\noversight not only feasible but also pedagogically appropriate. In this\npractitioner paper, we present the design, deployment, and evaluation of a\n`bot' module that is controlled by the instructor, and integrated into an\nonline discussion forum. The bot assists the instructor by generating draft\nresponses to student questions, which are reviewed, modified, and approved\nbefore release. Key features include the ability to leverage course materials,\naccess archived discussions, and publish responses anonymously to encourage\nopen participation. We report our experiences using this tool in a 12-week\nsecond-year software engineering course on object-oriented programming.\nInstructor feedback confirmed the tool successfully alleviated workload but\nhighlighted a need for improvement in handling complex, context-dependent\nqueries. We report the features that were viewed as most beneficial, and\nsuggest avenues for future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into computing education\noffers many potential benefits to student learning, and several novel\npedagogical approaches have been reported in the literature. However LLMs also\npresent challenges, one of the most commonly cited being that of student\nover-reliance. This challenge is compounded by the fact that LLMs are always\navailable to provide instant help and solutions to students, which can\nundermine their ability to independently solve problems and diagnose and\nresolve errors. Providing instructor oversight of LLM-generated content can\nmitigate this problem, however it is often not practical in real-time learning\ncontexts. Online class discussion forums, which are widely used in computing\neducation, present an opportunity for exploring instructor oversight because\nthey operate asynchronously. Unlike real-time interactions, the discussion\nforum format aligns with the expectation that responses may take time, making\noversight not only feasible but also pedagogically appropriate. In this\npractitioner paper, we present the design, deployment, and evaluation of a\n`bot' module that is controlled by the instructor, and integrated into an\nonline discussion forum. The bot assists the instructor by generating draft\nresponses to student questions, which are reviewed, modified, and approved\nbefore release. Key features include the ability to leverage course materials,\naccess archived discussions, and publish responses anonymously to encourage\nopen participation. We report our experiences using this tool in a 12-week\nsecond-year software engineering course on object-oriented programming.\nInstructor feedback confirmed the tool successfully alleviated workload but\nhighlighted a need for improvement in handling complex, context-dependent\nqueries. We report the features that were viewed as most beneficial, and\nsuggest avenues for future exploration."
                },
                "authors": [
                    {
                        "name": "Shuying Qiao"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Nasser Giacaman"
                    }
                ],
                "author_detail": {
                    "name": "Nasser Giacaman"
                },
                "author": "Nasser Giacaman",
                "arxiv_comment": "Accepted to ACE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09046v1",
                "updated": "2024-12-12T08:15:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    15,
                    16,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:15:16Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    15,
                    16,
                    3,
                    347,
                    0
                ],
                "title": "Multi-Task Learning with LLMs for Implicit Sentiment Analysis:\n  Data-level and Task-level Automatic Weight Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Task Learning with LLMs for Implicit Sentiment Analysis:\n  Data-level and Task-level Automatic Weight Learning"
                },
                "summary": "Implicit sentiment analysis (ISA) presents significant challenges due to the\nabsence of salient cue words. Previous methods have struggled with insufficient\ndata and limited reasoning capabilities to infer underlying opinions.\nIntegrating multi-task learning (MTL) with large language models (LLMs) offers\nthe potential to enable models of varying sizes to reliably perceive and\nrecognize genuine opinions in ISA. However, existing MTL approaches are\nconstrained by two sources of uncertainty: data-level uncertainty, arising from\nhallucination problems in LLM-generated contextual information, and task-level\nuncertainty, stemming from the varying capacities of models to process\ncontextual information. To handle these uncertainties, we introduce MT-ISA, a\nnovel MTL framework that enhances ISA by leveraging the generation and\nreasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA\nconstructs auxiliary tasks using generative LLMs to supplement sentiment\nelements and incorporates automatic MTL to fully exploit auxiliary data. We\nintroduce data-level and task-level automatic weight learning (AWL), which\ndynamically identifies relationships and prioritizes more reliable data and\ncritical tasks, enabling models of varying sizes to adaptively learn\nfine-grained weights based on their reasoning capabilities. We investigate\nthree strategies for data-level AWL, while also introducing homoscedastic\nuncertainty for task-level AWL. Extensive experiments reveal that models of\nvarying sizes achieve an optimal balance between primary prediction and\nauxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit sentiment analysis (ISA) presents significant challenges due to the\nabsence of salient cue words. Previous methods have struggled with insufficient\ndata and limited reasoning capabilities to infer underlying opinions.\nIntegrating multi-task learning (MTL) with large language models (LLMs) offers\nthe potential to enable models of varying sizes to reliably perceive and\nrecognize genuine opinions in ISA. However, existing MTL approaches are\nconstrained by two sources of uncertainty: data-level uncertainty, arising from\nhallucination problems in LLM-generated contextual information, and task-level\nuncertainty, stemming from the varying capacities of models to process\ncontextual information. To handle these uncertainties, we introduce MT-ISA, a\nnovel MTL framework that enhances ISA by leveraging the generation and\nreasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA\nconstructs auxiliary tasks using generative LLMs to supplement sentiment\nelements and incorporates automatic MTL to fully exploit auxiliary data. We\nintroduce data-level and task-level automatic weight learning (AWL), which\ndynamically identifies relationships and prioritizes more reliable data and\ncritical tasks, enabling models of varying sizes to adaptively learn\nfine-grained weights based on their reasoning capabilities. We investigate\nthree strategies for data-level AWL, while also introducing homoscedastic\nuncertainty for task-level AWL. Extensive experiments reveal that models of\nvarying sizes achieve an optimal balance between primary prediction and\nauxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability\nof our approach."
                },
                "authors": [
                    {
                        "name": "Wenna Lai"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "Guandong Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "11 pages, 6 figures, and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16884v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16884v3",
                "updated": "2024-12-12T08:05:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    5,
                    10,
                    3,
                    347,
                    0
                ],
                "published": "2024-05-27T07:05:27Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    7,
                    5,
                    27,
                    0,
                    148,
                    0
                ],
                "title": "Match, Compare, or Select? An Investigation of Large Language Models for\n  Entity Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Match, Compare, or Select? An Investigation of Large Language Models for\n  Entity Matching"
                },
                "summary": "Entity matching (EM) is a critical step in entity resolution (ER). Recently,\nentity matching based on large language models (LLMs) has shown great promise.\nHowever, current LLM-based entity matching approaches typically follow a binary\nmatching paradigm that ignores the global consistency among record\nrelationships. In this paper, we investigate various methodologies for\nLLM-based entity matching that incorporate record interactions from different\nperspectives. Specifically, we comprehensively compare three representative\nstrategies: matching, comparing, and selecting, and analyze their respective\nadvantages and challenges in diverse scenarios. Based on our findings, we\nfurther design a compound entity matching framework (ComEM) that leverages the\ncomposition of multiple strategies and LLMs. ComEM benefits from the advantages\nof different sides and achieves improvements in both effectiveness and\nefficiency. Experimental results on 8 ER datasets and 10 LLMs verify the\nsuperiority of incorporating record interactions through the selecting\nstrategy, as well as the further cost-effectiveness brought by ComEM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching (EM) is a critical step in entity resolution (ER). Recently,\nentity matching based on large language models (LLMs) has shown great promise.\nHowever, current LLM-based entity matching approaches typically follow a binary\nmatching paradigm that ignores the global consistency among record\nrelationships. In this paper, we investigate various methodologies for\nLLM-based entity matching that incorporate record interactions from different\nperspectives. Specifically, we comprehensively compare three representative\nstrategies: matching, comparing, and selecting, and analyze their respective\nadvantages and challenges in diverse scenarios. Based on our findings, we\nfurther design a compound entity matching framework (ComEM) that leverages the\ncomposition of multiple strategies and LLMs. ComEM benefits from the advantages\nof different sides and achieves improvements in both effectiveness and\nefficiency. Experimental results on 8 ER datasets and 10 LLMs verify the\nsuperiority of incorporating record interactions through the selecting\nstrategy, as well as the further cost-effectiveness brought by ComEM."
                },
                "authors": [
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Xiaoyang Chen"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xuanang Chen"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Zhenyu Zeng"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "Accepted at COLING 2025. Our code is available at\n  https://github.com/tshu-w/ComEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16884v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16884v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13282v3",
                "updated": "2024-12-12T08:00:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    0,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-19T07:23:33Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    7,
                    23,
                    33,
                    2,
                    171,
                    0
                ],
                "title": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective"
                },
                "summary": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09974v2",
                "updated": "2024-12-12T07:52:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-15T06:08:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    8,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "A Framework for Using LLMs for Repository Mining Studies in Empirical\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Using LLMs for Repository Mining Studies in Empirical\n  Software Engineering"
                },
                "summary": "Context: The emergence of Large Language Models (LLMs) has significantly\ntransformed Software Engineering (SE) by providing innovative methods for\nanalyzing software repositories. Objectives: Our objective is to establish a\npractical framework for future SE researchers needing to enhance the data\ncollection and dataset while conducting software repository mining studies\nusing LLMs. Method: This experience report shares insights from two previous\nrepository mining studies, focusing on the methodologies used for creating,\nrefining, and validating prompts that enhance the output of LLMs, particularly\nin the context of data collection in empirical studies. Results: Our research\npackages a framework, coined Prompt Refinement and Insights for Mining\nEmpirical Software repositories (PRIMES), consisting of a checklist that can\nimprove LLM usage performance, enhance output quality, and minimize errors\nthrough iterative processes and comparisons among different LLMs. We also\nemphasize the significance of reproducibility by implementing mechanisms for\ntracking model results. Conclusion: Our findings indicate that standardizing\nprompt engineering and using PRIMES can enhance the reliability and\nreproducibility of studies utilizing LLMs. Ultimately, this work calls for\nfurther research to address challenges like hallucinations, model biases, and\ncost-effectiveness in integrating LLMs into workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: The emergence of Large Language Models (LLMs) has significantly\ntransformed Software Engineering (SE) by providing innovative methods for\nanalyzing software repositories. Objectives: Our objective is to establish a\npractical framework for future SE researchers needing to enhance the data\ncollection and dataset while conducting software repository mining studies\nusing LLMs. Method: This experience report shares insights from two previous\nrepository mining studies, focusing on the methodologies used for creating,\nrefining, and validating prompts that enhance the output of LLMs, particularly\nin the context of data collection in empirical studies. Results: Our research\npackages a framework, coined Prompt Refinement and Insights for Mining\nEmpirical Software repositories (PRIMES), consisting of a checklist that can\nimprove LLM usage performance, enhance output quality, and minimize errors\nthrough iterative processes and comparisons among different LLMs. We also\nemphasize the significance of reproducibility by implementing mechanisms for\ntracking model results. Conclusion: Our findings indicate that standardizing\nprompt engineering and using PRIMES can enhance the reliability and\nreproducibility of studies utilizing LLMs. Ultimately, this work calls for\nfurther research to address challenges like hallucinations, model biases, and\ncost-effectiveness in integrating LLMs into workflows."
                },
                "authors": [
                    {
                        "name": "Vincenzo de Martino"
                    },
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11147v2",
                "updated": "2024-12-12T07:50:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    50,
                    43,
                    3,
                    347,
                    0
                ],
                "published": "2024-09-17T12:58:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    58,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable few-shot learning\ncapabilities and unified the paradigm of NLP tasks through the in-context\nlearning (ICL) technique. Despite the success of ICL, the quality of the\nexemplar demonstrations can significantly influence the LLM's performance.\nExisting exemplar selection methods mainly focus on the semantic similarity\nbetween queries and candidate exemplars. On the other hand, the logical\nconnections between reasoning steps can be beneficial to depict the\nproblem-solving process as well. In this paper, we proposes a novel method\nnamed Reasoning Graph-enhanced Exemplar Retrieval (RGER). RGER first quires LLM\nto generate an initial response, then expresses intermediate problem-solving\nsteps to a graph structure. After that, it employs graph kernel to select\nexemplars with semantic and structural similarity. Extensive experiments\ndemonstrate the structural relationship is helpful to the alignment of queries\nand candidate exemplars. The efficacy of RGER on math and logit reasoning tasks\nshowcases its superiority over state-of-the-art retrieval-based approaches. Our\ncode is released at https://github.com/Yukang-Lin/RGER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable few-shot learning\ncapabilities and unified the paradigm of NLP tasks through the in-context\nlearning (ICL) technique. Despite the success of ICL, the quality of the\nexemplar demonstrations can significantly influence the LLM's performance.\nExisting exemplar selection methods mainly focus on the semantic similarity\nbetween queries and candidate exemplars. On the other hand, the logical\nconnections between reasoning steps can be beneficial to depict the\nproblem-solving process as well. In this paper, we proposes a novel method\nnamed Reasoning Graph-enhanced Exemplar Retrieval (RGER). RGER first quires LLM\nto generate an initial response, then expresses intermediate problem-solving\nsteps to a graph structure. After that, it employs graph kernel to select\nexemplars with semantic and structural similarity. Extensive experiments\ndemonstrate the structural relationship is helpful to the alignment of queries\nand candidate exemplars. The efficacy of RGER on math and logit reasoning tasks\nshowcases its superiority over state-of-the-art retrieval-based approaches. Our\ncode is released at https://github.com/Yukang-Lin/RGER."
                },
                "authors": [
                    {
                        "name": "Yukang Lin"
                    },
                    {
                        "name": "Bingchen Zhong"
                    },
                    {
                        "name": "Shuoran Jiang"
                    },
                    {
                        "name": "Joanna Siebert"
                    },
                    {
                        "name": "Qingcai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingcai Chen"
                },
                "author": "Qingcai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01268v2",
                "updated": "2024-12-12T07:29:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    29,
                    41,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-02T06:24:51Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    6,
                    24,
                    51,
                    2,
                    276,
                    0
                ],
                "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Unveiling AI's Potential Through Tools, Techniques, and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Unveiling AI's Potential Through Tools, Techniques, and\n  Applications"
                },
                "summary": "Artificial intelligence (AI), machine learning, and deep learning have become\ntransformative forces in big data analytics and management, enabling\ngroundbreaking advancements across diverse industries. This article delves into\nthe foundational concepts and cutting-edge developments in these fields, with a\nparticular focus on large language models (LLMs) and their role in natural\nlanguage processing, multimodal reasoning, and autonomous decision-making.\nHighlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores\ntheir applications in data analysis, model design, and optimization.\n  The integration of advanced algorithms like neural networks, reinforcement\nlearning, and generative models has enhanced the capabilities of AI systems to\nprocess, visualize, and interpret complex datasets. Additionally, the emergence\nof technologies like edge computing and automated machine learning (AutoML)\ndemocratizes access to AI, empowering users across skill levels to engage with\nintelligent systems. This work also underscores the importance of ethical\nconsiderations, transparency, and fairness in the deployment of AI\ntechnologies, paving the way for responsible innovation.\n  Through practical insights into hardware configurations, software\nenvironments, and real-world applications, this article serves as a\ncomprehensive resource for researchers and practitioners. By bridging\ntheoretical underpinnings with actionable strategies, it showcases the\npotential of AI and LLMs to revolutionize big data management and drive\nmeaningful advancements across domains such as healthcare, finance, and\nautonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI), machine learning, and deep learning have become\ntransformative forces in big data analytics and management, enabling\ngroundbreaking advancements across diverse industries. This article delves into\nthe foundational concepts and cutting-edge developments in these fields, with a\nparticular focus on large language models (LLMs) and their role in natural\nlanguage processing, multimodal reasoning, and autonomous decision-making.\nHighlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores\ntheir applications in data analysis, model design, and optimization.\n  The integration of advanced algorithms like neural networks, reinforcement\nlearning, and generative models has enhanced the capabilities of AI systems to\nprocess, visualize, and interpret complex datasets. Additionally, the emergence\nof technologies like edge computing and automated machine learning (AutoML)\ndemocratizes access to AI, empowering users across skill levels to engage with\nintelligent systems. This work also underscores the importance of ethical\nconsiderations, transparency, and fairness in the deployment of AI\ntechnologies, paving the way for responsible innovation.\n  Through practical insights into hardware configurations, software\nenvironments, and real-world applications, this article serves as a\ncomprehensive resource for researchers and practitioners. By bridging\ntheoretical underpinnings with actionable strategies, it showcases the\npotential of AI and LLMs to revolutionize big data management and drive\nmeaningful advancements across domains such as healthcare, finance, and\nautonomous systems."
                },
                "authors": [
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Xuanhe Pan"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Jinlang Wang"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Tianyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyang Wang"
                },
                "author": "Tianyang Wang",
                "arxiv_comment": "This book contains 155 pages and 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09012v1",
                "updated": "2024-12-12T07:23:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    23,
                    52,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:23:52Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    23,
                    52,
                    3,
                    347,
                    0
                ],
                "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Cryptic Crosswords Challenging for LLMs?"
                },
                "summary": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Sadallah"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "COLING 2025",
                "arxiv_journal_ref": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08985v1",
                "updated": "2024-12-12T06:38:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    38,
                    40,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T06:38:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    38,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "Assessing the Robustness of Retrieval-Augmented Generation Systems in\n  K-12 Educational Question Answering with Knowledge Discrepancies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Robustness of Retrieval-Augmented Generation Systems in\n  K-12 Educational Question Answering with Knowledge Discrepancies"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable\npotential as question answering systems in the K-12 Education domain, where\nknowledge is typically queried within the restricted scope of authoritative\ntextbooks. However, the discrepancy between textbooks and the parametric\nknowledge in Large Language Models (LLMs) could undermine the effectiveness of\nRAG systems. To systematically investigate the robustness of RAG systems under\nsuch knowledge discrepancies, we present EduKDQA, a question answering dataset\nthat simulates knowledge discrepancies in real applications by applying\nhypothetical knowledge updates in answers and source documents. EduKDQA\nincludes 3,005 questions covering five subjects, under a comprehensive question\ntypology from the perspective of context utilization and knowledge integration.\nWe conducted extensive experiments on retrieval and question answering\nperformance. We find that most RAG systems suffer from a substantial\nperformance drop in question answering with knowledge discrepancies, while\nquestions that require integration of contextual knowledge and parametric\nknowledge pose a challenge to LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable\npotential as question answering systems in the K-12 Education domain, where\nknowledge is typically queried within the restricted scope of authoritative\ntextbooks. However, the discrepancy between textbooks and the parametric\nknowledge in Large Language Models (LLMs) could undermine the effectiveness of\nRAG systems. To systematically investigate the robustness of RAG systems under\nsuch knowledge discrepancies, we present EduKDQA, a question answering dataset\nthat simulates knowledge discrepancies in real applications by applying\nhypothetical knowledge updates in answers and source documents. EduKDQA\nincludes 3,005 questions covering five subjects, under a comprehensive question\ntypology from the perspective of context utilization and knowledge integration.\nWe conducted extensive experiments on retrieval and question answering\nperformance. We find that most RAG systems suffer from a substantial\nperformance drop in question answering with knowledge discrepancies, while\nquestions that require integration of contextual knowledge and parametric\nknowledge pose a challenge to LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18417v2",
                "updated": "2024-12-12T06:18:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    18,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-09-27T03:15:07Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    15,
                    7,
                    4,
                    271,
                    0
                ],
                "title": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement\n  Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement\n  Learning from Human Feedback"
                },
                "summary": "This paper addresses the cost-efficiency aspect of Reinforcement Learning\nfrom Human Feedback (RLHF). RLHF leverages datasets of human preferences over\noutputs of large language models (LLM)s to instill human expectations into\nLLMs. Although preference annotation comes with a monetized cost, the economic\nutility of a preference dataset has not been considered by far. What\nexacerbates this situation is that, given complex intransitive or cyclic\nrelationships in preference datasets, existing algorithms for fine-tuning LLMs\nare still far from capturing comprehensive preferences. This raises severe\ncost-efficiency concerns in production environments, where preference data\naccumulate over time. In this paper, we discuss the fine-tuning of LLMs as a\nmonetized economy and introduce an auction mechanism to improve the efficiency\nof preference data collection in dollar terms. We show that introducing an\nauction mechanism can play an essential role in enhancing the cost-efficiency\nof RLHF, while maintaining satisfactory model performance. Experimental results\ndemonstrate that our proposed auction-based protocol is cost-effective for\nfine-tuning LLMs concentrating on high-quality feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the cost-efficiency aspect of Reinforcement Learning\nfrom Human Feedback (RLHF). RLHF leverages datasets of human preferences over\noutputs of large language models (LLM)s to instill human expectations into\nLLMs. Although preference annotation comes with a monetized cost, the economic\nutility of a preference dataset has not been considered by far. What\nexacerbates this situation is that, given complex intransitive or cyclic\nrelationships in preference datasets, existing algorithms for fine-tuning LLMs\nare still far from capturing comprehensive preferences. This raises severe\ncost-efficiency concerns in production environments, where preference data\naccumulate over time. In this paper, we discuss the fine-tuning of LLMs as a\nmonetized economy and introduce an auction mechanism to improve the efficiency\nof preference data collection in dollar terms. We show that introducing an\nauction mechanism can play an essential role in enhancing the cost-efficiency\nof RLHF, while maintaining satisfactory model performance. Experimental results\ndemonstrate that our proposed auction-based protocol is cost-effective for\nfine-tuning LLMs concentrating on high-quality feedback."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhang"
                    },
                    {
                        "name": "Jiuding Duan"
                    }
                ],
                "author_detail": {
                    "name": "Jiuding Duan"
                },
                "author": "Jiuding Duan",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08972v1",
                "updated": "2024-12-12T06:08:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    8,
                    46,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T06:08:46Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    8,
                    46,
                    3,
                    347,
                    0
                ],
                "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios"
                },
                "summary": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. These results highlight significant challenges in advancing\nLLMs' rule-guided reasoning capabilities in real-life applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. These results highlight significant challenges in advancing\nLLMs' rule-guided reasoning capabilities in real-life applications."
                },
                "authors": [
                    {
                        "name": "Ruiwen Zhou"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "Data and Codes are available at\n  https://github.com/skyriver-2000/RuleArena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08970v1",
                "updated": "2024-12-12T06:04:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    4,
                    31,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T06:04:31Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    4,
                    31,
                    3,
                    347,
                    0
                ],
                "title": "Reasoning-Aware Query-Focused Summarization over Multi-Table Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Aware Query-Focused Summarization over Multi-Table Data"
                },
                "summary": "Query-focused summarization over multi-table data is a challenging yet\ncritical task for extracting precise and relevant information from structured\ndata. Existing methods often rely on complex preprocessing steps and struggle\nto generalize across domains or handle the logical reasoning required for\nmulti-table queries. In this paper, we propose QueryTableSummarizer++, an\nend-to-end generative framework leveraging large language models (LLMs)\nenhanced with table-aware pre-training, query-aligned fine-tuning, and\nreinforcement learning with feedback. Our method eliminates the need for\nintermediate serialization steps and directly generates query-relevant\nsummaries. Experiments on a benchmark dataset demonstrate that\nQueryTableSummarizer++ significantly outperforms state-of-the-art baselines in\nterms of BLEU, ROUGE, and F1-score. Additional analyses highlight its\nscalability, generalization across domains, and robust handling of complex\nqueries. Human evaluation further validates the superior quality and practical\napplicability of the generated summaries, establishing QueryTableSummarizer++\nas a highly effective solution for multi-table summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-focused summarization over multi-table data is a challenging yet\ncritical task for extracting precise and relevant information from structured\ndata. Existing methods often rely on complex preprocessing steps and struggle\nto generalize across domains or handle the logical reasoning required for\nmulti-table queries. In this paper, we propose QueryTableSummarizer++, an\nend-to-end generative framework leveraging large language models (LLMs)\nenhanced with table-aware pre-training, query-aligned fine-tuning, and\nreinforcement learning with feedback. Our method eliminates the need for\nintermediate serialization steps and directly generates query-relevant\nsummaries. Experiments on a benchmark dataset demonstrate that\nQueryTableSummarizer++ significantly outperforms state-of-the-art baselines in\nterms of BLEU, ROUGE, and F1-score. Additional analyses highlight its\nscalability, generalization across domains, and robust handling of complex\nqueries. Human evaluation further validates the superior quality and practical\napplicability of the generated summaries, establishing QueryTableSummarizer++\nas a highly effective solution for multi-table summarization tasks."
                },
                "authors": [
                    {
                        "name": "Xiaochuan Lin"
                    },
                    {
                        "name": "Xiangyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyong Chen"
                },
                "author": "Xiangyong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08955v1",
                "updated": "2024-12-12T05:36:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    36,
                    51,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T05:36:51Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    36,
                    51,
                    3,
                    347,
                    0
                ],
                "title": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Cross-lingual in-context learning (XICL) has emerged as a transformative\nparadigm for leveraging large language models (LLMs) to tackle multilingual\ntasks, especially for low-resource languages. However, existing approaches\noften rely on external retrievers or task-specific fine-tuning, limiting their\nscalability and generalizability. In this paper, we propose a novel\nself-supervised framework that harnesses the generative capabilities of LLMs to\ninternally select and utilize task-relevant examples. Our method introduces two\nkey objectives: a retrieval-generation alignment loss to optimize the quality\nof selected examples and a semantic coherence loss to ensure cross-lingual\nconsistency. Through extensive experiments on multilingual benchmarks, our\napproach achieves state-of-the-art performance, significantly outperforming\nexisting baselines. Further analysis highlights its robustness across diverse\nlanguage families and its ability to generalize to unseen tasks. Human\nevaluations confirm the superior fluency, relevance, and semantic correctness\nof outputs generated by our method. This work provides a scalable, effective,\nand generalizable solution for cross-lingual in-context learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-lingual in-context learning (XICL) has emerged as a transformative\nparadigm for leveraging large language models (LLMs) to tackle multilingual\ntasks, especially for low-resource languages. However, existing approaches\noften rely on external retrievers or task-specific fine-tuning, limiting their\nscalability and generalizability. In this paper, we propose a novel\nself-supervised framework that harnesses the generative capabilities of LLMs to\ninternally select and utilize task-relevant examples. Our method introduces two\nkey objectives: a retrieval-generation alignment loss to optimize the quality\nof selected examples and a semantic coherence loss to ensure cross-lingual\nconsistency. Through extensive experiments on multilingual benchmarks, our\napproach achieves state-of-the-art performance, significantly outperforming\nexisting baselines. Further analysis highlights its robustness across diverse\nlanguage families and its ability to generalize to unseen tasks. Human\nevaluations confirm the superior fluency, relevance, and semantic correctness\nof outputs generated by our method. This work provides a scalable, effective,\nand generalizable solution for cross-lingual in-context learning."
                },
                "authors": [
                    {
                        "name": "Mateo Alejandro Rojas"
                    },
                    {
                        "name": "Rafael Carranza"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Carranza"
                },
                "author": "Rafael Carranza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15650v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15650v3",
                "updated": "2024-12-12T05:34:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    34,
                    36,
                    3,
                    347,
                    0
                ],
                "published": "2024-04-24T05:08:55Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    5,
                    8,
                    55,
                    2,
                    115,
                    0
                ],
                "title": "Return of EM: Entity-driven Answer Set Expansion for QA Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Return of EM: Entity-driven Answer Set Expansion for QA Evaluation"
                },
                "summary": "Recently, directly using large language models (LLMs) has been shown to be\nthe most reliable method to evaluate QA models. However, it suffers from\nlimited interpretability, high cost, and environmental harm. To address these,\nwe propose to use soft EM with entity-driven answer set expansion. Our approach\nexpands the gold answer set to include diverse surface forms, based on the\nobservation that the surface forms often follow particular patterns depending\non the entity type. The experimental results show that our method outperforms\ntraditional evaluation methods by a large margin. Moreover, the reliability of\nour evaluation method is comparable to that of LLM-based ones, while offering\nthe benefits of high interpretability and reduced environmental harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, directly using large language models (LLMs) has been shown to be\nthe most reliable method to evaluate QA models. However, it suffers from\nlimited interpretability, high cost, and environmental harm. To address these,\nwe propose to use soft EM with entity-driven answer set expansion. Our approach\nexpands the gold answer set to include diverse surface forms, based on the\nobservation that the surface forms often follow particular patterns depending\non the entity type. The experimental results show that our method outperforms\ntraditional evaluation methods by a large margin. Moreover, the reliability of\nour evaluation method is comparable to that of LLM-based ones, while offering\nthe benefits of high interpretability and reduced environmental harm."
                },
                "authors": [
                    {
                        "name": "Dongryeol Lee"
                    },
                    {
                        "name": "Minwoo Lee"
                    },
                    {
                        "name": "Kyungmin Min"
                    },
                    {
                        "name": "Joonsuk Park"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "Accepted at COLING 2025 (16 pages, 4 figures, 11 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15650v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15650v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06126v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06126v4",
                "updated": "2024-12-12T05:28:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    28,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-09T01:18:16Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    1,
                    18,
                    16,
                    4,
                    40,
                    0
                ],
                "title": "Learn To be Efficient: Build Structured Sparsity in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn To be Efficient: Build Structured Sparsity in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success with their\nbillion-level parameters, yet they incur high inference overheads. The\nemergence of activation sparsity in LLMs provides a natural approach to reduce\nthis cost by involving only parts of the parameters for inference. However,\nexisting methods only focus on utilizing this naturally formed activation\nsparsity in a post-training setting, overlooking the potential for further\namplifying this inherent sparsity. In this paper, we hypothesize that LLMs can\nlearn to be efficient by achieving more structured activation sparsity. To\nachieve this, we introduce a novel training algorithm, Learn-To-be-Efficient\n(LTE), designed to train efficiency-aware LLMs to learn to activate fewer\nneurons and achieve a better trade-off between sparsity and performance.\nFurthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based\nmodels, LTE can also be applied to LLMs like LLaMA using non-ReLU activations.\nExtensive evaluation on language understanding, language generation, and\ninstruction tuning tasks show that LTE consistently outperforms SOTA baselines.\nAlong with our hardware-aware custom kernel implementation, LTE reduces\nLLaMA2-7B inference latency by 25% at 50% sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success with their\nbillion-level parameters, yet they incur high inference overheads. The\nemergence of activation sparsity in LLMs provides a natural approach to reduce\nthis cost by involving only parts of the parameters for inference. However,\nexisting methods only focus on utilizing this naturally formed activation\nsparsity in a post-training setting, overlooking the potential for further\namplifying this inherent sparsity. In this paper, we hypothesize that LLMs can\nlearn to be efficient by achieving more structured activation sparsity. To\nachieve this, we introduce a novel training algorithm, Learn-To-be-Efficient\n(LTE), designed to train efficiency-aware LLMs to learn to activate fewer\nneurons and achieve a better trade-off between sparsity and performance.\nFurthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based\nmodels, LTE can also be applied to LLMs like LLaMA using non-ReLU activations.\nExtensive evaluation on language understanding, language generation, and\ninstruction tuning tasks show that LTE consistently outperforms SOTA baselines.\nAlong with our hardware-aware custom kernel implementation, LTE reduces\nLLaMA2-7B inference latency by 25% at 50% sparsity."
                },
                "authors": [
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Z. Morley Mao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Atul Prakash"
                    }
                ],
                "author_detail": {
                    "name": "Atul Prakash"
                },
                "author": "Atul Prakash",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06126v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06126v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12683v2",
                "updated": "2024-12-12T05:19:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    19,
                    43,
                    3,
                    347,
                    0
                ],
                "published": "2024-02-20T03:14:47Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    3,
                    14,
                    47,
                    1,
                    51,
                    0
                ],
                "title": "TorchCP: A Python Library for Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TorchCP: A Python Library for Conformal Prediction"
                },
                "summary": "Conformal Prediction (CP) has attracted great attention from the research\ncommunity due to its strict theoretical guarantees. However, researchers and\ndevelopers still face challenges of applicability and efficiency when applying\nCP algorithms to deep learning models. In this paper, we introduce \\torchcp, a\ncomprehensive PyTorch-based toolkit to strengthen the usability of CP for deep\nlearning models. \\torchcp implements a wide range of post-hoc and training\nmethods of conformal prediction for various machine learning tasks, including\nclassification, regression, GNN, and LLM. Moreover, we provide user-friendly\ninterfaces and extensive evaluations to easily integrate CP algorithms into\nspecific tasks. Our \\torchcp toolkit, built entirely with PyTorch, enables\nhigh-performance GPU acceleration for deep learning models and mini-batch\ncomputation on large-scale datasets. With the LGPL license, the code is\nopen-sourced at \\url{https://github.com/ml-stat-Sustech/TorchCP} and will be\ncontinuously updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction (CP) has attracted great attention from the research\ncommunity due to its strict theoretical guarantees. However, researchers and\ndevelopers still face challenges of applicability and efficiency when applying\nCP algorithms to deep learning models. In this paper, we introduce \\torchcp, a\ncomprehensive PyTorch-based toolkit to strengthen the usability of CP for deep\nlearning models. \\torchcp implements a wide range of post-hoc and training\nmethods of conformal prediction for various machine learning tasks, including\nclassification, regression, GNN, and LLM. Moreover, we provide user-friendly\ninterfaces and extensive evaluations to easily integrate CP algorithms into\nspecific tasks. Our \\torchcp toolkit, built entirely with PyTorch, enables\nhigh-performance GPU acceleration for deep learning models and mini-batch\ncomputation on large-scale datasets. With the LGPL license, the code is\nopen-sourced at \\url{https://github.com/ml-stat-Sustech/TorchCP} and will be\ncontinuously updated."
                },
                "authors": [
                    {
                        "name": "Jianguo Huang"
                    },
                    {
                        "name": "Jianqing Song"
                    },
                    {
                        "name": "Xuanning Zhou"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07870v5",
                "updated": "2024-12-12T05:02:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    2,
                    11,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-12T15:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    15,
                    26,
                    17,
                    1,
                    317,
                    0
                ],
                "title": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders"
                },
                "summary": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Jaya Krishna Mandivarapu"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Krishna Mandivarapu"
                },
                "author": "Jaya Krishna Mandivarapu",
                "arxiv_doi": "10.18653/v1/2024.customnlp4u-1.13",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.customnlp4u-1.13",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.07870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EMNLP CustomNLP4U 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07334v2",
                "updated": "2024-12-12T05:01:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    1,
                    4,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-10T09:25:39Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    25,
                    39,
                    1,
                    345,
                    0
                ],
                "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation"
                },
                "summary": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git"
                },
                "authors": [
                    {
                        "name": "Pedro H. V. Valois"
                    },
                    {
                        "name": "Lincon S. Souza"
                    },
                    {
                        "name": "Erica K. Shimomoto"
                    },
                    {
                        "name": "Kazuhiro Fukui"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Fukui"
                },
                "author": "Kazuhiro Fukui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07038v2",
                "updated": "2024-12-12T04:14:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    4,
                    14,
                    53,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-09T22:53:49Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    22,
                    53,
                    49,
                    0,
                    344,
                    0
                ],
                "title": "Optimizing Beam-Plasma Interactions Through Jitter Analysis Using\n  Start-to-End Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Beam-Plasma Interactions Through Jitter Analysis Using\n  Start-to-End Simulations"
                },
                "summary": "Traditional accelerators, while effective, suffer from extensive spatial and\nfinancial demands, necessitating the exploration of compact alternatives like\nPWFA, which significantly reduces the necessary accelerator length by utilizing\nthe wake generated by a high-speed pulse traveling through plasma. Our research\nfocuses on mitigating instabilities, particularly timing jitter, which\ncritically impacts the quality of accelerated beams. Through the deployment of\nImpact-T, Bmad, and Tao simulation tools at the FACET-II facility, we examined\nhow timing jitter influences key beam parameters, including peak currents and\nemittance, over various simulation scenarios. The findings reveal that even\nminute variations in accelerator settings can significantly influence beam\ncharacteristics, underscoring the importance of precise control in beam\ndynamics. The outcomes contribute to enhancing the reliability and precision of\nPWFA systems, promising improved applications in both scientific research and\nmedical therapies. Future research directions include integrating machine\nlearning techniques to refine control strategies further and reduce\nexperimental redundancies, highlighting the evolving synergy between\naccelerator physics and computational data science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional accelerators, while effective, suffer from extensive spatial and\nfinancial demands, necessitating the exploration of compact alternatives like\nPWFA, which significantly reduces the necessary accelerator length by utilizing\nthe wake generated by a high-speed pulse traveling through plasma. Our research\nfocuses on mitigating instabilities, particularly timing jitter, which\ncritically impacts the quality of accelerated beams. Through the deployment of\nImpact-T, Bmad, and Tao simulation tools at the FACET-II facility, we examined\nhow timing jitter influences key beam parameters, including peak currents and\nemittance, over various simulation scenarios. The findings reveal that even\nminute variations in accelerator settings can significantly influence beam\ncharacteristics, underscoring the importance of precise control in beam\ndynamics. The outcomes contribute to enhancing the reliability and precision of\nPWFA systems, promising improved applications in both scientific research and\nmedical therapies. Future research directions include integrating machine\nlearning techniques to refine control strategies further and reduce\nexperimental redundancies, highlighting the evolving synergy between\naccelerator physics and computational data science."
                },
                "authors": [
                    {
                        "name": "Robin Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Robin Hwang"
                },
                "author": "Robin Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08908v1",
                "updated": "2024-12-12T03:44:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    44,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:44:17Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    44,
                    17,
                    3,
                    347,
                    0
                ],
                "title": "WiFo: Wireless Foundation Model for Channel Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiFo: Wireless Foundation Model for Channel Prediction"
                },
                "summary": "Channel prediction permits to acquire channel state information (CSI) without\nsignaling overhead. However, almost all existing channel prediction methods\nnecessitate the deployment of a dedicated model to accommodate a specific\nconfiguration. Leveraging the powerful modeling and multi-task learning\ncapabilities of foundation models, we propose the first space-time-frequency\n(STF) wireless foundation model (WiFo) to address time-frequency channel\nprediction tasks in a one-for-all manner. Specifically, WiFo is initially\npre-trained over massive and extensive diverse CSI datasets. Then, the model\nwill be instantly used for channel prediction under various CSI configurations\nwithout any fine-tuning. We propose a masked autoencoder (MAE)-based network\nstructure for WiFo to handle heterogeneous STF CSI data, and design several\nmask reconstruction tasks for self-supervised pre-training to capture the\ninherent 3D variations of CSI. To fully unleash its predictive power, we build\na large-scale heterogeneous simulated CSI dataset consisting of 160K CSI\nsamples for pre-training. Simulations validate its superior unified learning\nperformance across multiple datasets and demonstrate its state-of-the-art\n(SOTA) zero-shot generalization performance via comparisons with other\nfull-shot baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel prediction permits to acquire channel state information (CSI) without\nsignaling overhead. However, almost all existing channel prediction methods\nnecessitate the deployment of a dedicated model to accommodate a specific\nconfiguration. Leveraging the powerful modeling and multi-task learning\ncapabilities of foundation models, we propose the first space-time-frequency\n(STF) wireless foundation model (WiFo) to address time-frequency channel\nprediction tasks in a one-for-all manner. Specifically, WiFo is initially\npre-trained over massive and extensive diverse CSI datasets. Then, the model\nwill be instantly used for channel prediction under various CSI configurations\nwithout any fine-tuning. We propose a masked autoencoder (MAE)-based network\nstructure for WiFo to handle heterogeneous STF CSI data, and design several\nmask reconstruction tasks for self-supervised pre-training to capture the\ninherent 3D variations of CSI. To fully unleash its predictive power, we build\na large-scale heterogeneous simulated CSI dataset consisting of 160K CSI\nsamples for pre-training. Simulations validate its superior unified learning\nperformance across multiple datasets and demonstrate its state-of-the-art\n(SOTA) zero-shot generalization performance via comparisons with other\nfull-shot baselines."
                },
                "authors": [
                    {
                        "name": "Boxun Liu"
                    },
                    {
                        "name": "Shijian Gao"
                    },
                    {
                        "name": "Xuanyu Liu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Liuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liuqing Yang"
                },
                "author": "Liuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11641v2",
                "updated": "2024-12-12T03:26:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    26,
                    57,
                    3,
                    347,
                    0
                ],
                "published": "2024-01-22T01:06:17Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    1,
                    6,
                    17,
                    0,
                    22,
                    0
                ],
                "title": "Revolutionizing Finance with LLMs: An Overview of Applications and\n  Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Finance with LLMs: An Overview of Applications and\n  Insights"
                },
                "summary": "In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry."
                },
                "authors": [
                    {
                        "name": "Huaqin Zhao"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Tianze Yang"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Shaochen Xu"
                    },
                    {
                        "name": "Haixing Dai"
                    },
                    {
                        "name": "Lin Zhao"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Gengchen Mai"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10613v2",
                "updated": "2024-12-12T03:24:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    24,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-20T07:48:19Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    48,
                    19,
                    1,
                    233,
                    0
                ],
                "title": "Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval"
                },
                "summary": "Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably lead to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably lead to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models."
                },
                "authors": [
                    {
                        "name": "Guangyuan Ma"
                    },
                    {
                        "name": "Yongliang Ma"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Ming Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Accepted by AAAI25. Source code is available at\n  https://github.com/tdro-llm/tdro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08885v1",
                "updated": "2024-12-12T02:48:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    48,
                    20,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T02:48:20Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    48,
                    20,
                    3,
                    347,
                    0
                ],
                "title": "Residual Channel Boosts Contrastive Learning for Radio Frequency\n  Fingerprint Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual Channel Boosts Contrastive Learning for Radio Frequency\n  Fingerprint Identification"
                },
                "summary": "In order to address the issue of limited data samples for the deployment of\npre-trained models in unseen environments, this paper proposes a residual\nchannel-based data augmentation strategy for Radio Frequency Fingerprint\nIdentification (RFFI), coupled with a lightweight SimSiam contrastive learning\nframework. By applying least square (LS) and minimum mean square error (MMSE)\nchannel estimations followed by equalization, signals with different residual\nchannel effects are generated. These residual channels enable the model to\nlearn more effective representations. Then the pre-trained model is fine-tuned\nwith 1% samples in a novel environment for RFFI. Experimental results\ndemonstrate that our method significantly enhances both feature extraction\nability and generalization while requiring fewer samples and less time, making\nit suitable for practical wireless security applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to address the issue of limited data samples for the deployment of\npre-trained models in unseen environments, this paper proposes a residual\nchannel-based data augmentation strategy for Radio Frequency Fingerprint\nIdentification (RFFI), coupled with a lightweight SimSiam contrastive learning\nframework. By applying least square (LS) and minimum mean square error (MMSE)\nchannel estimations followed by equalization, signals with different residual\nchannel effects are generated. These residual channels enable the model to\nlearn more effective representations. Then the pre-trained model is fine-tuned\nwith 1% samples in a novel environment for RFFI. Experimental results\ndemonstrate that our method significantly enhances both feature extraction\nability and generalization while requiring fewer samples and less time, making\nit suitable for practical wireless security applications."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Guanxiong Shen"
                    },
                    {
                        "name": "Hongyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Chen"
                },
                "author": "Hongyang Chen",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07561v2",
                "updated": "2024-12-12T02:47:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    47,
                    5,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-10T02:58:52Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    2,
                    58,
                    52,
                    3,
                    284,
                    0
                ],
                "title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System\n  Powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System\n  Powered by Large Language Models"
                },
                "summary": "The rise of various social platforms has transformed journalism. The growing\ndemand for news content has led to the increased use of large language models\n(LLMs) in news production due to their speed and cost-effectiveness. However,\nLLMs still encounter limitations in professionalism and ethical judgment in\nnews generation. Additionally, predicting public feedback is usually difficult\nbefore news is released. To tackle these challenges, we introduce AI-Press, an\nautomated news drafting and polishing system based on multi-agent collaboration\nand Retrieval-Augmented Generation. We develop a feedback simulation system\nthat generates public feedback considering demographic distributions. Through\nextensive quantitative and qualitative evaluations, our system shows\nsignificant improvements in news-generating capabilities and verifies the\neffectiveness of public feedback simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of various social platforms has transformed journalism. The growing\ndemand for news content has led to the increased use of large language models\n(LLMs) in news production due to their speed and cost-effectiveness. However,\nLLMs still encounter limitations in professionalism and ethical judgment in\nnews generation. Additionally, predicting public feedback is usually difficult\nbefore news is released. To tackle these challenges, we introduce AI-Press, an\nautomated news drafting and polishing system based on multi-agent collaboration\nand Retrieval-Augmented Generation. We develop a feedback simulation system\nthat generates public feedback considering demographic distributions. Through\nextensive quantitative and qualitative evaluations, our system shows\nsignificant improvements in news-generating capabilities and verifies the\neffectiveness of public feedback simulation."
                },
                "authors": [
                    {
                        "name": "Xiawei Liu"
                    },
                    {
                        "name": "Shiyue Yang"
                    },
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Haoyu Kuang"
                    },
                    {
                        "name": "Libo Sun"
                    },
                    {
                        "name": "Yihang Yang"
                    },
                    {
                        "name": "Siming Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11802v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11802v4",
                "updated": "2024-12-12T02:45:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    45,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-18T14:01:45Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    1,
                    45,
                    0,
                    78,
                    0
                ],
                "title": "Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark\n  for Evaluating Long-Context Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark\n  for Evaluating Long-Context Large Language Models"
                },
                "summary": "Despite recent efforts to develop large language models with robust\nlong-context capabilities, the lack of long-context benchmarks means that\nrelatively little is known about their performance. To alleviate this gap, in\nthis paper, we propose \\textbf{Counting-Stars}, a multi-evidence,\nposition-aware, and scalable benchmark designed to evaluate the multi-evidence\nretrieval capabilities of long-context LLMs. \\textbf{Counting-Stars} comprises\ntwo counting-based multiple pieces of evidence retrieval tasks: searching and\nreasoning. Using Counting-Stars, we conducted experiments to evaluate several\nlong-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4,\nand Moonshot-v1. Extensive experimental results demonstrate that Gemini 1.5 Pro\nachieves the best overall results, while GPT-4 Turbo exhibits the most stable\nperformance across various tasks. Furthermore, our analysis of these LLMs,\nwhich have been extended to handle long-context scenarios, indicates that\nsignificant room for improvement remains as the length of the input context and\nthe complexity of the tasks increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts to develop large language models with robust\nlong-context capabilities, the lack of long-context benchmarks means that\nrelatively little is known about their performance. To alleviate this gap, in\nthis paper, we propose \\textbf{Counting-Stars}, a multi-evidence,\nposition-aware, and scalable benchmark designed to evaluate the multi-evidence\nretrieval capabilities of long-context LLMs. \\textbf{Counting-Stars} comprises\ntwo counting-based multiple pieces of evidence retrieval tasks: searching and\nreasoning. Using Counting-Stars, we conducted experiments to evaluate several\nlong-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4,\nand Moonshot-v1. Extensive experimental results demonstrate that Gemini 1.5 Pro\nachieves the best overall results, while GPT-4 Turbo exhibits the most stable\nperformance across various tasks. Furthermore, our analysis of these LLMs,\nwhich have been extended to handle long-context scenarios, indicates that\nsignificant room for improvement remains as the length of the input context and\nthe complexity of the tasks increase."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Xuan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Luo"
                },
                "author": "Xuan Luo",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11802v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07762v2",
                "updated": "2024-12-12T02:41:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    41,
                    45,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-12T12:52:04Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    12,
                    52,
                    4,
                    1,
                    317,
                    0
                ],
                "title": "ASER: Activation Smoothing and Error Reconstruction for Large Language\n  Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASER: Activation Smoothing and Error Reconstruction for Large Language\n  Model Quantization"
                },
                "summary": "Quantization stands as a pivotal technique for large language model (LLM)\nserving, yet it poses significant challenges particularly in achieving\neffective low-bit quantization. The limited numerical mapping makes the\nquantized model produce a non-trivial error, bringing out intolerable\nperformance degration. This paper is anchored in the basic idea of model\ncompression objectives, and delves into the layer-wise error distribution of\nLLMs during post-training quantization. Subsequently, we introduce ASER, an\nalgorithm consisting of (1) Error Reconstruction: low-rank compensation for\nquantization error with LoRA-style matrices constructed by whitening SVD; (2)\nActivation Smoothing: outlier extraction to gain smooth activation and better\nerror compensation. ASER is capable of quantizing typical LLMs to low-bit ones,\nparticularly preserving accuracy even in W4A8 per-channel setup. Experimental\nresults show that ASER is competitive among the state-of-the-art quantization\nalgorithms, showing potential to activation quantization, with minor overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization stands as a pivotal technique for large language model (LLM)\nserving, yet it poses significant challenges particularly in achieving\neffective low-bit quantization. The limited numerical mapping makes the\nquantized model produce a non-trivial error, bringing out intolerable\nperformance degration. This paper is anchored in the basic idea of model\ncompression objectives, and delves into the layer-wise error distribution of\nLLMs during post-training quantization. Subsequently, we introduce ASER, an\nalgorithm consisting of (1) Error Reconstruction: low-rank compensation for\nquantization error with LoRA-style matrices constructed by whitening SVD; (2)\nActivation Smoothing: outlier extraction to gain smooth activation and better\nerror compensation. ASER is capable of quantizing typical LLMs to low-bit ones,\nparticularly preserving accuracy even in W4A8 per-channel setup. Experimental\nresults show that ASER is competitive among the state-of-the-art quantization\nalgorithms, showing potential to activation quantization, with minor overhead."
                },
                "authors": [
                    {
                        "name": "Weibo Zhao"
                    },
                    {
                        "name": "Yubin Shi"
                    },
                    {
                        "name": "Xinyu Lyu"
                    },
                    {
                        "name": "Wanchen Sui"
                    },
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12644v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12644v4",
                "updated": "2024-12-12T02:37:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    37,
                    52,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-18T14:12:27Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    12,
                    27,
                    1,
                    170,
                    0
                ],
                "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models Aligned with Human Cognitive Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models Aligned with Human Cognitive Principles"
                },
                "summary": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human\ncognitive principles and designed to assess LLMs by examining the cognitive\ndemands of various tasks. The HPT utilizes the Hierarchical Prompting Framework\n(HPF), which structures five unique prompting strategies in a hierarchical\norder based on their cognitive requirement on LLMs when compared to human\nmental capabilities. It assesses the complexity of tasks with the Hierarchical\nPrompting Index (HPI), which demonstrates the cognitive competencies of LLMs\nacross diverse datasets and offers insights into the cognitive demands that\ndatasets place on different LLMs. This approach enables a comprehensive\nevaluation of an LLMs problem solving abilities and the intricacy of a dataset,\noffering a standardized metric for task complexity. Extensive experiments with\nmultiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%\ncompared to baseline performance, with GSM8k being the most cognitively complex\ntask among reasoning and coding tasks with an average HPI of 3.20 confirming\nthe effectiveness of HPT. To support future research and reproducibility in\nthis domain, the implementations of HPT and HPF are available here.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human\ncognitive principles and designed to assess LLMs by examining the cognitive\ndemands of various tasks. The HPT utilizes the Hierarchical Prompting Framework\n(HPF), which structures five unique prompting strategies in a hierarchical\norder based on their cognitive requirement on LLMs when compared to human\nmental capabilities. It assesses the complexity of tasks with the Hierarchical\nPrompting Index (HPI), which demonstrates the cognitive competencies of LLMs\nacross diverse datasets and offers insights into the cognitive demands that\ndatasets place on different LLMs. This approach enables a comprehensive\nevaluation of an LLMs problem solving abilities and the intricacy of a dataset,\noffering a standardized metric for task complexity. Extensive experiments with\nmultiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%\ncompared to baseline performance, with GSM8k being the most cognitively complex\ntask among reasoning and coding tasks with an average HPI of 3.20 confirming\nthe effectiveness of HPT. To support future research and reproducibility in\nthis domain, the implementations of HPT and HPF are available here."
                },
                "authors": [
                    {
                        "name": "Devichand Budagam"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Mahsa Khoshnoodi"
                    },
                    {
                        "name": "Sankalp KJ"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12644v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12644v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08275v2",
                "updated": "2024-12-12T02:29:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    2,
                    29,
                    45,
                    3,
                    347,
                    0
                ],
                "published": "2023-10-12T12:24:52Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    12,
                    24,
                    52,
                    3,
                    285,
                    0
                ],
                "title": "Harnessing the Power of LLM to Support Binary Taint Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Power of LLM to Support Binary Taint Analysis"
                },
                "summary": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs."
                },
                "authors": [
                    {
                        "name": "Puzhuo Liu"
                    },
                    {
                        "name": "Chengnian Sun"
                    },
                    {
                        "name": "Yaowen Zheng"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Yuncheng Wang"
                    },
                    {
                        "name": "Zhenyang Xu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "arxiv_comment": "36 pages,16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]