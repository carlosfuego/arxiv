[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v1",
                "updated": "2024-08-11T18:40:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve \\(\\frac{1}{16}\\) token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\n\\url{https://github.com/andy-yang-1/DoubleSparse}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve \\(\\frac{1}{16}\\) token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\n\\url{https://github.com/andy-yang-1/DoubleSparse}."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v1",
                "updated": "2024-07-27T16:20:21Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v2",
                "updated": "2024-07-21T14:08:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    8,
                    42,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04877v2",
                "updated": "2024-07-16T09:05:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    5,
                    39,
                    1,
                    198,
                    0
                ],
                "published": "2023-05-08T17:20:30Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    17,
                    20,
                    30,
                    0,
                    128,
                    0
                ],
                "title": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer"
                },
                "summary": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate."
                },
                "authors": [
                    {
                        "name": "Tomer Bucher"
                    },
                    {
                        "name": "Harel Nahari"
                    },
                    {
                        "name": "Hanan Herzig Sheinfux"
                    },
                    {
                        "name": "Ron Ruimy"
                    },
                    {
                        "name": "Arthur Niedermayr"
                    },
                    {
                        "name": "Raphael Dahan"
                    },
                    {
                        "name": "Qinghui Yan"
                    },
                    {
                        "name": "Yuval Adiv"
                    },
                    {
                        "name": "Michael Yannai"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Yaniv Kurman"
                    },
                    {
                        "name": "Sang Tae Park"
                    },
                    {
                        "name": "Daniel J. Masiel"
                    },
                    {
                        "name": "Eli Janzen"
                    },
                    {
                        "name": "James H. Edgar"
                    },
                    {
                        "name": "Fabrizio Carbone"
                    },
                    {
                        "name": "Guy Bartal"
                    },
                    {
                        "name": "Shai Tsesses"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Giovanni Maria Vanacore"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_doi": "10.1038/s41566-024-01451-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41566-024-01451-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11483v1",
                "updated": "2024-07-16T08:18:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T08:18:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models"
                },
                "summary": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement."
                },
                "authors": [
                    {
                        "name": "Jialin Hu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Zhiliang Shuai"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11338v1",
                "updated": "2024-07-16T03:08:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T03:08:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si"
                },
                "summary": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Harshal Jason D'Souza"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Rama Satya Sandilya"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "author": "Pavan Nukala",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v3",
                "updated": "2024-07-15T22:33:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    22,
                    33,
                    58,
                    0,
                    197,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "This study presents the first privacy aware semantic cache for LLMs\n  based on Federated Learning. MeanCache is the first cache that can handle\n  contextual queries efficiently. Total pages 14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05740v2",
                "updated": "2024-07-15T18:38:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    38,
                    54,
                    0,
                    197,
                    0
                ],
                "published": "2023-07-11T19:08:06Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    19,
                    8,
                    6,
                    1,
                    192,
                    0
                ],
                "title": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network"
                },
                "summary": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes."
                },
                "authors": [
                    {
                        "name": "Raghavendra Kanakagiri"
                    },
                    {
                        "name": "Edgar Solomonik"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Solomonik"
                },
                "author": "Edgar Solomonik",
                "arxiv_doi": "10.1145/3626183.3659985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626183.3659985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.3; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v1",
                "updated": "2024-07-15T17:25:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v1",
                "updated": "2024-07-15T14:09:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.08313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08313v1",
                "updated": "2024-08-15T17:59:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:59:57Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Symbolic Graphics Programs?"
                },
                "summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "Technical Report v1 (44 pages, 23 figures, project page:\n  https://sgp-bench.github.io/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08302v1",
                "updated": "2024-08-15T17:55:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    55,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:55:45Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    55,
                    45,
                    3,
                    228,
                    0
                ],
                "title": "Benchmarking the Capabilities of Large Language Models in Transportation\n  System Engineering: Accuracy, Consistency, and Reasoning Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Capabilities of Large Language Models in Transportation\n  System Engineering: Accuracy, Consistency, and Reasoning Behaviors"
                },
                "summary": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges."
                },
                "authors": [
                    {
                        "name": "Usman Syed"
                    },
                    {
                        "name": "Ethan Light"
                    },
                    {
                        "name": "Xingang Guo"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yanfeng Ouyang"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08300v1",
                "updated": "2024-08-15T17:54:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    54,
                    31,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:54:31Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    54,
                    31,
                    3,
                    228,
                    0
                ],
                "title": "HELP: Hierarchical Embeddings-based Log Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELP: Hierarchical Embeddings-based Log Parsing"
                },
                "summary": "Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing."
                },
                "authors": [
                    {
                        "name": "Andy Xu"
                    },
                    {
                        "name": "Arno Gau"
                    }
                ],
                "author_detail": {
                    "name": "Arno Gau"
                },
                "author": "Arno Gau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08295v1",
                "updated": "2024-08-15T17:50:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    50,
                    7,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:50:07Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    50,
                    7,
                    3,
                    228,
                    0
                ],
                "title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual\n  Learning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual\n  Learning with Pre-training"
                },
                "summary": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA."
                },
                "authors": [
                    {
                        "name": "Gengwei Zhang"
                    },
                    {
                        "name": "Liyuan Wang"
                    },
                    {
                        "name": "Guoliang Kang"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "This paper is an extension of our ICCV 23 paper (arXiv:2303.05118)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11907v2",
                "updated": "2024-08-15T17:37:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    37,
                    36,
                    3,
                    228,
                    0
                ],
                "published": "2024-02-19T07:46:40Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    7,
                    46,
                    40,
                    0,
                    50,
                    0
                ],
                "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive\n  Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive\n  Prompt Distillation"
                },
                "summary": "Aligning large language models (LLMs) with human expectations without\nhuman-annotated preference data is an important problem. In this paper, we\npropose a method to evaluate the response preference by using the output\nprobabilities of response pairs under contrastive prompt pairs, which could\nachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based\non this, we propose an automatic alignment method, Direct Large Model Alignment\n(DLMA). First, we use contrastive prompt pairs to automatically generate\npreference data. Then, we continue to evaluate the generated preference data\nusing contrastive prompt pairs and calculate a self-rewarding score. Finally,\nwe use the DPO algorithm to effectively align LLMs by combining this\nself-rewarding score. In the experimental stage, our DLMA method could surpass\nthe \\texttt{RLHF} method without relying on human-annotated preference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human expectations without\nhuman-annotated preference data is an important problem. In this paper, we\npropose a method to evaluate the response preference by using the output\nprobabilities of response pairs under contrastive prompt pairs, which could\nachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based\non this, we propose an automatic alignment method, Direct Large Model Alignment\n(DLMA). First, we use contrastive prompt pairs to automatically generate\npreference data. Then, we continue to evaluate the generated preference data\nusing contrastive prompt pairs and calculate a self-rewarding score. Finally,\nwe use the DPO algorithm to effectively align LLMs by combining this\nself-rewarding score. In the experimental stage, our DLMA method could surpass\nthe \\texttt{RLHF} method without relying on human-annotated preference data."
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Haoping Bai"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "arxiv_comment": "24 pages, 5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08282v1",
                "updated": "2024-08-15T17:33:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    33,
                    32,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:33:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    33,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through\n  Grounded Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through\n  Grounded Language Model"
                },
                "summary": "Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning."
                },
                "authors": [
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Arturo Laurenzi"
                    },
                    {
                        "name": "Nikos Tsagarakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Tsagarakis"
                },
                "author": "Nikos Tsagarakis",
                "arxiv_comment": "Paper accepted by IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08274v1",
                "updated": "2024-08-15T17:19:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    19,
                    12,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:19:12Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    19,
                    12,
                    3,
                    228,
                    0
                ],
                "title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for\n  Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for\n  Mixture of Experts"
                },
                "summary": "The Mixture of Experts (MoE) framework has become a popular architecture for\nlarge language models due to its superior performance over dense models.\nHowever, training MoEs from scratch in a large-scale regime is prohibitively\nexpensive. Existing methods mitigate this by pre-training multiple dense expert\nmodels independently and using them to initialize an MoE. This is done by using\nexperts' feed-forward network (FFN) to initialize the MoE's experts while\nmerging other parameters. However, this method limits the reuse of dense model\nparameters to only the FFN layers, thereby constraining the advantages when\n\"upcycling\" these models into MoEs. We propose BAM (Branch-Attend-Mix), a\nsimple yet effective method that addresses this shortcoming. BAM makes full use\nof specialized dense models by not only using their FFN to initialize the MoE\nlayers but also leveraging experts' attention parameters fully by initializing\nthem into a soft-variant of Mixture of Attention (MoA) layers. We explore two\nmethods for upcycling attention parameters: 1) initializing separate attention\nexperts from dense models including all attention parameters for the best model\nperformance; and 2) sharing key and value parameters across all experts to\nfacilitate for better inference efficiency. To further improve efficiency, we\nadopt a parallel attention transformer architecture to MoEs, which allows the\nattention experts and FFN experts to be computed concurrently. Our experiments\non seed models ranging from 590 million to 2 billion parameters demonstrate\nthat BAM surpasses baselines in both perplexity and downstream task\nperformance, within the same computational and data constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) framework has become a popular architecture for\nlarge language models due to its superior performance over dense models.\nHowever, training MoEs from scratch in a large-scale regime is prohibitively\nexpensive. Existing methods mitigate this by pre-training multiple dense expert\nmodels independently and using them to initialize an MoE. This is done by using\nexperts' feed-forward network (FFN) to initialize the MoE's experts while\nmerging other parameters. However, this method limits the reuse of dense model\nparameters to only the FFN layers, thereby constraining the advantages when\n\"upcycling\" these models into MoEs. We propose BAM (Branch-Attend-Mix), a\nsimple yet effective method that addresses this shortcoming. BAM makes full use\nof specialized dense models by not only using their FFN to initialize the MoE\nlayers but also leveraging experts' attention parameters fully by initializing\nthem into a soft-variant of Mixture of Attention (MoA) layers. We explore two\nmethods for upcycling attention parameters: 1) initializing separate attention\nexperts from dense models including all attention parameters for the best model\nperformance; and 2) sharing key and value parameters across all experts to\nfacilitate for better inference efficiency. To further improve efficiency, we\nadopt a parallel attention transformer architecture to MoEs, which allows the\nattention experts and FFN experts to be computed concurrently. Our experiments\non seed models ranging from 590 million to 2 billion parameters demonstrate\nthat BAM surpasses baselines in both perplexity and downstream task\nperformance, within the same computational and data constraints."
                },
                "authors": [
                    {
                        "name": "Qizhen Zhang"
                    },
                    {
                        "name": "Nikolas Gritsch"
                    },
                    {
                        "name": "Dwaraknath Gnaneshwar"
                    },
                    {
                        "name": "Simon Guo"
                    },
                    {
                        "name": "David Cairuz"
                    },
                    {
                        "name": "Bharat Venkitesh"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Phil Blunsom"
                    },
                    {
                        "name": "Sebastian Ruder"
                    },
                    {
                        "name": "Ahmet Ustun"
                    },
                    {
                        "name": "Acyr Locatelli"
                    }
                ],
                "author_detail": {
                    "name": "Acyr Locatelli"
                },
                "author": "Acyr Locatelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01870v4",
                "updated": "2024-08-15T17:07:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    7,
                    52,
                    3,
                    228,
                    0
                ],
                "published": "2023-12-04T13:06:24Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    13,
                    6,
                    24,
                    0,
                    338,
                    0
                ],
                "title": "Extreme-value modelling of migratory bird arrival dates: Insights from\n  citizen science data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme-value modelling of migratory bird arrival dates: Insights from\n  citizen science data"
                },
                "summary": "Citizen science mobilises many observers and gathers huge datasets but often\nwithout strict sampling protocols, resulting in observation biases due to\nheterogeneous sampling effort, which can lead to biased statistical inferences.\nWe develop a spatiotemporal Bayesian hierarchical model for bias-corrected\nestimation of arrival dates of the first migratory bird individuals at a\nbreeding site. Higher sampling effort could be correlated with earlier observed\ndates. We implement data fusion of two citizen-science datasets with\nfundamentally different protocols (BBS, eBird) and map posterior distributions\nof the latent process, which contains four spatial components with Gaussian\nprocess priors: species niche; sampling effort; position and scale parameters\nof annual first arrival date. The data layer includes four response variables:\ncounts of observed eBird locations (Poisson); presence-absence at observed\neBird locations (Binomial); BBS occurrence counts (Poisson); first arrival\ndates (Generalised Extreme-Value). We devise a Markov Chain Monte Carlo scheme\nand check by simulation that the latent process components are identifiable. We\napply our model to several migratory bird species in the northeastern US for\n2001--2021 and find that the sampling effort significantly modulates the\nobserved first arrival date. We exploit this relationship to effectively\nbias-correct predictions of the true first arrivals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citizen science mobilises many observers and gathers huge datasets but often\nwithout strict sampling protocols, resulting in observation biases due to\nheterogeneous sampling effort, which can lead to biased statistical inferences.\nWe develop a spatiotemporal Bayesian hierarchical model for bias-corrected\nestimation of arrival dates of the first migratory bird individuals at a\nbreeding site. Higher sampling effort could be correlated with earlier observed\ndates. We implement data fusion of two citizen-science datasets with\nfundamentally different protocols (BBS, eBird) and map posterior distributions\nof the latent process, which contains four spatial components with Gaussian\nprocess priors: species niche; sampling effort; position and scale parameters\nof annual first arrival date. The data layer includes four response variables:\ncounts of observed eBird locations (Poisson); presence-absence at observed\neBird locations (Binomial); BBS occurrence counts (Poisson); first arrival\ndates (Generalised Extreme-Value). We devise a Markov Chain Monte Carlo scheme\nand check by simulation that the latent process components are identifiable. We\napply our model to several migratory bird species in the northeastern US for\n2001--2021 and find that the sampling effort significantly modulates the\nobserved first arrival date. We exploit this relationship to effectively\nbias-correct predictions of the true first arrivals."
                },
                "authors": [
                    {
                        "name": "Jonathan Koh"
                    },
                    {
                        "name": "Thomas Opitz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Opitz"
                },
                "author": "Thomas Opitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08264v1",
                "updated": "2024-08-15T17:07:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    7,
                    40,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:07:40Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    7,
                    40,
                    3,
                    228,
                    0
                ],
                "title": "InVAErt networks for amortized inference and identifiability analysis of\n  lumped parameter hemodynamic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InVAErt networks for amortized inference and identifiability analysis of\n  lumped parameter hemodynamic models"
                },
                "summary": "Estimation of cardiovascular model parameters from electronic health records\n(EHR) poses a significant challenge primarily due to lack of identifiability.\nStructural non-identifiability arises when a manifold in the space of\nparameters is mapped to a common output, while practical non-identifiability\ncan result due to limited data, model misspecification, or noise corruption. To\naddress the resulting ill-posed inverse problem, optimization-based or Bayesian\ninference approaches typically use regularization, thereby limiting the\npossibility of discovering multiple solutions. In this study, we use inVAErt\nnetworks, a neural network-based, data-driven framework for enhanced digital\ntwin analysis of stiff dynamical systems. We demonstrate the flexibility and\neffectiveness of inVAErt networks in the context of physiological inversion of\na six-compartment lumped parameter hemodynamic model from synthetic data to\nreal data with missing components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of cardiovascular model parameters from electronic health records\n(EHR) poses a significant challenge primarily due to lack of identifiability.\nStructural non-identifiability arises when a manifold in the space of\nparameters is mapped to a common output, while practical non-identifiability\ncan result due to limited data, model misspecification, or noise corruption. To\naddress the resulting ill-posed inverse problem, optimization-based or Bayesian\ninference approaches typically use regularization, thereby limiting the\npossibility of discovering multiple solutions. In this study, we use inVAErt\nnetworks, a neural network-based, data-driven framework for enhanced digital\ntwin analysis of stiff dynamical systems. We demonstrate the flexibility and\neffectiveness of inVAErt networks in the context of physiological inversion of\na six-compartment lumped parameter hemodynamic model from synthetic data to\nreal data with missing components."
                },
                "authors": [
                    {
                        "name": "Guoxiang Grayson Tong"
                    },
                    {
                        "name": "Carlos A. Sing Long"
                    },
                    {
                        "name": "Daniele E. Schiavazzi"
                    }
                ],
                "author_detail": {
                    "name": "Daniele E. Schiavazzi"
                },
                "author": "Daniele E. Schiavazzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08252v1",
                "updated": "2024-08-15T16:47:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    47,
                    59,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T16:47:59Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    47,
                    59,
                    3,
                    228,
                    0
                ],
                "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models\n  with Soft Value-Based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models\n  with Soft Value-Based Decoding"
                },
                "summary": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}."
                },
                "authors": [
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Yulai Zhao"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Gabriele Scalia"
                    },
                    {
                        "name": "Gokcen Eraslan"
                    },
                    {
                        "name": "Surag Nair"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "Aviv Regev"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Masatoshi Uehara"
                    }
                ],
                "author_detail": {
                    "name": "Masatoshi Uehara"
                },
                "author": "Masatoshi Uehara",
                "arxiv_comment": "The code is available at https://github.com/masa-ue/SVDD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08247v1",
                "updated": "2024-08-15T16:33:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    33,
                    7,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T16:33:07Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    33,
                    7,
                    3,
                    228,
                    0
                ],
                "title": "Bayesian Inference analysis of jet quenching using inclusive jet and\n  hadron suppression measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference analysis of jet quenching using inclusive jet and\n  hadron suppression measurements"
                },
                "summary": "The JETSCAPE Collaboration reports a new determination of the jet transport\nparameter $\\hat{q}$ in the Quark-Gluon Plasma (QGP) using Bayesian Inference,\nincorporating all available inclusive hadron and jet yield suppression data\nmeasured in heavy-ion collisions at RHIC and the LHC. This multi-observable\nanalysis extends the previously published JETSCAPE Bayesian Inference\ndetermination of $\\hat{q}$, which was based solely on a selection of inclusive\nhadron suppression data. JETSCAPE is a modular framework incorporating detailed\ndynamical models of QGP formation and evolution, and jet propagation and\ninteraction in the QGP. Virtuality-dependent partonic energy loss in the QGP is\nmodeled as a thermalized weakly-coupled plasma, with parameters determined from\nBayesian calibration using soft-sector observables. This Bayesian calibration\nof $\\hat{q}$ utilizes Active Learning, a machine--learning approach, for\nefficient exploitation of computing resources. The experimental data included\nin this analysis span a broad range in collision energy and centrality, and in\ntransverse momentum. In order to explore the systematic dependence of the\nextracted parameter posterior distributions, several different calibrations are\nreported, based on combined jet and hadron data; on jet or hadron data\nseparately; and on restricted kinematic or centrality ranges of the jet and\nhadron data. Tension is observed in comparison of these variations, providing\nnew insights into the physics of jet transport in the QGP and its theoretical\nformulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The JETSCAPE Collaboration reports a new determination of the jet transport\nparameter $\\hat{q}$ in the Quark-Gluon Plasma (QGP) using Bayesian Inference,\nincorporating all available inclusive hadron and jet yield suppression data\nmeasured in heavy-ion collisions at RHIC and the LHC. This multi-observable\nanalysis extends the previously published JETSCAPE Bayesian Inference\ndetermination of $\\hat{q}$, which was based solely on a selection of inclusive\nhadron suppression data. JETSCAPE is a modular framework incorporating detailed\ndynamical models of QGP formation and evolution, and jet propagation and\ninteraction in the QGP. Virtuality-dependent partonic energy loss in the QGP is\nmodeled as a thermalized weakly-coupled plasma, with parameters determined from\nBayesian calibration using soft-sector observables. This Bayesian calibration\nof $\\hat{q}$ utilizes Active Learning, a machine--learning approach, for\nefficient exploitation of computing resources. The experimental data included\nin this analysis span a broad range in collision energy and centrality, and in\ntransverse momentum. In order to explore the systematic dependence of the\nextracted parameter posterior distributions, several different calibrations are\nreported, based on combined jet and hadron data; on jet or hadron data\nseparately; and on restricted kinematic or centrality ranges of the jet and\nhadron data. Tension is observed in comparison of these variations, providing\nnew insights into the physics of jet transport in the QGP and its theoretical\nformulation."
                },
                "authors": [
                    {
                        "name": "R. Ehlers"
                    },
                    {
                        "name": "Y. Chen"
                    },
                    {
                        "name": "J. Mulligan"
                    },
                    {
                        "name": "Y. Ji"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "S. Mak"
                    },
                    {
                        "name": "P. M. Jacobs"
                    },
                    {
                        "name": "A. Majumder"
                    },
                    {
                        "name": "A. Angerami"
                    },
                    {
                        "name": "R. Arora"
                    },
                    {
                        "name": "S. A. Bass"
                    },
                    {
                        "name": "R. Datta"
                    },
                    {
                        "name": "L. Du"
                    },
                    {
                        "name": "H. Elfner"
                    },
                    {
                        "name": "R. J. Fries"
                    },
                    {
                        "name": "C. Gale"
                    },
                    {
                        "name": "Y. He"
                    },
                    {
                        "name": "B. V. Jacak"
                    },
                    {
                        "name": "S. Jeon"
                    },
                    {
                        "name": "F. Jonas"
                    },
                    {
                        "name": "L. Kasper"
                    },
                    {
                        "name": "M. Kordell II"
                    },
                    {
                        "name": "R. Kunnawalkam-Elayavalli"
                    },
                    {
                        "name": "J. Latessa"
                    },
                    {
                        "name": "Y. -J. Lee"
                    },
                    {
                        "name": "R. Lemmon"
                    },
                    {
                        "name": "M. Luzum"
                    },
                    {
                        "name": "A. Mankolli"
                    },
                    {
                        "name": "C. Martin"
                    },
                    {
                        "name": "H. Mehryar"
                    },
                    {
                        "name": "T. Mengel"
                    },
                    {
                        "name": "C. Nattrass"
                    },
                    {
                        "name": "J. Norman"
                    },
                    {
                        "name": "C. Parker"
                    },
                    {
                        "name": "J. -F. Paquet"
                    },
                    {
                        "name": "J. H. Putschke"
                    },
                    {
                        "name": "H. Roch"
                    },
                    {
                        "name": "G. Roland"
                    },
                    {
                        "name": "B. Schenke"
                    },
                    {
                        "name": "L. Schwiebert"
                    },
                    {
                        "name": "A. Sengupta"
                    },
                    {
                        "name": "C. Shen"
                    },
                    {
                        "name": "M. Singh"
                    },
                    {
                        "name": "C. Sirimanna"
                    },
                    {
                        "name": "D. Soeder"
                    },
                    {
                        "name": "R. A. Soltz"
                    },
                    {
                        "name": "I. Soudi"
                    },
                    {
                        "name": "Y. Tachibana"
                    },
                    {
                        "name": "J. Velkovska"
                    },
                    {
                        "name": "G. Vujanovic"
                    },
                    {
                        "name": "X. -N. Wang"
                    },
                    {
                        "name": "X. Wu"
                    },
                    {
                        "name": "W. Zhao"
                    }
                ],
                "author_detail": {
                    "name": "W. Zhao"
                },
                "arxiv_affiliation": "JETSCAPE Collaboration",
                "author": "W. Zhao",
                "arxiv_comment": "20 pages, 10 figures, 2 tables, submitted to PRC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19577v3",
                "updated": "2024-08-15T16:14:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    14,
                    29,
                    3,
                    228,
                    0
                ],
                "published": "2024-03-28T17:03:44Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    17,
                    3,
                    44,
                    3,
                    88,
                    0
                ],
                "title": "A Public and Reproducible Assessment of the Topics API on Real Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Public and Reproducible Assessment of the Topics API on Real Data"
                },
                "summary": "The Topics API for the web is Google's privacy-enhancing alternative to\nreplace third-party cookies. Results of prior work have led to an ongoing\ndiscussion between Google and research communities about the capability of\nTopics to trade off both utility and privacy. The central point of contention\nis largely around the realism of the datasets used in these analyses and their\nreproducibility; researchers using data collected on a small sample of users or\ngenerating synthetic datasets, while Google's results are inferred from a\nprivate dataset. In this paper, we complement prior research by performing a\nreproducible assessment of the latest version of the Topics API on the largest\nand publicly available dataset of real browsing histories. First, we measure\nhow unique and stable real users' interests are over time. Then, we evaluate if\nTopics can be used to fingerprint the users from these real browsing traces by\nadapting methodologies from prior privacy studies. Finally, we call on web\nactors to perform and enable reproducible evaluations by releasing anonymized\ndistributions. We find that for the 1207 real users in this dataset, the\nprobability of being re-identified across websites is of 2%, 3%, and 4% after\n1, 2, and 3 observations of their topics by advertisers, respectively. This\npaper shows on real data that Topics does not provide the same privacy\nguarantees to all users and that the information leakage worsens over time,\nfurther highlighting the need for public and reproducible evaluations of the\nclaims made by new web proposals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Topics API for the web is Google's privacy-enhancing alternative to\nreplace third-party cookies. Results of prior work have led to an ongoing\ndiscussion between Google and research communities about the capability of\nTopics to trade off both utility and privacy. The central point of contention\nis largely around the realism of the datasets used in these analyses and their\nreproducibility; researchers using data collected on a small sample of users or\ngenerating synthetic datasets, while Google's results are inferred from a\nprivate dataset. In this paper, we complement prior research by performing a\nreproducible assessment of the latest version of the Topics API on the largest\nand publicly available dataset of real browsing histories. First, we measure\nhow unique and stable real users' interests are over time. Then, we evaluate if\nTopics can be used to fingerprint the users from these real browsing traces by\nadapting methodologies from prior privacy studies. Finally, we call on web\nactors to perform and enable reproducible evaluations by releasing anonymized\ndistributions. We find that for the 1207 real users in this dataset, the\nprobability of being re-identified across websites is of 2%, 3%, and 4% after\n1, 2, and 3 observations of their topics by advertisers, respectively. This\npaper shows on real data that Topics does not provide the same privacy\nguarantees to all users and that the information leakage worsens over time,\nfurther highlighting the need for public and reproducible evaluations of the\nclaims made by new web proposals."
                },
                "authors": [
                    {
                        "name": "Yohan Beugin"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "author": "Patrick McDaniel",
                "arxiv_comment": "Accepted at SecWeb 2024: Workshop on Designing Security for the Web\n  ---- Revisions: simulation bug fixed and new Topics classifier (v5), refer to\n  the updated quantitative results in the latest version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20242v2",
                "updated": "2024-08-15T16:08:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    8,
                    6,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-16T13:13:16Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    13,
                    16,
                    1,
                    198,
                    0
                ],
                "title": "The Threats of Embodied Multimodal LLMs: Jailbreaking Robotic\n  Manipulation in the Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Threats of Embodied Multimodal LLMs: Jailbreaking Robotic\n  Manipulation in the Physical World"
                },
                "summary": "Embodied artificial intelligence (AI) represents an artificial intelligence\nsystem that interacts with the physical world through sensors and actuators,\nseamlessly integrating perception and action. This design enables AI to learn\nfrom and operate within complex, real-world environments. Large Language Models\n(LLMs) deeply explore language instructions, playing a crucial role in devising\nplans for complex tasks. Consequently, they have progressively shown immense\npotential in empowering embodied AI, with LLM-based embodied AI emerging as a\nfocal point of research within the community. It is foreseeable that, over the\nnext decade, LLM-based embodied AI robots are expected to proliferate widely,\nbecoming commonplace in homes and industries. However, a critical safety issue\nthat has long been hiding in plain sight is: could LLM-based embodied AI\nperpetrate harmful behaviors? Our research investigates for the first time how\nto induce threatening actions in embodied AI, confirming the severe risks posed\nby these soon-to-be-marketed robots, which starkly contravene Asimov's Three\nLaws of Robotics and threaten human safety. Specifically, we formulate the\nconcept of embodied AI jailbreaking and expose three critical security\nvulnerabilities: first, jailbreaking robotics through compromised LLM; second,\nsafety misalignment between action and language spaces; and third, deceptive\nprompts leading to unaware hazardous behaviors. We also analyze potential\nmitigation measures and advocate for community awareness regarding the safety\nof embodied AI applications in the physical world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (AI) represents an artificial intelligence\nsystem that interacts with the physical world through sensors and actuators,\nseamlessly integrating perception and action. This design enables AI to learn\nfrom and operate within complex, real-world environments. Large Language Models\n(LLMs) deeply explore language instructions, playing a crucial role in devising\nplans for complex tasks. Consequently, they have progressively shown immense\npotential in empowering embodied AI, with LLM-based embodied AI emerging as a\nfocal point of research within the community. It is foreseeable that, over the\nnext decade, LLM-based embodied AI robots are expected to proliferate widely,\nbecoming commonplace in homes and industries. However, a critical safety issue\nthat has long been hiding in plain sight is: could LLM-based embodied AI\nperpetrate harmful behaviors? Our research investigates for the first time how\nto induce threatening actions in embodied AI, confirming the severe risks posed\nby these soon-to-be-marketed robots, which starkly contravene Asimov's Three\nLaws of Robotics and threaten human safety. Specifically, we formulate the\nconcept of embodied AI jailbreaking and expose three critical security\nvulnerabilities: first, jailbreaking robotics through compromised LLM; second,\nsafety misalignment between action and language spaces; and third, deceptive\nprompts leading to unaware hazardous behaviors. We also analyze potential\nmitigation measures and advocate for community awareness regarding the safety\nof embodied AI applications in the physical world."
                },
                "authors": [
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "Preliminary version (17 pages, 4 figures). Work in progress,\n  revisions ongoing. Appreciate understanding and welcome any feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10686v2",
                "updated": "2024-08-15T16:04:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    4,
                    29,
                    3,
                    228,
                    0
                ],
                "published": "2024-02-16T13:41:18Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    13,
                    41,
                    18,
                    4,
                    47,
                    0
                ],
                "title": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks"
                },
                "summary": "In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the state-of-the-art likelihood ratio attack\n(LiRA) within an information-theoretical framework that allows the\ninvestigation of the impact of the aleatoric uncertainty in the true data\ngeneration process, of the epistemic uncertainty caused by a limited training\ndata set, and of the calibration level of the target model. We compare three\ndifferent settings, in which the attacker receives decreasingly informative\nfeedback from the target model: confidence vector (CV) disclosure, in which the\noutput probability vector is released; true label confidence (TLC) disclosure,\nin which only the probability assigned to the true label is made available by\nthe model; and decision set (DS) disclosure, in which an adaptive prediction\nset is produced as in conformal prediction. We derive bounds on the advantage\nof an MIA adversary with the aim of offering insights into the impact of\nuncertainty and calibration on the effectiveness of MIAs. Simulation results\ndemonstrate that the derived analytical bounds predict well the effectiveness\nof MIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the state-of-the-art likelihood ratio attack\n(LiRA) within an information-theoretical framework that allows the\ninvestigation of the impact of the aleatoric uncertainty in the true data\ngeneration process, of the epistemic uncertainty caused by a limited training\ndata set, and of the calibration level of the target model. We compare three\ndifferent settings, in which the attacker receives decreasingly informative\nfeedback from the target model: confidence vector (CV) disclosure, in which the\noutput probability vector is released; true label confidence (TLC) disclosure,\nin which only the probability assigned to the true label is made available by\nthe model; and decision set (DS) disclosure, in which an adaptive prediction\nset is produced as in conformal prediction. We derive bounds on the advantage\nof an MIA adversary with the aim of offering insights into the impact of\nuncertainty and calibration on the effectiveness of MIAs. Simulation results\ndemonstrate that the derived analytical bounds predict well the effectiveness\nof MIAs."
                },
                "authors": [
                    {
                        "name": "Meiyi Zhu"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Chunyan Feng"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "13 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08231v1",
                "updated": "2024-08-15T15:56:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    56,
                    23,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:56:23Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    56,
                    23,
                    3,
                    228,
                    0
                ],
                "title": "DaRec: A Disentangled Alignment Framework for Large Language Model and\n  Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaRec: A Disentangled Alignment Framework for Large Language Model and\n  Recommender System"
                },
                "summary": "Benefiting from the strong reasoning capabilities, Large language models\n(LLMs) have demonstrated remarkable performance in recommender systems. Various\nefforts have been made to distill knowledge from LLMs to enhance collaborative\nmodels, employing techniques like contrastive learning for representation\nalignment. In this work, we prove that directly aligning the representations of\nLLMs and collaborative models is sub-optimal for enhancing downstream\nrecommendation tasks performance, based on the information theorem.\nConsequently, the challenge of effectively aligning semantic representations\nbetween collaborative models and LLMs remains unresolved. Inspired by this\nviewpoint, we propose a novel plug-and-play alignment framework for LLMs and\ncollaborative models. Specifically, we first disentangle the latent\nrepresentations of both LLMs and collaborative models into specific and shared\ncomponents via projection layers and representation regularization.\nSubsequently, we perform both global and local structure alignment on the\nshared representations to facilitate knowledge transfer. Additionally, we\ntheoretically prove that the specific and shared representations contain more\npertinent and less irrelevant information, which can enhance the effectiveness\nof downstream recommendation tasks. Extensive experimental results on benchmark\ndatasets demonstrate that our method is superior to existing state-of-the-art\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the strong reasoning capabilities, Large language models\n(LLMs) have demonstrated remarkable performance in recommender systems. Various\nefforts have been made to distill knowledge from LLMs to enhance collaborative\nmodels, employing techniques like contrastive learning for representation\nalignment. In this work, we prove that directly aligning the representations of\nLLMs and collaborative models is sub-optimal for enhancing downstream\nrecommendation tasks performance, based on the information theorem.\nConsequently, the challenge of effectively aligning semantic representations\nbetween collaborative models and LLMs remains unresolved. Inspired by this\nviewpoint, we propose a novel plug-and-play alignment framework for LLMs and\ncollaborative models. Specifically, we first disentangle the latent\nrepresentations of both LLMs and collaborative models into specific and shared\ncomponents via projection layers and representation regularization.\nSubsequently, we perform both global and local structure alignment on the\nshared representations to facilitate knowledge transfer. Additionally, we\ntheoretically prove that the specific and shared representations contain more\npertinent and less irrelevant information, which can enhance the effectiveness\nof downstream recommendation tasks. Extensive experimental results on benchmark\ndatasets demonstrate that our method is superior to existing state-of-the-art\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xihong Yang"
                    },
                    {
                        "name": "Heming Jing"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Huakang Niu"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xinwang Liu"
                    },
                    {
                        "name": "En Zhu"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Erxue Min"
                    }
                ],
                "author_detail": {
                    "name": "Erxue Min"
                },
                "author": "Erxue Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04312v2",
                "updated": "2024-08-15T15:43:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    43,
                    36,
                    3,
                    228,
                    0
                ],
                "published": "2023-10-06T15:19:30Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    15,
                    19,
                    30,
                    4,
                    279,
                    0
                ],
                "title": "Bespoke scapegoats: Scientific advisory bodies and blame avoidance in\n  the Covid-19 pandemic and beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bespoke scapegoats: Scientific advisory bodies and blame avoidance in\n  the Covid-19 pandemic and beyond"
                },
                "summary": "Scholars have not asked why so many governments created ad hoc scientific\nadvisory bodies (ahSABs) to address the Covid-19 pandemic instead of relying on\nexisting public health infrastructure. We address this neglected question with\nan exploratory study of the US, UK, Sweden, Italy, Poland, and Uganda. Drawing\non our case studies and the blame-avoidance literature, we find that ahSABs are\ncreated to excuse unpopular policies and take the blame should things go wrong.\nThus, membership typically represents a narrow range of perspectives. An ahSAB\nis a good scapegoat because it does little to reduce government discretion and\nhas limited ability to deflect blame back to government. Our explanation of our\ndeviant case of Sweden, that did not create and ahSAB, reinforces our general\nprinciples. We draw the policy inference that ahSAB membership should be vetted\nby the legislature to ensure broad membership.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scholars have not asked why so many governments created ad hoc scientific\nadvisory bodies (ahSABs) to address the Covid-19 pandemic instead of relying on\nexisting public health infrastructure. We address this neglected question with\nan exploratory study of the US, UK, Sweden, Italy, Poland, and Uganda. Drawing\non our case studies and the blame-avoidance literature, we find that ahSABs are\ncreated to excuse unpopular policies and take the blame should things go wrong.\nThus, membership typically represents a narrow range of perspectives. An ahSAB\nis a good scapegoat because it does little to reduce government discretion and\nhas limited ability to deflect blame back to government. Our explanation of our\ndeviant case of Sweden, that did not create and ahSAB, reinforces our general\nprinciples. We draw the policy inference that ahSAB membership should be vetted\nby the legislature to ensure broad membership."
                },
                "authors": [
                    {
                        "name": "Roger Koppl"
                    },
                    {
                        "name": "Kira Pronin"
                    },
                    {
                        "name": "Nick Cowen"
                    },
                    {
                        "name": "Marta Podemska-Mikluch"
                    },
                    {
                        "name": "Pablo Paniagua Prieto"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Paniagua Prieto"
                },
                "author": "Pablo Paniagua Prieto",
                "arxiv_comment": "2 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19442v2",
                "updated": "2024-08-15T15:43:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    43,
                    12,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-27T18:00:01Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    18,
                    0,
                    1,
                    3,
                    179,
                    0
                ],
                "title": "Ringdown amplitudes of nonspinning eccentric binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ringdown amplitudes of nonspinning eccentric binaries"
                },
                "summary": "Closed-form expressions for the ringdown complex amplitudes of nonspinning\nunequal-mass binaries in arbitrarily eccentric orbits are presented. They are\nbuilt upon 237 numerical simulations contained within the RIT catalog, through\nthe parameterisation introduced in [Phys. Rev. Lett. 132, 101401]. Global fits\nfor the complex amplitudes, associated to linear quasinormal mode frequencies\nof the dominant ringdown modes, are obtained in a factorised form immediately\napplicable to any existing quasi-circular model. Similarly to merger\namplitudes, ringdown ones increase by more than 50% compared to the circular\ncase for high impact parameters (medium eccentricities), while strongly\nsuppressed in the low impact parameter (highly eccentric) limit. Such reduction\ncan be explained by a transition between an \"orbital-type\" and an \"infall-type\"\ndynamics. The amplitudes (phases) fits accuracy lies around a few percent\n(deciradians) for the majority of the dataset, comparable to the accuracy of\ncurrent state-of-the-art quasi-circular ringdown models, and well within\ncurrent statistical errors of current LIGO-Virgo-Kagra ringdown observations.\nThese expressions constitute another building block towards the construction of\ncomplete general-relativistic inspiral-merger-ringdown semi-analytical\ntemplates, and allow to extend numerically-informed spectroscopic analyses\nbeyond the circular limit. Such generalisations are key to achieve accurate\ninference of compact binaries astrophysical properties, and tame astrophysical\nsystematics within observational investigations of strong-field general\nrelativistic dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closed-form expressions for the ringdown complex amplitudes of nonspinning\nunequal-mass binaries in arbitrarily eccentric orbits are presented. They are\nbuilt upon 237 numerical simulations contained within the RIT catalog, through\nthe parameterisation introduced in [Phys. Rev. Lett. 132, 101401]. Global fits\nfor the complex amplitudes, associated to linear quasinormal mode frequencies\nof the dominant ringdown modes, are obtained in a factorised form immediately\napplicable to any existing quasi-circular model. Similarly to merger\namplitudes, ringdown ones increase by more than 50% compared to the circular\ncase for high impact parameters (medium eccentricities), while strongly\nsuppressed in the low impact parameter (highly eccentric) limit. Such reduction\ncan be explained by a transition between an \"orbital-type\" and an \"infall-type\"\ndynamics. The amplitudes (phases) fits accuracy lies around a few percent\n(deciradians) for the majority of the dataset, comparable to the accuracy of\ncurrent state-of-the-art quasi-circular ringdown models, and well within\ncurrent statistical errors of current LIGO-Virgo-Kagra ringdown observations.\nThese expressions constitute another building block towards the construction of\ncomplete general-relativistic inspiral-merger-ringdown semi-analytical\ntemplates, and allow to extend numerically-informed spectroscopic analyses\nbeyond the circular limit. Such generalisations are key to achieve accurate\ninference of compact binaries astrophysical properties, and tame astrophysical\nsystematics within observational investigations of strong-field general\nrelativistic dynamics."
                },
                "authors": [
                    {
                        "name": "Gregorio Carullo"
                    }
                ],
                "author_detail": {
                    "name": "Gregorio Carullo"
                },
                "author": "Gregorio Carullo",
                "arxiv_comment": "20 pages, 11 figures | v2: added 320 mode and public data release;\n  journal submitted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17143v2",
                "updated": "2024-08-15T15:40:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    40,
                    15,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-26T04:12:08Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    4,
                    12,
                    8,
                    4,
                    117,
                    0
                ],
                "title": "Quantifying Memorization and Detecting Training Data of Pre-trained\n  Language Models using Japanese Newspaper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memorization and Detecting Training Data of Pre-trained\n  Language Models using Japanese Newspaper"
                },
                "summary": "Dominant pre-trained language models (PLMs) have demonstrated the potential\nrisk of memorizing and outputting the training data. While this concern has\nbeen discussed mainly in English, it is also practically important to focus on\ndomain-specific PLMs. In this study, we pre-trained domain-specific GPT-2\nmodels using a limited corpus of Japanese newspaper articles and evaluated\ntheir behavior. Experiments replicated the empirical finding that memorization\nof PLMs is related to the duplication in the training data, model size, and\nprompt length, in Japanese the same as in previous English studies.\nFurthermore, we attempted membership inference attacks, demonstrating that the\ntraining data can be detected even in Japanese, which is the same trend as in\nEnglish. The study warns that domain-specific PLMs, sometimes trained with\nvaluable private data, can ''copy and paste'' on a large scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dominant pre-trained language models (PLMs) have demonstrated the potential\nrisk of memorizing and outputting the training data. While this concern has\nbeen discussed mainly in English, it is also practically important to focus on\ndomain-specific PLMs. In this study, we pre-trained domain-specific GPT-2\nmodels using a limited corpus of Japanese newspaper articles and evaluated\ntheir behavior. Experiments replicated the empirical finding that memorization\nof PLMs is related to the duplication in the training data, model size, and\nprompt length, in Japanese the same as in previous English studies.\nFurthermore, we attempted membership inference attacks, demonstrating that the\ntraining data can be detected even in Japanese, which is the same trend as in\nEnglish. The study warns that domain-specific PLMs, sometimes trained with\nvaluable private data, can ''copy and paste'' on a large scale."
                },
                "authors": [
                    {
                        "name": "Shotaro Ishihara"
                    },
                    {
                        "name": "Hiromu Takahashi"
                    }
                ],
                "author_detail": {
                    "name": "Hiromu Takahashi"
                },
                "author": "Hiromu Takahashi",
                "arxiv_comment": "The 17th International Natural Language Generation Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08217v1",
                "updated": "2024-08-15T15:28:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    28,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:28:37Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    28,
                    37,
                    3,
                    228,
                    0
                ],
                "title": "RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train\n  and Deploy Edge Classifiers for Computational Social Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train\n  and Deploy Edge Classifiers for Computational Social Science"
                },
                "summary": "Large language models (LLMs) have enhanced our ability to rapidly analyze and\nclassify unstructured natural language data. However, concerns regarding cost,\nnetwork limitations, and security constraints have posed challenges for their\nintegration into work processes. In this study, we adopt a systems design\napproach to employing LLMs as imperfect data annotators for downstream\nsupervised learning tasks, introducing novel system intervention measures aimed\nat improving classification performance. Our methodology outperforms\nLLM-generated labels in seven of eight tests, demonstrating an effective\nstrategy for incorporating LLMs into the design and deployment of specialized,\nsupervised learning models present in many industry use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enhanced our ability to rapidly analyze and\nclassify unstructured natural language data. However, concerns regarding cost,\nnetwork limitations, and security constraints have posed challenges for their\nintegration into work processes. In this study, we adopt a systems design\napproach to employing LLMs as imperfect data annotators for downstream\nsupervised learning tasks, introducing novel system intervention measures aimed\nat improving classification performance. Our methodology outperforms\nLLM-generated labels in seven of eight tests, demonstrating an effective\nstrategy for incorporating LLMs into the design and deployment of specialized,\nsupervised learning models present in many industry use cases."
                },
                "authors": [
                    {
                        "name": "David Farr"
                    },
                    {
                        "name": "Nico Manzonelli"
                    },
                    {
                        "name": "Iain Cruickshank"
                    },
                    {
                        "name": "Jevin West"
                    }
                ],
                "author_detail": {
                    "name": "Jevin West"
                },
                "author": "Jevin West",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v3",
                "updated": "2024-08-15T15:24:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    24,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08212v1",
                "updated": "2024-08-15T15:23:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    23,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:23:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    23,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Covert Bias: The Severity of Social Views' Unalignment Towards Implicit\n  and Explicit Opinion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covert Bias: The Severity of Social Views' Unalignment Towards Implicit\n  and Explicit Opinion"
                },
                "summary": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models.To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models.To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity."
                },
                "authors": [
                    {
                        "name": "Abeer Aldayel"
                    },
                    {
                        "name": "Areej Alokaili"
                    },
                    {
                        "name": "Rehab Alahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Rehab Alahmadi"
                },
                "author": "Rehab Alahmadi",
                "arxiv_comment": "This work is under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15508v2",
                "updated": "2024-08-15T15:22:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    22,
                    57,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T09:45:16Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    9,
                    45,
                    16,
                    0,
                    204,
                    0
                ],
                "title": "Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners"
                },
                "summary": "Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application."
                },
                "authors": [
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Jaji Wu"
                    },
                    {
                        "name": "Jun Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jun Cheng"
                },
                "author": "Jun Cheng",
                "arxiv_comment": "Effecient Quantization Methods for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04275v2",
                "updated": "2024-08-15T15:20:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    20,
                    53,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-08T07:20:42Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    7,
                    20,
                    42,
                    3,
                    221,
                    0
                ],
                "title": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models"
                },
                "summary": "Multimodal large language models (LLMs) have demonstrated significant\npotential in a wide range of AI applications. Yet, training multimodal LLMs\nsuffers from low efficiency and scalability, due to the inherent model\nheterogeneity and data heterogeneity across different modalities.\n  We present DistTrain, an efficient and adaptive framework to reform the\ntraining of multimodal large language models on large-scale clusters. The core\nof DistTrain is the disaggregated training technique that exploits the\ncharacteristics of multimodal LLM training to achieve high efficiency and\nscalability. Specifically, it leverages disaggregated model orchestration and\ndisaggregated data reordering to address model and data heterogeneity\nrespectively. We also tailor system optimization for multimodal LLM training to\noverlap GPU communication and computation. We evaluate DistTrain across\ndifferent sizes of multimodal LLMs on a large-scale production cluster with\nthousands of GPUs. The experimental results show that DistTrain achieves 54.7%\nModel FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs\nand outperforms Megatron-LM by up to 2.2$\\times$ on throughput. The ablation\nstudy shows the main techniques of DistTrain are both effective and\nlightweight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) have demonstrated significant\npotential in a wide range of AI applications. Yet, training multimodal LLMs\nsuffers from low efficiency and scalability, due to the inherent model\nheterogeneity and data heterogeneity across different modalities.\n  We present DistTrain, an efficient and adaptive framework to reform the\ntraining of multimodal large language models on large-scale clusters. The core\nof DistTrain is the disaggregated training technique that exploits the\ncharacteristics of multimodal LLM training to achieve high efficiency and\nscalability. Specifically, it leverages disaggregated model orchestration and\ndisaggregated data reordering to address model and data heterogeneity\nrespectively. We also tailor system optimization for multimodal LLM training to\noverlap GPU communication and computation. We evaluate DistTrain across\ndifferent sizes of multimodal LLMs on a large-scale production cluster with\nthousands of GPUs. The experimental results show that DistTrain achieves 54.7%\nModel FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs\nand outperforms Megatron-LM by up to 2.2$\\times$ on throughput. The ablation\nstudy shows the main techniques of DistTrain are both effective and\nlightweight."
                },
                "authors": [
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08210v1",
                "updated": "2024-08-15T15:19:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    19,
                    11,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:19:11Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    19,
                    11,
                    3,
                    228,
                    0
                ],
                "title": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large\n  Language Models"
                },
                "summary": "Recent advances in AI have been significantly driven by the capabilities of\nlarge language models (LLMs) to solve complex problems in ways that resemble\nhuman thinking. However, there is an ongoing debate about the extent to which\nLLMs are capable of actual reasoning. Central to this debate are two key\nprobabilistic concepts that are essential for connecting causes to their\neffects: the probability of necessity (PN) and the probability of sufficiency\n(PS). This paper introduces a framework that is both theoretical and practical,\naimed at assessing how effectively LLMs are able to replicate real-world\nreasoning mechanisms using these probabilistic measures. By viewing LLMs as\nabstract machines that process information through a natural language\ninterface, we examine the conditions under which it is possible to compute\nsuitable approximations of PN and PS. Our research marks an important step\ntowards gaining a deeper understanding of when LLMs are capable of reasoning,\nas illustrated by a series of math examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI have been significantly driven by the capabilities of\nlarge language models (LLMs) to solve complex problems in ways that resemble\nhuman thinking. However, there is an ongoing debate about the extent to which\nLLMs are capable of actual reasoning. Central to this debate are two key\nprobabilistic concepts that are essential for connecting causes to their\neffects: the probability of necessity (PN) and the probability of sufficiency\n(PS). This paper introduces a framework that is both theoretical and practical,\naimed at assessing how effectively LLMs are able to replicate real-world\nreasoning mechanisms using these probabilistic measures. By viewing LLMs as\nabstract machines that process information through a natural language\ninterface, we examine the conditions under which it is possible to compute\nsuitable approximations of PN and PS. Our research marks an important step\ntowards gaining a deeper understanding of when LLMs are capable of reasoning,\nas illustrated by a series of math examples."
                },
                "authors": [
                    {
                        "name": "Javier González"
                    },
                    {
                        "name": "Aditya V. Nori"
                    }
                ],
                "author_detail": {
                    "name": "Aditya V. Nori"
                },
                "author": "Aditya V. Nori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08208v1",
                "updated": "2024-08-15T15:18:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    18,
                    46,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    18,
                    46,
                    3,
                    228,
                    0
                ],
                "title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems fundamentally rely on users' historical\ninteraction sequences, which are often contaminated by noisy interactions.\nIdentifying these noisy interactions accurately without additional information\nis particularly difficult due to the lack of explicit supervisory signals to\ndenote noise. Large Language Models (LLMs), equipped with extensive open\nknowledge and semantic reasoning abilities, present a promising avenue to\nbridge this information gap. However, employing LLMs for denoising in\nsequential recommendation introduces notable challenges: 1) Direct application\nof pretrained LLMs may not be competent for the denoising task, frequently\ngenerating nonsensical responses; 2) Even after fine-tuning, the reliability of\nLLM outputs remains questionable, especially given the complexity of the task\nand th inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing the\ncorrected sequences to be flexibly applied across various recommendation\nmodels. Extensive experiments validate the superiority of LLM4DSR over existing\nmethods across three datasets and three recommendation backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems fundamentally rely on users' historical\ninteraction sequences, which are often contaminated by noisy interactions.\nIdentifying these noisy interactions accurately without additional information\nis particularly difficult due to the lack of explicit supervisory signals to\ndenote noise. Large Language Models (LLMs), equipped with extensive open\nknowledge and semantic reasoning abilities, present a promising avenue to\nbridge this information gap. However, employing LLMs for denoising in\nsequential recommendation introduces notable challenges: 1) Direct application\nof pretrained LLMs may not be competent for the denoising task, frequently\ngenerating nonsensical responses; 2) Even after fine-tuning, the reliability of\nLLM outputs remains questionable, especially given the complexity of the task\nand th inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing the\ncorrected sequences to be flexibly applied across various recommendation\nmodels. Extensive experiments validate the superiority of LLM4DSR over existing\nmethods across three datasets and three recommendation backbones."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yudi Wu"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08188v1",
                "updated": "2024-08-15T14:46:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T14:46:13Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "title": "Scaling Up Natural Language Understanding for Multi-Robots Through the\n  Lens of Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Natural Language Understanding for Multi-Robots Through the\n  Lens of Hierarchy"
                },
                "summary": "Long-horizon planning is hindered by challenges such as uncertainty\naccumulation, computational complexity, delayed rewards and incomplete\ninformation. This work proposes an approach to exploit the task hierarchy from\nhuman instructions to facilitate multi-robot planning. Using Large Language\nModels (LLMs), we propose a two-step approach to translate multi-sentence\ninstructions into a structured language, Hierarchical Linear Temporal Logic\n(LTL), which serves as a formal representation for planning. Initially, LLMs\ntransform the instructions into a hierarchical representation defined as\nHierarchical Task Tree, capturing the logical and temporal relations among\ntasks. Following this, a domain-specific fine-tuning of LLM translates\nsub-tasks of each task into flat LTL formulas, aggregating them to form\nhierarchical LTL specifications. These specifications are then leveraged for\nplanning using off-the-shelf planners. Our framework not only bridges the gap\nbetween instructions and algorithmic planning but also showcases the potential\nof LLMs in harnessing hierarchical reasoning to automate multi-robot task\nplanning. Through evaluations in both simulation and real-world experiments\ninvolving human participants, we demonstrate that our method can handle more\ncomplex instructions compared to existing methods. The results indicate that\nour approach achieves higher success rates and lower costs in multi-robot task\nallocation and plan generation. Demos videos are available at\nhttps://youtu.be/7WOrDKxIMIs .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon planning is hindered by challenges such as uncertainty\naccumulation, computational complexity, delayed rewards and incomplete\ninformation. This work proposes an approach to exploit the task hierarchy from\nhuman instructions to facilitate multi-robot planning. Using Large Language\nModels (LLMs), we propose a two-step approach to translate multi-sentence\ninstructions into a structured language, Hierarchical Linear Temporal Logic\n(LTL), which serves as a formal representation for planning. Initially, LLMs\ntransform the instructions into a hierarchical representation defined as\nHierarchical Task Tree, capturing the logical and temporal relations among\ntasks. Following this, a domain-specific fine-tuning of LLM translates\nsub-tasks of each task into flat LTL formulas, aggregating them to form\nhierarchical LTL specifications. These specifications are then leveraged for\nplanning using off-the-shelf planners. Our framework not only bridges the gap\nbetween instructions and algorithmic planning but also showcases the potential\nof LLMs in harnessing hierarchical reasoning to automate multi-robot task\nplanning. Through evaluations in both simulation and real-world experiments\ninvolving human participants, we demonstrate that our method can handle more\ncomplex instructions compared to existing methods. The results indicate that\nour approach achieves higher success rates and lower costs in multi-robot task\nallocation and plan generation. Demos videos are available at\nhttps://youtu.be/7WOrDKxIMIs ."
                },
                "authors": [
                    {
                        "name": "Shaojun Xu"
                    },
                    {
                        "name": "Xusheng Luo"
                    },
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Letian Leng"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06162v3",
                "updated": "2024-08-15T13:59:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    59,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-09T09:34:25Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    9,
                    34,
                    25,
                    1,
                    100,
                    0
                ],
                "title": "Characterizing Multimodal Long-form Summarization: A Case Study on\n  Financial Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Multimodal Long-form Summarization: A Case Study on\n  Financial Reports"
                },
                "summary": "As large language models (LLMs) expand the power of natural language\nprocessing to handle long inputs, rigorous and systematic analyses are\nnecessary to understand their abilities and behavior. A salient application is\nsummarization, due to its ubiquity and controversy (e.g., researchers have\ndeclared the death of summarization). In this paper, we use financial report\nsummarization as a case study because financial reports are not only long but\nalso use numbers and tables extensively. We propose a computational framework\nfor characterizing multimodal long-form summarization and investigate the\nbehavior of Claude 2.0/2.1, GPT-4/3.5, and Cohere. We find that GPT-3.5 and\nCohere fail to perform this summarization task meaningfully. For Claude 2 and\nGPT-4, we analyze the extractiveness of the summary and identify a position\nbias in LLMs. This position bias disappears after shuffling the input for\nClaude, which suggests that Claude seems to recognize important information. We\nalso conduct a comprehensive investigation on the use of numeric data in\nLLM-generated summaries and offer a taxonomy of numeric hallucination. We\nemploy prompt engineering to improve GPT-4's use of numbers with limited\nsuccess. Overall, our analyses highlight the strong capability of Claude 2 in\nhandling long multimodal inputs compared to GPT-4. The generated summaries and\nevaluation code are available at\nhttps://github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) expand the power of natural language\nprocessing to handle long inputs, rigorous and systematic analyses are\nnecessary to understand their abilities and behavior. A salient application is\nsummarization, due to its ubiquity and controversy (e.g., researchers have\ndeclared the death of summarization). In this paper, we use financial report\nsummarization as a case study because financial reports are not only long but\nalso use numbers and tables extensively. We propose a computational framework\nfor characterizing multimodal long-form summarization and investigate the\nbehavior of Claude 2.0/2.1, GPT-4/3.5, and Cohere. We find that GPT-3.5 and\nCohere fail to perform this summarization task meaningfully. For Claude 2 and\nGPT-4, we analyze the extractiveness of the summary and identify a position\nbias in LLMs. This position bias disappears after shuffling the input for\nClaude, which suggests that Claude seems to recognize important information. We\nalso conduct a comprehensive investigation on the use of numeric data in\nLLM-generated summaries and offer a taxonomy of numeric hallucination. We\nemploy prompt engineering to improve GPT-4's use of numbers with limited\nsuccess. Overall, our analyses highlight the strong capability of Claude 2 in\nhandling long multimodal inputs compared to GPT-4. The generated summaries and\nevaluation code are available at\nhttps://github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization."
                },
                "authors": [
                    {
                        "name": "Tianyu Cao"
                    },
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Danial Dervovic"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08158v1",
                "updated": "2024-08-15T13:48:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    48,
                    44,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:48:44Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    48,
                    44,
                    3,
                    228,
                    0
                ],
                "title": "EmBARDiment: an Embodied AI Agent for Productivity in XR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmBARDiment: an Embodied AI Agent for Productivity in XR"
                },
                "summary": "XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents."
                },
                "authors": [
                    {
                        "name": "Riccardo Bovo"
                    },
                    {
                        "name": "Steven Abreu"
                    },
                    {
                        "name": "Karan Ahuja"
                    },
                    {
                        "name": "Eric J Gonzalez"
                    },
                    {
                        "name": "Li-Te Cheng"
                    },
                    {
                        "name": "Mar Gonzalez-Franco"
                    }
                ],
                "author_detail": {
                    "name": "Mar Gonzalez-Franco"
                },
                "author": "Mar Gonzalez-Franco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09228v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09228v5",
                "updated": "2024-08-15T13:46:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    46,
                    6,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-14T12:15:21Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    12,
                    15,
                    21,
                    6,
                    105,
                    0
                ],
                "title": "A Survey on Integration of Large Language Models with Intelligent Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Integration of Large Language Models with Intelligent Robots"
                },
                "summary": "In recent years, the integration of large language models (LLMs) has\nrevolutionized the field of robotics, enabling robots to communicate,\nunderstand, and reason with human-like proficiency. This paper explores the\nmultifaceted impact of LLMs on robotics, addressing key challenges and\nopportunities for leveraging these models across various domains. By\ncategorizing and analyzing LLM applications within core robotics elements --\ncommunication, perception, planning, and control -- we aim to provide\nactionable insights for researchers seeking to integrate LLMs into their\nrobotic systems. Our investigation focuses on LLMs developed post-GPT-3.5,\nprimarily in text-based modalities while also considering multimodal approaches\nfor perception and control. We offer comprehensive guidelines and examples for\nprompt engineering, facilitating beginners' access to LLM-based robotics\nsolutions. Through tutorial-level examples and structured prompt construction,\nwe illustrate how LLM-guided enhancements can be seamlessly integrated into\nrobotics applications. This survey serves as a roadmap for researchers\nnavigating the evolving landscape of LLM-driven robotics, offering a\ncomprehensive overview and practical guidance for harnessing the power of\nlanguage models in robotics development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the integration of large language models (LLMs) has\nrevolutionized the field of robotics, enabling robots to communicate,\nunderstand, and reason with human-like proficiency. This paper explores the\nmultifaceted impact of LLMs on robotics, addressing key challenges and\nopportunities for leveraging these models across various domains. By\ncategorizing and analyzing LLM applications within core robotics elements --\ncommunication, perception, planning, and control -- we aim to provide\nactionable insights for researchers seeking to integrate LLMs into their\nrobotic systems. Our investigation focuses on LLMs developed post-GPT-3.5,\nprimarily in text-based modalities while also considering multimodal approaches\nfor perception and control. We offer comprehensive guidelines and examples for\nprompt engineering, facilitating beginners' access to LLM-based robotics\nsolutions. Through tutorial-level examples and structured prompt construction,\nwe illustrate how LLM-guided enhancements can be seamlessly integrated into\nrobotics applications. This survey serves as a roadmap for researchers\nnavigating the evolving landscape of LLM-driven robotics, offering a\ncomprehensive overview and practical guidance for harnessing the power of\nlanguage models in robotics development."
                },
                "authors": [
                    {
                        "name": "Yeseung Kim"
                    },
                    {
                        "name": "Dohyun Kim"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Jisang Park"
                    },
                    {
                        "name": "Nayoung Oh"
                    },
                    {
                        "name": "Daehyung Park"
                    }
                ],
                "author_detail": {
                    "name": "Daehyung Park"
                },
                "author": "Daehyung Park",
                "arxiv_doi": "10.1007/s11370-024-00550-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11370-024-00550-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.09228v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09228v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 5 figures, Published in Intelligent Service Robotics (ISR)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09314v3",
                "updated": "2024-08-15T13:40:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    40,
                    16,
                    3,
                    228,
                    0
                ],
                "published": "2024-05-15T13:14:05Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    13,
                    14,
                    5,
                    2,
                    136,
                    0
                ],
                "title": "Themis: Automatic and Efficient Deep Learning System Testing with Strong\n  Fault Detection Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Themis: Automatic and Efficient Deep Learning System Testing with Strong\n  Fault Detection Capability"
                },
                "summary": "Deep Learning Systems (DLSs) have been widely applied in safety-critical\ntasks such as autopilot. However, when a perturbed input is fed into a DLS for\ninference, the DLS often has incorrect outputs (i.e., faults). DLS testing\ntechniques (e.g., DeepXplore) detect such faults by generating perturbed inputs\nto explore data flows that induce faults. Since a DLS often has infinitely many\ndata flows, existing techniques require developers to manually specify a set of\nactivation values in a DLS's neurons for exploring fault-inducing data flows.\nUnfortunately, recent studies show that such manual effort is tedious and can\ndetect only a tiny proportion of fault-inducing data flows.\n  In this paper, we present Themis, the first automatic DLS testing system,\nwhich attains strong fault detection capability by ensuring a full coverage of\nfault-inducing data flows at a high probability. Themis carries a new workflow\nfor automatically and systematically revealing data flows whose internal\nneurons' outputs vary substantially when the inputs are slightly perturbed, as\nthese data flows are likely fault-inducing. We evaluated Themis on ten\ndifferent DLSs and found that on average the number of faults detected by\nThemis was 3.78X more than four notable DLS testing techniques. By retraining\nall evaluated DLSs with the detected faults, Themis also increased (regained)\nthese DLSs' accuracies on average 14.7X higher than all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Systems (DLSs) have been widely applied in safety-critical\ntasks such as autopilot. However, when a perturbed input is fed into a DLS for\ninference, the DLS often has incorrect outputs (i.e., faults). DLS testing\ntechniques (e.g., DeepXplore) detect such faults by generating perturbed inputs\nto explore data flows that induce faults. Since a DLS often has infinitely many\ndata flows, existing techniques require developers to manually specify a set of\nactivation values in a DLS's neurons for exploring fault-inducing data flows.\nUnfortunately, recent studies show that such manual effort is tedious and can\ndetect only a tiny proportion of fault-inducing data flows.\n  In this paper, we present Themis, the first automatic DLS testing system,\nwhich attains strong fault detection capability by ensuring a full coverage of\nfault-inducing data flows at a high probability. Themis carries a new workflow\nfor automatically and systematically revealing data flows whose internal\nneurons' outputs vary substantially when the inputs are slightly perturbed, as\nthese data flows are likely fault-inducing. We evaluated Themis on ten\ndifferent DLSs and found that on average the number of faults detected by\nThemis was 3.78X more than four notable DLS testing techniques. By retraining\nall evaluated DLSs with the detected faults, Themis also increased (regained)\nthese DLSs' accuracies on average 14.7X higher than all baselines."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Tsz On Li"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Heming Cui"
                    }
                ],
                "author_detail": {
                    "name": "Heming Cui"
                },
                "author": "Heming Cui",
                "arxiv_comment": "Remove Tsz on due to project license",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08152v1",
                "updated": "2024-08-15T13:40:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    40,
                    3,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:40:03Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    40,
                    3,
                    3,
                    228,
                    0
                ],
                "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for\n  Reinforcement Learning and Monte-Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for\n  Reinforcement Learning and Monte-Carlo Tree Search"
                },
                "summary": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$)."
                },
                "authors": [
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Z. Z. Ren"
                    },
                    {
                        "name": "Junxiao Song"
                    },
                    {
                        "name": "Zhihong Shao"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Dejian Yang"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Z. F. Wu"
                    },
                    {
                        "name": "Fuli Luo"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.12532v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.12532v5",
                "updated": "2024-08-15T13:38:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    38,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2022-06-25T02:15:22Z",
                "published_parsed": [
                    2022,
                    6,
                    25,
                    2,
                    15,
                    22,
                    5,
                    176,
                    0
                ],
                "title": "Inferring Effect Ordering Without Causal Effect Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Effect Ordering Without Causal Effect Estimation"
                },
                "summary": "Predictive models are often employed to guide interventions across various\ndomains, such as advertising, customer retention, and personalized medicine.\nThese models often do not estimate the actual effects of interventions but\nserve as proxies, suggesting potential effectiveness based on predicted\noutcomes. Our paper addresses the critical question of when and how these\npredictive models can be interpreted causally, specifically focusing on using\nthe models for inferring effect ordering rather than precise effect sizes. We\nformalize two assumptions, full latent mediation and latent monotonicity, that\nare jointly sufficient for inferring effect ordering without direct causal\neffect estimation. We explore the utility of these assumptions in assessing the\nfeasibility of proxies for inferring effect ordering in scenarios where there\nis no data on how individuals behave when intervened or no data on the primary\noutcome of interest. Additionally, we provide practical guidelines for\npractitioners to make their own assessments about proxies. Our findings reveal\nnot only when it is possible to reasonably infer effect ordering from proxies,\nbut also conditions under which modeling these proxies can outperform direct\neffect estimation. This study underscores the importance of broadening causal\ninference to encompass alternative causal interpretations beyond effect\nestimation, offering a foundation for future research to enhance\ndecision-making processes when direct effect estimation is not feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive models are often employed to guide interventions across various\ndomains, such as advertising, customer retention, and personalized medicine.\nThese models often do not estimate the actual effects of interventions but\nserve as proxies, suggesting potential effectiveness based on predicted\noutcomes. Our paper addresses the critical question of when and how these\npredictive models can be interpreted causally, specifically focusing on using\nthe models for inferring effect ordering rather than precise effect sizes. We\nformalize two assumptions, full latent mediation and latent monotonicity, that\nare jointly sufficient for inferring effect ordering without direct causal\neffect estimation. We explore the utility of these assumptions in assessing the\nfeasibility of proxies for inferring effect ordering in scenarios where there\nis no data on how individuals behave when intervened or no data on the primary\noutcome of interest. Additionally, we provide practical guidelines for\npractitioners to make their own assessments about proxies. Our findings reveal\nnot only when it is possible to reasonably infer effect ordering from proxies,\nbut also conditions under which modeling these proxies can outperform direct\neffect estimation. This study underscores the importance of broadening causal\ninference to encompass alternative causal interpretations beyond effect\nestimation, offering a foundation for future research to enhance\ndecision-making processes when direct effect estimation is not feasible."
                },
                "authors": [
                    {
                        "name": "Carlos Fernández-Loría"
                    },
                    {
                        "name": "Jorge Loría"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Loría"
                },
                "author": "Jorge Loría",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.12532v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.12532v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08149v1",
                "updated": "2024-08-15T13:35:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    35,
                    59,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    35,
                    59,
                    3,
                    228,
                    0
                ],
                "title": "Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks"
                },
                "summary": "Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08147v1",
                "updated": "2024-08-15T13:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    32,
                    25,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:32:25Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    32,
                    25,
                    3,
                    228,
                    0
                ],
                "title": "P/D-Serve: Serving Disaggregated Large Language Model at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P/D-Serve: Serving Disaggregated Large Language Model at Scale"
                },
                "summary": "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs."
                },
                "authors": [
                    {
                        "name": "Yibo Jin"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Yipeng Ma"
                    },
                    {
                        "name": "Yicheng Shan"
                    },
                    {
                        "name": "Zhengfan Yuan"
                    },
                    {
                        "name": "Cailong Li"
                    },
                    {
                        "name": "Yajing Sun"
                    },
                    {
                        "name": "Tiandeng Wu"
                    },
                    {
                        "name": "Xing Chu"
                    },
                    {
                        "name": "Ruizhi Huan"
                    },
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Xiao You"
                    },
                    {
                        "name": "Wenting Zhou"
                    },
                    {
                        "name": "Yunpeng Ye"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Xiangkun Xu"
                    },
                    {
                        "name": "Yongsheng Zhang"
                    },
                    {
                        "name": "Tiantian Dong"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Xijian Ju"
                    },
                    {
                        "name": "Jianxun Song"
                    },
                    {
                        "name": "Haoliang Cheng"
                    },
                    {
                        "name": "Xiaojing Li"
                    },
                    {
                        "name": "Jiandong Ding"
                    },
                    {
                        "name": "Hefei Guo"
                    },
                    {
                        "name": "Zhengyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyong Zhang"
                },
                "author": "Zhengyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08146v1",
                "updated": "2024-08-15T13:29:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    29,
                    48,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:29:48Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    29,
                    48,
                    3,
                    228,
                    0
                ],
                "title": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft\n  Heads with Adversarial Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft\n  Heads with Adversarial Learning"
                },
                "summary": "Large Language Models (LLMs) exhibit high inference latency due to their\nautoregressive decoding nature. While the draft head in speculative decoding\nmitigates this issue, its full potential remains unexplored. In this paper, we\nintroduce KOALA (K-layer Optimized Adversarial Learning Architecture), an\northogonal approach to the draft head. By transforming the conventional\nsingle-layer draft head into a multi-layer architecture and incorporating\nadversarial learning into the traditional supervised training, KOALA\nsignificantly improves the accuracy of the draft head in predicting subsequent\ntokens, thus more closely mirroring the functionality of LLMs. Although this\nimprovement comes at the cost of slightly increased drafting overhead, KOALA\nsubstantially unlocks the draft head's potential, greatly enhancing speculative\ndecoding. We conducted comprehensive evaluations of KOALA, including both\nautoregressive and non-autoregressive draft heads across various tasks,\ndemonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is\n10.57%-14.09% faster than the original draft heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit high inference latency due to their\nautoregressive decoding nature. While the draft head in speculative decoding\nmitigates this issue, its full potential remains unexplored. In this paper, we\nintroduce KOALA (K-layer Optimized Adversarial Learning Architecture), an\northogonal approach to the draft head. By transforming the conventional\nsingle-layer draft head into a multi-layer architecture and incorporating\nadversarial learning into the traditional supervised training, KOALA\nsignificantly improves the accuracy of the draft head in predicting subsequent\ntokens, thus more closely mirroring the functionality of LLMs. Although this\nimprovement comes at the cost of slightly increased drafting overhead, KOALA\nsubstantially unlocks the draft head's potential, greatly enhancing speculative\ndecoding. We conducted comprehensive evaluations of KOALA, including both\nautoregressive and non-autoregressive draft heads across various tasks,\ndemonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is\n10.57%-14.09% faster than the original draft heads."
                },
                "authors": [
                    {
                        "name": "Kaiqi Zhang"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Rui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rui Chen"
                },
                "author": "Rui Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08144v1",
                "updated": "2024-08-15T13:28:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    28,
                    18,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:28:18Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    28,
                    18,
                    3,
                    228,
                    0
                ],
                "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU"
                },
                "summary": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "So-Eon Kim"
                    },
                    {
                        "name": "Seong-Bae Park"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    }
                ],
                "author_detail": {
                    "name": "Soyeon Caren Han"
                },
                "author": "Soyeon Caren Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08133v1",
                "updated": "2024-08-15T13:07:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    7,
                    51,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:07:51Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    7,
                    51,
                    3,
                    228,
                    0
                ],
                "title": "EXPLAIN, AGREE, LEARN: Scaling Learning for Neural Probabilistic Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXPLAIN, AGREE, LEARN: Scaling Learning for Neural Probabilistic Logic"
                },
                "summary": "Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm\nby combining the perceptive and learning capabilities of neural networks with\nthe robustness of probabilistic logic. Learning corresponds to likelihood\noptimization of the neural networks. However, to obtain the likelihood exactly,\nexpensive probabilistic logic inference is required. To scale learning to more\ncomplex systems, we therefore propose to instead optimize a sampling based\nobjective. We prove that the objective has a bounded error with respect to the\nlikelihood, which vanishes when increasing the sample count. Furthermore, the\nerror vanishes faster by exploiting a new concept of sample diversity. We then\ndevelop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective.\nEXPLAIN samples explanations for the data. AGREE reweighs each explanation in\nconcordance with the neural component. LEARN uses the reweighed explanations as\na signal for learning. In contrast to previous NeSy methods, EXAL can scale to\nlarger problem sizes while retaining theoretical guarantees on the error.\nExperimentally, our theoretical claims are verified and EXAL outperforms recent\nNeSy methods when scaling up the MNIST addition and Warcraft pathfinding\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm\nby combining the perceptive and learning capabilities of neural networks with\nthe robustness of probabilistic logic. Learning corresponds to likelihood\noptimization of the neural networks. However, to obtain the likelihood exactly,\nexpensive probabilistic logic inference is required. To scale learning to more\ncomplex systems, we therefore propose to instead optimize a sampling based\nobjective. We prove that the objective has a bounded error with respect to the\nlikelihood, which vanishes when increasing the sample count. Furthermore, the\nerror vanishes faster by exploiting a new concept of sample diversity. We then\ndevelop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective.\nEXPLAIN samples explanations for the data. AGREE reweighs each explanation in\nconcordance with the neural component. LEARN uses the reweighed explanations as\na signal for learning. In contrast to previous NeSy methods, EXAL can scale to\nlarger problem sizes while retaining theoretical guarantees on the error.\nExperimentally, our theoretical claims are verified and EXAL outperforms recent\nNeSy methods when scaling up the MNIST addition and Warcraft pathfinding\nproblems."
                },
                "authors": [
                    {
                        "name": "Victor Verreet"
                    },
                    {
                        "name": "Lennert De Smet"
                    },
                    {
                        "name": "Luc De Raedt"
                    },
                    {
                        "name": "Emanuele Sansone"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Sansone"
                },
                "author": "Emanuele Sansone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2107.13783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2107.13783v2",
                "updated": "2024-08-15T12:55:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    55,
                    16,
                    3,
                    228,
                    0
                ],
                "published": "2021-07-29T07:25:35Z",
                "published_parsed": [
                    2021,
                    7,
                    29,
                    7,
                    25,
                    35,
                    3,
                    210,
                    0
                ],
                "title": "Efficiently resolving rotational ambiguity in Bayesian matrix sampling\n  with matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently resolving rotational ambiguity in Bayesian matrix sampling\n  with matching"
                },
                "summary": "A wide class of Bayesian models involve unidentifiable random matrices that\ndisplay rotational ambiguity, with the Gaussian factor model being a typical\nexample. A rich variety of Markov chain Monte Carlo (MCMC) algorithms have been\nproposed for sampling the parameters of these models. However, without\nidentifiability constraints, reliable posterior summaries of the parameters\ncannot be obtained directly from the MCMC output. As an alternative, we propose\na computationally efficient post-processing algorithm that allows inference on\nnon-identifiable parameters. We first orthogonalize the posterior samples using\nVarimax and then tackle label and sign switching with a greedy matching\nalgorithm. We compare the performance and computational complexity with other\nmethods using a simulation study and chemical exposures data. The algorithm\nimplementation is available in the infinitefactor R package on CRAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide class of Bayesian models involve unidentifiable random matrices that\ndisplay rotational ambiguity, with the Gaussian factor model being a typical\nexample. A rich variety of Markov chain Monte Carlo (MCMC) algorithms have been\nproposed for sampling the parameters of these models. However, without\nidentifiability constraints, reliable posterior summaries of the parameters\ncannot be obtained directly from the MCMC output. As an alternative, we propose\na computationally efficient post-processing algorithm that allows inference on\nnon-identifiable parameters. We first orthogonalize the posterior samples using\nVarimax and then tackle label and sign switching with a greedy matching\nalgorithm. We compare the performance and computational complexity with other\nmethods using a simulation study and chemical exposures data. The algorithm\nimplementation is available in the infinitefactor R package on CRAN."
                },
                "authors": [
                    {
                        "name": "Evan Poworoznek"
                    },
                    {
                        "name": "Niccolo Anceschi"
                    },
                    {
                        "name": "Federico Ferrari"
                    },
                    {
                        "name": "David Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David Dunson"
                },
                "author": "David Dunson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2107.13783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2107.13783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08119v1",
                "updated": "2024-08-15T12:38:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    38,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T12:38:10Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    38,
                    10,
                    3,
                    228,
                    0
                ],
                "title": "The Unreasonable Effectiveness of Solving Inverse Problems with Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Effectiveness of Solving Inverse Problems with Neural\n  Networks"
                },
                "summary": "Finding model parameters from data is an essential task in science and\nengineering, from weather and climate forecasts to plasma control. Previous\nworks have employed neural networks to greatly accelerate finding solutions to\ninverse problems. Of particular interest are end-to-end models which utilize\ndifferentiable simulations in order to backpropagate feedback from the\nsimulated process to the network weights and enable roll-out of multiple time\nsteps. So far, it has been assumed that, while model inference is faster than\nclassical optimization, this comes at the cost of a decrease in solution\naccuracy. We show that this is generally not true. In fact, neural networks\ntrained to learn solutions to inverse problems can find better solutions than\nclassical optimizers even on their training set. To demonstrate this, we\nperform both a theoretical analysis as well an extensive empirical evaluation\non challenging problems involving local minima, chaos, and zero-gradient\nregions. Our findings suggest an alternative use for neural networks: rather\nthan generalizing to new data for fast inference, they can also be used to find\nbetter solutions on known data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding model parameters from data is an essential task in science and\nengineering, from weather and climate forecasts to plasma control. Previous\nworks have employed neural networks to greatly accelerate finding solutions to\ninverse problems. Of particular interest are end-to-end models which utilize\ndifferentiable simulations in order to backpropagate feedback from the\nsimulated process to the network weights and enable roll-out of multiple time\nsteps. So far, it has been assumed that, while model inference is faster than\nclassical optimization, this comes at the cost of a decrease in solution\naccuracy. We show that this is generally not true. In fact, neural networks\ntrained to learn solutions to inverse problems can find better solutions than\nclassical optimizers even on their training set. To demonstrate this, we\nperform both a theoretical analysis as well an extensive empirical evaluation\non challenging problems involving local minima, chaos, and zero-gradient\nregions. Our findings suggest an alternative use for neural networks: rather\nthan generalizing to new data for fast inference, they can also be used to find\nbetter solutions on known data."
                },
                "authors": [
                    {
                        "name": "Philipp Holl"
                    },
                    {
                        "name": "Nils Thuerey"
                    }
                ],
                "author_detail": {
                    "name": "Nils Thuerey"
                },
                "author": "Nils Thuerey",
                "arxiv_comment": "Source code to follow soon: https://ge.in.tum.de",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15706v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15706v6",
                "updated": "2024-08-15T12:25:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    25,
                    39,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T15:16:47Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    16,
                    47,
                    0,
                    204,
                    0
                ],
                "title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition"
                },
                "summary": "Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action."
                },
                "authors": [
                    {
                        "name": "Jinfu Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mengyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Liu"
                },
                "author": "Mengyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15706v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15706v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08114v1",
                "updated": "2024-08-15T12:23:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    23,
                    53,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T12:23:53Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    23,
                    53,
                    3,
                    228,
                    0
                ],
                "title": "Quantifying the informativity of emission lines to infer physical\n  conditions in giant molecular clouds. I. Application to model predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the informativity of emission lines to infer physical\n  conditions in giant molecular clouds. I. Application to model predictions"
                },
                "summary": "Observations of ionic, atomic, or molecular lines are performed to improve\nour understanding of the interstellar medium (ISM). However, the potential of a\nline to constrain the physical conditions of the ISM is difficult to assess\nquantitatively, because of the complexity of the ISM physics. The situation is\neven more complex when trying to assess which combinations of lines are the\nmost useful. Therefore, observation campaigns usually try to observe as many\nlines as possible for as much time as possible. We search for a quantitative\nstatistical criterion to evaluate the constraining power of a (or combination\nof) tracer(s) with respect to physical conditions in order to improve our\nunderstanding of the statistical relationships between ISM tracers and physical\nconditions and helps observers to motivate their observation proposals. The\nbest tracers are obtained by comparing the mutual information between a\nphysical parameter and different sets of lines. We apply this method to\nsimulations of radio molecular lines emitted by a photodissociation region\nsimilar to the Horsehead Nebula that would be observed at the IRAM 30m\ntelescope. We search for the best lines to constrain the visual extinction\n$A_v^{tot}$ or the far UV illumination $G_0$. The most informative lines change\nwith the physical regime (e.g., cloud extinction). Short integration time of\nthe CO isotopologue $J=1-0$ lines already yields much information on the total\ncolumn density most regimes. The best set of lines to constrain the visual\nextinction does not necessarily combine the most informative individual lines.\nPrecise constraints on $G_0$ are more difficult to achieve with molecular\nlines. They require spectral lines emitted at the cloud surface (e.g., [CII]\nand [CI] lines). This approach allows one to better explore the knowledge\nprovided by ISM codes, and to guide future observation campaigns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of ionic, atomic, or molecular lines are performed to improve\nour understanding of the interstellar medium (ISM). However, the potential of a\nline to constrain the physical conditions of the ISM is difficult to assess\nquantitatively, because of the complexity of the ISM physics. The situation is\neven more complex when trying to assess which combinations of lines are the\nmost useful. Therefore, observation campaigns usually try to observe as many\nlines as possible for as much time as possible. We search for a quantitative\nstatistical criterion to evaluate the constraining power of a (or combination\nof) tracer(s) with respect to physical conditions in order to improve our\nunderstanding of the statistical relationships between ISM tracers and physical\nconditions and helps observers to motivate their observation proposals. The\nbest tracers are obtained by comparing the mutual information between a\nphysical parameter and different sets of lines. We apply this method to\nsimulations of radio molecular lines emitted by a photodissociation region\nsimilar to the Horsehead Nebula that would be observed at the IRAM 30m\ntelescope. We search for the best lines to constrain the visual extinction\n$A_v^{tot}$ or the far UV illumination $G_0$. The most informative lines change\nwith the physical regime (e.g., cloud extinction). Short integration time of\nthe CO isotopologue $J=1-0$ lines already yields much information on the total\ncolumn density most regimes. The best set of lines to constrain the visual\nextinction does not necessarily combine the most informative individual lines.\nPrecise constraints on $G_0$ are more difficult to achieve with molecular\nlines. They require spectral lines emitted at the cloud surface (e.g., [CII]\nand [CI] lines). This approach allows one to better explore the knowledge\nprovided by ISM codes, and to guide future observation campaigns."
                },
                "authors": [
                    {
                        "name": "Lucas Einig"
                    },
                    {
                        "name": "Pierre Palud"
                    },
                    {
                        "name": "Antoine Roueff"
                    },
                    {
                        "name": "Jérôme Pety"
                    },
                    {
                        "name": "Emeric Bron"
                    },
                    {
                        "name": "Franck Le Petit"
                    },
                    {
                        "name": "Maryvonne Gerin"
                    },
                    {
                        "name": "Jocelyn Chanussot"
                    },
                    {
                        "name": "Pierre Chainais"
                    },
                    {
                        "name": "Pierre-Antoine Thouvenin"
                    },
                    {
                        "name": "David Languignon"
                    },
                    {
                        "name": "Ivana Bešlić"
                    },
                    {
                        "name": "Simon Coudé"
                    },
                    {
                        "name": "Helena Mazurek"
                    },
                    {
                        "name": "Jan H. Orkisz"
                    },
                    {
                        "name": "Miriam G. Santa-Maria"
                    },
                    {
                        "name": "Léontine Ségal"
                    },
                    {
                        "name": "Antoine Zakardjian"
                    },
                    {
                        "name": "Sébastien Bardeau"
                    },
                    {
                        "name": "Karine Demyk"
                    },
                    {
                        "name": "Victor de Souza Magalhes"
                    },
                    {
                        "name": "Javier R. Goicoechea"
                    },
                    {
                        "name": "Pierre Gratier"
                    },
                    {
                        "name": "Viviana V. Guzmán"
                    },
                    {
                        "name": "Annie Hughes"
                    },
                    {
                        "name": "François Levrier"
                    },
                    {
                        "name": "Jacques Le Bourlot"
                    },
                    {
                        "name": "Dariusz C. Lis"
                    },
                    {
                        "name": "Harvey S. Liszt"
                    },
                    {
                        "name": "Nicolas Peretto"
                    },
                    {
                        "name": "Evelyne Roueff"
                    },
                    {
                        "name": "Albrecht Sievers"
                    }
                ],
                "author_detail": {
                    "name": "Albrecht Sievers"
                },
                "author": "Albrecht Sievers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08105v1",
                "updated": "2024-08-15T12:04:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    4,
                    32,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T12:04:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    4,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language\n  Models to Infer Causal Links Between Siamese Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language\n  Models to Infer Causal Links Between Siamese Images"
                },
                "summary": "Large Language Models (LLMs) have showcased exceptional ability in causal\nreasoning from textual information. However, will these causalities remain\nstraightforward for Vision Large Language Models (VLLMs) when only visual hints\nare provided? Motivated by this, we propose a novel Multimodal Causal Reasoning\nbenchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect\nrelationship when solely relying on visual cues such as action, appearance,\nclothing, and environment. Specifically, we introduce a prompt-driven image\nsynthesis approach to create siamese images with embedded semantic causality\nand visual cues, which can effectively evaluate VLLMs' causal reasoning\ncapabilities. Additionally, we develop tailored metrics from multiple\nperspectives, including image-level match, phrase-level understanding, and\nsentence-level explanation, to comprehensively assess VLLMs' comprehension\nabilities. Our extensive experiments reveal that the current state-of-the-art\nVLLMs are not as skilled at multimodal causal reasoning as we might have hoped.\nFurthermore, we perform a comprehensive analysis to understand these models'\nshortcomings from different views and suggest directions for future research.\nWe hope MuCR can serve as a valuable resource and foundational benchmark in\nmultimodal causal reasoning research. The project is available at:\nhttps://github.com/Zhiyuan-Li-John/MuCR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased exceptional ability in causal\nreasoning from textual information. However, will these causalities remain\nstraightforward for Vision Large Language Models (VLLMs) when only visual hints\nare provided? Motivated by this, we propose a novel Multimodal Causal Reasoning\nbenchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect\nrelationship when solely relying on visual cues such as action, appearance,\nclothing, and environment. Specifically, we introduce a prompt-driven image\nsynthesis approach to create siamese images with embedded semantic causality\nand visual cues, which can effectively evaluate VLLMs' causal reasoning\ncapabilities. Additionally, we develop tailored metrics from multiple\nperspectives, including image-level match, phrase-level understanding, and\nsentence-level explanation, to comprehensively assess VLLMs' comprehension\nabilities. Our extensive experiments reveal that the current state-of-the-art\nVLLMs are not as skilled at multimodal causal reasoning as we might have hoped.\nFurthermore, we perform a comprehensive analysis to understand these models'\nshortcomings from different views and suggest directions for future research.\nWe hope MuCR can serve as a valuable resource and foundational benchmark in\nmultimodal causal reasoning research. The project is available at:\nhttps://github.com/Zhiyuan-Li-John/MuCR"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Dongnan Liu"
                    },
                    {
                        "name": "Chaoyi Zhang"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Jieting Long"
                    },
                    {
                        "name": "Weidong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Cai"
                },
                "author": "Weidong Cai",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.11188v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.11188v4",
                "updated": "2024-08-15T11:52:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    52,
                    20,
                    3,
                    228,
                    0
                ],
                "published": "2023-06-19T22:46:20Z",
                "published_parsed": [
                    2023,
                    6,
                    19,
                    22,
                    46,
                    20,
                    0,
                    170,
                    0
                ],
                "title": "Invariant correlation under marginal transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant correlation under marginal transforms"
                },
                "summary": "A useful property of independent samples is that their correlation remains\nthe same after applying marginal transforms. This invariance property plays a\nfundamental role in statistical inference, but does not hold in general for\ndependent samples. In this paper, we study this invariance property on the\nPearson correlation coefficient and its applications. A multivariate random\nvector is said to have an invariant correlation if its pairwise correlation\ncoefficients remain unchanged under any common marginal transforms. For a\nbivariate case, we characterize all models of such a random vector via a\ncertain combination of comonotonicity -- the strongest form of positive\ndependence -- and independence. In particular, we show that the class of\nexchangeable copulas with invariant correlation is precisely described by what\nwe call positive Fr\\'echet copulas. In the general multivariate case, we\ncharacterize the set of all invariant correlation matrices via the clique\npartition polytope. We also propose a positive regression dependent model that\nadmits any prescribed invariant correlation matrix. Finally, we show that all\nour characterization results of invariant correlation, except one special case,\nremain the same if the common marginal transforms are confined to the set of\nincreasing ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A useful property of independent samples is that their correlation remains\nthe same after applying marginal transforms. This invariance property plays a\nfundamental role in statistical inference, but does not hold in general for\ndependent samples. In this paper, we study this invariance property on the\nPearson correlation coefficient and its applications. A multivariate random\nvector is said to have an invariant correlation if its pairwise correlation\ncoefficients remain unchanged under any common marginal transforms. For a\nbivariate case, we characterize all models of such a random vector via a\ncertain combination of comonotonicity -- the strongest form of positive\ndependence -- and independence. In particular, we show that the class of\nexchangeable copulas with invariant correlation is precisely described by what\nwe call positive Fr\\'echet copulas. In the general multivariate case, we\ncharacterize the set of all invariant correlation matrices via the clique\npartition polytope. We also propose a positive regression dependent model that\nadmits any prescribed invariant correlation matrix. Finally, we show that all\nour characterization results of invariant correlation, except one special case,\nremain the same if the common marginal transforms are confined to the set of\nincreasing ones."
                },
                "authors": [
                    {
                        "name": "Takaaki Koike"
                    },
                    {
                        "name": "Liyuan Lin"
                    },
                    {
                        "name": "Ruodu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruodu Wang"
                },
                "author": "Ruodu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.11188v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.11188v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08089v1",
                "updated": "2024-08-15T11:33:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    33,
                    20,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:33:20Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    33,
                    20,
                    3,
                    228,
                    0
                ],
                "title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents"
                },
                "summary": "In this paper, we present a simulation system called AgentCourt that\nsimulates the entire courtroom process. The judge, plaintiff's lawyer, defense\nlawyer, and other participants are autonomous agents driven by large language\nmodels (LLMs). Our core goal is to enable lawyer agents to learn how to argue a\ncase, as well as improving their overall legal skills, through courtroom\nprocess simulation. To achieve this goal, we propose an adversarial\nevolutionary approach for the lawyer-agent. Since AgentCourt can simulate the\noccurrence and development of court hearings based on a knowledge base and LLM,\nthe lawyer agents can continuously learn and accumulate experience from real\ncourt cases. The simulation experiments show that after two lawyer-agents have\nengaged in a thousand adversarial legal cases in AgentCourt (which can take a\ndecade for real-world lawyers), compared to their pre-evolutionary state, the\nevolved lawyer agents exhibit consistent improvement in their ability to handle\nlegal tasks. To enhance the credibility of our experimental results, we\nenlisted a panel of professional lawyers to evaluate our simulations. The\nevaluation indicates that the evolved lawyer agents exhibit notable\nadvancements in responsiveness, as well as expertise and logical rigor. This\nwork paves the way for advancing LLM-driven agent technology in legal\nscenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a simulation system called AgentCourt that\nsimulates the entire courtroom process. The judge, plaintiff's lawyer, defense\nlawyer, and other participants are autonomous agents driven by large language\nmodels (LLMs). Our core goal is to enable lawyer agents to learn how to argue a\ncase, as well as improving their overall legal skills, through courtroom\nprocess simulation. To achieve this goal, we propose an adversarial\nevolutionary approach for the lawyer-agent. Since AgentCourt can simulate the\noccurrence and development of court hearings based on a knowledge base and LLM,\nthe lawyer agents can continuously learn and accumulate experience from real\ncourt cases. The simulation experiments show that after two lawyer-agents have\nengaged in a thousand adversarial legal cases in AgentCourt (which can take a\ndecade for real-world lawyers), compared to their pre-evolutionary state, the\nevolved lawyer agents exhibit consistent improvement in their ability to handle\nlegal tasks. To enhance the credibility of our experimental results, we\nenlisted a panel of professional lawyers to evaluate our simulations. The\nevaluation indicates that the evolved lawyer agents exhibit notable\nadvancements in responsiveness, as well as expertise and logical rigor. This\nwork paves the way for advancing LLM-driven agent technology in legal\nscenarios. Code is available at https://github.com/relic-yuexi/AgentCourt."
                },
                "authors": [
                    {
                        "name": "Guhong Chen"
                    },
                    {
                        "name": "Liyang Fan"
                    },
                    {
                        "name": "Zihan Gong"
                    },
                    {
                        "name": "Nan Xie"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08088v1",
                "updated": "2024-08-15T11:32:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    32,
                    46,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:32:46Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    32,
                    46,
                    3,
                    228,
                    0
                ],
                "title": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber\n  Threat Intelligence Credibility Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber\n  Threat Intelligence Credibility Assessment"
                },
                "summary": "Cyber threat intelligence is a critical tool that many organizations and\nindividuals use to protect themselves from sophisticated, organized,\npersistent, and weaponized cyber attacks. However, few studies have focused on\nthe quality assessment of threat intelligence provided by intelligence\nplatforms, and this work still requires manual analysis by cybersecurity\nexperts. In this paper, we propose a knowledge graph-based verifier, a novel\nCyber Threat Intelligence (CTI) quality assessment framework that combines\nknowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs\nto automatically extract OSCTI key claims to be verified and utilizes a\nknowledge graph consisting of paragraphs for fact-checking. This method differs\nfrom the traditional way of constructing complex knowledge graphs with entities\nas nodes. By constructing knowledge graphs with paragraphs as nodes and\nsemantic similarity as edges, it effectively enhances the semantic\nunderstanding ability of the model and simplifies labeling requirements.\nAdditionally, to fill the gap in the research field, we created and made public\nthe first dataset for threat intelligence assessment from heterogeneous\nsources. To the best of our knowledge, this work is the first to create a\ndataset on threat intelligence reliability verification, providing a reference\nfor future research. Experimental results show that KGV (Knowledge Graph\nVerifier) significantly improves the performance of LLMs in intelligence\nquality assessment. Compared with traditional methods, we reduce a large amount\nof data annotation while the model still exhibits strong reasoning\ncapabilities. Finally, our method can achieve XXX accuracy in network threat\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber threat intelligence is a critical tool that many organizations and\nindividuals use to protect themselves from sophisticated, organized,\npersistent, and weaponized cyber attacks. However, few studies have focused on\nthe quality assessment of threat intelligence provided by intelligence\nplatforms, and this work still requires manual analysis by cybersecurity\nexperts. In this paper, we propose a knowledge graph-based verifier, a novel\nCyber Threat Intelligence (CTI) quality assessment framework that combines\nknowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs\nto automatically extract OSCTI key claims to be verified and utilizes a\nknowledge graph consisting of paragraphs for fact-checking. This method differs\nfrom the traditional way of constructing complex knowledge graphs with entities\nas nodes. By constructing knowledge graphs with paragraphs as nodes and\nsemantic similarity as edges, it effectively enhances the semantic\nunderstanding ability of the model and simplifies labeling requirements.\nAdditionally, to fill the gap in the research field, we created and made public\nthe first dataset for threat intelligence assessment from heterogeneous\nsources. To the best of our knowledge, this work is the first to create a\ndataset on threat intelligence reliability verification, providing a reference\nfor future research. Experimental results show that KGV (Knowledge Graph\nVerifier) significantly improves the performance of LLMs in intelligence\nquality assessment. Compared with traditional methods, we reduce a large amount\nof data annotation while the model still exhibits strong reasoning\ncapabilities. Finally, our method can achieve XXX accuracy in network threat\nassessment."
                },
                "authors": [
                    {
                        "name": "Zongzong Wu"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Yufeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Li"
                },
                "author": "Yufeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08083v1",
                "updated": "2024-08-15T11:16:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    16,
                    21,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:16:21Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    16,
                    21,
                    3,
                    228,
                    0
                ],
                "title": "Confidence-weighted integration of human and machine judgments for\n  superior decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-weighted integration of human and machine judgments for\n  superior decision-making"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools in various\ndomains. Recent studies have shown that LLMs can surpass humans in certain\ntasks, such as predicting the outcomes of neuroscience studies. What role does\nthis leave for humans in the overall decision process? One possibility is that\nhumans, despite performing worse than LLMs, can still add value when teamed\nwith them. A human and machine team can surpass each individual teammate when\nteam members' confidence is well-calibrated and team members diverge in which\ntasks they find difficult (i.e., calibration and diversity are needed). We\nsimplified and extended a Bayesian approach to combining judgments using a\nlogistic regression framework that integrates confidence-weighted judgments for\nany number of team members. Using this straightforward method, we demonstrated\nin a neuroscience forecasting task that, even when humans were inferior to\nLLMs, their combination with one or more LLMs consistently improved team\nperformance. Our hope is that this simple and effective strategy for\nintegrating the judgments of humans and machines will lead to productive\ncollaborations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools in various\ndomains. Recent studies have shown that LLMs can surpass humans in certain\ntasks, such as predicting the outcomes of neuroscience studies. What role does\nthis leave for humans in the overall decision process? One possibility is that\nhumans, despite performing worse than LLMs, can still add value when teamed\nwith them. A human and machine team can surpass each individual teammate when\nteam members' confidence is well-calibrated and team members diverge in which\ntasks they find difficult (i.e., calibration and diversity are needed). We\nsimplified and extended a Bayesian approach to combining judgments using a\nlogistic regression framework that integrates confidence-weighted judgments for\nany number of team members. Using this straightforward method, we demonstrated\nin a neuroscience forecasting task that, even when humans were inferior to\nLLMs, their combination with one or more LLMs consistently improved team\nperformance. Our hope is that this simple and effective strategy for\nintegrating the judgments of humans and machines will lead to productive\ncollaborations."
                },
                "authors": [
                    {
                        "name": "Felipe Yáñez"
                    },
                    {
                        "name": "Xiaoliang Luo"
                    },
                    {
                        "name": "Omar Valerio Minero"
                    },
                    {
                        "name": "Bradley C. Love"
                    }
                ],
                "author_detail": {
                    "name": "Bradley C. Love"
                },
                "author": "Bradley C. Love",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08074v1",
                "updated": "2024-08-15T11:01:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    1,
                    35,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:01:35Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    1,
                    35,
                    3,
                    228,
                    0
                ],
                "title": "A Survey on Integrated Sensing, Communication, and Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Integrated Sensing, Communication, and Computation"
                },
                "summary": "The forthcoming generation of wireless technology, 6G, promises a\nrevolutionary leap beyond traditional data-centric services. It aims to usher\nin an era of ubiquitous intelligent services, where everything is\ninterconnected and intelligent. This vision requires the seamless integration\nof three fundamental modules: Sensing for information acquisition,\ncommunication for information sharing, and computation for information\nprocessing and decision-making. These modules are intricately linked,\nespecially in complex tasks such as edge learning and inference. However, the\nperformance of these modules is interdependent, creating a resource competition\nfor time, energy, and bandwidth. Existing techniques like integrated\ncommunication and computation (ICC), integrated sensing and computation (ISC),\nand integrated sensing and communication (ISAC) have made partial strides in\naddressing this challenge, but they fall short of meeting the extreme\nperformance requirements. To overcome these limitations, it is essential to\ndevelop new techniques that comprehensively integrate sensing, communication,\nand computation. This integrated approach, known as Integrated Sensing,\nCommunication, and Computation (ISCC), offers a systematic perspective for\nenhancing task performance. This paper begins with a comprehensive survey of\nhistoric and related techniques such as ICC, ISC, and ISAC, highlighting their\nstrengths and limitations. It then explores the state-of-the-art signal designs\nfor ISCC, along with network resource management strategies specifically\ntailored for ISCC. Furthermore, this paper discusses the exciting research\nopportunities that lie ahead for implementing ISCC in future advanced networks.\nBy embracing ISCC, we can unlock the full potential of intelligent\nconnectivity, paving the way for groundbreaking applications and services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The forthcoming generation of wireless technology, 6G, promises a\nrevolutionary leap beyond traditional data-centric services. It aims to usher\nin an era of ubiquitous intelligent services, where everything is\ninterconnected and intelligent. This vision requires the seamless integration\nof three fundamental modules: Sensing for information acquisition,\ncommunication for information sharing, and computation for information\nprocessing and decision-making. These modules are intricately linked,\nespecially in complex tasks such as edge learning and inference. However, the\nperformance of these modules is interdependent, creating a resource competition\nfor time, energy, and bandwidth. Existing techniques like integrated\ncommunication and computation (ICC), integrated sensing and computation (ISC),\nand integrated sensing and communication (ISAC) have made partial strides in\naddressing this challenge, but they fall short of meeting the extreme\nperformance requirements. To overcome these limitations, it is essential to\ndevelop new techniques that comprehensively integrate sensing, communication,\nand computation. This integrated approach, known as Integrated Sensing,\nCommunication, and Computation (ISCC), offers a systematic perspective for\nenhancing task performance. This paper begins with a comprehensive survey of\nhistoric and related techniques such as ICC, ISC, and ISAC, highlighting their\nstrengths and limitations. It then explores the state-of-the-art signal designs\nfor ISCC, along with network resource management strategies specifically\ntailored for ISCC. Furthermore, this paper discusses the exciting research\nopportunities that lie ahead for implementing ISCC in future advanced networks.\nBy embracing ISCC, we can unlock the full potential of intelligent\nconnectivity, paving the way for groundbreaking applications and services."
                },
                "authors": [
                    {
                        "name": "Dingzhu Wen"
                    },
                    {
                        "name": "Yong Zhou"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yuanming Shi"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08072v1",
                "updated": "2024-08-15T10:44:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T10:44:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05661v2",
                "updated": "2024-08-15T10:27:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    27,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-09T06:30:28Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    6,
                    30,
                    28,
                    6,
                    161,
                    0
                ],
                "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked\n  Language Modelling methods for learning Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked\n  Language Modelling methods for learning Speech Representations"
                },
                "summary": "In recent years, self-supervised pre-training methods have gained significant\ntraction in learning high-level information from raw speech. Among these\nmethods, HuBERT has demonstrated SOTA performance in automatic speech\nrecognition (ASR). However, HuBERT's performance lags behind data2vec due to\ndisparities in pre-training strategies. In this paper, we propose (i) a Swap\nmethod to address pre-training and inference mismatch observed in HuBERT and\n(ii) incorporates Multicluster masked prediction loss for more effective\nutilization of the models capacity. The resulting method is, MS-HuBERT, an\nend-to-end self-supervised pre-training method for learning robust speech\nrepresentations. It beats vanilla HuBERT on the ASR Librispeech benchmark on\naverage by a 5% margin when evaluated on different finetuning splits.\nAdditionally, we demonstrate that the learned embeddings obtained during\npre-training encode essential information for improving performance of content\nbased tasks such as ASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, self-supervised pre-training methods have gained significant\ntraction in learning high-level information from raw speech. Among these\nmethods, HuBERT has demonstrated SOTA performance in automatic speech\nrecognition (ASR). However, HuBERT's performance lags behind data2vec due to\ndisparities in pre-training strategies. In this paper, we propose (i) a Swap\nmethod to address pre-training and inference mismatch observed in HuBERT and\n(ii) incorporates Multicluster masked prediction loss for more effective\nutilization of the models capacity. The resulting method is, MS-HuBERT, an\nend-to-end self-supervised pre-training method for learning robust speech\nrepresentations. It beats vanilla HuBERT on the ASR Librispeech benchmark on\naverage by a 5% margin when evaluated on different finetuning splits.\nAdditionally, we demonstrate that the learned embeddings obtained during\npre-training encode essential information for improving performance of content\nbased tasks such as ASR."
                },
                "authors": [
                    {
                        "name": "Hemant Yadav"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "4 pages, submitted to interspeech2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08066v1",
                "updated": "2024-08-15T10:15:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    15,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T10:15:37Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    15,
                    37,
                    3,
                    228,
                    0
                ],
                "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense\n  Retrieval"
                },
                "summary": "In the information retrieval (IR) area, dense retrieval (DR) models use deep\nlearning techniques to encode queries and passages into embedding space to\ncompute their semantic relations. It is important for DR models to balance both\nefficiency and effectiveness. Pre-trained language models (PLMs), especially\nTransformer-based PLMs, have been proven to be effective encoders of DR models.\nHowever, the self-attention component in Transformer-based PLM results in a\ncomputational complexity that grows quadratically with sequence length, and\nthus exhibits a slow inference speed for long-text retrieval. Some recently\nproposed non-Transformer PLMs, especially the Mamba architecture PLMs, have\ndemonstrated not only comparable effectiveness to Transformer-based PLMs on\ngenerative language tasks but also better efficiency due to linear time scaling\nin sequence length. This paper implements the Mamba Retriever to explore\nwhether Mamba can serve as an effective and efficient encoder of DR model for\nIR tasks. We fine-tune the Mamba Retriever on the classic short-text MS MARCO\npassage ranking dataset and the long-text LoCoV0 dataset. Experimental results\nshow that (1) on the MS MARCO passage ranking dataset and BEIR, the Mamba\nRetriever achieves comparable or better effectiveness compared to\nTransformer-based retrieval models, and the effectiveness grows with the size\nof the Mamba model; (2) on the long-text LoCoV0 dataset, the Mamba Retriever\ncan extend to longer text length than its pre-trained length after fine-tuning\non retrieval task, and it has comparable or better effectiveness compared to\nother long-text retrieval models; (3) the Mamba Retriever has superior\ninference speed for long-text retrieval. In conclusion, Mamba Retriever is both\neffective and efficient, making it a practical model, especially for long-text\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the information retrieval (IR) area, dense retrieval (DR) models use deep\nlearning techniques to encode queries and passages into embedding space to\ncompute their semantic relations. It is important for DR models to balance both\nefficiency and effectiveness. Pre-trained language models (PLMs), especially\nTransformer-based PLMs, have been proven to be effective encoders of DR models.\nHowever, the self-attention component in Transformer-based PLM results in a\ncomputational complexity that grows quadratically with sequence length, and\nthus exhibits a slow inference speed for long-text retrieval. Some recently\nproposed non-Transformer PLMs, especially the Mamba architecture PLMs, have\ndemonstrated not only comparable effectiveness to Transformer-based PLMs on\ngenerative language tasks but also better efficiency due to linear time scaling\nin sequence length. This paper implements the Mamba Retriever to explore\nwhether Mamba can serve as an effective and efficient encoder of DR model for\nIR tasks. We fine-tune the Mamba Retriever on the classic short-text MS MARCO\npassage ranking dataset and the long-text LoCoV0 dataset. Experimental results\nshow that (1) on the MS MARCO passage ranking dataset and BEIR, the Mamba\nRetriever achieves comparable or better effectiveness compared to\nTransformer-based retrieval models, and the effectiveness grows with the size\nof the Mamba model; (2) on the long-text LoCoV0 dataset, the Mamba Retriever\ncan extend to longer text length than its pre-trained length after fine-tuning\non retrieval task, and it has comparable or better effectiveness compared to\nother long-text retrieval models; (3) the Mamba Retriever has superior\ninference speed for long-text retrieval. In conclusion, Mamba Retriever is both\neffective and efficient, making it a practical model, especially for long-text\nretrieval."
                },
                "authors": [
                    {
                        "name": "Hanqi Zhang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Lang Mei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09170v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09170v4",
                "updated": "2024-08-15T10:12:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    12,
                    54,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-14T07:19:27Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    7,
                    19,
                    27,
                    6,
                    105,
                    0
                ],
                "title": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking"
                },
                "summary": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaoshu Chen"
                    },
                    {
                        "name": "Sihang Zhou"
                    },
                    {
                        "name": "Ke Liang"
                    },
                    {
                        "name": "Xinwang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinwang Liu"
                },
                "author": "Xinwang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09170v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09170v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20770v2",
                "updated": "2024-08-15T10:03:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    40,
                    3,
                    228,
                    0
                ],
                "published": "2024-05-24T07:23:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    7,
                    23,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Sentinel: LLM Agent for Adversarial Purification"
                },
                "summary": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness."
                },
                "authors": [
                    {
                        "name": "Guang Lin"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13764v3",
                "updated": "2024-08-15T10:03:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2023-12-21T11:43:41Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    11,
                    43,
                    41,
                    3,
                    355,
                    0
                ],
                "title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties"
                },
                "summary": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab."
                },
                "authors": [
                    {
                        "name": "Junfei Xiao"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Wenxuan Li"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "arxiv_comment": "Accepted to ECCV 2024. Code is available at\n  https://github.com/lambert-x/ProLab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08062v1",
                "updated": "2024-08-15T10:03:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    30,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T10:03:30Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    30,
                    3,
                    228,
                    0
                ],
                "title": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo"
                },
                "summary": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms."
                },
                "authors": [
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Rogers"
                },
                "author": "Timothy J. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08056v1",
                "updated": "2024-08-15T09:50:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    50,
                    11,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T09:50:11Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    50,
                    11,
                    3,
                    228,
                    0
                ],
                "title": "DATTA: Towards Diversity Adaptive Test-Time Adaptation in Dynamic Wild\n  World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DATTA: Towards Diversity Adaptive Test-Time Adaptation in Dynamic Wild\n  World"
                },
                "summary": "Test-time adaptation (TTA) effectively addresses distribution shifts between\ntraining and testing data by adjusting models on test samples, which is crucial\nfor improving model inference in real-world applications. However, traditional\nTTA methods typically follow a fixed pattern to address the dynamic data\npatterns (low-diversity or high-diversity patterns) often leading to\nperformance degradation and consequently a decline in Quality of Experience\n(QoE). The primary issues we observed are:Different scenarios require different\nnormalization methods (e.g., Instance Normalization is optimal in mixed domains\nbut not in static domains). Model fine-tuning can potentially harm the model\nand waste time.Hence, it is crucial to design strategies for effectively\nmeasuring and managing distribution diversity to minimize its negative impact\non model performance. Based on these observations, this paper proposes a new\ngeneral method, named Diversity Adaptive Test-Time Adaptation (DATTA), aimed at\nimproving QoE. DATTA dynamically selects the best batch normalization methods\nand fine-tuning strategies by leveraging the Diversity Score to differentiate\nbetween high and low diversity score batches. It features three key components:\nDiversity Discrimination (DD) to assess batch diversity, Diversity Adaptive\nBatch Normalization (DABN) to tailor normalization methods based on DD\ninsights, and Diversity Adaptive Fine-Tuning (DAFT) to selectively fine-tune\nthe model. Experimental results show that our method achieves up to a 21%\nincrease in accuracy compared to state-of-the-art methodologies, indicating\nthat our method maintains good model performance while demonstrating its\nrobustness. Our code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) effectively addresses distribution shifts between\ntraining and testing data by adjusting models on test samples, which is crucial\nfor improving model inference in real-world applications. However, traditional\nTTA methods typically follow a fixed pattern to address the dynamic data\npatterns (low-diversity or high-diversity patterns) often leading to\nperformance degradation and consequently a decline in Quality of Experience\n(QoE). The primary issues we observed are:Different scenarios require different\nnormalization methods (e.g., Instance Normalization is optimal in mixed domains\nbut not in static domains). Model fine-tuning can potentially harm the model\nand waste time.Hence, it is crucial to design strategies for effectively\nmeasuring and managing distribution diversity to minimize its negative impact\non model performance. Based on these observations, this paper proposes a new\ngeneral method, named Diversity Adaptive Test-Time Adaptation (DATTA), aimed at\nimproving QoE. DATTA dynamically selects the best batch normalization methods\nand fine-tuning strategies by leveraging the Diversity Score to differentiate\nbetween high and low diversity score batches. It features three key components:\nDiversity Discrimination (DD) to assess batch diversity, Diversity Adaptive\nBatch Normalization (DABN) to tailor normalization methods based on DD\ninsights, and Diversity Adaptive Fine-Tuning (DAFT) to selectively fine-tune\nthe model. Experimental results show that our method achieves up to a 21%\nincrease in accuracy compared to state-of-the-art methodologies, indicating\nthat our method maintains good model performance while demonstrating its\nrobustness. Our code will be released soon."
                },
                "authors": [
                    {
                        "name": "Chuyang Ye"
                    },
                    {
                        "name": "Dongyan Wei"
                    },
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanyi Pang"
                    },
                    {
                        "name": "Yixi Lin"
                    },
                    {
                        "name": "Jiarong Liao"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Xianghua Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Jingyan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyan Jiang"
                },
                "author": "Jingyan Jiang",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08054v1",
                "updated": "2024-08-15T09:48:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T09:48:45Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "title": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework"
                },
                "summary": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting."
                },
                "authors": [
                    {
                        "name": "Changyu Du"
                    },
                    {
                        "name": "Sebastian Esser"
                    },
                    {
                        "name": "Stavros Nousias"
                    },
                    {
                        "name": "André Borrmann"
                    }
                ],
                "author_detail": {
                    "name": "André Borrmann"
                },
                "author": "André Borrmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07600v3",
                "updated": "2024-08-15T09:34:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    34,
                    34,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-11T09:39:58Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    9,
                    39,
                    58,
                    3,
                    102,
                    0
                ],
                "title": "Implicit and Explicit Language Guidance for Diffusion-based Visual\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit and Explicit Language Guidance for Diffusion-based Visual\n  Perception"
                },
                "summary": "Text-to-image diffusion models have shown powerful ability on conditional\nimage synthesis. With large-scale vision-language pre-training, diffusion\nmodels are able to generate high-quality images with rich texture and\nreasonable structure under different text prompts. However, it is an open\nproblem to adapt the pre-trained diffusion model for visual perception. In this\npaper, we propose an implicit and explicit language guidance framework for\ndiffusion-based perception, named IEDP. Our IEDP comprises an implicit language\nguidance branch and an explicit language guidance branch. The implicit branch\nemploys frozen CLIP image encoder to directly generate implicit text embeddings\nthat are fed to diffusion model, without using explicit text prompts. The\nexplicit branch utilizes the ground-truth labels of corresponding images as\ntext prompts to condition feature extraction of diffusion model. During\ntraining, we jointly train diffusion model by sharing the model weights of\nthese two branches. As a result, implicit and explicit branches can jointly\nguide feature learning. During inference, we only employ implicit branch for\nfinal prediction, which does not require any ground-truth labels. Experiments\nare performed on two typical perception tasks, including semantic segmentation\nand depth estimation. Our IEDP achieves promising performance on both tasks.\nFor semantic segmentation, our IEDP has the mIoU$^\\text{ss}$ score of 55.9% on\nAD20K validation set, which outperforms the baseline method VPD by 2.2%. For\ndepth estimation, our IEDP outperforms the baseline method VPD with a relative\ngain of 11.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models have shown powerful ability on conditional\nimage synthesis. With large-scale vision-language pre-training, diffusion\nmodels are able to generate high-quality images with rich texture and\nreasonable structure under different text prompts. However, it is an open\nproblem to adapt the pre-trained diffusion model for visual perception. In this\npaper, we propose an implicit and explicit language guidance framework for\ndiffusion-based perception, named IEDP. Our IEDP comprises an implicit language\nguidance branch and an explicit language guidance branch. The implicit branch\nemploys frozen CLIP image encoder to directly generate implicit text embeddings\nthat are fed to diffusion model, without using explicit text prompts. The\nexplicit branch utilizes the ground-truth labels of corresponding images as\ntext prompts to condition feature extraction of diffusion model. During\ntraining, we jointly train diffusion model by sharing the model weights of\nthese two branches. As a result, implicit and explicit branches can jointly\nguide feature learning. During inference, we only employ implicit branch for\nfinal prediction, which does not require any ground-truth labels. Experiments\nare performed on two typical perception tasks, including semantic segmentation\nand depth estimation. Our IEDP achieves promising performance on both tasks.\nFor semantic segmentation, our IEDP has the mIoU$^\\text{ss}$ score of 55.9% on\nAD20K validation set, which outperforms the baseline method VPD by 2.2%. For\ndepth estimation, our IEDP outperforms the baseline method VPD with a relative\ngain of 11.0%."
                },
                "authors": [
                    {
                        "name": "Hefeng Wang"
                    },
                    {
                        "name": "Jiale Cao"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Aiping Yang"
                    },
                    {
                        "name": "Yanwei Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Pang"
                },
                "author": "Yanwei Pang",
                "arxiv_comment": "Accepted by IEEE TMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07537v2",
                "updated": "2024-08-15T09:30:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    30,
                    35,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T13:14:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    14,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report"
                },
                "summary": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions)."
                },
                "authors": [
                    {
                        "name": "Winnie Bahati Mbaka"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08047v1",
                "updated": "2024-08-15T09:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    26,
                    26,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T09:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    26,
                    26,
                    3,
                    228,
                    0
                ],
                "title": "An Efficient Continuous Control Perspective for\n  Reinforcement-Learning-based Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Continuous Control Perspective for\n  Reinforcement-Learning-based Sequential Recommendation"
                },
                "summary": "Sequential recommendation, where user preference is dynamically inferred from\nsequential historical behaviors, is a critical task in recommender systems\n(RSs). To further optimize long-term user engagement, offline\nreinforcement-learning-based RSs have become a mainstream technique as they\nprovide an additional advantage in avoiding global explorations that may harm\nonline users' experiences. However, previous studies mainly focus on discrete\naction and policy spaces, which might have difficulties in handling\ndramatically growing items efficiently.\n  To mitigate this issue, in this paper, we aim to design an algorithmic\nframework applicable to continuous policies. To facilitate the control in the\nlow-dimensional but dense user preference space, we propose an\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous\n\\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested\nassumption, we first propose the novel unified action representation abstracted\nfrom normalized user and item spaces. Then, we develop the corresponding policy\nevaluation and policy improvement procedures. During this process, strategic\nexploration and directional control in terms of unified actions are carefully\ndesigned and crucial to final recommendation decisions. Moreover, beneficial\nfrom unified actions, the conservatism regularization for policies and value\nfunctions are combined and perfectly compatible with the continuous framework.\nThe resulting dual regularization ensures the successful offline training of\nRL-based recommendation policies. Finally, we conduct extensive experiments to\nvalidate the effectiveness of our framework. The results show that compared to\nthe discrete baselines, our ECoC is trained far more efficiently. Meanwhile,\nthe final policies outperform baselines in both capturing the offline data and\ngaining long-term rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation, where user preference is dynamically inferred from\nsequential historical behaviors, is a critical task in recommender systems\n(RSs). To further optimize long-term user engagement, offline\nreinforcement-learning-based RSs have become a mainstream technique as they\nprovide an additional advantage in avoiding global explorations that may harm\nonline users' experiences. However, previous studies mainly focus on discrete\naction and policy spaces, which might have difficulties in handling\ndramatically growing items efficiently.\n  To mitigate this issue, in this paper, we aim to design an algorithmic\nframework applicable to continuous policies. To facilitate the control in the\nlow-dimensional but dense user preference space, we propose an\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous\n\\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested\nassumption, we first propose the novel unified action representation abstracted\nfrom normalized user and item spaces. Then, we develop the corresponding policy\nevaluation and policy improvement procedures. During this process, strategic\nexploration and directional control in terms of unified actions are carefully\ndesigned and crucial to final recommendation decisions. Moreover, beneficial\nfrom unified actions, the conservatism regularization for policies and value\nfunctions are combined and perfectly compatible with the continuous framework.\nThe resulting dual regularization ensures the successful offline training of\nRL-based recommendation policies. Finally, we conduct extensive experiments to\nvalidate the effectiveness of our framework. The results show that compared to\nthe discrete baselines, our ECoC is trained far more efficiently. Meanwhile,\nthe final policies outperform baselines in both capturing the offline data and\ngaining long-term rewards."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Likang Wu"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yang"
                },
                "author": "Yu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08027v1",
                "updated": "2024-08-15T08:50:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    50,
                    58,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T08:50:58Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    50,
                    58,
                    3,
                    228,
                    0
                ],
                "title": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words"
                },
                "summary": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words."
                },
                "authors": [
                    {
                        "name": "Kento Nozawa"
                    },
                    {
                        "name": "Takashi Masuko"
                    },
                    {
                        "name": "Toru Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toru Taniguchi"
                },
                "author": "Toru Taniguchi",
                "arxiv_comment": "13 pages, 1 figure, and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08021v1",
                "updated": "2024-08-15T08:37:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    37,
                    24,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T08:37:24Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    37,
                    24,
                    3,
                    228,
                    0
                ],
                "title": "DIVE: Towards Descriptive and Diverse Visual Commonsense Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIVE: Towards Descriptive and Diverse Visual Commonsense Generation"
                },
                "summary": "Towards human-level visual understanding, visual commonsense generation has\nbeen introduced to generate commonsense inferences beyond images. However,\ncurrent research on visual commonsense generation has overlooked an important\nhuman cognitive ability: generating descriptive and diverse inferences. In this\nwork, we propose a novel visual commonsense generation framework, called DIVE,\nwhich aims to improve the descriptiveness and diversity of generated\ninferences. DIVE involves two methods, generic inference filtering and\ncontrastive retrieval learning, which address the limitations of existing\nvisual commonsense resources and training objectives. Experimental results\nverify that DIVE outperforms state-of-the-art models for visual commonsense\ngeneration in terms of both descriptiveness and diversity, while showing a\nsuperior quality in generating unique and novel inferences. Notably, DIVE\nachieves human-level descriptiveness and diversity on Visual Commonsense\nGraphs. Furthermore, human evaluations confirm that DIVE aligns closely with\nhuman judgments on descriptiveness and diversity\\footnote{Our code and dataset\nare available at https://github.com/Park-ing-lot/DIVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards human-level visual understanding, visual commonsense generation has\nbeen introduced to generate commonsense inferences beyond images. However,\ncurrent research on visual commonsense generation has overlooked an important\nhuman cognitive ability: generating descriptive and diverse inferences. In this\nwork, we propose a novel visual commonsense generation framework, called DIVE,\nwhich aims to improve the descriptiveness and diversity of generated\ninferences. DIVE involves two methods, generic inference filtering and\ncontrastive retrieval learning, which address the limitations of existing\nvisual commonsense resources and training objectives. Experimental results\nverify that DIVE outperforms state-of-the-art models for visual commonsense\ngeneration in terms of both descriptiveness and diversity, while showing a\nsuperior quality in generating unique and novel inferences. Notably, DIVE\nachieves human-level descriptiveness and diversity on Visual Commonsense\nGraphs. Furthermore, human evaluations confirm that DIVE aligns closely with\nhuman judgments on descriptiveness and diversity\\footnote{Our code and dataset\nare available at https://github.com/Park-ing-lot/DIVE."
                },
                "authors": [
                    {
                        "name": "Jun-Hyung Park"
                    },
                    {
                        "name": "Hyuntae Park"
                    },
                    {
                        "name": "Youjin Kang"
                    },
                    {
                        "name": "Eojin Jeon"
                    },
                    {
                        "name": "SangKeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "SangKeun Lee"
                },
                "arxiv_affiliation": "Korea University",
                "author": "SangKeun Lee",
                "arxiv_doi": "10.18653/v1/2023.emnlp-main.601",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.601",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 10 figuers, EMNLP 2023 (main)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08019v1",
                "updated": "2024-08-15T08:34:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    34,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T08:34:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    34,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Accelerating High-Fidelity Waveform Generation via Adversarial Flow\n  Matching Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating High-Fidelity Waveform Generation via Adversarial Flow\n  Matching Optimization"
                },
                "summary": "This paper introduces PeriodWave-Turbo, a high-fidelity and high-efficient\nwaveform generation model via adversarial flow matching optimization. Recently,\nconditional flow matching (CFM) generative models have been successfully\nadopted for waveform generation tasks, leveraging a single vector field\nestimation objective for training. Although these models can generate\nhigh-fidelity waveform signals, they require significantly more ODE steps\ncompared to GAN-based models, which only need a single generation step.\nAdditionally, the generated samples often lack high-frequency information due\nto noisy vector field estimation, which fails to ensure high-frequency\nreproduction. To address this limitation, we enhance pre-trained CFM-based\ngenerative models by incorporating a fixed-step generator modification. We\nutilized reconstruction losses and adversarial feedback to accelerate\nhigh-fidelity waveform generation. Through adversarial flow matching\noptimization, it only requires 1,000 steps of fine-tuning to achieve\nstate-of-the-art performance across various objective metrics. Moreover, we\nsignificantly reduce inference speed from 16 steps to 2 or 4 steps.\nAdditionally, by scaling up the backbone of PeriodWave from 29M to 70M\nparameters for improved generalization, PeriodWave-Turbo achieves unprecedented\nperformance, with a perceptual evaluation of speech quality (PESQ) score of\n4.454 on the LibriTTS dataset. Audio samples, source code and checkpoints will\nbe available at https://github.com/sh-lee-prml/PeriodWave.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PeriodWave-Turbo, a high-fidelity and high-efficient\nwaveform generation model via adversarial flow matching optimization. Recently,\nconditional flow matching (CFM) generative models have been successfully\nadopted for waveform generation tasks, leveraging a single vector field\nestimation objective for training. Although these models can generate\nhigh-fidelity waveform signals, they require significantly more ODE steps\ncompared to GAN-based models, which only need a single generation step.\nAdditionally, the generated samples often lack high-frequency information due\nto noisy vector field estimation, which fails to ensure high-frequency\nreproduction. To address this limitation, we enhance pre-trained CFM-based\ngenerative models by incorporating a fixed-step generator modification. We\nutilized reconstruction losses and adversarial feedback to accelerate\nhigh-fidelity waveform generation. Through adversarial flow matching\noptimization, it only requires 1,000 steps of fine-tuning to achieve\nstate-of-the-art performance across various objective metrics. Moreover, we\nsignificantly reduce inference speed from 16 steps to 2 or 4 steps.\nAdditionally, by scaling up the backbone of PeriodWave from 29M to 70M\nparameters for improved generalization, PeriodWave-Turbo achieves unprecedented\nperformance, with a perceptual evaluation of speech quality (PESQ) score of\n4.454 on the LibriTTS dataset. Audio samples, source code and checkpoints will\nbe available at https://github.com/sh-lee-prml/PeriodWave."
                },
                "authors": [
                    {
                        "name": "Sang-Hoon Lee"
                    },
                    {
                        "name": "Ha-Yeong Choi"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "9 pages, 9 tables, 1 figure,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14938v2",
                "updated": "2024-08-15T08:20:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    20,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2023-11-25T05:55:39Z",
                "published_parsed": [
                    2023,
                    11,
                    25,
                    5,
                    55,
                    39,
                    5,
                    329,
                    0
                ],
                "title": "The Cosmic Dipole in the Quaia Sample of Quasars: A Bayesian Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cosmic Dipole in the Quaia Sample of Quasars: A Bayesian Analysis"
                },
                "summary": "We present a Bayesian analysis of the Quaia sample of 1.3 million quasars as\na test of the cosmological principle. This principle postulates that the\nuniverse is homogeneous and isotropic on sufficiently large scales, forming the\nbasis of prevailing cosmological models. However, recent analyses of quasar\nsamples have found a matter dipole inconsistent with the inferred kinematic\ndipole of the Cosmic Microwave Background (CMB), representing a tension with\nthe expectations of the cosmological principle. Here, we explore various\nhypotheses for the distribution of quasars in Quaia, finding that the sample is\ninfluenced by selection effects with significant contamination near the\ngalactic plane. After excising these regions, we find significant evidence that\nthe Quaia quasar dipole is consistent with the CMB dipole, both in terms of the\nexpected amplitude and direction. This result is in conflict with recent\nanalyses, lending support to the cosmological principle and the interpretation\nthat the observed dipole is due to our local departure from the Hubble flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Bayesian analysis of the Quaia sample of 1.3 million quasars as\na test of the cosmological principle. This principle postulates that the\nuniverse is homogeneous and isotropic on sufficiently large scales, forming the\nbasis of prevailing cosmological models. However, recent analyses of quasar\nsamples have found a matter dipole inconsistent with the inferred kinematic\ndipole of the Cosmic Microwave Background (CMB), representing a tension with\nthe expectations of the cosmological principle. Here, we explore various\nhypotheses for the distribution of quasars in Quaia, finding that the sample is\ninfluenced by selection effects with significant contamination near the\ngalactic plane. After excising these regions, we find significant evidence that\nthe Quaia quasar dipole is consistent with the CMB dipole, both in terms of the\nexpected amplitude and direction. This result is in conflict with recent\nanalyses, lending support to the cosmological principle and the interpretation\nthat the observed dipole is due to our local departure from the Hubble flow."
                },
                "authors": [
                    {
                        "name": "Vasudev Mittal"
                    },
                    {
                        "name": "Oliver T. Oayda"
                    },
                    {
                        "name": "Geraint F. Lewis"
                    }
                ],
                "author_detail": {
                    "name": "Geraint F. Lewis"
                },
                "author": "Geraint F. Lewis",
                "arxiv_doi": "10.1093/mnras/stad3706",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stad3706",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.14938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 7 figures, accepted for publication in MNRAS; updated to\n  include published correction (2 pages, 2 figures)",
                "arxiv_journal_ref": "MNRAS, Volume 527, Issue 3, January 2024, Pages 8497-8510;\n  Correction: MNRAS, Volume 530, Issue 4, June 2024, Pages 4763-4764",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08010v1",
                "updated": "2024-08-15T08:19:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    19,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T08:19:08Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    19,
                    8,
                    3,
                    228,
                    0
                ],
                "title": "Perturbing a quantum black hole",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbing a quantum black hole"
                },
                "summary": "We analyze the analytic structure of correlators in the field theory dual to\nthe quantum Ba\\~{n}ados-Teitelboim-Zanelli (qBTZ) black hole, a braneworld\nmodel incorporating exact backreaction from quantum conformal matter. We first\ncompute the quasi-normal mode (QNM) spectrum of operators with dimension\n$\\Delta$ and spin $s=0,\\pm 1/2$. The leading QNMs and their overtones display\nqualitatively different behavior depending on the branch of qBTZ solution;\nbranch 1 is a conical singularity dressed with a horizon while branch 2 is a\nquantum-corrected BTZ black hole. Hence the relaxation of probe matter\ndistinguishes the type of singularity cloaked by the horizon. We then turn to\npole-skipping locations where Green's functions are not unique. At these\npoints, frequency is proportional to temperature, but momentum exhibits complex\ntemperature dependence due to quantum effects. Under the assumption that the\npole-skipping point closest to the origin reflects quantum chaos, we infer the\nlikely behavior of the quantum Lyapunov exponent and butterfly velocity in the\ndual theory. Finally, we examine pole collisions in complex momentum space,\nshowing that quantum corrections imprint a unique signature on the analytic\nstructure of the poles in retarded Green's functions, resulting in\nlevel-crossing phenomena that differ notably from the level-touching phenomena\nin the uncorrected BTZ geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the analytic structure of correlators in the field theory dual to\nthe quantum Ba\\~{n}ados-Teitelboim-Zanelli (qBTZ) black hole, a braneworld\nmodel incorporating exact backreaction from quantum conformal matter. We first\ncompute the quasi-normal mode (QNM) spectrum of operators with dimension\n$\\Delta$ and spin $s=0,\\pm 1/2$. The leading QNMs and their overtones display\nqualitatively different behavior depending on the branch of qBTZ solution;\nbranch 1 is a conical singularity dressed with a horizon while branch 2 is a\nquantum-corrected BTZ black hole. Hence the relaxation of probe matter\ndistinguishes the type of singularity cloaked by the horizon. We then turn to\npole-skipping locations where Green's functions are not unique. At these\npoints, frequency is proportional to temperature, but momentum exhibits complex\ntemperature dependence due to quantum effects. Under the assumption that the\npole-skipping point closest to the origin reflects quantum chaos, we infer the\nlikely behavior of the quantum Lyapunov exponent and butterfly velocity in the\ndual theory. Finally, we examine pole collisions in complex momentum space,\nshowing that quantum corrections imprint a unique signature on the analytic\nstructure of the poles in retarded Green's functions, resulting in\nlevel-crossing phenomena that differ notably from the level-touching phenomena\nin the uncorrected BTZ geometry."
                },
                "authors": [
                    {
                        "name": "Casey Cartwright"
                    },
                    {
                        "name": "Umut Gürsoy"
                    },
                    {
                        "name": "Juan F. Pedraza"
                    },
                    {
                        "name": "Guim Planella Planas"
                    }
                ],
                "author_detail": {
                    "name": "Guim Planella Planas"
                },
                "author": "Guim Planella Planas",
                "arxiv_comment": "70 pages, 27 figures. comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08000v1",
                "updated": "2024-08-15T07:57:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    57,
                    28,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:57:28Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    57,
                    28,
                    3,
                    228,
                    0
                ],
                "title": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and\n  3D Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and\n  3D Editing"
                },
                "summary": "Novel View Synthesis (NVS) and 3D generation have recently achieved prominent\nimprovements. However, these works mainly focus on confined categories or\nsynthetic 3D assets, which are discouraged from generalizing to challenging\nin-the-wild scenes and fail to be employed with 2D synthesis directly.\nMoreover, these methods heavily depended on camera poses, limiting their\nreal-world applications. To overcome these issues, we propose MVInpainter,\nre-formulating the 3D editing as a multi-view 2D inpainting task. Specifically,\nMVInpainter partially inpaints multi-view images with the reference guidance\nrather than intractably generating an entirely novel view from scratch, which\nlargely simplifies the difficulty of in-the-wild NVS and leverages unmasked\nclues instead of explicit pose conditions. To ensure cross-view consistency,\nMVInpainter is enhanced by video priors from motion components and appearance\nguidance from concatenated reference key&value attention. Furthermore,\nMVInpainter incorporates slot attention to aggregate high-level optical flow\nfeatures from unmasked regions to control the camera movement with pose-free\ntraining and inference. Sufficient scene-level experiments on both\nobject-centric and forward-facing datasets verify the effectiveness of\nMVInpainter, including diverse tasks, such as multi-view object removal,\nsynthesis, insertion, and replacement. The project page is\nhttps://ewrfcas.github.io/MVInpainter/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel View Synthesis (NVS) and 3D generation have recently achieved prominent\nimprovements. However, these works mainly focus on confined categories or\nsynthetic 3D assets, which are discouraged from generalizing to challenging\nin-the-wild scenes and fail to be employed with 2D synthesis directly.\nMoreover, these methods heavily depended on camera poses, limiting their\nreal-world applications. To overcome these issues, we propose MVInpainter,\nre-formulating the 3D editing as a multi-view 2D inpainting task. Specifically,\nMVInpainter partially inpaints multi-view images with the reference guidance\nrather than intractably generating an entirely novel view from scratch, which\nlargely simplifies the difficulty of in-the-wild NVS and leverages unmasked\nclues instead of explicit pose conditions. To ensure cross-view consistency,\nMVInpainter is enhanced by video priors from motion components and appearance\nguidance from concatenated reference key&value attention. Furthermore,\nMVInpainter incorporates slot attention to aggregate high-level optical flow\nfeatures from unmasked regions to control the camera movement with pose-free\ntraining and inference. Sufficient scene-level experiments on both\nobject-centric and forward-facing datasets verify the effectiveness of\nMVInpainter, including diverse tasks, such as multi-view object removal,\nsynthesis, insertion, and replacement. The project page is\nhttps://ewrfcas.github.io/MVInpainter/."
                },
                "authors": [
                    {
                        "name": "Chenjie Cao"
                    },
                    {
                        "name": "Chaohui Yu"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Xiangyang Xue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Xue"
                },
                "author": "Xiangyang Xue",
                "arxiv_comment": "Project page: https://ewrfcas.github.io/MVInpainter/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05735v2",
                "updated": "2024-08-15T07:48:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2023-08-10T17:56:31Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    17,
                    56,
                    31,
                    3,
                    222,
                    0
                ],
                "title": "A high black hole to host mass ratio in a lensed AGN in the early\n  Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A high black hole to host mass ratio in a lensed AGN in the early\n  Universe"
                },
                "summary": "Early JWST observations have uncovered a new population of red sources that\nmight represent a previously overlooked phase of supermassive black hole growth\n(Kocevski et al. 2023; Matthee et al. 2023, Labb\\'e et al. 2023). One of the\nmost intriguing examples is an extremely red, point-like object that was found\nto be triply-imaged by the strong lensing (SL) cluster Abell 2744 (Furtak et\nal. 2023). Here we present deep JWST/NIRSpec observations of this object,\nAbell2744-QSO1. The spectroscopy confirms that the three images are of the same\nobject, and that it is a highly reddened ($A_V\\simeq3$) broad emission-line\nActive Galactic Nucleus (AGN) at a redshift of\n$z_{\\mathrm{spec}}=7.0451\\pm0.0005$. From the width of H$\\beta$\n($\\mathrm{FWHM}=2800\\pm250\\,\\frac{\\mathrm{km}}{\\mathrm{s}}$) we derive a black\nhole mass of $M_{\\mathrm{BH}}=4_{-1}^{+2}\\times10^7\\,\\mathrm{M}_{\\odot}$. We\ninfer a very high ratio of black hole to galaxy mass of at least 3%, an order\nof magnitude more than is seen in local galaxies (Bennert et al. 2011), and\npossibly as high as 100%. The lack of strong metal lines in the spectrum\ntogether with the high bolometric luminosity\n($L_{\\mathrm{bol}}=(1.1\\pm0.3)\\times10^{45}\\,\\frac{\\mathrm{erg}}{\\mathrm{s}}$)\nindicate that we are seeing the black hole in a phase of rapid growth,\naccreting at 30% of the Eddington limit. The rapid growth and high black hole\nto galaxy mass ratio of A2744-QSO1 suggest that it may represent the missing\nlink between black hole seeds (Volonteri et al. 2021) and the first luminous\nquasars (Fan et al. 2022).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early JWST observations have uncovered a new population of red sources that\nmight represent a previously overlooked phase of supermassive black hole growth\n(Kocevski et al. 2023; Matthee et al. 2023, Labb\\'e et al. 2023). One of the\nmost intriguing examples is an extremely red, point-like object that was found\nto be triply-imaged by the strong lensing (SL) cluster Abell 2744 (Furtak et\nal. 2023). Here we present deep JWST/NIRSpec observations of this object,\nAbell2744-QSO1. The spectroscopy confirms that the three images are of the same\nobject, and that it is a highly reddened ($A_V\\simeq3$) broad emission-line\nActive Galactic Nucleus (AGN) at a redshift of\n$z_{\\mathrm{spec}}=7.0451\\pm0.0005$. From the width of H$\\beta$\n($\\mathrm{FWHM}=2800\\pm250\\,\\frac{\\mathrm{km}}{\\mathrm{s}}$) we derive a black\nhole mass of $M_{\\mathrm{BH}}=4_{-1}^{+2}\\times10^7\\,\\mathrm{M}_{\\odot}$. We\ninfer a very high ratio of black hole to galaxy mass of at least 3%, an order\nof magnitude more than is seen in local galaxies (Bennert et al. 2011), and\npossibly as high as 100%. The lack of strong metal lines in the spectrum\ntogether with the high bolometric luminosity\n($L_{\\mathrm{bol}}=(1.1\\pm0.3)\\times10^{45}\\,\\frac{\\mathrm{erg}}{\\mathrm{s}}$)\nindicate that we are seeing the black hole in a phase of rapid growth,\naccreting at 30% of the Eddington limit. The rapid growth and high black hole\nto galaxy mass ratio of A2744-QSO1 suggest that it may represent the missing\nlink between black hole seeds (Volonteri et al. 2021) and the first luminous\nquasars (Fan et al. 2022)."
                },
                "authors": [
                    {
                        "name": "Lukas J. Furtak"
                    },
                    {
                        "name": "Ivo Labbé"
                    },
                    {
                        "name": "Adi Zitrin"
                    },
                    {
                        "name": "Jenny E. Greene"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Iryna Chemerynska"
                    },
                    {
                        "name": "Vasily Kokorev"
                    },
                    {
                        "name": "Tim B. Miller"
                    },
                    {
                        "name": "Andy D. Goulding"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Rachel Bezanson"
                    },
                    {
                        "name": "Gabriel B. Brammer"
                    },
                    {
                        "name": "Sam E. Cutler"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Richard Pan"
                    },
                    {
                        "name": "Sedona H. Price"
                    },
                    {
                        "name": "Bingjie Wang"
                    },
                    {
                        "name": "John R. Weaver"
                    },
                    {
                        "name": "Katherine E. Whitaker"
                    },
                    {
                        "name": "Hakim Atek"
                    },
                    {
                        "name": "Ákos Bogdán"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Pieter van Dokkum"
                    },
                    {
                        "name": "Ryan Endsley"
                    },
                    {
                        "name": "Yoshinobu Fudamoto"
                    },
                    {
                        "name": "Seiji Fujimoto"
                    },
                    {
                        "name": "Karl Glazebrook"
                    },
                    {
                        "name": "Stéphanie Juneau"
                    },
                    {
                        "name": "Danilo Marchesini"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Erica Nelson"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Adèle Plat"
                    },
                    {
                        "name": "David J. Setton"
                    },
                    {
                        "name": "Daniel P. Stark"
                    },
                    {
                        "name": "Christina C. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Christina C. Williams"
                },
                "author": "Christina C. Williams",
                "arxiv_comment": "Published in Nature. Updated to the accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07998v1",
                "updated": "2024-08-15T07:47:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    47,
                    27,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:47:27Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    47,
                    27,
                    3,
                    228,
                    0
                ],
                "title": "Machine learning from limited data: Predicting biological dynamics under\n  a time-varying external input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning from limited data: Predicting biological dynamics under\n  a time-varying external input"
                },
                "summary": "Reservoir computing (RC) is known as a powerful machine learning approach for\nlearning complex dynamics from limited data. Here, we use RC to predict highly\nstochastic dynamics of cell shapes. We find that RC is able to predict the\nsteady state climate from very limited data. Furthermore, the RC learns the\ntimescale of transients from only four observations. We find that these\ncapabilities of the RC to act as a dynamic twin allows us to also infer\nimportant statistics of cell shape dynamics of unobserved conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir computing (RC) is known as a powerful machine learning approach for\nlearning complex dynamics from limited data. Here, we use RC to predict highly\nstochastic dynamics of cell shapes. We find that RC is able to predict the\nsteady state climate from very limited data. Furthermore, the RC learns the\ntimescale of transients from only four observations. We find that these\ncapabilities of the RC to act as a dynamic twin allows us to also infer\nimportant statistics of cell shape dynamics of unobserved conditions."
                },
                "authors": [
                    {
                        "name": "Hoony Kang"
                    },
                    {
                        "name": "Keshav Srinivasan"
                    },
                    {
                        "name": "Wolfgang Losert"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Losert"
                },
                "author": "Wolfgang Losert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07990v1",
                "updated": "2024-08-15T07:37:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    37,
                    24,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:37:24Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    37,
                    24,
                    3,
                    228,
                    0
                ],
                "title": "FuseChat: Knowledge Fusion of Chat Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseChat: Knowledge Fusion of Chat Models"
                },
                "summary": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}."
                },
                "authors": [
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07989v1",
                "updated": "2024-08-15T07:30:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    30,
                    47,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:30:47Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    30,
                    47,
                    3,
                    228,
                    0
                ],
                "title": "IIU: Independent Inference Units for Knowledge-based Visual Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IIU: Independent Inference Units for Knowledge-based Visual Question\n  Answering"
                },
                "summary": "Knowledge-based visual question answering requires external knowledge beyond\nvisible content to answer the question correctly. One limitation of existing\nmethods is that they focus more on modeling the inter-modal and intra-modal\ncorrelations, which entangles complex multimodal clues by implicit embeddings\nand lacks interpretability and generalization ability. The key challenge to\nsolve the above problem is to separate the information and process it\nseparately at the functional level. By reusing each processing unit, the\ngeneralization ability of the model to deal with different data can be\nincreased. In this paper, we propose Independent Inference Units (IIU) for\nfine-grained multi-modal reasoning to decompose intra-modal information by the\nfunctionally independent units. Specifically, IIU processes each\nsemantic-specific intra-modal clue by an independent inference unit, which also\ncollects complementary information by communication from different units. To\nfurther reduce the impact of redundant information, we propose a memory update\nmodule to maintain semantic-relevant memory along with the reasoning process\ngradually. In comparison with existing non-pretrained multi-modal reasoning\nmodels on standard datasets, our model achieves a new state-of-the-art,\nenhancing performance by 3%, and surpassing basic pretrained multi-modal\nmodels. The experimental results show that our IIU model is effective in\ndisentangling intra-modal clues as well as reasoning units to provide\nexplainable reasoning evidence. Our code is available at\nhttps://github.com/Lilidamowang/IIU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based visual question answering requires external knowledge beyond\nvisible content to answer the question correctly. One limitation of existing\nmethods is that they focus more on modeling the inter-modal and intra-modal\ncorrelations, which entangles complex multimodal clues by implicit embeddings\nand lacks interpretability and generalization ability. The key challenge to\nsolve the above problem is to separate the information and process it\nseparately at the functional level. By reusing each processing unit, the\ngeneralization ability of the model to deal with different data can be\nincreased. In this paper, we propose Independent Inference Units (IIU) for\nfine-grained multi-modal reasoning to decompose intra-modal information by the\nfunctionally independent units. Specifically, IIU processes each\nsemantic-specific intra-modal clue by an independent inference unit, which also\ncollects complementary information by communication from different units. To\nfurther reduce the impact of redundant information, we propose a memory update\nmodule to maintain semantic-relevant memory along with the reasoning process\ngradually. In comparison with existing non-pretrained multi-modal reasoning\nmodels on standard datasets, our model achieves a new state-of-the-art,\nenhancing performance by 3%, and surpassing basic pretrained multi-modal\nmodels. The experimental results show that our IIU model is effective in\ndisentangling intra-modal clues as well as reasoning units to provide\nexplainable reasoning evidence. Our code is available at\nhttps://github.com/Lilidamowang/IIU."
                },
                "authors": [
                    {
                        "name": "Yili Li"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Keke Gai"
                    },
                    {
                        "name": "Gang Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Gang Xiong"
                },
                "author": "Gang Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12017v2",
                "updated": "2024-08-15T07:12:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    12,
                    33,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-27T07:16:46Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    16,
                    46,
                    3,
                    179,
                    0
                ],
                "title": "Follow-Up Questions Improve Documents Generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Up Questions Improve Documents Generated by Large Language Models"
                },
                "summary": "This study investigates the impact of Large Language Models (LLMs) generating\nfollow-up questions in response to user requests for short (1-page) text\ndocuments. Users interacted with a novel web-based AI system designed to ask\nfollow-up questions. Users requested documents they would like the AI to\nproduce. The AI then generated follow-up questions to clarify the user's needs\nor offer additional insights before generating the requested documents. After\nanswering the questions, users were shown a document generated using both the\ninitial request and the questions and answers, and a document generated using\nonly the initial request. Users indicated which document they preferred and\ngave feedback about their experience with the question-answering process. The\nfindings of this study show clear benefits to question-asking both in document\npreference and in the qualitative user experience. This study further shows\nthat users found more value in questions which were thought-provoking,\nopen-ended, or offered unique insights into the user's request as opposed to\nsimple information-gathering questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of Large Language Models (LLMs) generating\nfollow-up questions in response to user requests for short (1-page) text\ndocuments. Users interacted with a novel web-based AI system designed to ask\nfollow-up questions. Users requested documents they would like the AI to\nproduce. The AI then generated follow-up questions to clarify the user's needs\nor offer additional insights before generating the requested documents. After\nanswering the questions, users were shown a document generated using both the\ninitial request and the questions and answers, and a document generated using\nonly the initial request. Users indicated which document they preferred and\ngave feedback about their experience with the question-answering process. The\nfindings of this study show clear benefits to question-asking both in document\npreference and in the qualitative user experience. This study further shows\nthat users found more value in questions which were thought-provoking,\nopen-ended, or offered unique insights into the user's request as opposed to\nsimple information-gathering questions."
                },
                "authors": [
                    {
                        "name": "Bernadette J Tix"
                    }
                ],
                "author_detail": {
                    "name": "Bernadette J Tix"
                },
                "author": "Bernadette J Tix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07983v1",
                "updated": "2024-08-15T07:09:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    9,
                    51,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:09:51Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    9,
                    51,
                    3,
                    228,
                    0
                ],
                "title": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal\n  Knowledge in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal\n  Knowledge in Large Language Models"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have led to\nsignificant improvements in various natural language processing tasks. However,\nthe evaluation of LLMs' legal knowledge, particularly in non-English languages\nsuch as Arabic, remains under-explored. To address this gap, we introduce\nArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal\nknowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval\nconsists of multiple tasks sourced from Saudi legal documents and synthesized\nquestions. In this work, we aim to analyze the capabilities required to solve\nlegal problems in Arabic and benchmark the performance of state-of-the-art\nLLMs. We explore the impact of in-context learning and investigate various\nevaluation methods. Additionally, we explore workflows for generating questions\nwith automatic validation to enhance the dataset's quality. We benchmark\nmultilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We\nalso share our methodology for creating the dataset and validation, which can\nbe generalized to other domains. We hope to accelerate AI research in the\nArabic Legal domain by releasing the ArabLegalEval dataset and code:\nhttps://github.com/Thiqah/ArabLegalEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have led to\nsignificant improvements in various natural language processing tasks. However,\nthe evaluation of LLMs' legal knowledge, particularly in non-English languages\nsuch as Arabic, remains under-explored. To address this gap, we introduce\nArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal\nknowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval\nconsists of multiple tasks sourced from Saudi legal documents and synthesized\nquestions. In this work, we aim to analyze the capabilities required to solve\nlegal problems in Arabic and benchmark the performance of state-of-the-art\nLLMs. We explore the impact of in-context learning and investigate various\nevaluation methods. Additionally, we explore workflows for generating questions\nwith automatic validation to enhance the dataset's quality. We benchmark\nmultilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We\nalso share our methodology for creating the dataset and validation, which can\nbe generalized to other domains. We hope to accelerate AI research in the\nArabic Legal domain by releasing the ArabLegalEval dataset and code:\nhttps://github.com/Thiqah/ArabLegalEval"
                },
                "authors": [
                    {
                        "name": "Faris Hijazi"
                    },
                    {
                        "name": "Somayah AlHarbi"
                    },
                    {
                        "name": "Abdulaziz AlHussein"
                    },
                    {
                        "name": "Harethah Abu Shairah"
                    },
                    {
                        "name": "Reem AlZahrani"
                    },
                    {
                        "name": "Hebah AlShamlan"
                    },
                    {
                        "name": "Omar Knio"
                    },
                    {
                        "name": "George Turkiyyah"
                    }
                ],
                "author_detail": {
                    "name": "George Turkiyyah"
                },
                "arxiv_affiliation": "KAUST",
                "author": "George Turkiyyah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07982v1",
                "updated": "2024-08-15T07:03:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    3,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:03:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    3,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera"
                },
                "summary": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry."
                },
                "authors": [
                    {
                        "name": "Hiroki Tanioka"
                    },
                    {
                        "name": "Tetsushi Ueta"
                    },
                    {
                        "name": "Masahiko Sano"
                    }
                ],
                "author_detail": {
                    "name": "Masahiko Sano"
                },
                "author": "Masahiko Sano",
                "arxiv_comment": "4 pages, 5 figures, 1 table, The 1st InterAI: Interactive AI for\n  Human-Centered Robotics workshop in conjuction with IEEE Ro-MAN 2024,\n  Pasadona, LA, USA, Aug. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07981v1",
                "updated": "2024-08-15T07:00:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    0,
                    20,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:00:20Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    0,
                    20,
                    3,
                    228,
                    0
                ],
                "title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured\n  Surgical Video Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured\n  Surgical Video Learning"
                },
                "summary": "Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Garrett Skinner"
                    },
                    {
                        "name": "Gene Yang"
                    },
                    {
                        "name": "Brian R Quaranto"
                    },
                    {
                        "name": "Steven D Schwaitzberg"
                    },
                    {
                        "name": "Peter C W Kim"
                    },
                    {
                        "name": "Jinjun Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Jinjun Xiong"
                },
                "author": "Jinjun Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03337v3",
                "updated": "2024-08-15T06:58:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    58,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T07:19:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    19,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements"
                },
                "summary": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI"
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07977v1",
                "updated": "2024-08-15T06:43:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    43,
                    52,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T06:43:52Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    43,
                    52,
                    3,
                    228,
                    0
                ],
                "title": "Cortical network reconfiguration aligns with shifts of basal ganglia and\n  cerebellar influence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortical network reconfiguration aligns with shifts of basal ganglia and\n  cerebellar influence"
                },
                "summary": "Mammalian functional architecture flexibly adapts, transitioning from\nintegration where information is distributed across the cortex, to segregation\nwhere information is focal in densely connected communities of brain regions.\nThis flexibility in cortical brain networks is hypothesized to be driven by\ncontrol signals originating from subcortical pathways, with the basal ganglia\nshifting the cortex towards integrated processing states and the cerebellum\ntowards segregated states. In a sample of healthy human participants (N=242),\nwe used fMRI to measure temporal variation in global brain networks while\nparticipants performed two tasks with similar cognitive demands (Stroop and\nMulti-Source Inference Task (MSIT)). Using the modularity index, we determined\ncortical networks shifted from integration (low modularity) at rest to high\nmodularity during easier i.e. congruent (segregation). Increased task\ndifficulty (incongruent) resulted in lower modularity in comparison to the\neasier counterpart indicating more integration of the cortical network.\nInfluence of basal ganglia and cerebellum was measured using eigenvector\ncentrality. Results correlated with decreases and increases in cortical\nmodularity respectively, with only the basal ganglia influence preceding\ncortical integration. Our results support the theory the basal ganglia shifts\ncortical networks to integrated states due to environmental demand. Cerebellar\ninfluence correlates with shifts to segregated cortical states, though may not\nplay a causal role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammalian functional architecture flexibly adapts, transitioning from\nintegration where information is distributed across the cortex, to segregation\nwhere information is focal in densely connected communities of brain regions.\nThis flexibility in cortical brain networks is hypothesized to be driven by\ncontrol signals originating from subcortical pathways, with the basal ganglia\nshifting the cortex towards integrated processing states and the cerebellum\ntowards segregated states. In a sample of healthy human participants (N=242),\nwe used fMRI to measure temporal variation in global brain networks while\nparticipants performed two tasks with similar cognitive demands (Stroop and\nMulti-Source Inference Task (MSIT)). Using the modularity index, we determined\ncortical networks shifted from integration (low modularity) at rest to high\nmodularity during easier i.e. congruent (segregation). Increased task\ndifficulty (incongruent) resulted in lower modularity in comparison to the\neasier counterpart indicating more integration of the cortical network.\nInfluence of basal ganglia and cerebellum was measured using eigenvector\ncentrality. Results correlated with decreases and increases in cortical\nmodularity respectively, with only the basal ganglia influence preceding\ncortical integration. Our results support the theory the basal ganglia shifts\ncortical networks to integrated states due to environmental demand. Cerebellar\ninfluence correlates with shifts to segregated cortical states, though may not\nplay a causal role."
                },
                "authors": [
                    {
                        "name": "Kimberly Nestor"
                    },
                    {
                        "name": "Javier Rasero"
                    },
                    {
                        "name": "Richard Betzel"
                    },
                    {
                        "name": "Peter J. Gianaros"
                    },
                    {
                        "name": "Timothy Verstynen"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Verstynen"
                },
                "author": "Timothy Verstynen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07975v1",
                "updated": "2024-08-15T06:40:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    40,
                    38,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T06:40:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    40,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual\n  Grounding and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual\n  Grounding and Large Language Models"
                },
                "summary": "This paper investigates the task of the open-ended interactive robotic\nmanipulation on table-top scenarios. While recent Large Language Models (LLMs)\nenhance robots' comprehension of user instructions, their lack of visual\ngrounding constrains their ability to physically interact with the environment.\nThis is because the robot needs to locate the target object for manipulation\nwithin the physical workspace. To this end, we introduce an interactive robotic\nmanipulation framework called Polaris, which integrates perception and\ninteraction by utilizing GPT-4 alongside grounded vision models. For precise\nmanipulation, it is essential that such grounded vision models produce detailed\nobject pose for the target object, rather than merely identifying pixels\nbelonging to them in the image. Consequently, we propose a novel\nSynthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes\nrendered synthetic data for training and is then transferred to real-world\nmanipulation tasks. The real-world performance demonstrates the efficacy of our\nproposed pipeline and underscores its potential for extension to more general\ncategories. Moreover, real-robot experiments have showcased the impressive\nperformance of our framework in grasping and executing multiple manipulation\ntasks. This indicates its potential to generalize to scenarios beyond the\ntabletop. More information and video results are available here:\nhttps://star-uu-wang.github.io/Polaris/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the task of the open-ended interactive robotic\nmanipulation on table-top scenarios. While recent Large Language Models (LLMs)\nenhance robots' comprehension of user instructions, their lack of visual\ngrounding constrains their ability to physically interact with the environment.\nThis is because the robot needs to locate the target object for manipulation\nwithin the physical workspace. To this end, we introduce an interactive robotic\nmanipulation framework called Polaris, which integrates perception and\ninteraction by utilizing GPT-4 alongside grounded vision models. For precise\nmanipulation, it is essential that such grounded vision models produce detailed\nobject pose for the target object, rather than merely identifying pixels\nbelonging to them in the image. Consequently, we propose a novel\nSynthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes\nrendered synthetic data for training and is then transferred to real-world\nmanipulation tasks. The real-world performance demonstrates the efficacy of our\nproposed pipeline and underscores its potential for extension to more general\ncategories. Moreover, real-robot experiments have showcased the impressive\nperformance of our framework in grasping and executing multiple manipulation\ntasks. This indicates its potential to generalize to scenarios beyond the\ntabletop. More information and video results are available here:\nhttps://star-uu-wang.github.io/Polaris/"
                },
                "authors": [
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Junqiu Yu"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "arxiv_comment": "Accepted by IROS 2024. 8 pages, 5 figures. See\n  https://star-uu-wang.github.io/Polaris/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07971v1",
                "updated": "2024-08-15T06:36:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    36,
                    27,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T06:36:27Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    36,
                    27,
                    3,
                    228,
                    0
                ],
                "title": "Predicting Lung Cancer Patient Prognosis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Lung Cancer Patient Prognosis with Large Language Models"
                },
                "summary": "Prognosis prediction is crucial for determining optimal treatment plans for\nlung cancer patients. Traditionally, such predictions relied on models\ndeveloped from retrospective patient data. Recently, large language models\n(LLMs) have gained attention for their ability to process and generate text\nbased on extensive learned knowledge. In this study, we evaluate the potential\nof GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.\nWe collected two prognosis datasets, i.e., survival and post-operative\ncomplication datasets, and designed multiple tasks to assess the models'\nperformance comprehensively. Logistic regression models were also developed as\nbaselines for comparison. The experimental results demonstrate that LLMs can\nachieve competitive, and in some tasks superior, performance in lung cancer\nprognosis prediction compared to data-driven logistic regression models despite\nnot using additional patient data. These findings suggest that LLMs can be\neffective tools for prognosis prediction in lung cancer, particularly when\npatient data is limited or unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prognosis prediction is crucial for determining optimal treatment plans for\nlung cancer patients. Traditionally, such predictions relied on models\ndeveloped from retrospective patient data. Recently, large language models\n(LLMs) have gained attention for their ability to process and generate text\nbased on extensive learned knowledge. In this study, we evaluate the potential\nof GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.\nWe collected two prognosis datasets, i.e., survival and post-operative\ncomplication datasets, and designed multiple tasks to assess the models'\nperformance comprehensively. Logistic regression models were also developed as\nbaselines for comparison. The experimental results demonstrate that LLMs can\nachieve competitive, and in some tasks superior, performance in lung cancer\nprognosis prediction compared to data-driven logistic regression models despite\nnot using additional patient data. These findings suggest that LLMs can be\neffective tools for prognosis prediction in lung cancer, particularly when\npatient data is limited or unavailable."
                },
                "authors": [
                    {
                        "name": "Danqing Hu"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Nan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wu"
                },
                "author": "Nan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07944v1",
                "updated": "2024-08-15T05:35:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    35,
                    52,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T05:35:52Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    35,
                    52,
                    3,
                    228,
                    0
                ],
                "title": "Training Spatial-Frequency Visual Prompts and Probabilistic Clusters for\n  Accurate Black-Box Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Spatial-Frequency Visual Prompts and Probabilistic Clusters for\n  Accurate Black-Box Transfer Learning"
                },
                "summary": "Despite the growing prevalence of black-box pre-trained models (PTMs) such as\nprediction API services, there remains a significant challenge in directly\napplying general models to real-world scenarios due to the data distribution\ngap. Considering a data deficiency and constrained computational resource\nscenario, this paper proposes a novel parameter-efficient transfer learning\nframework for vision recognition models in the black-box setting. Our framework\nincorporates two novel training techniques. First, we align the input space\n(i.e., image) of PTMs to the target data distribution by generating visual\nprompts of spatial and frequency domain. Along with the novel spatial-frequency\nhybrid visual prompter, we design a novel training technique based on\nprobabilistic clusters, which can enhance class separation in the output space\n(i.e., prediction probabilities). In experiments, our model demonstrates\nsuperior performance in a few-shot transfer learning setting across extensive\nvisual recognition datasets, surpassing state-of-the-art baselines.\nAdditionally, we show that the proposed method efficiently reduces\ncomputational costs for training and inference phases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing prevalence of black-box pre-trained models (PTMs) such as\nprediction API services, there remains a significant challenge in directly\napplying general models to real-world scenarios due to the data distribution\ngap. Considering a data deficiency and constrained computational resource\nscenario, this paper proposes a novel parameter-efficient transfer learning\nframework for vision recognition models in the black-box setting. Our framework\nincorporates two novel training techniques. First, we align the input space\n(i.e., image) of PTMs to the target data distribution by generating visual\nprompts of spatial and frequency domain. Along with the novel spatial-frequency\nhybrid visual prompter, we design a novel training technique based on\nprobabilistic clusters, which can enhance class separation in the output space\n(i.e., prediction probabilities). In experiments, our model demonstrates\nsuperior performance in a few-shot transfer learning setting across extensive\nvisual recognition datasets, surpassing state-of-the-art baselines.\nAdditionally, we show that the proposed method efficiently reduces\ncomputational costs for training and inference phases."
                },
                "authors": [
                    {
                        "name": "Wonwoo Cho"
                    },
                    {
                        "name": "Kangyeol Kim"
                    },
                    {
                        "name": "Saemee Choi"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "ACM Multimedia 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04024v2",
                "updated": "2024-08-15T04:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    4,
                    53,
                    51,
                    3,
                    228,
                    0
                ],
                "published": "2024-03-06T20:10:41Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    20,
                    10,
                    41,
                    2,
                    66,
                    0
                ],
                "title": "Enhancing chest X-ray datasets with privacy-preserving large language\n  models and multi-type annotations: a data-driven approach for improved\n  classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing chest X-ray datasets with privacy-preserving large language\n  models and multi-type annotations: a data-driven approach for improved\n  classification"
                },
                "summary": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports for dataset releases. However, there is still\nroom for improvement in label quality. These labelers typically output only\npresence labels, sometimes with binary uncertainty indicators, which limits\ntheir usefulness. Supervised deep learning models have also been developed for\nreport labeling but lack adaptability, similar to rule-based systems. In this\nwork, we present MAPLEZ (Medical report Annotations with Privacy-preserving\nLarge language model using Expeditious Zero shot answers), a novel approach\nleveraging a locally executable Large Language Model (LLM) to extract and\nenhance findings labels on CXR reports. MAPLEZ extracts not only binary labels\nindicating the presence or absence of a finding but also the location,\nseverity, and radiologists' uncertainty about the finding. Over eight\nabnormalities from five test sets, we show that our method can extract these\nannotations with an increase of 3.6 percentage points (pp) in macro F1 score\nfor categorical presence annotations and more than 20 pp increase in F1 score\nfor the location annotations over competing labelers. Additionally, using the\ncombination of improved annotations and multi-type annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.1 pp in AUROC over models trained with\nannotations from the best alternative approach. We share code and annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports for dataset releases. However, there is still\nroom for improvement in label quality. These labelers typically output only\npresence labels, sometimes with binary uncertainty indicators, which limits\ntheir usefulness. Supervised deep learning models have also been developed for\nreport labeling but lack adaptability, similar to rule-based systems. In this\nwork, we present MAPLEZ (Medical report Annotations with Privacy-preserving\nLarge language model using Expeditious Zero shot answers), a novel approach\nleveraging a locally executable Large Language Model (LLM) to extract and\nenhance findings labels on CXR reports. MAPLEZ extracts not only binary labels\nindicating the presence or absence of a finding but also the location,\nseverity, and radiologists' uncertainty about the finding. Over eight\nabnormalities from five test sets, we show that our method can extract these\nannotations with an increase of 3.6 percentage points (pp) in macro F1 score\nfor categorical presence annotations and more than 20 pp increase in F1 score\nfor the location annotations over competing labelers. Additionally, using the\ncombination of improved annotations and multi-type annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.1 pp in AUROC over models trained with\nannotations from the best alternative approach. We share code and annotations."
                },
                "authors": [
                    {
                        "name": "Ricardo Bigolin Lanfredi"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Ronald Summers"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Summers"
                },
                "author": "Ronald Summers",
                "arxiv_comment": "Code and data:\n  https://github.com/rsummers11/CADLab/tree/master/MAPLEZ_LLM_report_labeler/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18276v2",
                "updated": "2024-08-15T04:43:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    4,
                    43,
                    5,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-23T21:18:31Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    21,
                    18,
                    31,
                    1,
                    205,
                    0
                ],
                "title": "Rome was Not Built in a Single Step: Hierarchical Prompting for\n  LLM-based Chip Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rome was Not Built in a Single Step: Hierarchical Prompting for\n  LLM-based Chip Design"
                },
                "summary": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM"
                },
                "authors": [
                    {
                        "name": "Andre Nakkab"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Siddharth Garg"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Garg"
                },
                "author": "Siddharth Garg",
                "arxiv_comment": "Accepted at MLCAD '24. 10 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03388v2",
                "updated": "2024-08-15T04:24:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    4,
                    24,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-06T18:18:37Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    18,
                    18,
                    37,
                    1,
                    219,
                    0
                ],
                "title": "A Non-negative VAE:the Generalized Gamma Belief Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Non-negative VAE:the Generalized Gamma Belief Network"
                },
                "summary": "The gamma belief network (GBN), often regarded as a deep topic model, has\ndemonstrated its potential for uncovering multi-layer interpretable latent\nrepresentations in text data. Its notable capability to acquire interpretable\nlatent factors is partially attributed to sparse and non-negative\ngamma-distributed latent variables. However, the existing GBN and its\nvariations are constrained by the linear generative model, thereby limiting\ntheir expressiveness and applicability. To address this limitation, we\nintroduce the generalized gamma belief network (Generalized GBN) in this paper,\nwhich extends the original linear generative model to a more expressive\nnon-linear generative model. Since the parameters of the Generalized GBN no\nlonger possess an analytic conditional posterior, we further propose an\nupward-downward Weibull inference network to approximate the posterior\ndistribution of the latent variables. The parameters of both the generative\nmodel and the inference network are jointly trained within the variational\ninference framework. Finally, we conduct comprehensive experiments on both\nexpressivity and disentangled representation learning tasks to evaluate the\nperformance of the Generalized GBN against state-of-the-art Gaussian\nvariational autoencoders serving as baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gamma belief network (GBN), often regarded as a deep topic model, has\ndemonstrated its potential for uncovering multi-layer interpretable latent\nrepresentations in text data. Its notable capability to acquire interpretable\nlatent factors is partially attributed to sparse and non-negative\ngamma-distributed latent variables. However, the existing GBN and its\nvariations are constrained by the linear generative model, thereby limiting\ntheir expressiveness and applicability. To address this limitation, we\nintroduce the generalized gamma belief network (Generalized GBN) in this paper,\nwhich extends the original linear generative model to a more expressive\nnon-linear generative model. Since the parameters of the Generalized GBN no\nlonger possess an analytic conditional posterior, we further propose an\nupward-downward Weibull inference network to approximate the posterior\ndistribution of the latent variables. The parameters of both the generative\nmodel and the inference network are jointly trained within the variational\ninference framework. Finally, we conduct comprehensive experiments on both\nexpressivity and disentangled representation learning tasks to evaluate the\nperformance of the Generalized GBN against state-of-the-art Gaussian\nvariational autoencoders serving as baselines."
                },
                "authors": [
                    {
                        "name": "Zhibin Duan"
                    },
                    {
                        "name": "Tiansheng Wen"
                    },
                    {
                        "name": "Muyao Wang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingyuan Zhou"
                },
                "author": "Mingyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07907v1",
                "updated": "2024-08-15T03:25:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    25,
                    56,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T03:25:56Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    25,
                    56,
                    3,
                    228,
                    0
                ],
                "title": "AIE: Auction Information Enhanced Framework for CTR Prediction in Online\n  Advertising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIE: Auction Information Enhanced Framework for CTR Prediction in Online\n  Advertising"
                },
                "summary": "Click-Through Rate (CTR) prediction is a fundamental technique for online\nadvertising recommendation and the complex online competitive auction process\nalso brings many difficulties to CTR optimization. Recent studies have shown\nthat introducing posterior auction information contributes to the performance\nof CTR prediction. However, existing work doesn't fully capitalize on the\nbenefits of auction information and overlooks the data bias brought by the\nauction, leading to biased and suboptimal results. To address these\nlimitations, we propose Auction Information Enhanced Framework (AIE) for CTR\nprediction in online advertising, which delves into the problem of insufficient\nutilization of auction signals and first reveals the auction bias.\nSpecifically, AIE introduces two pluggable modules, namely Adaptive\nMarket-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which\nwork collaboratively to excavate the posterior auction signals better and\nenhance the performance of CTR prediction. Furthermore, the two proposed\nmodules are lightweight, model-agnostic, and friendly to inference latency.\nExtensive experiments are conducted on a public dataset and an industrial\ndataset to demonstrate the effectiveness and compatibility of AIE. Besides, a\none-month online A/B test in a large-scale advertising platform shows that AIE\nimproves the base model by 5.76% and 2.44% in terms of eCPM and CTR,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction is a fundamental technique for online\nadvertising recommendation and the complex online competitive auction process\nalso brings many difficulties to CTR optimization. Recent studies have shown\nthat introducing posterior auction information contributes to the performance\nof CTR prediction. However, existing work doesn't fully capitalize on the\nbenefits of auction information and overlooks the data bias brought by the\nauction, leading to biased and suboptimal results. To address these\nlimitations, we propose Auction Information Enhanced Framework (AIE) for CTR\nprediction in online advertising, which delves into the problem of insufficient\nutilization of auction signals and first reveals the auction bias.\nSpecifically, AIE introduces two pluggable modules, namely Adaptive\nMarket-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which\nwork collaboratively to excavate the posterior auction signals better and\nenhance the performance of CTR prediction. Furthermore, the two proposed\nmodules are lightweight, model-agnostic, and friendly to inference latency.\nExtensive experiments are conducted on a public dataset and an industrial\ndataset to demonstrate the effectiveness and compatibility of AIE. Besides, a\none-month online A/B test in a large-scale advertising platform shows that AIE\nimproves the base model by 5.76% and 2.44% in terms of eCPM and CTR,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Muyu Zhang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_doi": "10.1145/3640457.3688136",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688136",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.09792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.09792v2",
                "updated": "2024-08-15T03:25:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    25,
                    15,
                    3,
                    228,
                    0
                ],
                "published": "2022-07-20T10:09:53Z",
                "published_parsed": [
                    2022,
                    7,
                    20,
                    10,
                    9,
                    53,
                    2,
                    201,
                    0
                ],
                "title": "Unsupervised Industrial Anomaly Detection via Pattern Generative and\n  Contrastive Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Industrial Anomaly Detection via Pattern Generative and\n  Contrastive Networks"
                },
                "summary": "It is hard to collect enough flaw images for training deep learning network\nin industrial production. Therefore, existing industrial anomaly detection\nmethods prefer to use CNN-based unsupervised detection and localization network\nto achieve this task. However, these methods always fail when there are\nvarieties happened in new signals since traditional end-to-end networks suffer\nbarriers of fitting nonlinear model in high-dimensional space. Moreover, they\nhave a memory library by clustering the feature of normal images essentially,\nwhich cause it is not robust to texture change. To this end, we propose the\nVision Transformer based (VIT-based) unsupervised anomaly detection network. It\nutilizes a hierarchical task learning and human experience to enhance its\ninterpretability. Our network consists of pattern generation and comparison\nnetworks. Pattern generation network uses two VIT-based encoder modules to\nextract the feature of two consecutive image patches, then uses VIT-based\ndecoder module to learn the human designed style of these features and predict\nthe third image patch. After this, we use the Siamese-based network to compute\nthe similarity of the generation image patch and original image patch. Finally,\nwe refine the anomaly localization by the bi-directional inference strategy.\nComparison experiments on public dataset MVTec dataset show our method achieves\n99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we\ngive a qualitative illustration on our own leather and cloth datasets. The\naccurate segment results strongly prove the accuracy of our method in anomaly\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is hard to collect enough flaw images for training deep learning network\nin industrial production. Therefore, existing industrial anomaly detection\nmethods prefer to use CNN-based unsupervised detection and localization network\nto achieve this task. However, these methods always fail when there are\nvarieties happened in new signals since traditional end-to-end networks suffer\nbarriers of fitting nonlinear model in high-dimensional space. Moreover, they\nhave a memory library by clustering the feature of normal images essentially,\nwhich cause it is not robust to texture change. To this end, we propose the\nVision Transformer based (VIT-based) unsupervised anomaly detection network. It\nutilizes a hierarchical task learning and human experience to enhance its\ninterpretability. Our network consists of pattern generation and comparison\nnetworks. Pattern generation network uses two VIT-based encoder modules to\nextract the feature of two consecutive image patches, then uses VIT-based\ndecoder module to learn the human designed style of these features and predict\nthe third image patch. After this, we use the Siamese-based network to compute\nthe similarity of the generation image patch and original image patch. Finally,\nwe refine the anomaly localization by the bi-directional inference strategy.\nComparison experiments on public dataset MVTec dataset show our method achieves\n99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we\ngive a qualitative illustration on our own leather and cloth datasets. The\naccurate segment results strongly prove the accuracy of our method in anomaly\ndetection."
                },
                "authors": [
                    {
                        "name": "Jianfeng Huang"
                    },
                    {
                        "name": "Chenyang Li"
                    },
                    {
                        "name": "Yimin Lin"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_doi": "10.1007/978-981-97-5609-4_8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-97-5609-4_8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2207.09792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.09792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Advanced Intelligent Computing Technology and Applications. ICIC\n  2024. Lecture Notes in Computer Science, vol 14871",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07904v1",
                "updated": "2024-08-15T03:19:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    19,
                    41,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T03:19:41Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    19,
                    41,
                    3,
                    228,
                    0
                ],
                "title": "Assessing Language Models' Worldview for Fiction Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Language Models' Worldview for Fiction Generation"
                },
                "summary": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant\napplications in computational creativity. One such application is fictional\nstory generation. Fiction is a narrative that occurs in a story world that is\nslightly different than ours. With LLMs becoming writing partners, we question\nhow suitable they are to generate fiction. This study investigates the ability\nof LLMs to maintain a state of world essential to generate fiction. Through a\nseries of questions to nine LLMs, we find that only two models exhibit\nconsistent worldview, while the rest are self-conflicting. Subsequent analysis\nof stories generated by four models revealed a strikingly uniform narrative\npattern. This uniformity across models further suggests a lack of `state'\nnecessary for fiction. We highlight the limitations of current LLMs in fiction\nwriting and advocate for future research to test and create story worlds for\nLLMs to reside in. All code, dataset, and the generated responses can be found\nin https://github.com/tanny411/llm-reliability-and-consistency-evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant\napplications in computational creativity. One such application is fictional\nstory generation. Fiction is a narrative that occurs in a story world that is\nslightly different than ours. With LLMs becoming writing partners, we question\nhow suitable they are to generate fiction. This study investigates the ability\nof LLMs to maintain a state of world essential to generate fiction. Through a\nseries of questions to nine LLMs, we find that only two models exhibit\nconsistent worldview, while the rest are self-conflicting. Subsequent analysis\nof stories generated by four models revealed a strikingly uniform narrative\npattern. This uniformity across models further suggests a lack of `state'\nnecessary for fiction. We highlight the limitations of current LLMs in fiction\nwriting and advocate for future research to test and create story worlds for\nLLMs to reside in. All code, dataset, and the generated responses can be found\nin https://github.com/tanny411/llm-reliability-and-consistency-evaluation."
                },
                "authors": [
                    {
                        "name": "Aisha Khatun"
                    },
                    {
                        "name": "Daniel G. Brown"
                    }
                ],
                "author_detail": {
                    "name": "Daniel G. Brown"
                },
                "author": "Daniel G. Brown",
                "arxiv_comment": "Short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07896v1",
                "updated": "2024-08-15T02:55:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    55,
                    30,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:55:30Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    55,
                    30,
                    3,
                    228,
                    0
                ],
                "title": "The doctor will polygraph you now: ethical concerns with AI for\n  fact-checking patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The doctor will polygraph you now: ethical concerns with AI for\n  fact-checking patients"
                },
                "summary": "Clinical artificial intelligence (AI) methods have been proposed for\npredicting social behaviors which could be reasonably understood from\npatient-reported data. This raises ethical concerns about respect, privacy, and\npatient awareness/control over how their health data is used. Ethical concerns\nsurrounding clinical AI systems for social behavior verification were divided\ninto three main categories: (1) the use of patient data retrospectively without\ninformed consent for the specific task of verification, (2) the potential for\ninaccuracies or biases within such systems, and (3) the impact on trust in\npatient-provider relationships with the introduction of automated AI systems\nfor fact-checking. Additionally, this report showed the simulated misuse of a\nverification system and identified a potential LLM bias against\npatient-reported information in favor of multimodal data, published literature,\nand the outputs of other AI methods (i.e., AI self-trust). Finally,\nrecommendations were presented for mitigating the risk that AI verification\nsystems will cause harm to patients or undermine the purpose of the healthcare\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical artificial intelligence (AI) methods have been proposed for\npredicting social behaviors which could be reasonably understood from\npatient-reported data. This raises ethical concerns about respect, privacy, and\npatient awareness/control over how their health data is used. Ethical concerns\nsurrounding clinical AI systems for social behavior verification were divided\ninto three main categories: (1) the use of patient data retrospectively without\ninformed consent for the specific task of verification, (2) the potential for\ninaccuracies or biases within such systems, and (3) the impact on trust in\npatient-provider relationships with the introduction of automated AI systems\nfor fact-checking. Additionally, this report showed the simulated misuse of a\nverification system and identified a potential LLM bias against\npatient-reported information in favor of multimodal data, published literature,\nand the outputs of other AI methods (i.e., AI self-trust). Finally,\nrecommendations were presented for mitigating the risk that AI verification\nsystems will cause harm to patients or undermine the purpose of the healthcare\nsystem."
                },
                "authors": [
                    {
                        "name": "James Anibal"
                    },
                    {
                        "name": "Jasmine Gunkel"
                    },
                    {
                        "name": "Hannah Huth"
                    },
                    {
                        "name": "Hang Nguyen"
                    },
                    {
                        "name": "Shaheen Awan"
                    },
                    {
                        "name": "Yael Bensoussan"
                    },
                    {
                        "name": "Bradford Wood"
                    }
                ],
                "author_detail": {
                    "name": "Bradford Wood"
                },
                "author": "Bradford Wood",
                "arxiv_comment": "9 pages, 1 figure, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10596v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10596v3",
                "updated": "2024-08-15T02:38:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    38,
                    53,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-16T14:21:24Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    14,
                    21,
                    24,
                    1,
                    107,
                    0
                ],
                "title": "Formation of GW230529 from Isolated Binary Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formation of GW230529 from Isolated Binary Evolution"
                },
                "summary": "In this {\\em{Letter}}, we explore the formation of the mass-gap black\nhole-neutron star (mgBHNS) merger detected in gravitational wave (GW) event,\ni.e., GW230529, from the isolated binary evolution channel, and study potential\nsignatures of its electromagnetic counterparts. By adopting the `delayed'\nsupernova prescription and reasonable model realizations, our population\nsynthesis simulation results can simultaneously match the rate densities of\nmgBHNS and total BHNS mergers inferred from the population analyses, along with\nthe population distribution of the BH mass in BHNS mergers reported by the\nLIGO-Virgo-KAGRA Collaboration. Because GW230529 contributes significantly to\nthe inferred mgBHNS rate densities, we suggest that GW230529 can be explained\nthrough the isolated binary evolution channel. Considering the AP4 (DD2)\nequation of state, the probability that GW230529 can make tidal disruption is\n$12.8\\%$ ($63.2\\%$). If GW230529 is a disrupted event, its kilonova peak\napparent magnitude is predicted $\\sim23-24\\,{\\rm{mag}}$, and hence, can be\ndetected by the present survey projects and LSST. Since GW230529 could be an\noff-axis event inferred from the GW observation, its associated gamma-ray burst\n(GRB) might be too dim to be observed by $\\gamma$-ray detectors, interpreting\nthe lack of GRB observations. Our study suggests the existence of mgBHNS\nmergers formed through the isolated binary evolution channel due to the\ndiscovery of GW230529, indicating that BHNS mergers are still likely to be\nmultimessenger sources that emit GWs, GRBs, and kilonovae. Although mgBHNS\nmergers account for $\\sim50\\%$ cosmological BHNS population, we find that\n$\\gtrsim90\\%$ disrupted BHNS mergers are expected to originate from mgBHNS\nmergers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this {\\em{Letter}}, we explore the formation of the mass-gap black\nhole-neutron star (mgBHNS) merger detected in gravitational wave (GW) event,\ni.e., GW230529, from the isolated binary evolution channel, and study potential\nsignatures of its electromagnetic counterparts. By adopting the `delayed'\nsupernova prescription and reasonable model realizations, our population\nsynthesis simulation results can simultaneously match the rate densities of\nmgBHNS and total BHNS mergers inferred from the population analyses, along with\nthe population distribution of the BH mass in BHNS mergers reported by the\nLIGO-Virgo-KAGRA Collaboration. Because GW230529 contributes significantly to\nthe inferred mgBHNS rate densities, we suggest that GW230529 can be explained\nthrough the isolated binary evolution channel. Considering the AP4 (DD2)\nequation of state, the probability that GW230529 can make tidal disruption is\n$12.8\\%$ ($63.2\\%$). If GW230529 is a disrupted event, its kilonova peak\napparent magnitude is predicted $\\sim23-24\\,{\\rm{mag}}$, and hence, can be\ndetected by the present survey projects and LSST. Since GW230529 could be an\noff-axis event inferred from the GW observation, its associated gamma-ray burst\n(GRB) might be too dim to be observed by $\\gamma$-ray detectors, interpreting\nthe lack of GRB observations. Our study suggests the existence of mgBHNS\nmergers formed through the isolated binary evolution channel due to the\ndiscovery of GW230529, indicating that BHNS mergers are still likely to be\nmultimessenger sources that emit GWs, GRBs, and kilonovae. Although mgBHNS\nmergers account for $\\sim50\\%$ cosmological BHNS population, we find that\n$\\gtrsim90\\%$ disrupted BHNS mergers are expected to originate from mgBHNS\nmergers."
                },
                "authors": [
                    {
                        "name": "Jin-Ping Zhu"
                    },
                    {
                        "name": "Rui-Chong Hu"
                    },
                    {
                        "name": "Yacheng Kang"
                    },
                    {
                        "name": "Bing Zhang"
                    },
                    {
                        "name": "Hui Tong"
                    },
                    {
                        "name": "Lijing Shao"
                    },
                    {
                        "name": "Ying Qin"
                    }
                ],
                "author_detail": {
                    "name": "Ying Qin"
                },
                "author": "Ying Qin",
                "arxiv_comment": "Accepted for publication in ApJ, 15 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10596v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10596v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17900v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17900v5",
                "updated": "2024-08-15T02:33:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    33,
                    22,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-25T09:42:24Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    42,
                    24,
                    3,
                    207,
                    0
                ],
                "title": "The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer"
                },
                "summary": "Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions."
                },
                "authors": [
                    {
                        "name": "Danqing Hu"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Nan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wu"
                },
                "author": "Nan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17900v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17900v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07888v1",
                "updated": "2024-08-15T02:22:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    22,
                    48,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:22:48Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    22,
                    48,
                    3,
                    228,
                    0
                ],
                "title": "Fine-tuning Large Language Models with Human-inspired Learning\n  Strategies in Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models with Human-inspired Learning\n  Strategies in Medical Question Answering"
                },
                "summary": "Training Large Language Models (LLMs) incurs substantial data-related costs,\nmotivating the development of data-efficient training methods through optimised\ndata ordering and selection. Human-inspired learning strategies, such as\ncurriculum learning, offer possibilities for efficient training by organising\ndata according to common human learning practices. Despite evidence that\nfine-tuning with curriculum learning improves the performance of LLMs for\nnatural language understanding tasks, its effectiveness is typically assessed\nusing a single model. In this work, we extend previous research by evaluating\nboth curriculum-based and non-curriculum-based learning strategies across\nmultiple LLMs, using human-defined and automated data labels for medical\nquestion answering. Our results indicate a moderate impact of using\nhuman-inspired learning strategies for fine-tuning LLMs, with maximum accuracy\ngains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that\nthe effectiveness of these strategies varies significantly across different\nmodel-dataset combinations, emphasising that the benefits of a specific\nhuman-inspired strategy for fine-tuning LLMs do not generalise. Additionally,\nwe find evidence that curriculum learning using LLM-defined question difficulty\noutperforms human-defined difficulty, highlighting the potential of using\nmodel-generated measures for optimal curriculum design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) incurs substantial data-related costs,\nmotivating the development of data-efficient training methods through optimised\ndata ordering and selection. Human-inspired learning strategies, such as\ncurriculum learning, offer possibilities for efficient training by organising\ndata according to common human learning practices. Despite evidence that\nfine-tuning with curriculum learning improves the performance of LLMs for\nnatural language understanding tasks, its effectiveness is typically assessed\nusing a single model. In this work, we extend previous research by evaluating\nboth curriculum-based and non-curriculum-based learning strategies across\nmultiple LLMs, using human-defined and automated data labels for medical\nquestion answering. Our results indicate a moderate impact of using\nhuman-inspired learning strategies for fine-tuning LLMs, with maximum accuracy\ngains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that\nthe effectiveness of these strategies varies significantly across different\nmodel-dataset combinations, emphasising that the benefits of a specific\nhuman-inspired strategy for fine-tuning LLMs do not generalise. Additionally,\nwe find evidence that curriculum learning using LLM-defined question difficulty\noutperforms human-defined difficulty, highlighting the potential of using\nmodel-generated measures for optimal curriculum design."
                },
                "authors": [
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07955v2",
                "updated": "2024-08-15T02:18:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    18,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-01-15T20:42:16Z",
                "published_parsed": [
                    2024,
                    1,
                    15,
                    20,
                    42,
                    16,
                    0,
                    15,
                    0
                ],
                "title": "A Study on Large Language Models' Limitations in Multiple-Choice\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study on Large Language Models' Limitations in Multiple-Choice\n  Question Answering"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever."
                },
                "authors": [
                    {
                        "name": "Aisha Khatun"
                    },
                    {
                        "name": "Daniel G. Brown"
                    }
                ],
                "author_detail": {
                    "name": "Daniel G. Brown"
                },
                "author": "Daniel G. Brown",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v2",
                "updated": "2024-08-15T02:12:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    12,
                    52,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "We thank Shiyue Xu for pointing out the error in Equation 5 in the\n  previous draft: https://github.com/LuJunru/SamPO/issues/1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07884v1",
                "updated": "2024-08-15T02:07:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    7,
                    11,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:07:11Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    7,
                    11,
                    3,
                    228,
                    0
                ],
                "title": "Instruct Large Language Models to Generate Scientific Literature Survey\n  Step by Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct Large Language Models to Generate Scientific Literature Survey\n  Step by Step"
                },
                "summary": "Abstract. Automatically generating scientific literature surveys is a\nvaluable task that can significantly enhance research efficiency. However, the\ndiverse and complex nature of information within a literature survey poses\nsubstantial challenges for generative models. In this paper, we design a series\nof prompts to systematically leverage large language models (LLMs), enabling\nthe creation of comprehensive literature surveys through a step-by-step\napproach. Specifically, we design prompts to guide LLMs to sequentially\ngenerate the title, abstract, hierarchical headings, and the main content of\nthe literature survey. We argue that this design enables the generation of the\nheadings from a high-level perspective. During the content generation process,\nthis design effectively harnesses relevant information while minimizing costs\nby restricting the length of both input and output content in LLM queries. Our\nimplementation with Qwen-long achieved third place in the NLPCC 2024 Scientific\nLiterature Survey Generation evaluation task, with an overall score only 0.03%\nlower than the second-place team. Additionally, our soft heading recall is\n95.84%, the second best among the submissions. Thanks to the efficient prompt\ndesign and the low cost of the Qwen-long API, our method reduces the expense\nfor generating each literature survey to 0.1 RMB, enhancing the practical value\nof our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract. Automatically generating scientific literature surveys is a\nvaluable task that can significantly enhance research efficiency. However, the\ndiverse and complex nature of information within a literature survey poses\nsubstantial challenges for generative models. In this paper, we design a series\nof prompts to systematically leverage large language models (LLMs), enabling\nthe creation of comprehensive literature surveys through a step-by-step\napproach. Specifically, we design prompts to guide LLMs to sequentially\ngenerate the title, abstract, hierarchical headings, and the main content of\nthe literature survey. We argue that this design enables the generation of the\nheadings from a high-level perspective. During the content generation process,\nthis design effectively harnesses relevant information while minimizing costs\nby restricting the length of both input and output content in LLM queries. Our\nimplementation with Qwen-long achieved third place in the NLPCC 2024 Scientific\nLiterature Survey Generation evaluation task, with an overall score only 0.03%\nlower than the second-place team. Additionally, our soft heading recall is\n95.84%, the second best among the submissions. Thanks to the efficient prompt\ndesign and the low cost of the Qwen-long API, our method reduces the expense\nfor generating each literature survey to 0.1 RMB, enhancing the practical value\nof our method."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lai"
                    },
                    {
                        "name": "Yupeng Wu"
                    },
                    {
                        "name": "Yidan Wang"
                    },
                    {
                        "name": "Wenpeng Hu"
                    },
                    {
                        "name": "Chen Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zheng"
                },
                "author": "Chen Zheng",
                "arxiv_comment": "NLPCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14746v2",
                "updated": "2024-08-15T02:06:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    6,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2023-05-24T05:42:56Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    5,
                    42,
                    56,
                    2,
                    144,
                    0
                ],
                "title": "Wasserstein Gaussianization and Efficient Variational Bayes for Robust\n  Bayesian Synthetic Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wasserstein Gaussianization and Efficient Variational Bayes for Robust\n  Bayesian Synthetic Likelihood"
                },
                "summary": "The Bayesian Synthetic Likelihood (BSL) method is a widely-used tool for\nlikelihood-free Bayesian inference. This method assumes that some summary\nstatistics are normally distributed, which can be incorrect in many\napplications. We propose a transformation, called the Wasserstein\nGaussianization transformation, that uses a Wasserstein gradient flow to\napproximately transform the distribution of the summary statistics into a\nGaussian distribution. BSL also implicitly requires compatibility between\nsimulated summary statistics under the working model and the observed summary\nstatistics. A robust BSL variant which achieves this has been developed in the\nrecent literature. We combine the Wasserstein Gaussianization transformation\nwith robust BSL, and an efficient Variational Bayes procedure for posterior\napproximation, to develop a highly efficient and reliable approximate Bayesian\ninference method for likelihood-free problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian Synthetic Likelihood (BSL) method is a widely-used tool for\nlikelihood-free Bayesian inference. This method assumes that some summary\nstatistics are normally distributed, which can be incorrect in many\napplications. We propose a transformation, called the Wasserstein\nGaussianization transformation, that uses a Wasserstein gradient flow to\napproximately transform the distribution of the summary statistics into a\nGaussian distribution. BSL also implicitly requires compatibility between\nsimulated summary statistics under the working model and the observed summary\nstatistics. A robust BSL variant which achieves this has been developed in the\nrecent literature. We combine the Wasserstein Gaussianization transformation\nwith robust BSL, and an efficient Variational Bayes procedure for posterior\napproximation, to develop a highly efficient and reliable approximate Bayesian\ninference method for likelihood-free problems."
                },
                "authors": [
                    {
                        "name": "Nhat-Minh Nguyen"
                    },
                    {
                        "name": "Minh-Ngoc Tran"
                    },
                    {
                        "name": "Christopher Drovandi"
                    },
                    {
                        "name": "David Nott"
                    }
                ],
                "author_detail": {
                    "name": "David Nott"
                },
                "author": "David Nott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v2",
                "updated": "2024-08-15T01:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    49,
                    29,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07873v1",
                "updated": "2024-08-15T01:00:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    0,
                    28,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T01:00:28Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    0,
                    28,
                    3,
                    228,
                    0
                ],
                "title": "Words Matter: Reducing Stigma in Online Conversations about Substance\n  Use with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words Matter: Reducing Stigma in Online Conversations about Substance\n  Use with Large Language Models"
                },
                "summary": "Stigma is a barrier to treatment for individuals struggling with substance\nuse disorders (SUD), which leads to significantly lower treatment engagement\nrates. With only 7% of those affected receiving any form of help, societal\nstigma not only discourages individuals with SUD from seeking help but isolates\nthem, hindering their recovery journey and perpetuating a cycle of shame and\nself-doubt. This study investigates how stigma manifests on social media,\nparticularly Reddit, where anonymity can exacerbate discriminatory behaviors.\nWe analyzed over 1.2 million posts, identifying 3,207 that exhibited\nstigmatizing language towards people who use substances (PWUS). Using Informed\nand Stylized LLMs, we develop a model for de-stigmatization of these\nexpressions into empathetic language, resulting in 1,649 reformed phrase pairs.\nOur paper contributes to the field by proposing a computational framework for\nanalyzing stigma and destigmatizing online content, and delving into the\nlinguistic features that propagate stigma towards PWUS. Our work not only\nenhances understanding of stigma's manifestations online but also provides\npractical tools for fostering a more supportive digital environment for those\naffected by SUD. Code and data will be made publicly available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stigma is a barrier to treatment for individuals struggling with substance\nuse disorders (SUD), which leads to significantly lower treatment engagement\nrates. With only 7% of those affected receiving any form of help, societal\nstigma not only discourages individuals with SUD from seeking help but isolates\nthem, hindering their recovery journey and perpetuating a cycle of shame and\nself-doubt. This study investigates how stigma manifests on social media,\nparticularly Reddit, where anonymity can exacerbate discriminatory behaviors.\nWe analyzed over 1.2 million posts, identifying 3,207 that exhibited\nstigmatizing language towards people who use substances (PWUS). Using Informed\nand Stylized LLMs, we develop a model for de-stigmatization of these\nexpressions into empathetic language, resulting in 1,649 reformed phrase pairs.\nOur paper contributes to the field by proposing a computational framework for\nanalyzing stigma and destigmatizing online content, and delving into the\nlinguistic features that propagate stigma towards PWUS. Our work not only\nenhances understanding of stigma's manifestations online but also provides\npractical tools for fostering a more supportive digital environment for those\naffected by SUD. Code and data will be made publicly available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Layla Bouzoubaa"
                    },
                    {
                        "name": "Elham Aghakhani"
                    },
                    {
                        "name": "Rezvaneh Rezapour"
                    }
                ],
                "author_detail": {
                    "name": "Rezvaneh Rezapour"
                },
                "author": "Rezvaneh Rezapour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07850v1",
                "updated": "2024-08-14T23:31:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    31,
                    26,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:31:26Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    31,
                    26,
                    2,
                    227,
                    0
                ],
                "title": "Cyclical period changes in cataclysmic variables: a statistical study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclical period changes in cataclysmic variables: a statistical study"
                },
                "summary": "We report the results of a statistical study of cyclical period changes in\ncataclysmic variables (CVs). Assuming the third-body hypothesis as the cause of\nperiod changes, we estimate the third-body mass, $m_3$, and its separation from\nthe binary, $a_3$, for 21 CVs showing cyclical period changes from well-sampled\nobserved-minus-calculated diagrams covering more than a decade of observations.\nThe inferred $a_3$ values are independent of the binary orbital period,\n$P_\\mathrm{orb}$, whereas the $m_3$ values increase with $P_\\mathrm{orb}$ by an\norder of magnitude from the shortest period (oldest) to the longest period\n(youngest) systems, implying significant mass loss from the third body with\ntime. A model for the time evolution of the triple system is not able to\nsimultaneously explain the observed behavior of the $m_3(P_\\mathrm{orb})$ and\n$a_3(P_\\mathrm{orb})$ distributions because the combined mass loss from the\nbinary and the third body demands an increase in orbital separation by factors\n$\\sim 140$ as the binary evolves toward shorter $P_\\mathrm{orb}$'s, in clear\ndisagreement with the observed distribution. We conclude that the third-body\nhypothesis is statistically inconsistent and cannot be used to explain cyclical\nperiod changes observed in CVs. On the other hand, the diagram of the amplitude\nof the period change versus the CV donor-star mass is consistent both with the\nalternative hypothesis that the observed cyclical period changes are a\nconsequence of magnetic activity in the solar-type donor star, and with the\nstandard evolutionary scenario for CVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the results of a statistical study of cyclical period changes in\ncataclysmic variables (CVs). Assuming the third-body hypothesis as the cause of\nperiod changes, we estimate the third-body mass, $m_3$, and its separation from\nthe binary, $a_3$, for 21 CVs showing cyclical period changes from well-sampled\nobserved-minus-calculated diagrams covering more than a decade of observations.\nThe inferred $a_3$ values are independent of the binary orbital period,\n$P_\\mathrm{orb}$, whereas the $m_3$ values increase with $P_\\mathrm{orb}$ by an\norder of magnitude from the shortest period (oldest) to the longest period\n(youngest) systems, implying significant mass loss from the third body with\ntime. A model for the time evolution of the triple system is not able to\nsimultaneously explain the observed behavior of the $m_3(P_\\mathrm{orb})$ and\n$a_3(P_\\mathrm{orb})$ distributions because the combined mass loss from the\nbinary and the third body demands an increase in orbital separation by factors\n$\\sim 140$ as the binary evolves toward shorter $P_\\mathrm{orb}$'s, in clear\ndisagreement with the observed distribution. We conclude that the third-body\nhypothesis is statistically inconsistent and cannot be used to explain cyclical\nperiod changes observed in CVs. On the other hand, the diagram of the amplitude\nof the period change versus the CV donor-star mass is consistent both with the\nalternative hypothesis that the observed cyclical period changes are a\nconsequence of magnetic activity in the solar-type donor star, and with the\nstandard evolutionary scenario for CVs."
                },
                "authors": [
                    {
                        "name": "Leandro Souza"
                    },
                    {
                        "name": "Raymundo Baptista"
                    }
                ],
                "author_detail": {
                    "name": "Raymundo Baptista"
                },
                "author": "Raymundo Baptista",
                "arxiv_doi": "10.3847/1538-4357/ad6b0e",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad6b0e",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 2 figures, prepared with AASLatex; to appear in The\n  Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07420v2",
                "updated": "2024-08-14T23:19:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    19,
                    29,
                    2,
                    227,
                    0
                ],
                "published": "2024-05-13T01:39:25Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    1,
                    39,
                    25,
                    0,
                    134,
                    0
                ],
                "title": "Robust Inference for High-Dimensional Panel Data Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference for High-Dimensional Panel Data Models"
                },
                "summary": "In this paper, we propose a robust estimation and inferential method for\nhigh-dimensional panel data models. Specifically, (1) we investigate the case\nwhere the number of regressors can grow faster than the sample size, (2) we pay\nparticular attention to non-Gaussian, serially and cross-sectionally correlated\nand heteroskedastic error processes, and (3) we develop an estimation method\nfor high-dimensional long-run covariance matrix using a thresholded estimator.\n  Methodologically and technically, we develop two Nagaev-types of\nconcentration inequalities: one for a partial sum and the other for a quadratic\nform, subject to a set of easily verifiable conditions. Leveraging these two\ninequalities, we also derive a non-asymptotic bound for the LASSO estimator,\nachieve asymptotic normality via the node-wise LASSO regression, and establish\na sharp convergence rate for the thresholded heteroskedasticity and\nautocorrelation consistent (HAC) estimator.\n  Our study thus provides the relevant literature with a complete toolkit for\nconducting inference about the parameters of interest involved in a\nhigh-dimensional panel data framework. We also demonstrate the practical\nrelevance of these theoretical results by investigating a high-dimensional\npanel data model with interactive fixed effects. Moreover, we conduct extensive\nnumerical studies using simulated and real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a robust estimation and inferential method for\nhigh-dimensional panel data models. Specifically, (1) we investigate the case\nwhere the number of regressors can grow faster than the sample size, (2) we pay\nparticular attention to non-Gaussian, serially and cross-sectionally correlated\nand heteroskedastic error processes, and (3) we develop an estimation method\nfor high-dimensional long-run covariance matrix using a thresholded estimator.\n  Methodologically and technically, we develop two Nagaev-types of\nconcentration inequalities: one for a partial sum and the other for a quadratic\nform, subject to a set of easily verifiable conditions. Leveraging these two\ninequalities, we also derive a non-asymptotic bound for the LASSO estimator,\nachieve asymptotic normality via the node-wise LASSO regression, and establish\na sharp convergence rate for the thresholded heteroskedasticity and\nautocorrelation consistent (HAC) estimator.\n  Our study thus provides the relevant literature with a complete toolkit for\nconducting inference about the parameters of interest involved in a\nhigh-dimensional panel data framework. We also demonstrate the practical\nrelevance of these theoretical results by investigating a high-dimensional\npanel data model with interactive fixed effects. Moreover, we conduct extensive\nnumerical studies using simulated and real data examples."
                },
                "authors": [
                    {
                        "name": "Jiti Gao"
                    },
                    {
                        "name": "Bin Peng"
                    },
                    {
                        "name": "Yayi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yayi Yan"
                },
                "author": "Yayi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07846v1",
                "updated": "2024-08-14T23:02:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    2,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:02:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    2,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites"
                },
                "summary": "Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present \\textsc{AgoneTest}: an automated system for generating test\nsuites for Java projects and a comprehensive and principled methodology for\nevaluating the generated test suites. Starting from a state-of-the-art dataset\n(i.e., \\textsc{Methods2Test}), we built a new dataset for comparing\nhuman-written tests with those generated by LLMs. Our key contributions include\na scalable automated software system, a new dataset, and a detailed methodology\nfor evaluating test quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present \\textsc{AgoneTest}: an automated system for generating test\nsuites for Java projects and a comprehensive and principled methodology for\nevaluating the generated test suites. Starting from a state-of-the-art dataset\n(i.e., \\textsc{Methods2Test}), we built a new dataset for comparing\nhuman-written tests with those generated by LLMs. Our key contributions include\na scalable automated software system, a new dataset, and a detailed methodology\nfor evaluating test quality."
                },
                "authors": [
                    {
                        "name": "Andrea Lops"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Azzurra Ragone"
                    },
                    {
                        "name": "Michelantonio Trizio"
                    },
                    {
                        "name": "Claudio Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Bartolini"
                },
                "author": "Claudio Bartolini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00987v2",
                "updated": "2024-08-14T22:59:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    59,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-02T01:52:28Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    1,
                    52,
                    28,
                    1,
                    2,
                    0
                ],
                "title": "Inverting estimating equations for causal inference on quantiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverting estimating equations for causal inference on quantiles"
                },
                "summary": "The causal inference literature frequently focuses on estimating the mean of\nthe potential outcome, whereas quantiles of the potential outcome may carry\nimportant additional information. We propose a unified approach, based on the\ninverse estimating equations, to generalize a class of causal inference\nsolutions from estimating the mean of the potential outcome to its quantiles.\nWe assume that a moment function is available to identify the mean of the\nthreshold-transformed potential outcome, based on which a convenient\nconstruction of the estimating equation of quantiles of potential outcome is\nproposed. In addition, we give a general construction of the efficient\ninfluence functions of the mean and quantiles of potential outcomes, and\nexplicate their connection. We motivate estimators for the quantile estimands\nwith the efficient influence function, and develop their asymptotic properties\nwhen either parametric models or data-adaptive machine learners are used to\nestimate the nuisance functions. A broad implication of our results is that one\ncan rework the existing result for mean causal estimands to facilitate causal\ninference on quantiles. Our general results are illustrated by several\nanalytical and numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The causal inference literature frequently focuses on estimating the mean of\nthe potential outcome, whereas quantiles of the potential outcome may carry\nimportant additional information. We propose a unified approach, based on the\ninverse estimating equations, to generalize a class of causal inference\nsolutions from estimating the mean of the potential outcome to its quantiles.\nWe assume that a moment function is available to identify the mean of the\nthreshold-transformed potential outcome, based on which a convenient\nconstruction of the estimating equation of quantiles of potential outcome is\nproposed. In addition, we give a general construction of the efficient\ninfluence functions of the mean and quantiles of potential outcomes, and\nexplicate their connection. We motivate estimators for the quantile estimands\nwith the efficient influence function, and develop their asymptotic properties\nwhen either parametric models or data-adaptive machine learners are used to\nestimate the nuisance functions. A broad implication of our results is that one\ncan rework the existing result for mean causal estimands to facilitate causal\ninference on quantiles. Our general results are illustrated by several\nanalytical and numerical examples."
                },
                "authors": [
                    {
                        "name": "Chao Cheng"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07842v1",
                "updated": "2024-08-14T22:44:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    44,
                    27,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T22:44:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    44,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Quantile and Distribution Treatment Effects on the Treated with Possibly\n  Non-Continuous Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile and Distribution Treatment Effects on the Treated with Possibly\n  Non-Continuous Outcomes"
                },
                "summary": "Quantile and Distribution Treatment effects on the Treated (QTT/DTT) for\nnon-continuous outcomes are either not identified or inference thereon is\ninfeasible using existing methods. By introducing functional index parallel\ntrends and no anticipation assumptions, this paper identifies and provides\nuniform inference procedures for QTT/DTT. The inference procedure applies under\nboth the canonical two-group and staggered treatment designs with balanced\npanels, unbalanced panels, or repeated cross-sections. Monte Carlo experiments\ndemonstrate the proposed method's robust and competitive performance, while an\nempirical application illustrates its practical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile and Distribution Treatment effects on the Treated (QTT/DTT) for\nnon-continuous outcomes are either not identified or inference thereon is\ninfeasible using existing methods. By introducing functional index parallel\ntrends and no anticipation assumptions, this paper identifies and provides\nuniform inference procedures for QTT/DTT. The inference procedure applies under\nboth the canonical two-group and staggered treatment designs with balanced\npanels, unbalanced panels, or repeated cross-sections. Monte Carlo experiments\ndemonstrate the proposed method's robust and competitive performance, while an\nempirical application illustrates its practical utility."
                },
                "authors": [
                    {
                        "name": "Nelly K. Djuazon"
                    },
                    {
                        "name": "Emmanuel Selorm Tsyawo"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Selorm Tsyawo"
                },
                "author": "Emmanuel Selorm Tsyawo",
                "arxiv_comment": "First draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.08313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08313v1",
                "updated": "2024-08-15T17:59:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:59:57Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Symbolic Graphics Programs?"
                },
                "summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "Technical Report v1 (44 pages, 23 figures, project page:\n  https://sgp-bench.github.io/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08302v1",
                "updated": "2024-08-15T17:55:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    55,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:55:45Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    55,
                    45,
                    3,
                    228,
                    0
                ],
                "title": "Benchmarking the Capabilities of Large Language Models in Transportation\n  System Engineering: Accuracy, Consistency, and Reasoning Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Capabilities of Large Language Models in Transportation\n  System Engineering: Accuracy, Consistency, and Reasoning Behaviors"
                },
                "summary": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges."
                },
                "authors": [
                    {
                        "name": "Usman Syed"
                    },
                    {
                        "name": "Ethan Light"
                    },
                    {
                        "name": "Xingang Guo"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yanfeng Ouyang"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08300v1",
                "updated": "2024-08-15T17:54:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    54,
                    31,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:54:31Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    54,
                    31,
                    3,
                    228,
                    0
                ],
                "title": "HELP: Hierarchical Embeddings-based Log Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELP: Hierarchical Embeddings-based Log Parsing"
                },
                "summary": "Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing."
                },
                "authors": [
                    {
                        "name": "Andy Xu"
                    },
                    {
                        "name": "Arno Gau"
                    }
                ],
                "author_detail": {
                    "name": "Arno Gau"
                },
                "author": "Arno Gau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11907v2",
                "updated": "2024-08-15T17:37:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    37,
                    36,
                    3,
                    228,
                    0
                ],
                "published": "2024-02-19T07:46:40Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    7,
                    46,
                    40,
                    0,
                    50,
                    0
                ],
                "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive\n  Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive\n  Prompt Distillation"
                },
                "summary": "Aligning large language models (LLMs) with human expectations without\nhuman-annotated preference data is an important problem. In this paper, we\npropose a method to evaluate the response preference by using the output\nprobabilities of response pairs under contrastive prompt pairs, which could\nachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based\non this, we propose an automatic alignment method, Direct Large Model Alignment\n(DLMA). First, we use contrastive prompt pairs to automatically generate\npreference data. Then, we continue to evaluate the generated preference data\nusing contrastive prompt pairs and calculate a self-rewarding score. Finally,\nwe use the DPO algorithm to effectively align LLMs by combining this\nself-rewarding score. In the experimental stage, our DLMA method could surpass\nthe \\texttt{RLHF} method without relying on human-annotated preference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human expectations without\nhuman-annotated preference data is an important problem. In this paper, we\npropose a method to evaluate the response preference by using the output\nprobabilities of response pairs under contrastive prompt pairs, which could\nachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based\non this, we propose an automatic alignment method, Direct Large Model Alignment\n(DLMA). First, we use contrastive prompt pairs to automatically generate\npreference data. Then, we continue to evaluate the generated preference data\nusing contrastive prompt pairs and calculate a self-rewarding score. Finally,\nwe use the DPO algorithm to effectively align LLMs by combining this\nself-rewarding score. In the experimental stage, our DLMA method could surpass\nthe \\texttt{RLHF} method without relying on human-annotated preference data."
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Haoping Bai"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen",
                "arxiv_comment": "24 pages, 5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08282v1",
                "updated": "2024-08-15T17:33:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    33,
                    32,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T17:33:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    33,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through\n  Grounded Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through\n  Grounded Language Model"
                },
                "summary": "Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning."
                },
                "authors": [
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Arturo Laurenzi"
                    },
                    {
                        "name": "Nikos Tsagarakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Tsagarakis"
                },
                "author": "Nikos Tsagarakis",
                "arxiv_comment": "Paper accepted by IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08253v1",
                "updated": "2024-08-15T16:48:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    48,
                    16,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T16:48:16Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    48,
                    16,
                    3,
                    228,
                    0
                ],
                "title": "All-Optical Azimuthal Trapping of Dissipative Kerr Multi-Solitons for\n  Relative Noise Suppression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-Optical Azimuthal Trapping of Dissipative Kerr Multi-Solitons for\n  Relative Noise Suppression"
                },
                "summary": "Temporal cavity solitons, or dissipative Kerr solitons (DKS) in integrated\nmicroresonators, are essential for deployable metrology technologies. Such\napplications favor the lowest noise state, typically the single-DKS state where\none soliton is in the resonator. Other multi-DKS states can also be reached,\noffering better conversion efficiency and thermal stability, potentially\nsimplifying DKS-based technologies. Yet they exhibit more noise due to relative\nsoliton jitter, and are usually not compatible with targeted applications. We\ndemonstrate that Kerr-induced synchronization, an all-optical trapping\ntechnique, can azimuthally pin the multi-DKS state to a common reference field.\nThis method ensures repetition rate noise independent of the number of\nsolitons, making a multi-DKS state indistinguishable from a single-DKS state in\nthat regard, akin to trapped-soliton molecule behavior. Supported by\ntheoretical analysis and experimental demonstration in an integrated\nmicroresonator, this approach provides metrological capacity regardless of the\nnumber of cavity solitons, benefiting numerous DKS-based metrology\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal cavity solitons, or dissipative Kerr solitons (DKS) in integrated\nmicroresonators, are essential for deployable metrology technologies. Such\napplications favor the lowest noise state, typically the single-DKS state where\none soliton is in the resonator. Other multi-DKS states can also be reached,\noffering better conversion efficiency and thermal stability, potentially\nsimplifying DKS-based technologies. Yet they exhibit more noise due to relative\nsoliton jitter, and are usually not compatible with targeted applications. We\ndemonstrate that Kerr-induced synchronization, an all-optical trapping\ntechnique, can azimuthally pin the multi-DKS state to a common reference field.\nThis method ensures repetition rate noise independent of the number of\nsolitons, making a multi-DKS state indistinguishable from a single-DKS state in\nthat regard, akin to trapped-soliton molecule behavior. Supported by\ntheoretical analysis and experimental demonstration in an integrated\nmicroresonator, this approach provides metrological capacity regardless of the\nnumber of cavity solitons, benefiting numerous DKS-based metrology\napplications."
                },
                "authors": [
                    {
                        "name": "Pradyoth Shandilya"
                    },
                    {
                        "name": "Shao-Chien Ou"
                    },
                    {
                        "name": "Jordan Stone"
                    },
                    {
                        "name": "Curtis Menyuk"
                    },
                    {
                        "name": "Miro Erkintalo"
                    },
                    {
                        "name": "Kartik Srinivasan"
                    },
                    {
                        "name": "Gregory Moille"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Moille"
                },
                "author": "Gregory Moille",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10015v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10015v3",
                "updated": "2024-08-15T16:44:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    44,
                    6,
                    3,
                    228,
                    0
                ],
                "published": "2023-08-19T13:46:49Z",
                "published_parsed": [
                    2023,
                    8,
                    19,
                    13,
                    46,
                    49,
                    5,
                    231,
                    0
                ],
                "title": "DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection"
                },
                "summary": "Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. We have validated our proposed method on benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. We have validated our proposed method on benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy."
                },
                "authors": [
                    {
                        "name": "Anuj Rai"
                    },
                    {
                        "name": "Parsheel Kumar Tiwari"
                    },
                    {
                        "name": "Jyotishna Baishya"
                    },
                    {
                        "name": "Ram Prakash Sharma"
                    },
                    {
                        "name": "Somnath Dey"
                    }
                ],
                "author_detail": {
                    "name": "Somnath Dey"
                },
                "author": "Somnath Dey",
                "arxiv_comment": "arXiv admin note:",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10015v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10015v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20242v2",
                "updated": "2024-08-15T16:08:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    8,
                    6,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-16T13:13:16Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    13,
                    16,
                    1,
                    198,
                    0
                ],
                "title": "The Threats of Embodied Multimodal LLMs: Jailbreaking Robotic\n  Manipulation in the Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Threats of Embodied Multimodal LLMs: Jailbreaking Robotic\n  Manipulation in the Physical World"
                },
                "summary": "Embodied artificial intelligence (AI) represents an artificial intelligence\nsystem that interacts with the physical world through sensors and actuators,\nseamlessly integrating perception and action. This design enables AI to learn\nfrom and operate within complex, real-world environments. Large Language Models\n(LLMs) deeply explore language instructions, playing a crucial role in devising\nplans for complex tasks. Consequently, they have progressively shown immense\npotential in empowering embodied AI, with LLM-based embodied AI emerging as a\nfocal point of research within the community. It is foreseeable that, over the\nnext decade, LLM-based embodied AI robots are expected to proliferate widely,\nbecoming commonplace in homes and industries. However, a critical safety issue\nthat has long been hiding in plain sight is: could LLM-based embodied AI\nperpetrate harmful behaviors? Our research investigates for the first time how\nto induce threatening actions in embodied AI, confirming the severe risks posed\nby these soon-to-be-marketed robots, which starkly contravene Asimov's Three\nLaws of Robotics and threaten human safety. Specifically, we formulate the\nconcept of embodied AI jailbreaking and expose three critical security\nvulnerabilities: first, jailbreaking robotics through compromised LLM; second,\nsafety misalignment between action and language spaces; and third, deceptive\nprompts leading to unaware hazardous behaviors. We also analyze potential\nmitigation measures and advocate for community awareness regarding the safety\nof embodied AI applications in the physical world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (AI) represents an artificial intelligence\nsystem that interacts with the physical world through sensors and actuators,\nseamlessly integrating perception and action. This design enables AI to learn\nfrom and operate within complex, real-world environments. Large Language Models\n(LLMs) deeply explore language instructions, playing a crucial role in devising\nplans for complex tasks. Consequently, they have progressively shown immense\npotential in empowering embodied AI, with LLM-based embodied AI emerging as a\nfocal point of research within the community. It is foreseeable that, over the\nnext decade, LLM-based embodied AI robots are expected to proliferate widely,\nbecoming commonplace in homes and industries. However, a critical safety issue\nthat has long been hiding in plain sight is: could LLM-based embodied AI\nperpetrate harmful behaviors? Our research investigates for the first time how\nto induce threatening actions in embodied AI, confirming the severe risks posed\nby these soon-to-be-marketed robots, which starkly contravene Asimov's Three\nLaws of Robotics and threaten human safety. Specifically, we formulate the\nconcept of embodied AI jailbreaking and expose three critical security\nvulnerabilities: first, jailbreaking robotics through compromised LLM; second,\nsafety misalignment between action and language spaces; and third, deceptive\nprompts leading to unaware hazardous behaviors. We also analyze potential\nmitigation measures and advocate for community awareness regarding the safety\nof embodied AI applications in the physical world."
                },
                "authors": [
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "Preliminary version (17 pages, 4 figures). Work in progress,\n  revisions ongoing. Appreciate understanding and welcome any feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08231v1",
                "updated": "2024-08-15T15:56:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    56,
                    23,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:56:23Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    56,
                    23,
                    3,
                    228,
                    0
                ],
                "title": "DaRec: A Disentangled Alignment Framework for Large Language Model and\n  Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaRec: A Disentangled Alignment Framework for Large Language Model and\n  Recommender System"
                },
                "summary": "Benefiting from the strong reasoning capabilities, Large language models\n(LLMs) have demonstrated remarkable performance in recommender systems. Various\nefforts have been made to distill knowledge from LLMs to enhance collaborative\nmodels, employing techniques like contrastive learning for representation\nalignment. In this work, we prove that directly aligning the representations of\nLLMs and collaborative models is sub-optimal for enhancing downstream\nrecommendation tasks performance, based on the information theorem.\nConsequently, the challenge of effectively aligning semantic representations\nbetween collaborative models and LLMs remains unresolved. Inspired by this\nviewpoint, we propose a novel plug-and-play alignment framework for LLMs and\ncollaborative models. Specifically, we first disentangle the latent\nrepresentations of both LLMs and collaborative models into specific and shared\ncomponents via projection layers and representation regularization.\nSubsequently, we perform both global and local structure alignment on the\nshared representations to facilitate knowledge transfer. Additionally, we\ntheoretically prove that the specific and shared representations contain more\npertinent and less irrelevant information, which can enhance the effectiveness\nof downstream recommendation tasks. Extensive experimental results on benchmark\ndatasets demonstrate that our method is superior to existing state-of-the-art\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the strong reasoning capabilities, Large language models\n(LLMs) have demonstrated remarkable performance in recommender systems. Various\nefforts have been made to distill knowledge from LLMs to enhance collaborative\nmodels, employing techniques like contrastive learning for representation\nalignment. In this work, we prove that directly aligning the representations of\nLLMs and collaborative models is sub-optimal for enhancing downstream\nrecommendation tasks performance, based on the information theorem.\nConsequently, the challenge of effectively aligning semantic representations\nbetween collaborative models and LLMs remains unresolved. Inspired by this\nviewpoint, we propose a novel plug-and-play alignment framework for LLMs and\ncollaborative models. Specifically, we first disentangle the latent\nrepresentations of both LLMs and collaborative models into specific and shared\ncomponents via projection layers and representation regularization.\nSubsequently, we perform both global and local structure alignment on the\nshared representations to facilitate knowledge transfer. Additionally, we\ntheoretically prove that the specific and shared representations contain more\npertinent and less irrelevant information, which can enhance the effectiveness\nof downstream recommendation tasks. Extensive experimental results on benchmark\ndatasets demonstrate that our method is superior to existing state-of-the-art\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xihong Yang"
                    },
                    {
                        "name": "Heming Jing"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Huakang Niu"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xinwang Liu"
                    },
                    {
                        "name": "En Zhu"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Erxue Min"
                    }
                ],
                "author_detail": {
                    "name": "Erxue Min"
                },
                "author": "Erxue Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08217v1",
                "updated": "2024-08-15T15:28:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    28,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:28:37Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    28,
                    37,
                    3,
                    228,
                    0
                ],
                "title": "RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train\n  and Deploy Edge Classifiers for Computational Social Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train\n  and Deploy Edge Classifiers for Computational Social Science"
                },
                "summary": "Large language models (LLMs) have enhanced our ability to rapidly analyze and\nclassify unstructured natural language data. However, concerns regarding cost,\nnetwork limitations, and security constraints have posed challenges for their\nintegration into work processes. In this study, we adopt a systems design\napproach to employing LLMs as imperfect data annotators for downstream\nsupervised learning tasks, introducing novel system intervention measures aimed\nat improving classification performance. Our methodology outperforms\nLLM-generated labels in seven of eight tests, demonstrating an effective\nstrategy for incorporating LLMs into the design and deployment of specialized,\nsupervised learning models present in many industry use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enhanced our ability to rapidly analyze and\nclassify unstructured natural language data. However, concerns regarding cost,\nnetwork limitations, and security constraints have posed challenges for their\nintegration into work processes. In this study, we adopt a systems design\napproach to employing LLMs as imperfect data annotators for downstream\nsupervised learning tasks, introducing novel system intervention measures aimed\nat improving classification performance. Our methodology outperforms\nLLM-generated labels in seven of eight tests, demonstrating an effective\nstrategy for incorporating LLMs into the design and deployment of specialized,\nsupervised learning models present in many industry use cases."
                },
                "authors": [
                    {
                        "name": "David Farr"
                    },
                    {
                        "name": "Nico Manzonelli"
                    },
                    {
                        "name": "Iain Cruickshank"
                    },
                    {
                        "name": "Jevin West"
                    }
                ],
                "author_detail": {
                    "name": "Jevin West"
                },
                "author": "Jevin West",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v3",
                "updated": "2024-08-15T15:24:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    24,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08212v1",
                "updated": "2024-08-15T15:23:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    23,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:23:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    23,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Covert Bias: The Severity of Social Views' Unalignment Towards Implicit\n  and Explicit Opinion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covert Bias: The Severity of Social Views' Unalignment Towards Implicit\n  and Explicit Opinion"
                },
                "summary": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models.To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models.To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity."
                },
                "authors": [
                    {
                        "name": "Abeer Aldayel"
                    },
                    {
                        "name": "Areej Alokaili"
                    },
                    {
                        "name": "Rehab Alahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Rehab Alahmadi"
                },
                "author": "Rehab Alahmadi",
                "arxiv_comment": "This work is under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15508v2",
                "updated": "2024-08-15T15:22:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    22,
                    57,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T09:45:16Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    9,
                    45,
                    16,
                    0,
                    204,
                    0
                ],
                "title": "Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners"
                },
                "summary": "Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application."
                },
                "authors": [
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Jaji Wu"
                    },
                    {
                        "name": "Jun Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jun Cheng"
                },
                "author": "Jun Cheng",
                "arxiv_comment": "Effecient Quantization Methods for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04275v2",
                "updated": "2024-08-15T15:20:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    20,
                    53,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-08T07:20:42Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    7,
                    20,
                    42,
                    3,
                    221,
                    0
                ],
                "title": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models"
                },
                "summary": "Multimodal large language models (LLMs) have demonstrated significant\npotential in a wide range of AI applications. Yet, training multimodal LLMs\nsuffers from low efficiency and scalability, due to the inherent model\nheterogeneity and data heterogeneity across different modalities.\n  We present DistTrain, an efficient and adaptive framework to reform the\ntraining of multimodal large language models on large-scale clusters. The core\nof DistTrain is the disaggregated training technique that exploits the\ncharacteristics of multimodal LLM training to achieve high efficiency and\nscalability. Specifically, it leverages disaggregated model orchestration and\ndisaggregated data reordering to address model and data heterogeneity\nrespectively. We also tailor system optimization for multimodal LLM training to\noverlap GPU communication and computation. We evaluate DistTrain across\ndifferent sizes of multimodal LLMs on a large-scale production cluster with\nthousands of GPUs. The experimental results show that DistTrain achieves 54.7%\nModel FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs\nand outperforms Megatron-LM by up to 2.2$\\times$ on throughput. The ablation\nstudy shows the main techniques of DistTrain are both effective and\nlightweight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) have demonstrated significant\npotential in a wide range of AI applications. Yet, training multimodal LLMs\nsuffers from low efficiency and scalability, due to the inherent model\nheterogeneity and data heterogeneity across different modalities.\n  We present DistTrain, an efficient and adaptive framework to reform the\ntraining of multimodal large language models on large-scale clusters. The core\nof DistTrain is the disaggregated training technique that exploits the\ncharacteristics of multimodal LLM training to achieve high efficiency and\nscalability. Specifically, it leverages disaggregated model orchestration and\ndisaggregated data reordering to address model and data heterogeneity\nrespectively. We also tailor system optimization for multimodal LLM training to\noverlap GPU communication and computation. We evaluate DistTrain across\ndifferent sizes of multimodal LLMs on a large-scale production cluster with\nthousands of GPUs. The experimental results show that DistTrain achieves 54.7%\nModel FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs\nand outperforms Megatron-LM by up to 2.2$\\times$ on throughput. The ablation\nstudy shows the main techniques of DistTrain are both effective and\nlightweight."
                },
                "authors": [
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08210v1",
                "updated": "2024-08-15T15:19:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    19,
                    11,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:19:11Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    19,
                    11,
                    3,
                    228,
                    0
                ],
                "title": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large\n  Language Models"
                },
                "summary": "Recent advances in AI have been significantly driven by the capabilities of\nlarge language models (LLMs) to solve complex problems in ways that resemble\nhuman thinking. However, there is an ongoing debate about the extent to which\nLLMs are capable of actual reasoning. Central to this debate are two key\nprobabilistic concepts that are essential for connecting causes to their\neffects: the probability of necessity (PN) and the probability of sufficiency\n(PS). This paper introduces a framework that is both theoretical and practical,\naimed at assessing how effectively LLMs are able to replicate real-world\nreasoning mechanisms using these probabilistic measures. By viewing LLMs as\nabstract machines that process information through a natural language\ninterface, we examine the conditions under which it is possible to compute\nsuitable approximations of PN and PS. Our research marks an important step\ntowards gaining a deeper understanding of when LLMs are capable of reasoning,\nas illustrated by a series of math examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI have been significantly driven by the capabilities of\nlarge language models (LLMs) to solve complex problems in ways that resemble\nhuman thinking. However, there is an ongoing debate about the extent to which\nLLMs are capable of actual reasoning. Central to this debate are two key\nprobabilistic concepts that are essential for connecting causes to their\neffects: the probability of necessity (PN) and the probability of sufficiency\n(PS). This paper introduces a framework that is both theoretical and practical,\naimed at assessing how effectively LLMs are able to replicate real-world\nreasoning mechanisms using these probabilistic measures. By viewing LLMs as\nabstract machines that process information through a natural language\ninterface, we examine the conditions under which it is possible to compute\nsuitable approximations of PN and PS. Our research marks an important step\ntowards gaining a deeper understanding of when LLMs are capable of reasoning,\nas illustrated by a series of math examples."
                },
                "authors": [
                    {
                        "name": "Javier González"
                    },
                    {
                        "name": "Aditya V. Nori"
                    }
                ],
                "author_detail": {
                    "name": "Aditya V. Nori"
                },
                "author": "Aditya V. Nori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08208v1",
                "updated": "2024-08-15T15:18:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    18,
                    46,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    18,
                    46,
                    3,
                    228,
                    0
                ],
                "title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems fundamentally rely on users' historical\ninteraction sequences, which are often contaminated by noisy interactions.\nIdentifying these noisy interactions accurately without additional information\nis particularly difficult due to the lack of explicit supervisory signals to\ndenote noise. Large Language Models (LLMs), equipped with extensive open\nknowledge and semantic reasoning abilities, present a promising avenue to\nbridge this information gap. However, employing LLMs for denoising in\nsequential recommendation introduces notable challenges: 1) Direct application\nof pretrained LLMs may not be competent for the denoising task, frequently\ngenerating nonsensical responses; 2) Even after fine-tuning, the reliability of\nLLM outputs remains questionable, especially given the complexity of the task\nand th inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing the\ncorrected sequences to be flexibly applied across various recommendation\nmodels. Extensive experiments validate the superiority of LLM4DSR over existing\nmethods across three datasets and three recommendation backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems fundamentally rely on users' historical\ninteraction sequences, which are often contaminated by noisy interactions.\nIdentifying these noisy interactions accurately without additional information\nis particularly difficult due to the lack of explicit supervisory signals to\ndenote noise. Large Language Models (LLMs), equipped with extensive open\nknowledge and semantic reasoning abilities, present a promising avenue to\nbridge this information gap. However, employing LLMs for denoising in\nsequential recommendation introduces notable challenges: 1) Direct application\nof pretrained LLMs may not be competent for the denoising task, frequently\ngenerating nonsensical responses; 2) Even after fine-tuning, the reliability of\nLLM outputs remains questionable, especially given the complexity of the task\nand th inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing the\ncorrected sequences to be flexibly applied across various recommendation\nmodels. Extensive experiments validate the superiority of LLM4DSR over existing\nmethods across three datasets and three recommendation backbones."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yudi Wu"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08202v1",
                "updated": "2024-08-15T15:10:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    10,
                    1,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T15:10:01Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    10,
                    1,
                    3,
                    228,
                    0
                ],
                "title": "Towards Practical Human Motion Prediction with LiDAR Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Human Motion Prediction with LiDAR Point Clouds"
                },
                "summary": "Human motion prediction is crucial for human-centric multimedia understanding\nand interacting. Current methods typically rely on ground truth human poses as\nobserved input, which is not practical for real-world scenarios where only raw\nvisual sensor data is available. To implement these methods in practice, a\npre-phrase of pose estimation is essential. However, such two-stage approaches\noften lead to performance degradation due to the accumulation of errors.\nMoreover, reducing raw visual data to sparse keypoint representations\nsignificantly diminishes the density of information, resulting in the loss of\nfine-grained features. In this paper, we propose \\textit{LiDAR-HMP}, the first\nsingle-LiDAR-based 3D human motion prediction approach, which receives the raw\nLiDAR point cloud as input and forecasts future 3D human poses directly.\nBuilding upon our novel structure-aware body feature descriptor, LiDAR-HMP\nadaptively maps the observed motion manifold to future poses and effectively\nmodels the spatial-temporal correlations of human motions for further\nrefinement of prediction results. Extensive experiments show that our method\nachieves state-of-the-art performance on two public benchmarks and demonstrates\nremarkable robustness and efficacy in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion prediction is crucial for human-centric multimedia understanding\nand interacting. Current methods typically rely on ground truth human poses as\nobserved input, which is not practical for real-world scenarios where only raw\nvisual sensor data is available. To implement these methods in practice, a\npre-phrase of pose estimation is essential. However, such two-stage approaches\noften lead to performance degradation due to the accumulation of errors.\nMoreover, reducing raw visual data to sparse keypoint representations\nsignificantly diminishes the density of information, resulting in the loss of\nfine-grained features. In this paper, we propose \\textit{LiDAR-HMP}, the first\nsingle-LiDAR-based 3D human motion prediction approach, which receives the raw\nLiDAR point cloud as input and forecasts future 3D human poses directly.\nBuilding upon our novel structure-aware body feature descriptor, LiDAR-HMP\nadaptively maps the observed motion manifold to future poses and effectively\nmodels the spatial-temporal correlations of human motions for further\nrefinement of prediction results. Extensive experiments show that our method\nachieves state-of-the-art performance on two public benchmarks and demonstrates\nremarkable robustness and efficacy in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Xiao Han"
                    },
                    {
                        "name": "Yiming Ren"
                    },
                    {
                        "name": "Yichen Yao"
                    },
                    {
                        "name": "Yujing Sun"
                    },
                    {
                        "name": "Yuexin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuexin Ma"
                },
                "author": "Yuexin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08188v1",
                "updated": "2024-08-15T14:46:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T14:46:13Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "title": "Scaling Up Natural Language Understanding for Multi-Robots Through the\n  Lens of Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Natural Language Understanding for Multi-Robots Through the\n  Lens of Hierarchy"
                },
                "summary": "Long-horizon planning is hindered by challenges such as uncertainty\naccumulation, computational complexity, delayed rewards and incomplete\ninformation. This work proposes an approach to exploit the task hierarchy from\nhuman instructions to facilitate multi-robot planning. Using Large Language\nModels (LLMs), we propose a two-step approach to translate multi-sentence\ninstructions into a structured language, Hierarchical Linear Temporal Logic\n(LTL), which serves as a formal representation for planning. Initially, LLMs\ntransform the instructions into a hierarchical representation defined as\nHierarchical Task Tree, capturing the logical and temporal relations among\ntasks. Following this, a domain-specific fine-tuning of LLM translates\nsub-tasks of each task into flat LTL formulas, aggregating them to form\nhierarchical LTL specifications. These specifications are then leveraged for\nplanning using off-the-shelf planners. Our framework not only bridges the gap\nbetween instructions and algorithmic planning but also showcases the potential\nof LLMs in harnessing hierarchical reasoning to automate multi-robot task\nplanning. Through evaluations in both simulation and real-world experiments\ninvolving human participants, we demonstrate that our method can handle more\ncomplex instructions compared to existing methods. The results indicate that\nour approach achieves higher success rates and lower costs in multi-robot task\nallocation and plan generation. Demos videos are available at\nhttps://youtu.be/7WOrDKxIMIs .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon planning is hindered by challenges such as uncertainty\naccumulation, computational complexity, delayed rewards and incomplete\ninformation. This work proposes an approach to exploit the task hierarchy from\nhuman instructions to facilitate multi-robot planning. Using Large Language\nModels (LLMs), we propose a two-step approach to translate multi-sentence\ninstructions into a structured language, Hierarchical Linear Temporal Logic\n(LTL), which serves as a formal representation for planning. Initially, LLMs\ntransform the instructions into a hierarchical representation defined as\nHierarchical Task Tree, capturing the logical and temporal relations among\ntasks. Following this, a domain-specific fine-tuning of LLM translates\nsub-tasks of each task into flat LTL formulas, aggregating them to form\nhierarchical LTL specifications. These specifications are then leveraged for\nplanning using off-the-shelf planners. Our framework not only bridges the gap\nbetween instructions and algorithmic planning but also showcases the potential\nof LLMs in harnessing hierarchical reasoning to automate multi-robot task\nplanning. Through evaluations in both simulation and real-world experiments\ninvolving human participants, we demonstrate that our method can handle more\ncomplex instructions compared to existing methods. The results indicate that\nour approach achieves higher success rates and lower costs in multi-robot task\nallocation and plan generation. Demos videos are available at\nhttps://youtu.be/7WOrDKxIMIs ."
                },
                "authors": [
                    {
                        "name": "Shaojun Xu"
                    },
                    {
                        "name": "Xusheng Luo"
                    },
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Letian Leng"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06162v3",
                "updated": "2024-08-15T13:59:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    59,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-09T09:34:25Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    9,
                    34,
                    25,
                    1,
                    100,
                    0
                ],
                "title": "Characterizing Multimodal Long-form Summarization: A Case Study on\n  Financial Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Multimodal Long-form Summarization: A Case Study on\n  Financial Reports"
                },
                "summary": "As large language models (LLMs) expand the power of natural language\nprocessing to handle long inputs, rigorous and systematic analyses are\nnecessary to understand their abilities and behavior. A salient application is\nsummarization, due to its ubiquity and controversy (e.g., researchers have\ndeclared the death of summarization). In this paper, we use financial report\nsummarization as a case study because financial reports are not only long but\nalso use numbers and tables extensively. We propose a computational framework\nfor characterizing multimodal long-form summarization and investigate the\nbehavior of Claude 2.0/2.1, GPT-4/3.5, and Cohere. We find that GPT-3.5 and\nCohere fail to perform this summarization task meaningfully. For Claude 2 and\nGPT-4, we analyze the extractiveness of the summary and identify a position\nbias in LLMs. This position bias disappears after shuffling the input for\nClaude, which suggests that Claude seems to recognize important information. We\nalso conduct a comprehensive investigation on the use of numeric data in\nLLM-generated summaries and offer a taxonomy of numeric hallucination. We\nemploy prompt engineering to improve GPT-4's use of numbers with limited\nsuccess. Overall, our analyses highlight the strong capability of Claude 2 in\nhandling long multimodal inputs compared to GPT-4. The generated summaries and\nevaluation code are available at\nhttps://github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) expand the power of natural language\nprocessing to handle long inputs, rigorous and systematic analyses are\nnecessary to understand their abilities and behavior. A salient application is\nsummarization, due to its ubiquity and controversy (e.g., researchers have\ndeclared the death of summarization). In this paper, we use financial report\nsummarization as a case study because financial reports are not only long but\nalso use numbers and tables extensively. We propose a computational framework\nfor characterizing multimodal long-form summarization and investigate the\nbehavior of Claude 2.0/2.1, GPT-4/3.5, and Cohere. We find that GPT-3.5 and\nCohere fail to perform this summarization task meaningfully. For Claude 2 and\nGPT-4, we analyze the extractiveness of the summary and identify a position\nbias in LLMs. This position bias disappears after shuffling the input for\nClaude, which suggests that Claude seems to recognize important information. We\nalso conduct a comprehensive investigation on the use of numeric data in\nLLM-generated summaries and offer a taxonomy of numeric hallucination. We\nemploy prompt engineering to improve GPT-4's use of numbers with limited\nsuccess. Overall, our analyses highlight the strong capability of Claude 2 in\nhandling long multimodal inputs compared to GPT-4. The generated summaries and\nevaluation code are available at\nhttps://github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization."
                },
                "authors": [
                    {
                        "name": "Tianyu Cao"
                    },
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Danial Dervovic"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08158v1",
                "updated": "2024-08-15T13:48:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    48,
                    44,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:48:44Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    48,
                    44,
                    3,
                    228,
                    0
                ],
                "title": "EmBARDiment: an Embodied AI Agent for Productivity in XR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmBARDiment: an Embodied AI Agent for Productivity in XR"
                },
                "summary": "XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents."
                },
                "authors": [
                    {
                        "name": "Riccardo Bovo"
                    },
                    {
                        "name": "Steven Abreu"
                    },
                    {
                        "name": "Karan Ahuja"
                    },
                    {
                        "name": "Eric J Gonzalez"
                    },
                    {
                        "name": "Li-Te Cheng"
                    },
                    {
                        "name": "Mar Gonzalez-Franco"
                    }
                ],
                "author_detail": {
                    "name": "Mar Gonzalez-Franco"
                },
                "author": "Mar Gonzalez-Franco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09228v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09228v5",
                "updated": "2024-08-15T13:46:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    46,
                    6,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-14T12:15:21Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    12,
                    15,
                    21,
                    6,
                    105,
                    0
                ],
                "title": "A Survey on Integration of Large Language Models with Intelligent Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Integration of Large Language Models with Intelligent Robots"
                },
                "summary": "In recent years, the integration of large language models (LLMs) has\nrevolutionized the field of robotics, enabling robots to communicate,\nunderstand, and reason with human-like proficiency. This paper explores the\nmultifaceted impact of LLMs on robotics, addressing key challenges and\nopportunities for leveraging these models across various domains. By\ncategorizing and analyzing LLM applications within core robotics elements --\ncommunication, perception, planning, and control -- we aim to provide\nactionable insights for researchers seeking to integrate LLMs into their\nrobotic systems. Our investigation focuses on LLMs developed post-GPT-3.5,\nprimarily in text-based modalities while also considering multimodal approaches\nfor perception and control. We offer comprehensive guidelines and examples for\nprompt engineering, facilitating beginners' access to LLM-based robotics\nsolutions. Through tutorial-level examples and structured prompt construction,\nwe illustrate how LLM-guided enhancements can be seamlessly integrated into\nrobotics applications. This survey serves as a roadmap for researchers\nnavigating the evolving landscape of LLM-driven robotics, offering a\ncomprehensive overview and practical guidance for harnessing the power of\nlanguage models in robotics development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the integration of large language models (LLMs) has\nrevolutionized the field of robotics, enabling robots to communicate,\nunderstand, and reason with human-like proficiency. This paper explores the\nmultifaceted impact of LLMs on robotics, addressing key challenges and\nopportunities for leveraging these models across various domains. By\ncategorizing and analyzing LLM applications within core robotics elements --\ncommunication, perception, planning, and control -- we aim to provide\nactionable insights for researchers seeking to integrate LLMs into their\nrobotic systems. Our investigation focuses on LLMs developed post-GPT-3.5,\nprimarily in text-based modalities while also considering multimodal approaches\nfor perception and control. We offer comprehensive guidelines and examples for\nprompt engineering, facilitating beginners' access to LLM-based robotics\nsolutions. Through tutorial-level examples and structured prompt construction,\nwe illustrate how LLM-guided enhancements can be seamlessly integrated into\nrobotics applications. This survey serves as a roadmap for researchers\nnavigating the evolving landscape of LLM-driven robotics, offering a\ncomprehensive overview and practical guidance for harnessing the power of\nlanguage models in robotics development."
                },
                "authors": [
                    {
                        "name": "Yeseung Kim"
                    },
                    {
                        "name": "Dohyun Kim"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Jisang Park"
                    },
                    {
                        "name": "Nayoung Oh"
                    },
                    {
                        "name": "Daehyung Park"
                    }
                ],
                "author_detail": {
                    "name": "Daehyung Park"
                },
                "author": "Daehyung Park",
                "arxiv_doi": "10.1007/s11370-024-00550-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11370-024-00550-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.09228v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09228v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 5 figures, Published in Intelligent Service Robotics (ISR)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08147v1",
                "updated": "2024-08-15T13:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    32,
                    25,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:32:25Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    32,
                    25,
                    3,
                    228,
                    0
                ],
                "title": "P/D-Serve: Serving Disaggregated Large Language Model at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P/D-Serve: Serving Disaggregated Large Language Model at Scale"
                },
                "summary": "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs."
                },
                "authors": [
                    {
                        "name": "Yibo Jin"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Yipeng Ma"
                    },
                    {
                        "name": "Yicheng Shan"
                    },
                    {
                        "name": "Zhengfan Yuan"
                    },
                    {
                        "name": "Cailong Li"
                    },
                    {
                        "name": "Yajing Sun"
                    },
                    {
                        "name": "Tiandeng Wu"
                    },
                    {
                        "name": "Xing Chu"
                    },
                    {
                        "name": "Ruizhi Huan"
                    },
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Xiao You"
                    },
                    {
                        "name": "Wenting Zhou"
                    },
                    {
                        "name": "Yunpeng Ye"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Xiangkun Xu"
                    },
                    {
                        "name": "Yongsheng Zhang"
                    },
                    {
                        "name": "Tiantian Dong"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Xijian Ju"
                    },
                    {
                        "name": "Jianxun Song"
                    },
                    {
                        "name": "Haoliang Cheng"
                    },
                    {
                        "name": "Xiaojing Li"
                    },
                    {
                        "name": "Jiandong Ding"
                    },
                    {
                        "name": "Hefei Guo"
                    },
                    {
                        "name": "Zhengyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyong Zhang"
                },
                "author": "Zhengyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08146v1",
                "updated": "2024-08-15T13:29:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    29,
                    48,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:29:48Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    29,
                    48,
                    3,
                    228,
                    0
                ],
                "title": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft\n  Heads with Adversarial Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft\n  Heads with Adversarial Learning"
                },
                "summary": "Large Language Models (LLMs) exhibit high inference latency due to their\nautoregressive decoding nature. While the draft head in speculative decoding\nmitigates this issue, its full potential remains unexplored. In this paper, we\nintroduce KOALA (K-layer Optimized Adversarial Learning Architecture), an\northogonal approach to the draft head. By transforming the conventional\nsingle-layer draft head into a multi-layer architecture and incorporating\nadversarial learning into the traditional supervised training, KOALA\nsignificantly improves the accuracy of the draft head in predicting subsequent\ntokens, thus more closely mirroring the functionality of LLMs. Although this\nimprovement comes at the cost of slightly increased drafting overhead, KOALA\nsubstantially unlocks the draft head's potential, greatly enhancing speculative\ndecoding. We conducted comprehensive evaluations of KOALA, including both\nautoregressive and non-autoregressive draft heads across various tasks,\ndemonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is\n10.57%-14.09% faster than the original draft heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit high inference latency due to their\nautoregressive decoding nature. While the draft head in speculative decoding\nmitigates this issue, its full potential remains unexplored. In this paper, we\nintroduce KOALA (K-layer Optimized Adversarial Learning Architecture), an\northogonal approach to the draft head. By transforming the conventional\nsingle-layer draft head into a multi-layer architecture and incorporating\nadversarial learning into the traditional supervised training, KOALA\nsignificantly improves the accuracy of the draft head in predicting subsequent\ntokens, thus more closely mirroring the functionality of LLMs. Although this\nimprovement comes at the cost of slightly increased drafting overhead, KOALA\nsubstantially unlocks the draft head's potential, greatly enhancing speculative\ndecoding. We conducted comprehensive evaluations of KOALA, including both\nautoregressive and non-autoregressive draft heads across various tasks,\ndemonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is\n10.57%-14.09% faster than the original draft heads."
                },
                "authors": [
                    {
                        "name": "Kaiqi Zhang"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Rui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rui Chen"
                },
                "author": "Rui Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08144v1",
                "updated": "2024-08-15T13:28:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    28,
                    18,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T13:28:18Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    28,
                    18,
                    3,
                    228,
                    0
                ],
                "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU"
                },
                "summary": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "So-Eon Kim"
                    },
                    {
                        "name": "Seong-Bae Park"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    }
                ],
                "author_detail": {
                    "name": "Soyeon Caren Han"
                },
                "author": "Soyeon Caren Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15706v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15706v6",
                "updated": "2024-08-15T12:25:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    25,
                    39,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T15:16:47Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    16,
                    47,
                    0,
                    204,
                    0
                ],
                "title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition"
                },
                "summary": "Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action."
                },
                "authors": [
                    {
                        "name": "Jinfu Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mengyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Liu"
                },
                "author": "Mengyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15706v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15706v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08105v1",
                "updated": "2024-08-15T12:04:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    4,
                    32,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T12:04:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    4,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language\n  Models to Infer Causal Links Between Siamese Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language\n  Models to Infer Causal Links Between Siamese Images"
                },
                "summary": "Large Language Models (LLMs) have showcased exceptional ability in causal\nreasoning from textual information. However, will these causalities remain\nstraightforward for Vision Large Language Models (VLLMs) when only visual hints\nare provided? Motivated by this, we propose a novel Multimodal Causal Reasoning\nbenchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect\nrelationship when solely relying on visual cues such as action, appearance,\nclothing, and environment. Specifically, we introduce a prompt-driven image\nsynthesis approach to create siamese images with embedded semantic causality\nand visual cues, which can effectively evaluate VLLMs' causal reasoning\ncapabilities. Additionally, we develop tailored metrics from multiple\nperspectives, including image-level match, phrase-level understanding, and\nsentence-level explanation, to comprehensively assess VLLMs' comprehension\nabilities. Our extensive experiments reveal that the current state-of-the-art\nVLLMs are not as skilled at multimodal causal reasoning as we might have hoped.\nFurthermore, we perform a comprehensive analysis to understand these models'\nshortcomings from different views and suggest directions for future research.\nWe hope MuCR can serve as a valuable resource and foundational benchmark in\nmultimodal causal reasoning research. The project is available at:\nhttps://github.com/Zhiyuan-Li-John/MuCR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased exceptional ability in causal\nreasoning from textual information. However, will these causalities remain\nstraightforward for Vision Large Language Models (VLLMs) when only visual hints\nare provided? Motivated by this, we propose a novel Multimodal Causal Reasoning\nbenchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect\nrelationship when solely relying on visual cues such as action, appearance,\nclothing, and environment. Specifically, we introduce a prompt-driven image\nsynthesis approach to create siamese images with embedded semantic causality\nand visual cues, which can effectively evaluate VLLMs' causal reasoning\ncapabilities. Additionally, we develop tailored metrics from multiple\nperspectives, including image-level match, phrase-level understanding, and\nsentence-level explanation, to comprehensively assess VLLMs' comprehension\nabilities. Our extensive experiments reveal that the current state-of-the-art\nVLLMs are not as skilled at multimodal causal reasoning as we might have hoped.\nFurthermore, we perform a comprehensive analysis to understand these models'\nshortcomings from different views and suggest directions for future research.\nWe hope MuCR can serve as a valuable resource and foundational benchmark in\nmultimodal causal reasoning research. The project is available at:\nhttps://github.com/Zhiyuan-Li-John/MuCR"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Dongnan Liu"
                    },
                    {
                        "name": "Chaoyi Zhang"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Jieting Long"
                    },
                    {
                        "name": "Weidong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Cai"
                },
                "author": "Weidong Cai",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08090v1",
                "updated": "2024-08-15T11:33:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    33,
                    51,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:33:51Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    33,
                    51,
                    3,
                    228,
                    0
                ],
                "title": "UV-Plane Beam Mapping for Non-Terrestrial Networks in 3GPP System-Level\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UV-Plane Beam Mapping for Non-Terrestrial Networks in 3GPP System-Level\n  Simulations"
                },
                "summary": "Due to the high altitudes and large beam sizes of satellites, the curvature\nof the Earth's surface can impact system-level performance. To consider this,\n3GPP introduces the UV-plane beam mapping for system-level simulations of\nnon-terrestrial networks (NTNs). This paper aims to provide a comprehensive\nunderstanding of how beams and user equipments (UEs) are placed on the UV-plane\nand subsequently mapped to the Earth's surface. We present a general process of\nprojecting UEs on the UV-plane onto the Earth's surface. This process could\noffer a useful guideline for beam and UE deployment when evaluating the\nsystem-level performance of NTNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high altitudes and large beam sizes of satellites, the curvature\nof the Earth's surface can impact system-level performance. To consider this,\n3GPP introduces the UV-plane beam mapping for system-level simulations of\nnon-terrestrial networks (NTNs). This paper aims to provide a comprehensive\nunderstanding of how beams and user equipments (UEs) are placed on the UV-plane\nand subsequently mapped to the Earth's surface. We present a general process of\nprojecting UEs on the UV-plane onto the Earth's surface. This process could\noffer a useful guideline for beam and UE deployment when evaluating the\nsystem-level performance of NTNs."
                },
                "authors": [
                    {
                        "name": "Dong-Hyun Jung"
                    },
                    {
                        "name": "Sucheol Kim"
                    },
                    {
                        "name": "Miyeon Lee"
                    },
                    {
                        "name": "Joon-Gyu Ryu"
                    },
                    {
                        "name": "Junil Choi"
                    }
                ],
                "author_detail": {
                    "name": "Junil Choi"
                },
                "author": "Junil Choi",
                "arxiv_comment": "5 pages, 9 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08089v1",
                "updated": "2024-08-15T11:33:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    33,
                    20,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:33:20Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    33,
                    20,
                    3,
                    228,
                    0
                ],
                "title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents"
                },
                "summary": "In this paper, we present a simulation system called AgentCourt that\nsimulates the entire courtroom process. The judge, plaintiff's lawyer, defense\nlawyer, and other participants are autonomous agents driven by large language\nmodels (LLMs). Our core goal is to enable lawyer agents to learn how to argue a\ncase, as well as improving their overall legal skills, through courtroom\nprocess simulation. To achieve this goal, we propose an adversarial\nevolutionary approach for the lawyer-agent. Since AgentCourt can simulate the\noccurrence and development of court hearings based on a knowledge base and LLM,\nthe lawyer agents can continuously learn and accumulate experience from real\ncourt cases. The simulation experiments show that after two lawyer-agents have\nengaged in a thousand adversarial legal cases in AgentCourt (which can take a\ndecade for real-world lawyers), compared to their pre-evolutionary state, the\nevolved lawyer agents exhibit consistent improvement in their ability to handle\nlegal tasks. To enhance the credibility of our experimental results, we\nenlisted a panel of professional lawyers to evaluate our simulations. The\nevaluation indicates that the evolved lawyer agents exhibit notable\nadvancements in responsiveness, as well as expertise and logical rigor. This\nwork paves the way for advancing LLM-driven agent technology in legal\nscenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a simulation system called AgentCourt that\nsimulates the entire courtroom process. The judge, plaintiff's lawyer, defense\nlawyer, and other participants are autonomous agents driven by large language\nmodels (LLMs). Our core goal is to enable lawyer agents to learn how to argue a\ncase, as well as improving their overall legal skills, through courtroom\nprocess simulation. To achieve this goal, we propose an adversarial\nevolutionary approach for the lawyer-agent. Since AgentCourt can simulate the\noccurrence and development of court hearings based on a knowledge base and LLM,\nthe lawyer agents can continuously learn and accumulate experience from real\ncourt cases. The simulation experiments show that after two lawyer-agents have\nengaged in a thousand adversarial legal cases in AgentCourt (which can take a\ndecade for real-world lawyers), compared to their pre-evolutionary state, the\nevolved lawyer agents exhibit consistent improvement in their ability to handle\nlegal tasks. To enhance the credibility of our experimental results, we\nenlisted a panel of professional lawyers to evaluate our simulations. The\nevaluation indicates that the evolved lawyer agents exhibit notable\nadvancements in responsiveness, as well as expertise and logical rigor. This\nwork paves the way for advancing LLM-driven agent technology in legal\nscenarios. Code is available at https://github.com/relic-yuexi/AgentCourt."
                },
                "authors": [
                    {
                        "name": "Guhong Chen"
                    },
                    {
                        "name": "Liyang Fan"
                    },
                    {
                        "name": "Zihan Gong"
                    },
                    {
                        "name": "Nan Xie"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08088v1",
                "updated": "2024-08-15T11:32:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    32,
                    46,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:32:46Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    32,
                    46,
                    3,
                    228,
                    0
                ],
                "title": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber\n  Threat Intelligence Credibility Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber\n  Threat Intelligence Credibility Assessment"
                },
                "summary": "Cyber threat intelligence is a critical tool that many organizations and\nindividuals use to protect themselves from sophisticated, organized,\npersistent, and weaponized cyber attacks. However, few studies have focused on\nthe quality assessment of threat intelligence provided by intelligence\nplatforms, and this work still requires manual analysis by cybersecurity\nexperts. In this paper, we propose a knowledge graph-based verifier, a novel\nCyber Threat Intelligence (CTI) quality assessment framework that combines\nknowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs\nto automatically extract OSCTI key claims to be verified and utilizes a\nknowledge graph consisting of paragraphs for fact-checking. This method differs\nfrom the traditional way of constructing complex knowledge graphs with entities\nas nodes. By constructing knowledge graphs with paragraphs as nodes and\nsemantic similarity as edges, it effectively enhances the semantic\nunderstanding ability of the model and simplifies labeling requirements.\nAdditionally, to fill the gap in the research field, we created and made public\nthe first dataset for threat intelligence assessment from heterogeneous\nsources. To the best of our knowledge, this work is the first to create a\ndataset on threat intelligence reliability verification, providing a reference\nfor future research. Experimental results show that KGV (Knowledge Graph\nVerifier) significantly improves the performance of LLMs in intelligence\nquality assessment. Compared with traditional methods, we reduce a large amount\nof data annotation while the model still exhibits strong reasoning\ncapabilities. Finally, our method can achieve XXX accuracy in network threat\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber threat intelligence is a critical tool that many organizations and\nindividuals use to protect themselves from sophisticated, organized,\npersistent, and weaponized cyber attacks. However, few studies have focused on\nthe quality assessment of threat intelligence provided by intelligence\nplatforms, and this work still requires manual analysis by cybersecurity\nexperts. In this paper, we propose a knowledge graph-based verifier, a novel\nCyber Threat Intelligence (CTI) quality assessment framework that combines\nknowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs\nto automatically extract OSCTI key claims to be verified and utilizes a\nknowledge graph consisting of paragraphs for fact-checking. This method differs\nfrom the traditional way of constructing complex knowledge graphs with entities\nas nodes. By constructing knowledge graphs with paragraphs as nodes and\nsemantic similarity as edges, it effectively enhances the semantic\nunderstanding ability of the model and simplifies labeling requirements.\nAdditionally, to fill the gap in the research field, we created and made public\nthe first dataset for threat intelligence assessment from heterogeneous\nsources. To the best of our knowledge, this work is the first to create a\ndataset on threat intelligence reliability verification, providing a reference\nfor future research. Experimental results show that KGV (Knowledge Graph\nVerifier) significantly improves the performance of LLMs in intelligence\nquality assessment. Compared with traditional methods, we reduce a large amount\nof data annotation while the model still exhibits strong reasoning\ncapabilities. Finally, our method can achieve XXX accuracy in network threat\nassessment."
                },
                "authors": [
                    {
                        "name": "Zongzong Wu"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Yufeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Li"
                },
                "author": "Yufeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08083v1",
                "updated": "2024-08-15T11:16:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    16,
                    21,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T11:16:21Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    11,
                    16,
                    21,
                    3,
                    228,
                    0
                ],
                "title": "Confidence-weighted integration of human and machine judgments for\n  superior decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-weighted integration of human and machine judgments for\n  superior decision-making"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools in various\ndomains. Recent studies have shown that LLMs can surpass humans in certain\ntasks, such as predicting the outcomes of neuroscience studies. What role does\nthis leave for humans in the overall decision process? One possibility is that\nhumans, despite performing worse than LLMs, can still add value when teamed\nwith them. A human and machine team can surpass each individual teammate when\nteam members' confidence is well-calibrated and team members diverge in which\ntasks they find difficult (i.e., calibration and diversity are needed). We\nsimplified and extended a Bayesian approach to combining judgments using a\nlogistic regression framework that integrates confidence-weighted judgments for\nany number of team members. Using this straightforward method, we demonstrated\nin a neuroscience forecasting task that, even when humans were inferior to\nLLMs, their combination with one or more LLMs consistently improved team\nperformance. Our hope is that this simple and effective strategy for\nintegrating the judgments of humans and machines will lead to productive\ncollaborations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools in various\ndomains. Recent studies have shown that LLMs can surpass humans in certain\ntasks, such as predicting the outcomes of neuroscience studies. What role does\nthis leave for humans in the overall decision process? One possibility is that\nhumans, despite performing worse than LLMs, can still add value when teamed\nwith them. A human and machine team can surpass each individual teammate when\nteam members' confidence is well-calibrated and team members diverge in which\ntasks they find difficult (i.e., calibration and diversity are needed). We\nsimplified and extended a Bayesian approach to combining judgments using a\nlogistic regression framework that integrates confidence-weighted judgments for\nany number of team members. Using this straightforward method, we demonstrated\nin a neuroscience forecasting task that, even when humans were inferior to\nLLMs, their combination with one or more LLMs consistently improved team\nperformance. Our hope is that this simple and effective strategy for\nintegrating the judgments of humans and machines will lead to productive\ncollaborations."
                },
                "authors": [
                    {
                        "name": "Felipe Yáñez"
                    },
                    {
                        "name": "Xiaoliang Luo"
                    },
                    {
                        "name": "Omar Valerio Minero"
                    },
                    {
                        "name": "Bradley C. Love"
                    }
                ],
                "author_detail": {
                    "name": "Bradley C. Love"
                },
                "author": "Bradley C. Love",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07440v2",
                "updated": "2024-08-15T10:45:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    45,
                    18,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T10:18:42Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    18,
                    42,
                    2,
                    227,
                    0
                ],
                "title": "BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt\n  Learning"
                },
                "summary": "Medical foundation models are gaining prominence in the medical community for\ntheir ability to derive general representations from extensive collections of\nmedical image-text pairs. Recent research indicates that these models are\nsusceptible to backdoor attacks, which allow them to classify clean images\naccurately but fail when specific triggers are introduced. However, traditional\nbackdoor attacks necessitate a considerable amount of additional data to\nmaliciously pre-train a model. This requirement is often impractical in medical\nimaging applications due to the usual scarcity of data. Inspired by the latest\ndevelopments in learnable prompts, this work introduces a method to embed a\nbackdoor into the medical foundation model during the prompt learning phase. By\nincorporating learnable prompts within the text encoder and introducing\nimperceptible learnable noise trigger to the input images, we exploit the full\ncapabilities of the medical foundation models (Med-FM). Our method, BAPLe,\nrequires only a minimal subset of data to adjust the noise trigger and the text\nprompts for downstream tasks, enabling the creation of an effective backdoor\nattack. Through extensive experiments with four medical foundation models, each\npre-trained on different modalities and evaluated across six downstream\ndatasets, we demonstrate the efficacy of our approach. BAPLe achieves a high\nbackdoor success rate across all models and datasets, outperforming the\nbaseline backdoor attack methods. Our work highlights the vulnerability of\nMed-FMs towards backdoor attacks and strives to promote the safe adoption of\nMed-FMs before their deployment in real-world applications. Code is available\nat https://asif-hanif.github.io/baple/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical foundation models are gaining prominence in the medical community for\ntheir ability to derive general representations from extensive collections of\nmedical image-text pairs. Recent research indicates that these models are\nsusceptible to backdoor attacks, which allow them to classify clean images\naccurately but fail when specific triggers are introduced. However, traditional\nbackdoor attacks necessitate a considerable amount of additional data to\nmaliciously pre-train a model. This requirement is often impractical in medical\nimaging applications due to the usual scarcity of data. Inspired by the latest\ndevelopments in learnable prompts, this work introduces a method to embed a\nbackdoor into the medical foundation model during the prompt learning phase. By\nincorporating learnable prompts within the text encoder and introducing\nimperceptible learnable noise trigger to the input images, we exploit the full\ncapabilities of the medical foundation models (Med-FM). Our method, BAPLe,\nrequires only a minimal subset of data to adjust the noise trigger and the text\nprompts for downstream tasks, enabling the creation of an effective backdoor\nattack. Through extensive experiments with four medical foundation models, each\npre-trained on different modalities and evaluated across six downstream\ndatasets, we demonstrate the efficacy of our approach. BAPLe achieves a high\nbackdoor success rate across all models and datasets, outperforming the\nbaseline backdoor attack methods. Our work highlights the vulnerability of\nMed-FMs towards backdoor attacks and strives to promote the safe adoption of\nMed-FMs before their deployment in real-world applications. Code is available\nat https://asif-hanif.github.io/baple/."
                },
                "authors": [
                    {
                        "name": "Asif Hanif"
                    },
                    {
                        "name": "Fahad Shamshad"
                    },
                    {
                        "name": "Muhammad Awais"
                    },
                    {
                        "name": "Muzammal Naseer"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Karthik Nandakumar"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "MICCAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08072v1",
                "updated": "2024-08-15T10:44:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T10:44:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09170v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09170v4",
                "updated": "2024-08-15T10:12:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    12,
                    54,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-14T07:19:27Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    7,
                    19,
                    27,
                    6,
                    105,
                    0
                ],
                "title": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking"
                },
                "summary": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaoshu Chen"
                    },
                    {
                        "name": "Sihang Zhou"
                    },
                    {
                        "name": "Ke Liang"
                    },
                    {
                        "name": "Xinwang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinwang Liu"
                },
                "author": "Xinwang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09170v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09170v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20770v2",
                "updated": "2024-08-15T10:03:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    40,
                    3,
                    228,
                    0
                ],
                "published": "2024-05-24T07:23:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    7,
                    23,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Sentinel: LLM Agent for Adversarial Purification"
                },
                "summary": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness."
                },
                "authors": [
                    {
                        "name": "Guang Lin"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13764v3",
                "updated": "2024-08-15T10:03:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2023-12-21T11:43:41Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    11,
                    43,
                    41,
                    3,
                    355,
                    0
                ],
                "title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties"
                },
                "summary": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab."
                },
                "authors": [
                    {
                        "name": "Junfei Xiao"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Wenxuan Li"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "arxiv_comment": "Accepted to ECCV 2024. Code is available at\n  https://github.com/lambert-x/ProLab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16663v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16663v4",
                "updated": "2024-08-15T10:03:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    16,
                    3,
                    228,
                    0
                ],
                "published": "2024-04-25T15:04:27Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    15,
                    4,
                    27,
                    3,
                    116,
                    0
                ],
                "title": "Conditional Fairness for Generative AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Fairness for Generative AIs"
                },
                "summary": "The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires \"conditional fairness\"\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for accessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires \"conditional fairness\"\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for accessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems."
                },
                "authors": [
                    {
                        "name": "Chih-Hong Cheng"
                    },
                    {
                        "name": "Harald Ruess"
                    },
                    {
                        "name": "Changshun Wu"
                    },
                    {
                        "name": "Xingyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xingyu Zhao"
                },
                "author": "Xingyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16663v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16663v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08054v1",
                "updated": "2024-08-15T09:48:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T09:48:45Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "title": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework"
                },
                "summary": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting."
                },
                "authors": [
                    {
                        "name": "Changyu Du"
                    },
                    {
                        "name": "Sebastian Esser"
                    },
                    {
                        "name": "Stavros Nousias"
                    },
                    {
                        "name": "André Borrmann"
                    }
                ],
                "author_detail": {
                    "name": "André Borrmann"
                },
                "author": "André Borrmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07537v2",
                "updated": "2024-08-15T09:30:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    30,
                    35,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T13:14:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    14,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report"
                },
                "summary": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions)."
                },
                "authors": [
                    {
                        "name": "Winnie Bahati Mbaka"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08027v1",
                "updated": "2024-08-15T08:50:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    50,
                    58,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T08:50:58Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    50,
                    58,
                    3,
                    228,
                    0
                ],
                "title": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model-based Speech Recognition by\n  Contextualization for Rare and Ambiguous Words"
                },
                "summary": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a large language model (LLM) based automatic speech recognition\n(ASR) system that can be contextualized by providing keywords as prior\ninformation in text prompts. We adopt decoder-only architecture and use our\nin-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by\nJapanese and English texts as the decoder. We adopt a pre-trained Whisper\nencoder as an audio encoder, and the audio embeddings from the audio encoder\nare projected to the text embedding space by an adapter layer and concatenated\nwith text embeddings converted from text prompts to form inputs to the decoder.\nBy providing keywords as prior information in the text prompts, we can\ncontextualize our LLM-based ASR system without modifying the model architecture\nto transcribe ambiguous words in the input audio accurately. Experimental\nresults demonstrate that providing keywords to the decoder can significantly\nimprove the recognition performance of rare and ambiguous words."
                },
                "authors": [
                    {
                        "name": "Kento Nozawa"
                    },
                    {
                        "name": "Takashi Masuko"
                    },
                    {
                        "name": "Toru Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toru Taniguchi"
                },
                "author": "Toru Taniguchi",
                "arxiv_comment": "13 pages, 1 figure, and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07990v1",
                "updated": "2024-08-15T07:37:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    37,
                    24,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:37:24Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    37,
                    24,
                    3,
                    228,
                    0
                ],
                "title": "FuseChat: Knowledge Fusion of Chat Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseChat: Knowledge Fusion of Chat Models"
                },
                "summary": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}."
                },
                "authors": [
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07986v1",
                "updated": "2024-08-15T07:25:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    25,
                    52,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:25:52Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    25,
                    52,
                    3,
                    228,
                    0
                ],
                "title": "Experimental evaluation of offline reinforcement learning for HVAC\n  control in buildings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental evaluation of offline reinforcement learning for HVAC\n  control in buildings"
                },
                "summary": "Reinforcement learning (RL) techniques have been increasingly investigated\nfor dynamic HVAC control in buildings. However, most studies focus on exploring\nsolutions in online or off-policy scenarios without discussing in detail the\nimplementation feasibility or effectiveness of dealing with purely offline\ndatasets or trajectories. The lack of these works limits the real-world\ndeployment of RL-based HVAC controllers, especially considering the abundance\nof historical data. To this end, this paper comprehensively evaluates the\nstrengths and limitations of state-of-the-art offline RL algorithms by\nconducting analytical and numerical studies. The analysis is conducted from two\nperspectives: algorithms and dataset characteristics. As a prerequisite, the\nnecessity of applying offline RL algorithms is first confirmed in two building\nenvironments. The ability of observation history modeling to reduce violations\nand enhance performance is subsequently studied. Next, the performance of\nRL-based controllers under datasets with different qualitative and quantitative\nconditions is investigated, including constraint satisfaction and power\nconsumption. Finally, the sensitivity of certain hyperparameters is also\nevaluated. The results indicate that datasets of a certain suboptimality level\nand relatively small scale can be utilized to effectively train a\nwell-performed RL-based HVAC controller. Specifically, such controllers can\nreduce at most 28.5% violation ratios of indoor temperatures and achieve at\nmost 12.1% power savings compared to the baseline controller. In summary, this\npaper presents our well-structured investigations and new findings when\napplying offline reinforcement learning to building HVAC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) techniques have been increasingly investigated\nfor dynamic HVAC control in buildings. However, most studies focus on exploring\nsolutions in online or off-policy scenarios without discussing in detail the\nimplementation feasibility or effectiveness of dealing with purely offline\ndatasets or trajectories. The lack of these works limits the real-world\ndeployment of RL-based HVAC controllers, especially considering the abundance\nof historical data. To this end, this paper comprehensively evaluates the\nstrengths and limitations of state-of-the-art offline RL algorithms by\nconducting analytical and numerical studies. The analysis is conducted from two\nperspectives: algorithms and dataset characteristics. As a prerequisite, the\nnecessity of applying offline RL algorithms is first confirmed in two building\nenvironments. The ability of observation history modeling to reduce violations\nand enhance performance is subsequently studied. Next, the performance of\nRL-based controllers under datasets with different qualitative and quantitative\nconditions is investigated, including constraint satisfaction and power\nconsumption. Finally, the sensitivity of certain hyperparameters is also\nevaluated. The results indicate that datasets of a certain suboptimality level\nand relatively small scale can be utilized to effectively train a\nwell-performed RL-based HVAC controller. Specifically, such controllers can\nreduce at most 28.5% violation ratios of indoor temperatures and achieve at\nmost 12.1% power savings compared to the baseline controller. In summary, this\npaper presents our well-structured investigations and new findings when\napplying offline reinforcement learning to building HVAC systems."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Linyan Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yang"
                },
                "author": "Yu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12017v2",
                "updated": "2024-08-15T07:12:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    12,
                    33,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-27T07:16:46Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    16,
                    46,
                    3,
                    179,
                    0
                ],
                "title": "Follow-Up Questions Improve Documents Generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Up Questions Improve Documents Generated by Large Language Models"
                },
                "summary": "This study investigates the impact of Large Language Models (LLMs) generating\nfollow-up questions in response to user requests for short (1-page) text\ndocuments. Users interacted with a novel web-based AI system designed to ask\nfollow-up questions. Users requested documents they would like the AI to\nproduce. The AI then generated follow-up questions to clarify the user's needs\nor offer additional insights before generating the requested documents. After\nanswering the questions, users were shown a document generated using both the\ninitial request and the questions and answers, and a document generated using\nonly the initial request. Users indicated which document they preferred and\ngave feedback about their experience with the question-answering process. The\nfindings of this study show clear benefits to question-asking both in document\npreference and in the qualitative user experience. This study further shows\nthat users found more value in questions which were thought-provoking,\nopen-ended, or offered unique insights into the user's request as opposed to\nsimple information-gathering questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of Large Language Models (LLMs) generating\nfollow-up questions in response to user requests for short (1-page) text\ndocuments. Users interacted with a novel web-based AI system designed to ask\nfollow-up questions. Users requested documents they would like the AI to\nproduce. The AI then generated follow-up questions to clarify the user's needs\nor offer additional insights before generating the requested documents. After\nanswering the questions, users were shown a document generated using both the\ninitial request and the questions and answers, and a document generated using\nonly the initial request. Users indicated which document they preferred and\ngave feedback about their experience with the question-answering process. The\nfindings of this study show clear benefits to question-asking both in document\npreference and in the qualitative user experience. This study further shows\nthat users found more value in questions which were thought-provoking,\nopen-ended, or offered unique insights into the user's request as opposed to\nsimple information-gathering questions."
                },
                "authors": [
                    {
                        "name": "Bernadette J Tix"
                    }
                ],
                "author_detail": {
                    "name": "Bernadette J Tix"
                },
                "author": "Bernadette J Tix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07983v1",
                "updated": "2024-08-15T07:09:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    9,
                    51,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:09:51Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    9,
                    51,
                    3,
                    228,
                    0
                ],
                "title": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal\n  Knowledge in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal\n  Knowledge in Large Language Models"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have led to\nsignificant improvements in various natural language processing tasks. However,\nthe evaluation of LLMs' legal knowledge, particularly in non-English languages\nsuch as Arabic, remains under-explored. To address this gap, we introduce\nArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal\nknowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval\nconsists of multiple tasks sourced from Saudi legal documents and synthesized\nquestions. In this work, we aim to analyze the capabilities required to solve\nlegal problems in Arabic and benchmark the performance of state-of-the-art\nLLMs. We explore the impact of in-context learning and investigate various\nevaluation methods. Additionally, we explore workflows for generating questions\nwith automatic validation to enhance the dataset's quality. We benchmark\nmultilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We\nalso share our methodology for creating the dataset and validation, which can\nbe generalized to other domains. We hope to accelerate AI research in the\nArabic Legal domain by releasing the ArabLegalEval dataset and code:\nhttps://github.com/Thiqah/ArabLegalEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have led to\nsignificant improvements in various natural language processing tasks. However,\nthe evaluation of LLMs' legal knowledge, particularly in non-English languages\nsuch as Arabic, remains under-explored. To address this gap, we introduce\nArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal\nknowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval\nconsists of multiple tasks sourced from Saudi legal documents and synthesized\nquestions. In this work, we aim to analyze the capabilities required to solve\nlegal problems in Arabic and benchmark the performance of state-of-the-art\nLLMs. We explore the impact of in-context learning and investigate various\nevaluation methods. Additionally, we explore workflows for generating questions\nwith automatic validation to enhance the dataset's quality. We benchmark\nmultilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We\nalso share our methodology for creating the dataset and validation, which can\nbe generalized to other domains. We hope to accelerate AI research in the\nArabic Legal domain by releasing the ArabLegalEval dataset and code:\nhttps://github.com/Thiqah/ArabLegalEval"
                },
                "authors": [
                    {
                        "name": "Faris Hijazi"
                    },
                    {
                        "name": "Somayah AlHarbi"
                    },
                    {
                        "name": "Abdulaziz AlHussein"
                    },
                    {
                        "name": "Harethah Abu Shairah"
                    },
                    {
                        "name": "Reem AlZahrani"
                    },
                    {
                        "name": "Hebah AlShamlan"
                    },
                    {
                        "name": "Omar Knio"
                    },
                    {
                        "name": "George Turkiyyah"
                    }
                ],
                "author_detail": {
                    "name": "George Turkiyyah"
                },
                "arxiv_affiliation": "KAUST",
                "author": "George Turkiyyah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07982v1",
                "updated": "2024-08-15T07:03:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    3,
                    0,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:03:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    3,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera"
                },
                "summary": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry."
                },
                "authors": [
                    {
                        "name": "Hiroki Tanioka"
                    },
                    {
                        "name": "Tetsushi Ueta"
                    },
                    {
                        "name": "Masahiko Sano"
                    }
                ],
                "author_detail": {
                    "name": "Masahiko Sano"
                },
                "author": "Masahiko Sano",
                "arxiv_comment": "4 pages, 5 figures, 1 table, The 1st InterAI: Interactive AI for\n  Human-Centered Robotics workshop in conjuction with IEEE Ro-MAN 2024,\n  Pasadona, LA, USA, Aug. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07981v1",
                "updated": "2024-08-15T07:00:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    0,
                    20,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T07:00:20Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    0,
                    20,
                    3,
                    228,
                    0
                ],
                "title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured\n  Surgical Video Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured\n  Surgical Video Learning"
                },
                "summary": "Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Garrett Skinner"
                    },
                    {
                        "name": "Gene Yang"
                    },
                    {
                        "name": "Brian R Quaranto"
                    },
                    {
                        "name": "Steven D Schwaitzberg"
                    },
                    {
                        "name": "Peter C W Kim"
                    },
                    {
                        "name": "Jinjun Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Jinjun Xiong"
                },
                "author": "Jinjun Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03337v3",
                "updated": "2024-08-15T06:58:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    58,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T07:19:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    19,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements"
                },
                "summary": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI"
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07975v1",
                "updated": "2024-08-15T06:40:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    40,
                    38,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T06:40:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    40,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual\n  Grounding and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual\n  Grounding and Large Language Models"
                },
                "summary": "This paper investigates the task of the open-ended interactive robotic\nmanipulation on table-top scenarios. While recent Large Language Models (LLMs)\nenhance robots' comprehension of user instructions, their lack of visual\ngrounding constrains their ability to physically interact with the environment.\nThis is because the robot needs to locate the target object for manipulation\nwithin the physical workspace. To this end, we introduce an interactive robotic\nmanipulation framework called Polaris, which integrates perception and\ninteraction by utilizing GPT-4 alongside grounded vision models. For precise\nmanipulation, it is essential that such grounded vision models produce detailed\nobject pose for the target object, rather than merely identifying pixels\nbelonging to them in the image. Consequently, we propose a novel\nSynthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes\nrendered synthetic data for training and is then transferred to real-world\nmanipulation tasks. The real-world performance demonstrates the efficacy of our\nproposed pipeline and underscores its potential for extension to more general\ncategories. Moreover, real-robot experiments have showcased the impressive\nperformance of our framework in grasping and executing multiple manipulation\ntasks. This indicates its potential to generalize to scenarios beyond the\ntabletop. More information and video results are available here:\nhttps://star-uu-wang.github.io/Polaris/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the task of the open-ended interactive robotic\nmanipulation on table-top scenarios. While recent Large Language Models (LLMs)\nenhance robots' comprehension of user instructions, their lack of visual\ngrounding constrains their ability to physically interact with the environment.\nThis is because the robot needs to locate the target object for manipulation\nwithin the physical workspace. To this end, we introduce an interactive robotic\nmanipulation framework called Polaris, which integrates perception and\ninteraction by utilizing GPT-4 alongside grounded vision models. For precise\nmanipulation, it is essential that such grounded vision models produce detailed\nobject pose for the target object, rather than merely identifying pixels\nbelonging to them in the image. Consequently, we propose a novel\nSynthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes\nrendered synthetic data for training and is then transferred to real-world\nmanipulation tasks. The real-world performance demonstrates the efficacy of our\nproposed pipeline and underscores its potential for extension to more general\ncategories. Moreover, real-robot experiments have showcased the impressive\nperformance of our framework in grasping and executing multiple manipulation\ntasks. This indicates its potential to generalize to scenarios beyond the\ntabletop. More information and video results are available here:\nhttps://star-uu-wang.github.io/Polaris/"
                },
                "authors": [
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Junqiu Yu"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "arxiv_comment": "Accepted by IROS 2024. 8 pages, 5 figures. See\n  https://star-uu-wang.github.io/Polaris/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07971v1",
                "updated": "2024-08-15T06:36:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    36,
                    27,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T06:36:27Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    36,
                    27,
                    3,
                    228,
                    0
                ],
                "title": "Predicting Lung Cancer Patient Prognosis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Lung Cancer Patient Prognosis with Large Language Models"
                },
                "summary": "Prognosis prediction is crucial for determining optimal treatment plans for\nlung cancer patients. Traditionally, such predictions relied on models\ndeveloped from retrospective patient data. Recently, large language models\n(LLMs) have gained attention for their ability to process and generate text\nbased on extensive learned knowledge. In this study, we evaluate the potential\nof GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.\nWe collected two prognosis datasets, i.e., survival and post-operative\ncomplication datasets, and designed multiple tasks to assess the models'\nperformance comprehensively. Logistic regression models were also developed as\nbaselines for comparison. The experimental results demonstrate that LLMs can\nachieve competitive, and in some tasks superior, performance in lung cancer\nprognosis prediction compared to data-driven logistic regression models despite\nnot using additional patient data. These findings suggest that LLMs can be\neffective tools for prognosis prediction in lung cancer, particularly when\npatient data is limited or unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prognosis prediction is crucial for determining optimal treatment plans for\nlung cancer patients. Traditionally, such predictions relied on models\ndeveloped from retrospective patient data. Recently, large language models\n(LLMs) have gained attention for their ability to process and generate text\nbased on extensive learned knowledge. In this study, we evaluate the potential\nof GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.\nWe collected two prognosis datasets, i.e., survival and post-operative\ncomplication datasets, and designed multiple tasks to assess the models'\nperformance comprehensively. Logistic regression models were also developed as\nbaselines for comparison. The experimental results demonstrate that LLMs can\nachieve competitive, and in some tasks superior, performance in lung cancer\nprognosis prediction compared to data-driven logistic regression models despite\nnot using additional patient data. These findings suggest that LLMs can be\neffective tools for prognosis prediction in lung cancer, particularly when\npatient data is limited or unavailable."
                },
                "authors": [
                    {
                        "name": "Danqing Hu"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Nan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wu"
                },
                "author": "Nan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07962v1",
                "updated": "2024-08-15T06:18:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    18,
                    50,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T06:18:50Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    18,
                    50,
                    3,
                    228,
                    0
                ],
                "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via\n  MetaGradient-based Hyperparameter Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via\n  MetaGradient-based Hyperparameter Tuning"
                },
                "summary": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied\nsubcategories of trial-and-error-based methods with the intention to be\ndeployed on real-world systems. In safe RL, the goal is to maximize reward\nperformance while minimizing constraints, often achieved by setting bounds on\nconstraint functions and utilizing the Lagrangian method. However, deploying\nLagrangian-based safe RL in real-world scenarios is challenging due to the\nnecessity of threshold fine-tuning, as imprecise adjustments may lead to\nsuboptimal policy convergence. To mitigate this challenge, we propose a unified\nLagrangian-based model-free architecture called Meta Soft Actor-Critic\nLagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to\nautomatically update the safety-related hyperparameters. The proposed method is\ndesigned to address safe exploration and threshold adjustment with minimal\nhyperparameter tuning requirement. In our pipeline, the inner parameters are\nupdated through the conventional formulation and the hyperparameters are\nadjusted using the meta-objectives which are defined based on the updated\nparameters. Our results show that the agent can reliably adjust the safety\nperformance due to the relatively fast convergence rate of the safety\nthreshold. We evaluate the performance of Meta SAC-Lag in five simulated\nenvironments against Lagrangian baselines, and the results demonstrate its\ncapability to create synergy between parameters, yielding better or competitive\nresults. Furthermore, we conduct a real-world experiment involving a robotic\narm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is\nsuccessfully trained to execute the task, while minimizing effort constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied\nsubcategories of trial-and-error-based methods with the intention to be\ndeployed on real-world systems. In safe RL, the goal is to maximize reward\nperformance while minimizing constraints, often achieved by setting bounds on\nconstraint functions and utilizing the Lagrangian method. However, deploying\nLagrangian-based safe RL in real-world scenarios is challenging due to the\nnecessity of threshold fine-tuning, as imprecise adjustments may lead to\nsuboptimal policy convergence. To mitigate this challenge, we propose a unified\nLagrangian-based model-free architecture called Meta Soft Actor-Critic\nLagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to\nautomatically update the safety-related hyperparameters. The proposed method is\ndesigned to address safe exploration and threshold adjustment with minimal\nhyperparameter tuning requirement. In our pipeline, the inner parameters are\nupdated through the conventional formulation and the hyperparameters are\nadjusted using the meta-objectives which are defined based on the updated\nparameters. Our results show that the agent can reliably adjust the safety\nperformance due to the relatively fast convergence rate of the safety\nthreshold. We evaluate the performance of Meta SAC-Lag in five simulated\nenvironments against Lagrangian baselines, and the results demonstrate its\ncapability to create synergy between parameters, yielding better or competitive\nresults. Furthermore, we conduct a real-world experiment involving a robotic\narm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is\nsuccessfully trained to execute the task, while minimizing effort constraints."
                },
                "authors": [
                    {
                        "name": "Homayoun Honari"
                    },
                    {
                        "name": "Amir Mehdi Soufi Enayati"
                    },
                    {
                        "name": "Mehran Ghafarian Tamizi"
                    },
                    {
                        "name": "Homayoun Najjaran"
                    }
                ],
                "author_detail": {
                    "name": "Homayoun Najjaran"
                },
                "author": "Homayoun Najjaran",
                "arxiv_comment": "Main text accepted to the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2024, 10 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04024v2",
                "updated": "2024-08-15T04:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    4,
                    53,
                    51,
                    3,
                    228,
                    0
                ],
                "published": "2024-03-06T20:10:41Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    20,
                    10,
                    41,
                    2,
                    66,
                    0
                ],
                "title": "Enhancing chest X-ray datasets with privacy-preserving large language\n  models and multi-type annotations: a data-driven approach for improved\n  classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing chest X-ray datasets with privacy-preserving large language\n  models and multi-type annotations: a data-driven approach for improved\n  classification"
                },
                "summary": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports for dataset releases. However, there is still\nroom for improvement in label quality. These labelers typically output only\npresence labels, sometimes with binary uncertainty indicators, which limits\ntheir usefulness. Supervised deep learning models have also been developed for\nreport labeling but lack adaptability, similar to rule-based systems. In this\nwork, we present MAPLEZ (Medical report Annotations with Privacy-preserving\nLarge language model using Expeditious Zero shot answers), a novel approach\nleveraging a locally executable Large Language Model (LLM) to extract and\nenhance findings labels on CXR reports. MAPLEZ extracts not only binary labels\nindicating the presence or absence of a finding but also the location,\nseverity, and radiologists' uncertainty about the finding. Over eight\nabnormalities from five test sets, we show that our method can extract these\nannotations with an increase of 3.6 percentage points (pp) in macro F1 score\nfor categorical presence annotations and more than 20 pp increase in F1 score\nfor the location annotations over competing labelers. Additionally, using the\ncombination of improved annotations and multi-type annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.1 pp in AUROC over models trained with\nannotations from the best alternative approach. We share code and annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports for dataset releases. However, there is still\nroom for improvement in label quality. These labelers typically output only\npresence labels, sometimes with binary uncertainty indicators, which limits\ntheir usefulness. Supervised deep learning models have also been developed for\nreport labeling but lack adaptability, similar to rule-based systems. In this\nwork, we present MAPLEZ (Medical report Annotations with Privacy-preserving\nLarge language model using Expeditious Zero shot answers), a novel approach\nleveraging a locally executable Large Language Model (LLM) to extract and\nenhance findings labels on CXR reports. MAPLEZ extracts not only binary labels\nindicating the presence or absence of a finding but also the location,\nseverity, and radiologists' uncertainty about the finding. Over eight\nabnormalities from five test sets, we show that our method can extract these\nannotations with an increase of 3.6 percentage points (pp) in macro F1 score\nfor categorical presence annotations and more than 20 pp increase in F1 score\nfor the location annotations over competing labelers. Additionally, using the\ncombination of improved annotations and multi-type annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.1 pp in AUROC over models trained with\nannotations from the best alternative approach. We share code and annotations."
                },
                "authors": [
                    {
                        "name": "Ricardo Bigolin Lanfredi"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Ronald Summers"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Summers"
                },
                "author": "Ronald Summers",
                "arxiv_comment": "Code and data:\n  https://github.com/rsummers11/CADLab/tree/master/MAPLEZ_LLM_report_labeler/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18276v2",
                "updated": "2024-08-15T04:43:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    4,
                    43,
                    5,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-23T21:18:31Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    21,
                    18,
                    31,
                    1,
                    205,
                    0
                ],
                "title": "Rome was Not Built in a Single Step: Hierarchical Prompting for\n  LLM-based Chip Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rome was Not Built in a Single Step: Hierarchical Prompting for\n  LLM-based Chip Design"
                },
                "summary": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM"
                },
                "authors": [
                    {
                        "name": "Andre Nakkab"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Siddharth Garg"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Garg"
                },
                "author": "Siddharth Garg",
                "arxiv_comment": "Accepted at MLCAD '24. 10 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07904v1",
                "updated": "2024-08-15T03:19:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    19,
                    41,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T03:19:41Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    3,
                    19,
                    41,
                    3,
                    228,
                    0
                ],
                "title": "Assessing Language Models' Worldview for Fiction Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Language Models' Worldview for Fiction Generation"
                },
                "summary": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant\napplications in computational creativity. One such application is fictional\nstory generation. Fiction is a narrative that occurs in a story world that is\nslightly different than ours. With LLMs becoming writing partners, we question\nhow suitable they are to generate fiction. This study investigates the ability\nof LLMs to maintain a state of world essential to generate fiction. Through a\nseries of questions to nine LLMs, we find that only two models exhibit\nconsistent worldview, while the rest are self-conflicting. Subsequent analysis\nof stories generated by four models revealed a strikingly uniform narrative\npattern. This uniformity across models further suggests a lack of `state'\nnecessary for fiction. We highlight the limitations of current LLMs in fiction\nwriting and advocate for future research to test and create story worlds for\nLLMs to reside in. All code, dataset, and the generated responses can be found\nin https://github.com/tanny411/llm-reliability-and-consistency-evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant\napplications in computational creativity. One such application is fictional\nstory generation. Fiction is a narrative that occurs in a story world that is\nslightly different than ours. With LLMs becoming writing partners, we question\nhow suitable they are to generate fiction. This study investigates the ability\nof LLMs to maintain a state of world essential to generate fiction. Through a\nseries of questions to nine LLMs, we find that only two models exhibit\nconsistent worldview, while the rest are self-conflicting. Subsequent analysis\nof stories generated by four models revealed a strikingly uniform narrative\npattern. This uniformity across models further suggests a lack of `state'\nnecessary for fiction. We highlight the limitations of current LLMs in fiction\nwriting and advocate for future research to test and create story worlds for\nLLMs to reside in. All code, dataset, and the generated responses can be found\nin https://github.com/tanny411/llm-reliability-and-consistency-evaluation."
                },
                "authors": [
                    {
                        "name": "Aisha Khatun"
                    },
                    {
                        "name": "Daniel G. Brown"
                    }
                ],
                "author_detail": {
                    "name": "Daniel G. Brown"
                },
                "author": "Daniel G. Brown",
                "arxiv_comment": "Short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07896v1",
                "updated": "2024-08-15T02:55:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    55,
                    30,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:55:30Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    55,
                    30,
                    3,
                    228,
                    0
                ],
                "title": "The doctor will polygraph you now: ethical concerns with AI for\n  fact-checking patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The doctor will polygraph you now: ethical concerns with AI for\n  fact-checking patients"
                },
                "summary": "Clinical artificial intelligence (AI) methods have been proposed for\npredicting social behaviors which could be reasonably understood from\npatient-reported data. This raises ethical concerns about respect, privacy, and\npatient awareness/control over how their health data is used. Ethical concerns\nsurrounding clinical AI systems for social behavior verification were divided\ninto three main categories: (1) the use of patient data retrospectively without\ninformed consent for the specific task of verification, (2) the potential for\ninaccuracies or biases within such systems, and (3) the impact on trust in\npatient-provider relationships with the introduction of automated AI systems\nfor fact-checking. Additionally, this report showed the simulated misuse of a\nverification system and identified a potential LLM bias against\npatient-reported information in favor of multimodal data, published literature,\nand the outputs of other AI methods (i.e., AI self-trust). Finally,\nrecommendations were presented for mitigating the risk that AI verification\nsystems will cause harm to patients or undermine the purpose of the healthcare\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical artificial intelligence (AI) methods have been proposed for\npredicting social behaviors which could be reasonably understood from\npatient-reported data. This raises ethical concerns about respect, privacy, and\npatient awareness/control over how their health data is used. Ethical concerns\nsurrounding clinical AI systems for social behavior verification were divided\ninto three main categories: (1) the use of patient data retrospectively without\ninformed consent for the specific task of verification, (2) the potential for\ninaccuracies or biases within such systems, and (3) the impact on trust in\npatient-provider relationships with the introduction of automated AI systems\nfor fact-checking. Additionally, this report showed the simulated misuse of a\nverification system and identified a potential LLM bias against\npatient-reported information in favor of multimodal data, published literature,\nand the outputs of other AI methods (i.e., AI self-trust). Finally,\nrecommendations were presented for mitigating the risk that AI verification\nsystems will cause harm to patients or undermine the purpose of the healthcare\nsystem."
                },
                "authors": [
                    {
                        "name": "James Anibal"
                    },
                    {
                        "name": "Jasmine Gunkel"
                    },
                    {
                        "name": "Hannah Huth"
                    },
                    {
                        "name": "Hang Nguyen"
                    },
                    {
                        "name": "Shaheen Awan"
                    },
                    {
                        "name": "Yael Bensoussan"
                    },
                    {
                        "name": "Bradford Wood"
                    }
                ],
                "author_detail": {
                    "name": "Bradford Wood"
                },
                "author": "Bradford Wood",
                "arxiv_comment": "9 pages, 1 figure, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07894v1",
                "updated": "2024-08-15T02:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    52,
                    2,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    52,
                    2,
                    3,
                    228,
                    0
                ],
                "title": "System States Forecasting of Microservices with Dynamic Spatio-Temporal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System States Forecasting of Microservices with Dynamic Spatio-Temporal\n  Data"
                },
                "summary": "In the AIOps (Artificial Intelligence for IT Operations) era, accurately\nforecasting system states is crucial. In microservices systems, this task\nencounters the challenge of dynamic and complex spatio-temporal relationships\namong microservice instances, primarily due to dynamic deployments, diverse\ncall paths, and cascading effects among instances. Current time-series\nforecasting methods, which focus mainly on intrinsic patterns, are insufficient\nin environments where spatial relationships are critical. Similarly,\nspatio-temporal graph approaches often neglect the nature of temporal trend,\nconcentrating mostly on message passing between nodes. Moreover, current\nresearch in microservices domain frequently underestimates the importance of\nnetwork metrics and topological structures in capturing the evolving dynamics\nof systems. This paper introduces STMformer, a model tailored for forecasting\nsystem states in microservices environments, capable of handling multi-node and\nmultivariate time series. Our method leverages dynamic network connection data\nand topological information to assist in modeling the intricate spatio-temporal\nrelationships within the system. Additionally, we integrate the\nPatchCrossAttention module to compute the impact of cascading effects globally.\nWe have developed a dataset based on a microservices system and conducted\ncomprehensive experiments with STMformer against leading methods. In both\nshort-term and long-term forecasting tasks, our model consistently achieved a\n8.6% reduction in MAE(Mean Absolute Error) and a 2.2% reduction in MSE (Mean\nSquared Error). The source code is available at\nhttps://github.com/xuyifeiiie/STMformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the AIOps (Artificial Intelligence for IT Operations) era, accurately\nforecasting system states is crucial. In microservices systems, this task\nencounters the challenge of dynamic and complex spatio-temporal relationships\namong microservice instances, primarily due to dynamic deployments, diverse\ncall paths, and cascading effects among instances. Current time-series\nforecasting methods, which focus mainly on intrinsic patterns, are insufficient\nin environments where spatial relationships are critical. Similarly,\nspatio-temporal graph approaches often neglect the nature of temporal trend,\nconcentrating mostly on message passing between nodes. Moreover, current\nresearch in microservices domain frequently underestimates the importance of\nnetwork metrics and topological structures in capturing the evolving dynamics\nof systems. This paper introduces STMformer, a model tailored for forecasting\nsystem states in microservices environments, capable of handling multi-node and\nmultivariate time series. Our method leverages dynamic network connection data\nand topological information to assist in modeling the intricate spatio-temporal\nrelationships within the system. Additionally, we integrate the\nPatchCrossAttention module to compute the impact of cascading effects globally.\nWe have developed a dataset based on a microservices system and conducted\ncomprehensive experiments with STMformer against leading methods. In both\nshort-term and long-term forecasting tasks, our model consistently achieved a\n8.6% reduction in MAE(Mean Absolute Error) and a 2.2% reduction in MSE (Mean\nSquared Error). The source code is available at\nhttps://github.com/xuyifeiiie/STMformer."
                },
                "authors": [
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Jingguo Ge"
                    },
                    {
                        "name": "Haina Tang"
                    },
                    {
                        "name": "Shuai Ding"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Hui Li"
                    }
                ],
                "author_detail": {
                    "name": "Hui Li"
                },
                "author": "Hui Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07892v1",
                "updated": "2024-08-15T02:41:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    41,
                    25,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:41:25Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    41,
                    25,
                    3,
                    228,
                    0
                ],
                "title": "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online"
                },
                "summary": "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability (i.e., lifelike content and\navatars, agentic activity) from people online, and AI's increasing scalability\n(i.e., cost-effectiveness, accessibility). Drawing on a long history of\nresearch into anonymous credentials and \"proof-of-personhood\" systems,\npersonhood credentials give people a way to signal their trustworthiness on\nonline platforms, and offer service providers new tools for reducing misuse by\nbad actors. In contrast, existing countermeasures to automated deception --\nsuch as CAPTCHAs -- are inadequate against sophisticated AI, while stringent\nidentity verification solutions are insufficiently private for many use-cases.\nAfter surveying the benefits of personhood credentials, we also examine\ndeployment risks and design challenges. We conclude with actionable next steps\nfor policymakers, technologists, and standards bodies to consider in\nconsultation with the public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability (i.e., lifelike content and\navatars, agentic activity) from people online, and AI's increasing scalability\n(i.e., cost-effectiveness, accessibility). Drawing on a long history of\nresearch into anonymous credentials and \"proof-of-personhood\" systems,\npersonhood credentials give people a way to signal their trustworthiness on\nonline platforms, and offer service providers new tools for reducing misuse by\nbad actors. In contrast, existing countermeasures to automated deception --\nsuch as CAPTCHAs -- are inadequate against sophisticated AI, while stringent\nidentity verification solutions are insufficiently private for many use-cases.\nAfter surveying the benefits of personhood credentials, we also examine\ndeployment risks and design challenges. We conclude with actionable next steps\nfor policymakers, technologists, and standards bodies to consider in\nconsultation with the public."
                },
                "authors": [
                    {
                        "name": "Steven Adler"
                    },
                    {
                        "name": "Zoë Hitzig"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Catherine Brewer"
                    },
                    {
                        "name": "Wayne Chang"
                    },
                    {
                        "name": "Renée DiResta"
                    },
                    {
                        "name": "Eddy Lazzarin"
                    },
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Wendy Seltzer"
                    },
                    {
                        "name": "Divya Siddarth"
                    },
                    {
                        "name": "Nouran Soliman"
                    },
                    {
                        "name": "Tobin South"
                    },
                    {
                        "name": "Connor Spelliscy"
                    },
                    {
                        "name": "Manu Sporny"
                    },
                    {
                        "name": "Varya Srivastava"
                    },
                    {
                        "name": "John Bailey"
                    },
                    {
                        "name": "Brian Christian"
                    },
                    {
                        "name": "Andrew Critch"
                    },
                    {
                        "name": "Ronnie Falcon"
                    },
                    {
                        "name": "Heather Flanagan"
                    },
                    {
                        "name": "Kim Hamilton Duffy"
                    },
                    {
                        "name": "Eric Ho"
                    },
                    {
                        "name": "Claire R. Leibowicz"
                    },
                    {
                        "name": "Srikanth Nadhamuni"
                    },
                    {
                        "name": "Alan Z. Rozenshtein"
                    },
                    {
                        "name": "David Schnurr"
                    },
                    {
                        "name": "Evan Shapiro"
                    },
                    {
                        "name": "Lacey Strahm"
                    },
                    {
                        "name": "Andrew Trask"
                    },
                    {
                        "name": "Zoe Weinberg"
                    },
                    {
                        "name": "Cedric Whitney"
                    },
                    {
                        "name": "Tom Zick"
                    }
                ],
                "author_detail": {
                    "name": "Tom Zick"
                },
                "author": "Tom Zick",
                "arxiv_comment": "63 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17900v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17900v5",
                "updated": "2024-08-15T02:33:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    33,
                    22,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-25T09:42:24Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    42,
                    24,
                    3,
                    207,
                    0
                ],
                "title": "The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer"
                },
                "summary": "Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions."
                },
                "authors": [
                    {
                        "name": "Danqing Hu"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Nan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wu"
                },
                "author": "Nan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17900v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17900v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07888v1",
                "updated": "2024-08-15T02:22:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    22,
                    48,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:22:48Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    22,
                    48,
                    3,
                    228,
                    0
                ],
                "title": "Fine-tuning Large Language Models with Human-inspired Learning\n  Strategies in Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models with Human-inspired Learning\n  Strategies in Medical Question Answering"
                },
                "summary": "Training Large Language Models (LLMs) incurs substantial data-related costs,\nmotivating the development of data-efficient training methods through optimised\ndata ordering and selection. Human-inspired learning strategies, such as\ncurriculum learning, offer possibilities for efficient training by organising\ndata according to common human learning practices. Despite evidence that\nfine-tuning with curriculum learning improves the performance of LLMs for\nnatural language understanding tasks, its effectiveness is typically assessed\nusing a single model. In this work, we extend previous research by evaluating\nboth curriculum-based and non-curriculum-based learning strategies across\nmultiple LLMs, using human-defined and automated data labels for medical\nquestion answering. Our results indicate a moderate impact of using\nhuman-inspired learning strategies for fine-tuning LLMs, with maximum accuracy\ngains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that\nthe effectiveness of these strategies varies significantly across different\nmodel-dataset combinations, emphasising that the benefits of a specific\nhuman-inspired strategy for fine-tuning LLMs do not generalise. Additionally,\nwe find evidence that curriculum learning using LLM-defined question difficulty\noutperforms human-defined difficulty, highlighting the potential of using\nmodel-generated measures for optimal curriculum design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) incurs substantial data-related costs,\nmotivating the development of data-efficient training methods through optimised\ndata ordering and selection. Human-inspired learning strategies, such as\ncurriculum learning, offer possibilities for efficient training by organising\ndata according to common human learning practices. Despite evidence that\nfine-tuning with curriculum learning improves the performance of LLMs for\nnatural language understanding tasks, its effectiveness is typically assessed\nusing a single model. In this work, we extend previous research by evaluating\nboth curriculum-based and non-curriculum-based learning strategies across\nmultiple LLMs, using human-defined and automated data labels for medical\nquestion answering. Our results indicate a moderate impact of using\nhuman-inspired learning strategies for fine-tuning LLMs, with maximum accuracy\ngains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that\nthe effectiveness of these strategies varies significantly across different\nmodel-dataset combinations, emphasising that the benefits of a specific\nhuman-inspired strategy for fine-tuning LLMs do not generalise. Additionally,\nwe find evidence that curriculum learning using LLM-defined question difficulty\noutperforms human-defined difficulty, highlighting the potential of using\nmodel-generated measures for optimal curriculum design."
                },
                "authors": [
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07955v2",
                "updated": "2024-08-15T02:18:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    18,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-01-15T20:42:16Z",
                "published_parsed": [
                    2024,
                    1,
                    15,
                    20,
                    42,
                    16,
                    0,
                    15,
                    0
                ],
                "title": "A Study on Large Language Models' Limitations in Multiple-Choice\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study on Large Language Models' Limitations in Multiple-Choice\n  Question Answering"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever."
                },
                "authors": [
                    {
                        "name": "Aisha Khatun"
                    },
                    {
                        "name": "Daniel G. Brown"
                    }
                ],
                "author_detail": {
                    "name": "Daniel G. Brown"
                },
                "author": "Daniel G. Brown",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v2",
                "updated": "2024-08-15T02:12:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    12,
                    52,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "We thank Shiyue Xu for pointing out the error in Equation 5 in the\n  previous draft: https://github.com/LuJunru/SamPO/issues/1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07884v1",
                "updated": "2024-08-15T02:07:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    7,
                    11,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T02:07:11Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    7,
                    11,
                    3,
                    228,
                    0
                ],
                "title": "Instruct Large Language Models to Generate Scientific Literature Survey\n  Step by Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct Large Language Models to Generate Scientific Literature Survey\n  Step by Step"
                },
                "summary": "Abstract. Automatically generating scientific literature surveys is a\nvaluable task that can significantly enhance research efficiency. However, the\ndiverse and complex nature of information within a literature survey poses\nsubstantial challenges for generative models. In this paper, we design a series\nof prompts to systematically leverage large language models (LLMs), enabling\nthe creation of comprehensive literature surveys through a step-by-step\napproach. Specifically, we design prompts to guide LLMs to sequentially\ngenerate the title, abstract, hierarchical headings, and the main content of\nthe literature survey. We argue that this design enables the generation of the\nheadings from a high-level perspective. During the content generation process,\nthis design effectively harnesses relevant information while minimizing costs\nby restricting the length of both input and output content in LLM queries. Our\nimplementation with Qwen-long achieved third place in the NLPCC 2024 Scientific\nLiterature Survey Generation evaluation task, with an overall score only 0.03%\nlower than the second-place team. Additionally, our soft heading recall is\n95.84%, the second best among the submissions. Thanks to the efficient prompt\ndesign and the low cost of the Qwen-long API, our method reduces the expense\nfor generating each literature survey to 0.1 RMB, enhancing the practical value\nof our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract. Automatically generating scientific literature surveys is a\nvaluable task that can significantly enhance research efficiency. However, the\ndiverse and complex nature of information within a literature survey poses\nsubstantial challenges for generative models. In this paper, we design a series\nof prompts to systematically leverage large language models (LLMs), enabling\nthe creation of comprehensive literature surveys through a step-by-step\napproach. Specifically, we design prompts to guide LLMs to sequentially\ngenerate the title, abstract, hierarchical headings, and the main content of\nthe literature survey. We argue that this design enables the generation of the\nheadings from a high-level perspective. During the content generation process,\nthis design effectively harnesses relevant information while minimizing costs\nby restricting the length of both input and output content in LLM queries. Our\nimplementation with Qwen-long achieved third place in the NLPCC 2024 Scientific\nLiterature Survey Generation evaluation task, with an overall score only 0.03%\nlower than the second-place team. Additionally, our soft heading recall is\n95.84%, the second best among the submissions. Thanks to the efficient prompt\ndesign and the low cost of the Qwen-long API, our method reduces the expense\nfor generating each literature survey to 0.1 RMB, enhancing the practical value\nof our method."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lai"
                    },
                    {
                        "name": "Yupeng Wu"
                    },
                    {
                        "name": "Yidan Wang"
                    },
                    {
                        "name": "Wenpeng Hu"
                    },
                    {
                        "name": "Chen Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zheng"
                },
                "author": "Chen Zheng",
                "arxiv_comment": "NLPCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v2",
                "updated": "2024-08-15T01:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    49,
                    29,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07873v1",
                "updated": "2024-08-15T01:00:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    0,
                    28,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T01:00:28Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    0,
                    28,
                    3,
                    228,
                    0
                ],
                "title": "Words Matter: Reducing Stigma in Online Conversations about Substance\n  Use with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words Matter: Reducing Stigma in Online Conversations about Substance\n  Use with Large Language Models"
                },
                "summary": "Stigma is a barrier to treatment for individuals struggling with substance\nuse disorders (SUD), which leads to significantly lower treatment engagement\nrates. With only 7% of those affected receiving any form of help, societal\nstigma not only discourages individuals with SUD from seeking help but isolates\nthem, hindering their recovery journey and perpetuating a cycle of shame and\nself-doubt. This study investigates how stigma manifests on social media,\nparticularly Reddit, where anonymity can exacerbate discriminatory behaviors.\nWe analyzed over 1.2 million posts, identifying 3,207 that exhibited\nstigmatizing language towards people who use substances (PWUS). Using Informed\nand Stylized LLMs, we develop a model for de-stigmatization of these\nexpressions into empathetic language, resulting in 1,649 reformed phrase pairs.\nOur paper contributes to the field by proposing a computational framework for\nanalyzing stigma and destigmatizing online content, and delving into the\nlinguistic features that propagate stigma towards PWUS. Our work not only\nenhances understanding of stigma's manifestations online but also provides\npractical tools for fostering a more supportive digital environment for those\naffected by SUD. Code and data will be made publicly available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stigma is a barrier to treatment for individuals struggling with substance\nuse disorders (SUD), which leads to significantly lower treatment engagement\nrates. With only 7% of those affected receiving any form of help, societal\nstigma not only discourages individuals with SUD from seeking help but isolates\nthem, hindering their recovery journey and perpetuating a cycle of shame and\nself-doubt. This study investigates how stigma manifests on social media,\nparticularly Reddit, where anonymity can exacerbate discriminatory behaviors.\nWe analyzed over 1.2 million posts, identifying 3,207 that exhibited\nstigmatizing language towards people who use substances (PWUS). Using Informed\nand Stylized LLMs, we develop a model for de-stigmatization of these\nexpressions into empathetic language, resulting in 1,649 reformed phrase pairs.\nOur paper contributes to the field by proposing a computational framework for\nanalyzing stigma and destigmatizing online content, and delving into the\nlinguistic features that propagate stigma towards PWUS. Our work not only\nenhances understanding of stigma's manifestations online but also provides\npractical tools for fostering a more supportive digital environment for those\naffected by SUD. Code and data will be made publicly available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Layla Bouzoubaa"
                    },
                    {
                        "name": "Elham Aghakhani"
                    },
                    {
                        "name": "Rezvaneh Rezapour"
                    }
                ],
                "author_detail": {
                    "name": "Rezvaneh Rezapour"
                },
                "author": "Rezvaneh Rezapour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07846v1",
                "updated": "2024-08-14T23:02:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    2,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:02:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    2,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites"
                },
                "summary": "Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present \\textsc{AgoneTest}: an automated system for generating test\nsuites for Java projects and a comprehensive and principled methodology for\nevaluating the generated test suites. Starting from a state-of-the-art dataset\n(i.e., \\textsc{Methods2Test}), we built a new dataset for comparing\nhuman-written tests with those generated by LLMs. Our key contributions include\na scalable automated software system, a new dataset, and a detailed methodology\nfor evaluating test quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present \\textsc{AgoneTest}: an automated system for generating test\nsuites for Java projects and a comprehensive and principled methodology for\nevaluating the generated test suites. Starting from a state-of-the-art dataset\n(i.e., \\textsc{Methods2Test}), we built a new dataset for comparing\nhuman-written tests with those generated by LLMs. Our key contributions include\na scalable automated software system, a new dataset, and a detailed methodology\nfor evaluating test quality."
                },
                "authors": [
                    {
                        "name": "Andrea Lops"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Azzurra Ragone"
                    },
                    {
                        "name": "Michelantonio Trizio"
                    },
                    {
                        "name": "Claudio Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Bartolini"
                },
                "author": "Claudio Bartolini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06172v2",
                "updated": "2024-08-14T22:31:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    31,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-08T17:48:42Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    17,
                    48,
                    42,
                    0,
                    190,
                    0
                ],
                "title": "On Speeding Up Language Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Speeding Up Language Model Evaluation"
                },
                "summary": "Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem.\nFor example, selecting the right pre-trained LLM, prompt, and hyperparameters\nto attain the best performance for a task typically necessitates evaluating an\nexpoential number of candidates on large validation sets. This exhaustive\nevaluation can be time-consuming and costly, as both inference and evaluation\nof LLM-based approaches are resource-intensive. Worse, a lot of computation is\nwasted: Many hyper-parameter settings are non-competitive, and many samples\nfrom the validation set are highly correlated - providing little or no new\ninformation. So, if the goal is to identify the best method, it can be done far\nmore efficiently if the validation samples and methods are selected adaptively.\nIn this paper, we propose a novel method to address this challenge. We lean on\nlow-rank matrix factorization to fill in missing evaluations and on multi-armed\nbandits to sequentially identify the next (method, validation sample)-pair to\nevaluate. We carefully assess the efficacy of our approach on several\ncompetitive benchmark problems and show that it can identify the top-performing\nmethod using only 5-15% of the typically needed resources -- resulting in a\nstaggering 85-95% LLM cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem.\nFor example, selecting the right pre-trained LLM, prompt, and hyperparameters\nto attain the best performance for a task typically necessitates evaluating an\nexpoential number of candidates on large validation sets. This exhaustive\nevaluation can be time-consuming and costly, as both inference and evaluation\nof LLM-based approaches are resource-intensive. Worse, a lot of computation is\nwasted: Many hyper-parameter settings are non-competitive, and many samples\nfrom the validation set are highly correlated - providing little or no new\ninformation. So, if the goal is to identify the best method, it can be done far\nmore efficiently if the validation samples and methods are selected adaptively.\nIn this paper, we propose a novel method to address this challenge. We lean on\nlow-rank matrix factorization to fill in missing evaluations and on multi-armed\nbandits to sequentially identify the next (method, validation sample)-pair to\nevaluate. We carefully assess the efficacy of our approach on several\ncompetitive benchmark problems and show that it can identify the top-performing\nmethod using only 5-15% of the typically needed resources -- resulting in a\nstaggering 85-95% LLM cost savings."
                },
                "authors": [
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Christian K. Belardi"
                    },
                    {
                        "name": "Ruihan Wu"
                    },
                    {
                        "name": "Travis Zhang"
                    },
                    {
                        "name": "Carla P. Gomes"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07840v1",
                "updated": "2024-08-14T22:28:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    28,
                    19,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T22:28:19Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    28,
                    19,
                    2,
                    227,
                    0
                ],
                "title": "ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction\n  Based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction\n  Based on Large Language Model"
                },
                "summary": "In the realm of event prediction, temporal knowledge graph forecasting (TKGF)\nstands as a pivotal technique. Previous approaches face the challenges of not\nutilizing experience during testing and relying on a single short-term history,\nwhich limits adaptation to evolving data. In this paper, we introduce the\nOnline Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by\nintegrating dynamic causal rule mining (DCRM) and dual history augmented\ngeneration (DHAG). DCRM dynamically constructs causal rules from real-time\ndata, allowing for swift adaptation to new causal relationships. In parallel,\nDHAG merges short-term and long-term historical contexts, leveraging a\nbi-branch approach to enrich event prediction. Our framework demonstrates\nnotable performance enhancements across diverse datasets, with significant\nHit@k (k=1,3,10) improvements, showcasing its ability to augment large language\nmodels (LLMs) for event prediction without necessitating extensive retraining.\nThe ONSEP framework not only advances the field of TKGF but also underscores\nthe potential of neural-symbolic approaches in adapting to dynamic data\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of event prediction, temporal knowledge graph forecasting (TKGF)\nstands as a pivotal technique. Previous approaches face the challenges of not\nutilizing experience during testing and relying on a single short-term history,\nwhich limits adaptation to evolving data. In this paper, we introduce the\nOnline Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by\nintegrating dynamic causal rule mining (DCRM) and dual history augmented\ngeneration (DHAG). DCRM dynamically constructs causal rules from real-time\ndata, allowing for swift adaptation to new causal relationships. In parallel,\nDHAG merges short-term and long-term historical contexts, leveraging a\nbi-branch approach to enrich event prediction. Our framework demonstrates\nnotable performance enhancements across diverse datasets, with significant\nHit@k (k=1,3,10) improvements, showcasing its ability to augment large language\nmodels (LLMs) for event prediction without necessitating extensive retraining.\nThe ONSEP framework not only advances the field of TKGF but also underscores\nthe potential of neural-symbolic approaches in adapting to dynamic data\nenvironments."
                },
                "authors": [
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Jingwei Li"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Chengbao Liu"
                    },
                    {
                        "name": "Jie Tan"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tan"
                },
                "author": "Jie Tan",
                "arxiv_comment": "16 pages, ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04884v2",
                "updated": "2024-08-14T22:17:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    22,
                    17,
                    10,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-09T06:17:18Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    6,
                    17,
                    18,
                    4,
                    222,
                    0
                ],
                "title": "Enhancing Relevance of Embedding-based Retrieval at Walmart",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Relevance of Embedding-based Retrieval at Walmart"
                },
                "summary": "Embedding-based neural retrieval (EBR) is an effective search retrieval\nmethod in product search for tackling the vocabulary gap between customer\nsearch queries and products. The initial launch of our EBR system at Walmart\nyielded significant gains in relevance and add-to-cart rates [1]. However,\ndespite EBR generally retrieving more relevant products for reranking, we have\nobserved numerous instances of relevance degradation. Enhancing retrieval\nperformance is crucial, as it directly influences product reranking and affects\nthe customer shopping experience. Factors contributing to these degradations\ninclude false positives/negatives in the training data and the inability to\nhandle query misspellings. To address these issues, we present several\napproaches to further strengthen the capabilities of our EBR model in terms of\nretrieval relevance. We introduce a Relevance Reward Model (RRM) based on human\nrelevance feedback. We utilize RRM to remove noise from the training data and\ndistill it into our EBR model through a multi-objective loss. In addition, we\npresent the techniques to increase the performance of our EBR model, such as\ntypo-aware training, and semi-positive generation. The effectiveness of our EBR\nis demonstrated through offline relevance evaluation, online AB tests, and\nsuccessful deployments to live production.\n  [1] Alessandro Magnani, Feng Liu, Suthee Chaidaroon, Sachin Yadav, Praveen\nReddy Suram, Ajit Puthenputhussery, Sijie Chen, Min Xie, Anirudh Kashi, Tony\nLee, et al. 2022. Semantic retrieval at walmart. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining. 3495-3503.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-based neural retrieval (EBR) is an effective search retrieval\nmethod in product search for tackling the vocabulary gap between customer\nsearch queries and products. The initial launch of our EBR system at Walmart\nyielded significant gains in relevance and add-to-cart rates [1]. However,\ndespite EBR generally retrieving more relevant products for reranking, we have\nobserved numerous instances of relevance degradation. Enhancing retrieval\nperformance is crucial, as it directly influences product reranking and affects\nthe customer shopping experience. Factors contributing to these degradations\ninclude false positives/negatives in the training data and the inability to\nhandle query misspellings. To address these issues, we present several\napproaches to further strengthen the capabilities of our EBR model in terms of\nretrieval relevance. We introduce a Relevance Reward Model (RRM) based on human\nrelevance feedback. We utilize RRM to remove noise from the training data and\ndistill it into our EBR model through a multi-objective loss. In addition, we\npresent the techniques to increase the performance of our EBR model, such as\ntypo-aware training, and semi-positive generation. The effectiveness of our EBR\nis demonstrated through offline relevance evaluation, online AB tests, and\nsuccessful deployments to live production.\n  [1] Alessandro Magnani, Feng Liu, Suthee Chaidaroon, Sachin Yadav, Praveen\nReddy Suram, Ajit Puthenputhussery, Sijie Chen, Min Xie, Anirudh Kashi, Tony\nLee, et al. 2022. Semantic retrieval at walmart. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining. 3495-3503."
                },
                "authors": [
                    {
                        "name": "Juexin Lin"
                    },
                    {
                        "name": "Sachin Yadav"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Nicholas Rossi"
                    },
                    {
                        "name": "Praveen R. Suram"
                    },
                    {
                        "name": "Satya Chembolu"
                    },
                    {
                        "name": "Prijith Chandran"
                    },
                    {
                        "name": "Hrushikesh Mohapatra"
                    },
                    {
                        "name": "Tony Lee"
                    },
                    {
                        "name": "Alessandro Magnani"
                    },
                    {
                        "name": "Ciya Liao"
                    }
                ],
                "author_detail": {
                    "name": "Ciya Liao"
                },
                "author": "Ciya Liao",
                "arxiv_doi": "10.1145/3627673.3680047",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3680047",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 3 figures, CIKM 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00699v2",
                "updated": "2024-08-14T21:56:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    21,
                    56,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-31T14:32:02Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    14,
                    32,
                    2,
                    6,
                    91,
                    0
                ],
                "title": "How Much are Large Language Models Contaminated? A Comprehensive Survey\n  and the LLMSanitize Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much are Large Language Models Contaminated? A Comprehensive Survey\n  and the LLMSanitize Library"
                },
                "summary": "With the rise of Large Language Models (LLMs) in recent years, abundant new\nopportunities are emerging, but also new challenges, among which contamination\nis quickly becoming critical. Business applications and fundraising in AI have\nreached a scale at which a few percentage points gained on popular\nquestion-answering benchmarks could translate into dozens of millions of\ndollars, placing high pressure on model integrity. At the same time, it is\nbecoming harder and harder to keep track of the data that LLMs have seen; if\nnot impossible with closed-source models like GPT-4 and Claude-3 not divulging\nany information on the training set. As a result, contamination becomes a major\nissue: LLMs' performance may not be reliable anymore, as the high performance\nmay be at least partly due to their previous exposure to the data. This\nlimitation jeopardizes the entire progress in the field of NLP, yet, there\nremains a lack of methods on how to efficiently detect contamination.In this\npaper, we survey all recent work on contamination detection with LLMs, and help\nthe community track contamination levels of LLMs by releasing an open-source\nPython library named LLMSanitize implementing major contamination detection\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of Large Language Models (LLMs) in recent years, abundant new\nopportunities are emerging, but also new challenges, among which contamination\nis quickly becoming critical. Business applications and fundraising in AI have\nreached a scale at which a few percentage points gained on popular\nquestion-answering benchmarks could translate into dozens of millions of\ndollars, placing high pressure on model integrity. At the same time, it is\nbecoming harder and harder to keep track of the data that LLMs have seen; if\nnot impossible with closed-source models like GPT-4 and Claude-3 not divulging\nany information on the training set. As a result, contamination becomes a major\nissue: LLMs' performance may not be reliable anymore, as the high performance\nmay be at least partly due to their previous exposure to the data. This\nlimitation jeopardizes the entire progress in the field of NLP, yet, there\nremains a lack of methods on how to efficiently detect contamination.In this\npaper, we survey all recent work on contamination detection with LLMs, and help\nthe community track contamination levels of LLMs by releasing an open-source\nPython library named LLMSanitize implementing major contamination detection\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Mathieu Ravaut"
                    },
                    {
                        "name": "Bosheng Ding"
                    },
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Hailin Chen"
                    },
                    {
                        "name": "Xingxuan Li"
                    },
                    {
                        "name": "Ruochen Zhao"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "8 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v3",
                "updated": "2024-08-14T21:30:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    21,
                    30,
                    52,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.03119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.03119v2",
                "updated": "2024-08-14T21:11:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    21,
                    11,
                    5,
                    2,
                    227,
                    0
                ],
                "published": "2023-05-04T19:38:14Z",
                "published_parsed": [
                    2023,
                    5,
                    4,
                    19,
                    38,
                    14,
                    3,
                    124,
                    0
                ],
                "title": "Optimizing Autonomous Transfer Hub Networks: Quantifying the Potential\n  Impact of Self-Driving Trucks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Autonomous Transfer Hub Networks: Quantifying the Potential\n  Impact of Self-Driving Trucks"
                },
                "summary": "Autonomous trucks are expected to fundamentally transform the freight\ntransportation industry. In particular, Autonomous Transfer Hub Networks\n(ATHNs), which combine autonomous trucks on middle miles with human-driven\ntrucks on the first and last miles, are seen as the most likely deployment\npathway for this technology. This paper presents a framework to optimize ATHN\noperations and evaluate the benefits of autonomous trucking. By exploiting the\nproblem structure, this paper introduces a flow-based optimization model for\nthis purpose that can be solved by blackbox solvers in a matter of hours. The\nresulting framework is easy to apply and enables the data-driven analysis of\nlarge-scale systems. The power of this approach is demonstrated on a system\nthat spans all of the United States over a four-week horizon. The case study\nquantifies the potential impact of autonomous trucking and shows that ATHNs can\nhave significant benefits over traditional transportation networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous trucks are expected to fundamentally transform the freight\ntransportation industry. In particular, Autonomous Transfer Hub Networks\n(ATHNs), which combine autonomous trucks on middle miles with human-driven\ntrucks on the first and last miles, are seen as the most likely deployment\npathway for this technology. This paper presents a framework to optimize ATHN\noperations and evaluate the benefits of autonomous trucking. By exploiting the\nproblem structure, this paper introduces a flow-based optimization model for\nthis purpose that can be solved by blackbox solvers in a matter of hours. The\nresulting framework is easy to apply and enables the data-driven analysis of\nlarge-scale systems. The power of this approach is demonstrated on a system\nthat spans all of the United States over a four-week horizon. The case study\nquantifies the potential impact of autonomous trucking and shows that ATHNs can\nhave significant benefits over traditional transportation networks."
                },
                "authors": [
                    {
                        "name": "Chungjae Lee"
                    },
                    {
                        "name": "Kevin Dalmeijer"
                    },
                    {
                        "name": "Pascal Van Hentenryck"
                    },
                    {
                        "name": "Peibo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peibo Zhang"
                },
                "author": "Peibo Zhang",
                "arxiv_doi": "10.1016/j.ejtl.2024.100141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ejtl.2024.100141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.03119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.03119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07806v1",
                "updated": "2024-08-14T20:30:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    30,
                    34,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T20:30:34Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    30,
                    34,
                    2,
                    227,
                    0
                ],
                "title": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction"
                },
                "summary": "The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems."
                },
                "authors": [
                    {
                        "name": "Sadra Zargarzadeh"
                    },
                    {
                        "name": "Maryam Mirzaei"
                    },
                    {
                        "name": "Yafei Ou"
                    },
                    {
                        "name": "Mahdi Tavakoli"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Tavakoli"
                },
                "author": "Mahdi Tavakoli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07802v1",
                "updated": "2024-08-14T20:24:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    24,
                    3,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T20:24:03Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    24,
                    3,
                    2,
                    227,
                    0
                ],
                "title": "Kraken: Inherently Parallel Transformers For Efficient Multi-Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kraken: Inherently Parallel Transformers For Efficient Multi-Device\n  Inference"
                },
                "summary": "Large Transformer networks are increasingly used in settings where low\ninference latency can improve the end-user experience and enable new\napplications. However, autoregressive inference is resource intensive and\nrequires parallelism for efficiency. Parallelism introduces collective\ncommunication that is both expensive and represents a phase when hardware\nresources are underutilized. Towards mitigating this, Kraken is an evolution of\nthe standard Transformer architecture that is designed to complement existing\ntensor parallelism schemes for efficient inference on multi-device systems. By\nintroducing a fixed degree of intra-layer model parallelism, the architecture\nallows collective operations to be overlapped with compute, decreasing latency\nand increasing hardware utilization. When trained on OpenWebText, Kraken models\nreach a similar perplexity as standard Transformers while also preserving their\nlanguage modeling capabilities when evaluated on the SuperGLUE benchmark.\nImportantly, when tested on multi-GPU systems using TensorRT-LLM engines,\nKraken speeds up Time To First Token by a mean of 35.6% across a range of model\nsizes, context lengths, and degrees of tensor parallelism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Transformer networks are increasingly used in settings where low\ninference latency can improve the end-user experience and enable new\napplications. However, autoregressive inference is resource intensive and\nrequires parallelism for efficiency. Parallelism introduces collective\ncommunication that is both expensive and represents a phase when hardware\nresources are underutilized. Towards mitigating this, Kraken is an evolution of\nthe standard Transformer architecture that is designed to complement existing\ntensor parallelism schemes for efficient inference on multi-device systems. By\nintroducing a fixed degree of intra-layer model parallelism, the architecture\nallows collective operations to be overlapped with compute, decreasing latency\nand increasing hardware utilization. When trained on OpenWebText, Kraken models\nreach a similar perplexity as standard Transformers while also preserving their\nlanguage modeling capabilities when evaluated on the SuperGLUE benchmark.\nImportantly, when tested on multi-GPU systems using TensorRT-LLM engines,\nKraken speeds up Time To First Token by a mean of 35.6% across a range of model\nsizes, context lengths, and degrees of tensor parallelism."
                },
                "authors": [
                    {
                        "name": "Rohan Baskar Prabhakar"
                    },
                    {
                        "name": "Hengrui Zhang"
                    },
                    {
                        "name": "David Wentlzaff"
                    }
                ],
                "author_detail": {
                    "name": "David Wentlzaff"
                },
                "author": "David Wentlzaff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07791v1",
                "updated": "2024-08-14T20:03:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    3,
                    53,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T20:03:53Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    3,
                    53,
                    2,
                    227,
                    0
                ],
                "title": "An Efficient and Explanatory Image and Text Clustering System with\n  Multimodal Autoencoder Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient and Explanatory Image and Text Clustering System with\n  Multimodal Autoencoder Architecture"
                },
                "summary": "We demonstrate the efficiencies and explanatory abilities of extensions to\nthe common tools of Autoencoders and LLM interpreters, in the novel context of\ncomparing different cultural approaches to the same international news event.\nWe develop a new Convolutional-Recurrent Variational Autoencoder (CRVAE) model\nthat extends the modalities of previous CVAE models, by using fully-connected\nlatent layers to embed in parallel the CNN encodings of video frames, together\nwith the LSTM encodings of their related text derived from audio. We\nincorporate the model within a larger system that includes frame-caption\nalignment, latent space vector clustering, and a novel LLM-based cluster\ninterpreter. We measure, tune, and apply this system to the task of summarizing\na video into three to five thematic clusters, with each theme described by ten\nLLM-produced phrases. We apply this system to two news topics, COVID-19 and the\nWinter Olympics, and five other topics are in progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate the efficiencies and explanatory abilities of extensions to\nthe common tools of Autoencoders and LLM interpreters, in the novel context of\ncomparing different cultural approaches to the same international news event.\nWe develop a new Convolutional-Recurrent Variational Autoencoder (CRVAE) model\nthat extends the modalities of previous CVAE models, by using fully-connected\nlatent layers to embed in parallel the CNN encodings of video frames, together\nwith the LSTM encodings of their related text derived from audio. We\nincorporate the model within a larger system that includes frame-caption\nalignment, latent space vector clustering, and a novel LLM-based cluster\ninterpreter. We measure, tune, and apply this system to the task of summarizing\na video into three to five thematic clusters, with each theme described by ten\nLLM-produced phrases. We apply this system to two news topics, COVID-19 and the\nWinter Olympics, and five other topics are in progress."
                },
                "authors": [
                    {
                        "name": "Tiancheng Shi"
                    },
                    {
                        "name": "Yuanchen Wei"
                    },
                    {
                        "name": "John R. Kender"
                    }
                ],
                "author_detail": {
                    "name": "John R. Kender"
                },
                "author": "John R. Kender",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17435v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17435v4",
                "updated": "2024-08-14T19:23:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    19,
                    23,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-30T20:49:47Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    20,
                    49,
                    47,
                    1,
                    30,
                    0
                ],
                "title": "Can LLMs Replace Economic Choice Prediction Labs? The Case of\n  Language-based Persuasion Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Replace Economic Choice Prediction Labs? The Case of\n  Language-based Persuasion Games"
                },
                "summary": "Human choice prediction in economic contexts is crucial for applications in\nmarketing, finance, public policy, and more. This task, however, is often\nconstrained by the difficulties in acquiring human choice data. With most\nexperimental economics studies focusing on simple choice settings, the AI\ncommunity has explored whether LLMs can substitute for humans in these\npredictions and examined more complex experimental economics settings. However,\na key question remains: can LLMs generate training data for human choice\nprediction? We explore this in language-based persuasion games, a complex\neconomic setting involving natural language in strategic interactions. Our\nexperiments show that models trained on LLM-generated data can effectively\npredict human behavior in these games and even outperform models trained on\nactual human data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human choice prediction in economic contexts is crucial for applications in\nmarketing, finance, public policy, and more. This task, however, is often\nconstrained by the difficulties in acquiring human choice data. With most\nexperimental economics studies focusing on simple choice settings, the AI\ncommunity has explored whether LLMs can substitute for humans in these\npredictions and examined more complex experimental economics settings. However,\na key question remains: can LLMs generate training data for human choice\nprediction? We explore this in language-based persuasion games, a complex\neconomic setting involving natural language in strategic interactions. Our\nexperiments show that models trained on LLM-generated data can effectively\npredict human behavior in these games and even outperform models trained on\nactual human data."
                },
                "authors": [
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Omer Madmon"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    }
                ],
                "author_detail": {
                    "name": "Moshe Tennenholtz"
                },
                "author": "Moshe Tennenholtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17435v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17435v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07773v1",
                "updated": "2024-08-14T18:57:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    57,
                    5,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T18:57:05Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    57,
                    5,
                    2,
                    227,
                    0
                ],
                "title": "MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis"
                },
                "summary": "The complexity and heterogeneity of data in many real-world applications pose\nsignificant challenges for traditional machine learning and signal processing\ntechniques. For instance, in medicine, effective analysis of diverse\nphysiological signals is crucial for patient monitoring and clinical\ndecision-making and yet highly challenging. We introduce MedTsLLM, a general\nmultimodal large language model (LLM) framework that effectively integrates\ntime series data and rich contextual information in the form of text to analyze\nphysiological signals, performing three tasks with clinical relevance: semantic\nsegmentation, boundary detection, and anomaly detection in time series. These\ncritical tasks enable deeper analysis of physiological signals and can provide\nactionable insights for clinicians. We utilize a reprogramming layer to align\nembeddings of time series patches with a pretrained LLM's embedding space and\nmake effective use of raw time series, in conjunction with textual context.\nGiven the multivariate nature of medical datasets, we develop methods to handle\nmultiple covariates. We additionally tailor the text prompt to include\npatient-specific information. Our model outperforms state-of-the-art baselines,\nincluding deep learning models, other LLMs, and clinical methods across\nmultiple medical domains, specifically electrocardiograms and respiratory\nwaveforms. MedTsLLM presents a promising step towards harnessing the power of\nLLMs for medical time series analysis that can elevate data-driven tools for\nclinicians and improve patient outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity and heterogeneity of data in many real-world applications pose\nsignificant challenges for traditional machine learning and signal processing\ntechniques. For instance, in medicine, effective analysis of diverse\nphysiological signals is crucial for patient monitoring and clinical\ndecision-making and yet highly challenging. We introduce MedTsLLM, a general\nmultimodal large language model (LLM) framework that effectively integrates\ntime series data and rich contextual information in the form of text to analyze\nphysiological signals, performing three tasks with clinical relevance: semantic\nsegmentation, boundary detection, and anomaly detection in time series. These\ncritical tasks enable deeper analysis of physiological signals and can provide\nactionable insights for clinicians. We utilize a reprogramming layer to align\nembeddings of time series patches with a pretrained LLM's embedding space and\nmake effective use of raw time series, in conjunction with textual context.\nGiven the multivariate nature of medical datasets, we develop methods to handle\nmultiple covariates. We additionally tailor the text prompt to include\npatient-specific information. Our model outperforms state-of-the-art baselines,\nincluding deep learning models, other LLMs, and clinical methods across\nmultiple medical domains, specifically electrocardiograms and respiratory\nwaveforms. MedTsLLM presents a promising step towards harnessing the power of\nLLMs for medical time series analysis that can elevate data-driven tools for\nclinicians and improve patient outcomes."
                },
                "authors": [
                    {
                        "name": "Nimeesha Chan"
                    },
                    {
                        "name": "Felix Parker"
                    },
                    {
                        "name": "William Bennett"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Mung Yao Jia"
                    },
                    {
                        "name": "James Fackler"
                    },
                    {
                        "name": "Kimia Ghobadi"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Ghobadi"
                },
                "author": "Kimia Ghobadi",
                "arxiv_comment": "published in Proceedings of Machine Learning Research, MLHC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07772v1",
                "updated": "2024-08-14T18:49:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    49,
                    27,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T18:49:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    49,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Out-of-Distribution Learning with Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Learning with Human Feedback"
                },
                "summary": "Out-of-distribution (OOD) learning often relies heavily on statistical\napproaches or predefined assumptions about OOD data distributions, hindering\ntheir efficacy in addressing multifaceted challenges of OOD generalization and\nOOD detection in real-world deployment environments. This paper presents a\nnovel framework for OOD learning with human feedback, which can provide\ninvaluable insights into the nature of OOD shifts and guide effective model\nadaptation. Our framework capitalizes on the freely available unlabeled data in\nthe wild that captures the environmental test-time OOD distributions under both\ncovariate and semantic shifts. To harness such data, our key idea is to\nselectively provide human feedback and label a small number of informative\nsamples from the wild data distribution, which are then used to train a\nmulti-class classifier and an OOD detector. By exploiting human feedback, we\nenhance the robustness and reliability of machine learning models, equipping\nthem with the capability to handle OOD scenarios with greater precision. We\nprovide theoretical insights on the generalization error bounds to justify our\nalgorithm. Extensive experiments show the superiority of our method,\noutperforming the current state-of-the-art by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) learning often relies heavily on statistical\napproaches or predefined assumptions about OOD data distributions, hindering\ntheir efficacy in addressing multifaceted challenges of OOD generalization and\nOOD detection in real-world deployment environments. This paper presents a\nnovel framework for OOD learning with human feedback, which can provide\ninvaluable insights into the nature of OOD shifts and guide effective model\nadaptation. Our framework capitalizes on the freely available unlabeled data in\nthe wild that captures the environmental test-time OOD distributions under both\ncovariate and semantic shifts. To harness such data, our key idea is to\nselectively provide human feedback and label a small number of informative\nsamples from the wild data distribution, which are then used to train a\nmulti-class classifier and an OOD detector. By exploiting human feedback, we\nenhance the robustness and reliability of machine learning models, equipping\nthem with the capability to handle OOD scenarios with greater precision. We\nprovide theoretical insights on the generalization error bounds to justify our\nalgorithm. Extensive experiments show the superiority of our method,\noutperforming the current state-of-the-art by a significant margin."
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Xuefeng Du"
                    },
                    {
                        "name": "Katie Rainey"
                    },
                    {
                        "name": "Shibin Parameswaran"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14058v2",
                "updated": "2024-08-14T18:45:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    45,
                    17,
                    2,
                    227,
                    0
                ],
                "published": "2024-05-22T23:06:34Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    23,
                    6,
                    34,
                    2,
                    143,
                    0
                ],
                "title": "Formally Verifying Deep Reinforcement Learning Controllers with Lyapunov\n  Barrier Certificates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formally Verifying Deep Reinforcement Learning Controllers with Lyapunov\n  Barrier Certificates"
                },
                "summary": "Deep reinforcement learning (DRL) is a powerful machine learning paradigm for\ngenerating agents that control autonomous systems. However, the ``black box''\nnature of DRL agents limits their deployment in real-world safety-critical\napplications. A promising approach for providing strong guarantees on an\nagent's behavior is to use Neural Lyapunov Barrier (NLB) certificates, which\nare learned functions over the system whose properties indirectly imply that an\nagent behaves as desired. However, NLB-based certificates are typically\ndifficult to learn and even more difficult to verify, especially for complex\nsystems. In this work, we present a novel method for training and verifying\nNLB-based certificates for discrete-time systems. Specifically, we introduce a\ntechnique for certificate composition, which simplifies the verification of\nhighly-complex systems by strategically designing a sequence of certificates.\nWhen jointly verified with neural network verification engines, these\ncertificates provide a formal guarantee that a DRL agent both achieves its\ngoals and avoids unsafe behavior. Furthermore, we introduce a technique for\ncertificate filtering, which significantly simplifies the process of producing\nformally verified certificates. We demonstrate the merits of our approach with\na case study on providing safety and liveness guarantees for a DRL-controlled\nspacecraft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) is a powerful machine learning paradigm for\ngenerating agents that control autonomous systems. However, the ``black box''\nnature of DRL agents limits their deployment in real-world safety-critical\napplications. A promising approach for providing strong guarantees on an\nagent's behavior is to use Neural Lyapunov Barrier (NLB) certificates, which\nare learned functions over the system whose properties indirectly imply that an\nagent behaves as desired. However, NLB-based certificates are typically\ndifficult to learn and even more difficult to verify, especially for complex\nsystems. In this work, we present a novel method for training and verifying\nNLB-based certificates for discrete-time systems. Specifically, we introduce a\ntechnique for certificate composition, which simplifies the verification of\nhighly-complex systems by strategically designing a sequence of certificates.\nWhen jointly verified with neural network verification engines, these\ncertificates provide a formal guarantee that a DRL agent both achieves its\ngoals and avoids unsafe behavior. Furthermore, we introduce a technique for\ncertificate filtering, which significantly simplifies the process of producing\nformally verified certificates. We demonstrate the merits of our approach with\na case study on providing safety and liveness guarantees for a DRL-controlled\nspacecraft."
                },
                "authors": [
                    {
                        "name": "Udayan Mandal"
                    },
                    {
                        "name": "Guy Amir"
                    },
                    {
                        "name": "Haoze Wu"
                    },
                    {
                        "name": "Ieva Daukantas"
                    },
                    {
                        "name": "Fletcher Lee Newell"
                    },
                    {
                        "name": "Umberto J. Ravaioli"
                    },
                    {
                        "name": "Baoluo Meng"
                    },
                    {
                        "name": "Michael Durling"
                    },
                    {
                        "name": "Milan Ganai"
                    },
                    {
                        "name": "Tobey Shim"
                    },
                    {
                        "name": "Guy Katz"
                    },
                    {
                        "name": "Clark Barrett"
                    }
                ],
                "author_detail": {
                    "name": "Clark Barrett"
                },
                "author": "Clark Barrett",
                "arxiv_comment": "To appear in FMCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06537v2",
                "updated": "2024-08-14T18:38:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    38,
                    11,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T00:06:56Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    0,
                    6,
                    56,
                    1,
                    226,
                    0
                ],
                "title": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data"
                },
                "summary": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models."
                },
                "authors": [
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07758v1",
                "updated": "2024-08-14T18:14:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    14,
                    40,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T18:14:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    14,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "RAVE Checklist: Recommendations for Overcoming Challenges in\n  Retrospective Safety Studies of Automated Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAVE Checklist: Recommendations for Overcoming Challenges in\n  Retrospective Safety Studies of Automated Driving Systems"
                },
                "summary": "The public, regulators, and domain experts alike seek to understand the\neffect of deployed SAE level 4 automated driving system (ADS) technologies on\nsafety. The recent expansion of ADS technology deployments is paving the way\nfor early stage safety impact evaluations, whereby the observational data from\nboth an ADS and a representative benchmark fleet are compared to quantify\nsafety performance. In January 2024, a working group of experts across\nacademia, insurance, and industry came together in Washington, DC to discuss\nthe current and future challenges in performing such evaluations. A subset of\nthis working group then met, virtually, on multiple occasions to produce this\npaper. This paper presents the RAVE (Retrospective Automated Vehicle\nEvaluation) checklist, a set of fifteen recommendations for performing and\nevaluating retrospective ADS performance comparisons. The recommendations are\ncentered around the concepts of (1) quality and validity, (2) transparency, and\n(3) interpretation. Over time, it is anticipated there will be a large and\nvaried body of work evaluating the observed performance of these ADS fleets.\nEstablishing and promoting good scientific practices benefits the work of\nstakeholders, many of whom may not be subject matter experts. This working\ngroup's intentions are to: i) strengthen individual research studies and ii)\nmake the at-large community more informed on how to evaluate this collective\nbody of work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The public, regulators, and domain experts alike seek to understand the\neffect of deployed SAE level 4 automated driving system (ADS) technologies on\nsafety. The recent expansion of ADS technology deployments is paving the way\nfor early stage safety impact evaluations, whereby the observational data from\nboth an ADS and a representative benchmark fleet are compared to quantify\nsafety performance. In January 2024, a working group of experts across\nacademia, insurance, and industry came together in Washington, DC to discuss\nthe current and future challenges in performing such evaluations. A subset of\nthis working group then met, virtually, on multiple occasions to produce this\npaper. This paper presents the RAVE (Retrospective Automated Vehicle\nEvaluation) checklist, a set of fifteen recommendations for performing and\nevaluating retrospective ADS performance comparisons. The recommendations are\ncentered around the concepts of (1) quality and validity, (2) transparency, and\n(3) interpretation. Over time, it is anticipated there will be a large and\nvaried body of work evaluating the observed performance of these ADS fleets.\nEstablishing and promoting good scientific practices benefits the work of\nstakeholders, many of whom may not be subject matter experts. This working\ngroup's intentions are to: i) strengthen individual research studies and ii)\nmake the at-large community more informed on how to evaluate this collective\nbody of work."
                },
                "authors": [
                    {
                        "name": "John M. Scanlon"
                    },
                    {
                        "name": "Eric R. Teoh"
                    },
                    {
                        "name": "David G. Kidd"
                    },
                    {
                        "name": "Kristofer D. Kusano"
                    },
                    {
                        "name": "Jonas Bärgman"
                    },
                    {
                        "name": "Geoffrey Chi-Johnston"
                    },
                    {
                        "name": "Luigi Di Lillo"
                    },
                    {
                        "name": "Francesca Favaro"
                    },
                    {
                        "name": "Carol Flannagan"
                    },
                    {
                        "name": "Henrik Liers"
                    },
                    {
                        "name": "Bonnie Lin"
                    },
                    {
                        "name": "Magdalena Lindman"
                    },
                    {
                        "name": "Shane McLaughlin"
                    },
                    {
                        "name": "Miguel Perez"
                    },
                    {
                        "name": "Trent Victor"
                    }
                ],
                "author_detail": {
                    "name": "Trent Victor"
                },
                "author": "Trent Victor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07754v1",
                "updated": "2024-08-14T18:06:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    6,
                    29,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T18:06:29Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    6,
                    29,
                    2,
                    227,
                    0
                ],
                "title": "Local Cold Load Pick-up Estimation Using Customer Energy Consumption\n  Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Cold Load Pick-up Estimation Using Customer Energy Consumption\n  Measurements"
                },
                "summary": "Thermostatically-controlled loads have a significant impact on electricity\ndemand after service is restored following an outage, a phenomenon known as\ncold load pick-up (CLPU). Active management of CLPU is becoming an essential\ntool for distribution system operators who seek to defer network upgrades and\nspeed up post-outage customer restoration. One key functionality needed for\nactively managing CLPU is its forecast at various scales. The widespread\ndeployment of smart metering devices is also opening up new opportunities for\ndata-driven load modeling and forecast. In this paper, we propose an approach\nfor customer-side estimation of CLPU using time-stamped local load\nmeasurements. The proposed method uses Auto-Regressive Integrated Moving\nAverage (ARIMA) modeling for short-term foregone energy consumption forecast\nduring an outage. Forecasts are made on an hourly basis to estimate the energy\nto potentially recover after outages lasting up to several hours. Moreover, to\naccount for changing customer behavior and weather, the model order is adjusted\ndynamically. Simulation results based on actual smart meter measurements are\npresented for 50 residential customers over the duration of one year. These\nresults are validated using physical modeling of residential loads and are\nshown to match well the ARIMA-based forecasts. Additionally, accuracy and\nexecution speed has been compared with other state-of-the-art approaches for\ntime-series forecasting including Long Short Term Memory Network (LSTM) and\nHolt-Winters Exponential Smoothing (HWES). ARIMA-based forecast is found to\noffer superior performance both in terms of accuracy and computation speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermostatically-controlled loads have a significant impact on electricity\ndemand after service is restored following an outage, a phenomenon known as\ncold load pick-up (CLPU). Active management of CLPU is becoming an essential\ntool for distribution system operators who seek to defer network upgrades and\nspeed up post-outage customer restoration. One key functionality needed for\nactively managing CLPU is its forecast at various scales. The widespread\ndeployment of smart metering devices is also opening up new opportunities for\ndata-driven load modeling and forecast. In this paper, we propose an approach\nfor customer-side estimation of CLPU using time-stamped local load\nmeasurements. The proposed method uses Auto-Regressive Integrated Moving\nAverage (ARIMA) modeling for short-term foregone energy consumption forecast\nduring an outage. Forecasts are made on an hourly basis to estimate the energy\nto potentially recover after outages lasting up to several hours. Moreover, to\naccount for changing customer behavior and weather, the model order is adjusted\ndynamically. Simulation results based on actual smart meter measurements are\npresented for 50 residential customers over the duration of one year. These\nresults are validated using physical modeling of residential loads and are\nshown to match well the ARIMA-based forecasts. Additionally, accuracy and\nexecution speed has been compared with other state-of-the-art approaches for\ntime-series forecasting including Long Short Term Memory Network (LSTM) and\nHolt-Winters Exponential Smoothing (HWES). ARIMA-based forecast is found to\noffer superior performance both in terms of accuracy and computation speed."
                },
                "authors": [
                    {
                        "name": "Sanja Bajic"
                    },
                    {
                        "name": "François Bouffard"
                    },
                    {
                        "name": "Hannah Michalska"
                    },
                    {
                        "name": "Géza Joós"
                    }
                ],
                "author_detail": {
                    "name": "Géza Joós"
                },
                "author": "Géza Joós",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03862v3",
                "updated": "2024-08-14T18:01:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    1,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2024-05-06T21:20:35Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    21,
                    20,
                    35,
                    0,
                    127,
                    0
                ],
                "title": "Persona Inconstancy in Multi-Agent LLM Collaboration: Conformity,\n  Confabulation, and Impersonation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona Inconstancy in Multi-Agent LLM Collaboration: Conformity,\n  Confabulation, and Impersonation"
                },
                "summary": "Multi-agent AI systems can be used for simulating collective decision-making\nin scientific and practical applications. They can also be used to introduce a\ndiverse group discussion step in chatbot pipelines, enhancing the cultural\nsensitivity of the chatbot's responses. These applications, however, are\npredicated on the ability of AI agents to reliably adopt assigned personas and\nmimic human interactions. To see whether LLM agents satisfy these requirements,\nwe examine AI agent ensembles engaged in cross-national collaboration and\ndebate by analyzing their private responses and chat transcripts. Our findings\nsuggest that multi-agent discussions can support collective AI decisions that\nmore often reflect diverse perspectives, yet this effect is tempered by the\nagents' susceptibility to conformity due to perceived peer pressure and\noccasional challenges in maintaining consistent personas and opinions.\nInstructions that encourage debate in support of one's opinions rather than\ncollaboration increase the rate of inconstancy. Without addressing the factors\nwe identify, the full potential of multi-agent frameworks for producing more\nculturally diverse AI outputs or more realistic simulations of group\ndecision-making may remain untapped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent AI systems can be used for simulating collective decision-making\nin scientific and practical applications. They can also be used to introduce a\ndiverse group discussion step in chatbot pipelines, enhancing the cultural\nsensitivity of the chatbot's responses. These applications, however, are\npredicated on the ability of AI agents to reliably adopt assigned personas and\nmimic human interactions. To see whether LLM agents satisfy these requirements,\nwe examine AI agent ensembles engaged in cross-national collaboration and\ndebate by analyzing their private responses and chat transcripts. Our findings\nsuggest that multi-agent discussions can support collective AI decisions that\nmore often reflect diverse perspectives, yet this effect is tempered by the\nagents' susceptibility to conformity due to perceived peer pressure and\noccasional challenges in maintaining consistent personas and opinions.\nInstructions that encourage debate in support of one's opinions rather than\ncollaboration increase the rate of inconstancy. Without addressing the factors\nwe identify, the full potential of multi-agent frameworks for producing more\nculturally diverse AI outputs or more realistic simulations of group\ndecision-making may remain untapped."
                },
                "authors": [
                    {
                        "name": "Razan Baltaji"
                    },
                    {
                        "name": "Babak Hemmatian"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "16 pages, 8 figures, 3 tables",
                "arxiv_journal_ref": "The 2nd Workshop on Cross-Cultural Considerations in NLP (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03852v2",
                "updated": "2024-08-14T18:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    18,
                    0,
                    57,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-04T11:34:43Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    11,
                    34,
                    43,
                    3,
                    186,
                    0
                ],
                "title": "Low-latency machine learning FPGA accelerator for multi-qubit-state\n  discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency machine learning FPGA accelerator for multi-qubit-state\n  discrimination"
                },
                "summary": "Measuring a qubit state is a fundamental yet error-prone operation in quantum\ncomputing. These errors can arise from various sources, such as crosstalk,\nspontaneous state transitions, and excitations caused by the readout pulse.\nHere, we utilize an integrated approach to deploy neural networks onto\nfield-programmable gate arrays (FPGA). We demonstrate that implementing a fully\nconnected neural network accelerator for multi-qubit readout is advantageous,\nbalancing computational complexity with low latency requirements without\nsignificant loss in accuracy. The neural network is implemented by quantizing\nweights, activation functions, and inputs. The hardware accelerator performs\nfrequency-multiplexed readout of five superconducting qubits in less than 50 ns\non a radio frequency system on chip (RFSoC) ZCU111 FPGA, marking the advent of\nRFSoC-based low-latency multi-qubit readout using neural networks. These\nmodules can be implemented and integrated into existing quantum control and\nreadout platforms, making the RFSoC ZCU111 ready for experimental deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring a qubit state is a fundamental yet error-prone operation in quantum\ncomputing. These errors can arise from various sources, such as crosstalk,\nspontaneous state transitions, and excitations caused by the readout pulse.\nHere, we utilize an integrated approach to deploy neural networks onto\nfield-programmable gate arrays (FPGA). We demonstrate that implementing a fully\nconnected neural network accelerator for multi-qubit readout is advantageous,\nbalancing computational complexity with low latency requirements without\nsignificant loss in accuracy. The neural network is implemented by quantizing\nweights, activation functions, and inputs. The hardware accelerator performs\nfrequency-multiplexed readout of five superconducting qubits in less than 50 ns\non a radio frequency system on chip (RFSoC) ZCU111 FPGA, marking the advent of\nRFSoC-based low-latency multi-qubit readout using neural networks. These\nmodules can be implemented and integrated into existing quantum control and\nreadout platforms, making the RFSoC ZCU111 ready for experimental deployment."
                },
                "authors": [
                    {
                        "name": "Pradeep Kumar Gautam"
                    },
                    {
                        "name": "Shantharam Kalipatnapu"
                    },
                    {
                        "name": "Shankaranarayanan H"
                    },
                    {
                        "name": "Ujjawal Singhal"
                    },
                    {
                        "name": "Benjamin Lienhard"
                    },
                    {
                        "name": "Vibhor Singh"
                    },
                    {
                        "name": "Chetan Singh Thakur"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Singh Thakur"
                },
                "author": "Chetan Singh Thakur",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07702v1",
                "updated": "2024-08-14T17:59:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "title": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models"
                },
                "summary": "Schema linking is a crucial step in Text-to-SQL pipelines, which translate\nnatural language queries into SQL. The goal of schema linking is to retrieve\nrelevant tables and columns (signal) while disregarding irrelevant ones\n(noise). However, imperfect schema linking can often exclude essential columns\nneeded for accurate query generation. In this work, we revisit the need for\nschema linking when using the latest generation of large language models\n(LLMs). We find empirically that newer models are adept at identifying relevant\nschema elements during generation, without the need for explicit schema\nlinking. This allows Text-to-SQL pipelines to bypass schema linking entirely\nand instead pass the full database schema to the LLM, eliminating the risk of\nexcluding necessary information. Furthermore, as alternatives to schema\nlinking, we propose techniques that improve Text-to-SQL accuracy without\ncompromising on essential schema information. Our approach achieves 71.83\\%\nexecution accuracy on the BIRD benchmark, ranking first at the time of\nsubmission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a crucial step in Text-to-SQL pipelines, which translate\nnatural language queries into SQL. The goal of schema linking is to retrieve\nrelevant tables and columns (signal) while disregarding irrelevant ones\n(noise). However, imperfect schema linking can often exclude essential columns\nneeded for accurate query generation. In this work, we revisit the need for\nschema linking when using the latest generation of large language models\n(LLMs). We find empirically that newer models are adept at identifying relevant\nschema elements during generation, without the need for explicit schema\nlinking. This allows Text-to-SQL pipelines to bypass schema linking entirely\nand instead pass the full database schema to the LLM, eliminating the risk of\nexcluding necessary information. Furthermore, as alternatives to schema\nlinking, we propose techniques that improve Text-to-SQL accuracy without\ncompromising on essential schema information. Our approach achieves 71.83\\%\nexecution accuracy on the BIRD benchmark, ranking first at the time of\nsubmission."
                },
                "authors": [
                    {
                        "name": "Karime Maamari"
                    },
                    {
                        "name": "Fadhil Abubaker"
                    },
                    {
                        "name": "Daniel Jaroslawicz"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07692v1",
                "updated": "2024-08-14T17:50:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    50,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:50:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    50,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "On the Parameter Selection of Phase-transmittance Radial Basis Function\n  Neural Networks for Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Parameter Selection of Phase-transmittance Radial Basis Function\n  Neural Networks for Communication Systems"
                },
                "summary": "In the ever-evolving field of digital communication systems, complex-valued\nneural networks (CVNNs) have become a cornerstone, delivering exceptional\nperformance in tasks like equalization, channel estimation, beamforming, and\ndecoding. Among the myriad of CVNN architectures, the phase-transmittance\nradial basis function neural network (PT-RBF) stands out, especially when\noperating in noisy environments such as 5G MIMO systems. Despite its\ncapabilities, achieving convergence in multi-layered, multi-input, and\nmulti-output PT-RBFs remains a daunting challenge. Addressing this gap, this\npaper presents a novel Deep PT-RBF parameter initialization technique. Through\nrigorous simulations conforming to 3GPP TS 38 standards, our method not only\noutperforms conventional initialization strategies like random, $K$-means, and\nconstellation-based methods but is also the only approach to achieve successful\nconvergence in deep PT-RBF architectures. These findings pave the way to more\nrobust and efficient neural network deployments in complex digital\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the ever-evolving field of digital communication systems, complex-valued\nneural networks (CVNNs) have become a cornerstone, delivering exceptional\nperformance in tasks like equalization, channel estimation, beamforming, and\ndecoding. Among the myriad of CVNN architectures, the phase-transmittance\nradial basis function neural network (PT-RBF) stands out, especially when\noperating in noisy environments such as 5G MIMO systems. Despite its\ncapabilities, achieving convergence in multi-layered, multi-input, and\nmulti-output PT-RBFs remains a daunting challenge. Addressing this gap, this\npaper presents a novel Deep PT-RBF parameter initialization technique. Through\nrigorous simulations conforming to 3GPP TS 38 standards, our method not only\noutperforms conventional initialization strategies like random, $K$-means, and\nconstellation-based methods but is also the only approach to achieve successful\nconvergence in deep PT-RBF architectures. These findings pave the way to more\nrobust and efficient neural network deployments in complex digital\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Jonathan A. Soares"
                    },
                    {
                        "name": "Kayol S. Mayer"
                    },
                    {
                        "name": "Dalton S. Arantes"
                    }
                ],
                "author_detail": {
                    "name": "Dalton S. Arantes"
                },
                "author": "Dalton S. Arantes",
                "arxiv_comment": "IEEE International Conference on Machine Learning for Communication\n  and Networking (ICMLCN 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07665v1",
                "updated": "2024-08-14T16:55:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    55,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T16:55:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    55,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models"
                },
                "summary": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17762v2",
                "updated": "2024-08-14T16:00:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    0,
                    49,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-27T18:55:17Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    18,
                    55,
                    17,
                    1,
                    58,
                    0
                ],
                "title": "Massive Activations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Activations in Large Language Models"
                },
                "summary": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations."
                },
                "authors": [
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "First Conference on Language Modeling (COLM), 2024. Website at\n  https://eric-mingjie.github.io/massive-activations/index.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15363v2",
                "updated": "2024-08-14T15:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    32,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-01T15:17:39Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    15,
                    17,
                    39,
                    0,
                    92,
                    0
                ],
                "title": "Exploring LLM Multi-Agents for ICD Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM Multi-Agents for ICD Coding"
                },
                "summary": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes."
                },
                "authors": [
                    {
                        "name": "Rumeng Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "12pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07611v1",
                "updated": "2024-08-14T15:19:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T15:19:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions."
                },
                "authors": [
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Xuefeng Liang"
                    },
                    {
                        "name": "Yuhui Liu"
                    },
                    {
                        "name": "Kaihua Ni"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Zetian Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zetian Hu"
                },
                "author": "Zetian Hu",
                "arxiv_comment": "8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01419v2",
                "updated": "2024-08-14T14:42:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    42,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-05-02T16:08:08Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    8,
                    3,
                    123,
                    0
                ],
                "title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT"
                },
                "summary": "This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future."
                },
                "authors": [
                    {
                        "name": "Paola Vitolo"
                    },
                    {
                        "name": "George Psaltakis"
                    },
                    {
                        "name": "Michael Tomlinson"
                    },
                    {
                        "name": "Gian Domenico Licciardo"
                    },
                    {
                        "name": "Andreas G. Andreou"
                    }
                ],
                "author_detail": {
                    "name": "Andreas G. Andreou"
                },
                "author": "Andreas G. Andreou",
                "arxiv_comment": "This paper was presented at the IEEE/ACM International Conference on\n  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07589v1",
                "updated": "2024-08-14T14:41:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    41,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:41:13Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    41,
                    13,
                    2,
                    227,
                    0
                ],
                "title": "Optimizing UAV Trajectory for Emergency Response Operations under Real\n  3D Environments: Integrating Priority Levels and LoS Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing UAV Trajectory for Emergency Response Operations under Real\n  3D Environments: Integrating Priority Levels and LoS Constraints"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as a critical component in\nnext-generation wireless networks, particularly for disaster recovery\nscenarios, due to their flexibility, mobility, and rapid deployment\ncapabilities. This paper focuses on optimizing UAV trajectories to ensure\neffective communication in disaster-stricken areas using terahertz (THz) links.\nWe address specific challenges such as energy consumption, user priority\nlevels, and navigating complex urban environments to maintain Line of Sight\n(LoS) connections amidst 3D obstacles. Our contributions include the\ndevelopment of a detailed modeling approach using online 3D map data, the\nformulation of an optimal trajectory optimization problem, and the proposal of\na Genetic Algorithm (GA)-based method alongside an enhanced heuristic algorithm\nfor faster convergence. Through 3D simulations, we demonstrate the trade-off\nbetween minimizing total service time and prioritizing higher-weight nodes,\nshowing the impact of different priority weight factors on the trajectory time.\nThe proposed algorithms are evaluated using real-world data from the West Bay\narea of Doha, Qatar, demonstrating their effectiveness in optimizing UAV\ntrajectories for emergency response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) have emerged as a critical component in\nnext-generation wireless networks, particularly for disaster recovery\nscenarios, due to their flexibility, mobility, and rapid deployment\ncapabilities. This paper focuses on optimizing UAV trajectories to ensure\neffective communication in disaster-stricken areas using terahertz (THz) links.\nWe address specific challenges such as energy consumption, user priority\nlevels, and navigating complex urban environments to maintain Line of Sight\n(LoS) connections amidst 3D obstacles. Our contributions include the\ndevelopment of a detailed modeling approach using online 3D map data, the\nformulation of an optimal trajectory optimization problem, and the proposal of\na Genetic Algorithm (GA)-based method alongside an enhanced heuristic algorithm\nfor faster convergence. Through 3D simulations, we demonstrate the trade-off\nbetween minimizing total service time and prioritizing higher-weight nodes,\nshowing the impact of different priority weight factors on the trajectory time.\nThe proposed algorithms are evaluated using real-world data from the West Bay\narea of Doha, Qatar, demonstrating their effectiveness in optimizing UAV\ntrajectories for emergency response."
                },
                "authors": [
                    {
                        "name": "Mohammad Taghi Dabiri"
                    },
                    {
                        "name": "Mazen Hasna"
                    },
                    {
                        "name": "Saud Althunibat"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    }
                ],
                "author_detail": {
                    "name": "Khalid Qaraqe"
                },
                "author": "Khalid Qaraqe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07583v1",
                "updated": "2024-08-14T14:28:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:28:11Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "title": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey"
                },
                "summary": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development."
                },
                "authors": [
                    {
                        "name": "Hamza Kheddar"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Kheddar"
                },
                "author": "Hamza Kheddar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.04760 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01766v3",
                "updated": "2024-08-14T13:41:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    41,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-31T14:52:02Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    14,
                    52,
                    2,
                    2,
                    31,
                    0
                ],
                "title": "LLM Voting: Human Choices and AI Collective Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Voting: Human Choices and AI Collective Decision Making"
                },
                "summary": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes."
                },
                "authors": [
                    {
                        "name": "Joshua C. Yang"
                    },
                    {
                        "name": "Damian Dailisan"
                    },
                    {
                        "name": "Marcin Korecki"
                    },
                    {
                        "name": "Carina I. Hausladen"
                    },
                    {
                        "name": "Dirk Helbing"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Helbing"
                },
                "author": "Dirk Helbing",
                "arxiv_comment": "Accepted in AAAI Conference on AI, Ethics, and Society (AIES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 91B14, 91C20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07542v1",
                "updated": "2024-08-14T13:22:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    22,
                    14,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:22:14Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    22,
                    14,
                    2,
                    227,
                    0
                ],
                "title": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation"
                },
                "summary": "Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%."
                },
                "authors": [
                    {
                        "name": "Simon Kloker"
                    },
                    {
                        "name": "Herbertson Bukoli"
                    },
                    {
                        "name": "Twaha Kateete"
                    }
                ],
                "author_detail": {
                    "name": "Twaha Kateete"
                },
                "author": "Twaha Kateete",
                "arxiv_comment": "Presented at Ndejje University Second Annual Research Dissemination\n  Symposium 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10020v2",
                "updated": "2024-08-14T13:15:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    15,
                    0,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-15T05:06:21Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    5,
                    6,
                    21,
                    4,
                    75,
                    0
                ],
                "title": "Lost in Overlap: Exploring Watermark Collision in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Overlap: Exploring Watermark Collision in LLMs"
                },
                "summary": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications."
                },
                "authors": [
                    {
                        "name": "Yiyang Luo"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Chao Gu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Gu"
                },
                "author": "Chao Gu",
                "arxiv_comment": "Long Paper, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07531v1",
                "updated": "2024-08-14T13:03:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Development of a Multi-Agent Clinical Decision Support System for Korean\n  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in\n  Emergency Departments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Multi-Agent Clinical Decision Support System for Korean\n  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in\n  Emergency Departments"
                },
                "summary": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation."
                },
                "authors": [
                    {
                        "name": "Seungjun Han"
                    },
                    {
                        "name": "Wongyung Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wongyung Choi"
                },
                "author": "Wongyung Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07526v1",
                "updated": "2024-08-14T13:01:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    30,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:01:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Learning-based Models for Vulnerability Detection: An Extensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based Models for Vulnerability Detection: An Extensive Study"
                },
                "summary": "Though many deep learning-based models have made great progress in\nvulnerability detection, we have no good understanding of these models, which\nlimits the further advancement of model capability, understanding of the\nmechanism of model detection, and efficiency and safety of practical\napplication of models. In this paper, we extensively and comprehensively\ninvestigate two types of state-of-the-art learning-based approaches\n(sequence-based and graph-based) by conducting experiments on a recently built\nlarge-scale dataset. We investigate seven research questions from five\ndimensions, namely model capabilities, model interpretation, model stability,\nease of use of model, and model economy. We experimentally demonstrate the\npriority of sequence-based models and the limited abilities of both LLM\n(ChatGPT) and graph-based models. We explore the types of vulnerability that\nlearning-based models skilled in and reveal the instability of the models\nthough the input is subtlely semantical-equivalently changed. We empirically\nexplain what the models have learned. We summarize the pre-processing as well\nas requirements for easily using the models. Finally, we initially induce the\nvital information for economically and safely practical usage of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though many deep learning-based models have made great progress in\nvulnerability detection, we have no good understanding of these models, which\nlimits the further advancement of model capability, understanding of the\nmechanism of model detection, and efficiency and safety of practical\napplication of models. In this paper, we extensively and comprehensively\ninvestigate two types of state-of-the-art learning-based approaches\n(sequence-based and graph-based) by conducting experiments on a recently built\nlarge-scale dataset. We investigate seven research questions from five\ndimensions, namely model capabilities, model interpretation, model stability,\nease of use of model, and model economy. We experimentally demonstrate the\npriority of sequence-based models and the limited abilities of both LLM\n(ChatGPT) and graph-based models. We explore the types of vulnerability that\nlearning-based models skilled in and reveal the instability of the models\nthough the input is subtlely semantical-equivalently changed. We empirically\nexplain what the models have learned. We summarize the pre-processing as well\nas requirements for easily using the models. Finally, we initially induce the\nvital information for economically and safely practical usage of these models."
                },
                "authors": [
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Liyu Shen"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Shaohua Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Wang"
                },
                "author": "Shaohua Wang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00656v2",
                "updated": "2024-08-14T12:42:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    42,
                    44,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-31T12:01:32Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    12,
                    1,
                    32,
                    6,
                    91,
                    0
                ],
                "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavLLM: Towards Robust and Adaptive Speech Large Language Model"
                },
                "summary": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}."
                },
                "authors": [
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sanyuan Chen"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Hongkun Hao"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Sunit Sivasankaran"
                    },
                    {
                        "name": "Linquan Liu"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07505v1",
                "updated": "2024-08-14T12:32:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:32:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Large Language Models Know What Makes Exemplary Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Know What Makes Exemplary Contexts"
                },
                "summary": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval."
                },
                "authors": [
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jianda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianda Chen"
                },
                "author": "Jianda Chen",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07494v1",
                "updated": "2024-08-14T12:19:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    19,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:19:25Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    19,
                    25,
                    2,
                    227,
                    0
                ],
                "title": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs"
                },
                "summary": "We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U."
                },
                "authors": [
                    {
                        "name": "Jan Luca Scheerer"
                    },
                    {
                        "name": "Anton Lykov"
                    },
                    {
                        "name": "Moe Kayali"
                    },
                    {
                        "name": "Ilias Fountalis"
                    },
                    {
                        "name": "Dan Olteanu"
                    },
                    {
                        "name": "Nikolaos Vasiloglou"
                    },
                    {
                        "name": "Dan Suciu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Suciu"
                },
                "author": "Dan Suciu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07482v1",
                "updated": "2024-08-14T11:55:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:55:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice."
                },
                "authors": [
                    {
                        "name": "Ning Lu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Jiantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Ma"
                },
                "author": "Jiantao Ma",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]