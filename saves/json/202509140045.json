[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v1",
                "updated": "2025-09-08T00:57:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.09679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09679v1",
                "updated": "2025-09-11T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    51,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    51,
                    3,
                    254,
                    0
                ],
                "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms"
                },
                "summary": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot."
                },
                "authors": [
                    {
                        "name": "Bingxin Xu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "arxiv_comment": "Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09678v1",
                "updated": "2025-09-11T17:59:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    46,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:46Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    46,
                    3,
                    254,
                    0
                ],
                "title": "Cosmic $τ$ensions Indirectly Correlate with Reionization Optical\n  Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic $τ$ensions Indirectly Correlate with Reionization Optical\n  Depth"
                },
                "summary": "The reionization optical depth $\\tau_{\\rm reio}$ has interesting connections\nto existing cosmological anomalies. As first studied in the context of the\nHubble tension in our previous paper, a larger $\\tau_{\\rm reio}$, which could\nbe achieved by removing the Planck low-$\\ell$ polarization data, could boost\n$H_0$ slightly, resulting in a mild reduction of the tension between the early-\nand late-universe determinations of $H_0$. It has been shown later that a\nlarger $\\tau_{\\rm reio}$ could also relieve other anomalies including: the\ntension between BAO and CMB data, the neutrino mass tension, and the latest\nDESI plus supernovae data's tension with the standard cosmological constant\nscenario. In this paper, we systematically analyze the correlations between\n$\\tau_{\\rm reio}$ and relevant cosmological parameters in the existing cosmic\nobservation anomalies. In addition to Pearson correlation coefficients\nextracted directly from the covariance matrix, we also study partial\ncorrelation coefficients which measure intrinsic relationships between pairs of\nparameters removing the influence of other parameters. We show that $\\tau_{\\rm\nreio}$ has weak intrinsic correlations with the parameters responsible for the\ntensions and anomalies discussed. The large direct Pearson correlations that\nallow larger $\\tau_{\\rm reio}$ inferences to alleviate the cosmological\ntensions each arise from complicated networks through multiple parameters. As a\nresult, the relationships between $\\tau_{\\rm reio}$ and each anomaly are not\nindependent of each other. We also employ our method of computing correlations\nto clarify the impact of large scale polarization data, and comment also on the\neffects of CMB observations from ACT and SPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reionization optical depth $\\tau_{\\rm reio}$ has interesting connections\nto existing cosmological anomalies. As first studied in the context of the\nHubble tension in our previous paper, a larger $\\tau_{\\rm reio}$, which could\nbe achieved by removing the Planck low-$\\ell$ polarization data, could boost\n$H_0$ slightly, resulting in a mild reduction of the tension between the early-\nand late-universe determinations of $H_0$. It has been shown later that a\nlarger $\\tau_{\\rm reio}$ could also relieve other anomalies including: the\ntension between BAO and CMB data, the neutrino mass tension, and the latest\nDESI plus supernovae data's tension with the standard cosmological constant\nscenario. In this paper, we systematically analyze the correlations between\n$\\tau_{\\rm reio}$ and relevant cosmological parameters in the existing cosmic\nobservation anomalies. In addition to Pearson correlation coefficients\nextracted directly from the covariance matrix, we also study partial\ncorrelation coefficients which measure intrinsic relationships between pairs of\nparameters removing the influence of other parameters. We show that $\\tau_{\\rm\nreio}$ has weak intrinsic correlations with the parameters responsible for the\ntensions and anomalies discussed. The large direct Pearson correlations that\nallow larger $\\tau_{\\rm reio}$ inferences to alleviate the cosmological\ntensions each arise from complicated networks through multiple parameters. As a\nresult, the relationships between $\\tau_{\\rm reio}$ and each anomaly are not\nindependent of each other. We also employ our method of computing correlations\nto clarify the impact of large scale polarization data, and comment also on the\neffects of CMB observations from ACT and SPT."
                },
                "authors": [
                    {
                        "name": "Itamar J. Allali"
                    },
                    {
                        "name": "Lingfeng Li"
                    },
                    {
                        "name": "Praniti Singh"
                    },
                    {
                        "name": "JiJi Fan"
                    }
                ],
                "author_detail": {
                    "name": "JiJi Fan"
                },
                "author": "JiJi Fan",
                "arxiv_comment": "19 pages, 12 figures, 4 tables, plus appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09677v1",
                "updated": "2025-09-11T17:59:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    34,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:34Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    34,
                    3,
                    254,
                    0
                ],
                "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs"
                },
                "summary": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Akshit Sinha"
                    },
                    {
                        "name": "Arvindh Arun"
                    },
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09675v1",
                "updated": "2025-09-11T17:59:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    17,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:17Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    17,
                    3,
                    254,
                    0
                ],
                "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes."
                },
                "authors": [
                    {
                        "name": "Runpeng Dai"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Hongtu Zhu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09673v1",
                "updated": "2025-09-11T17:59:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    12,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:12Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    12,
                    3,
                    254,
                    0
                ],
                "title": "Cosmology inference with perturbative forward modeling at the field\n  level: a comparison with joint power spectrum and bispectrum analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology inference with perturbative forward modeling at the field\n  level: a comparison with joint power spectrum and bispectrum analyses"
                },
                "summary": "We extend field-level inference to jointly constrain the cosmological\nparameters $\\{A,\\omega_{\\rm cdm},H_0\\}$, in both real and redshift space. Our\nanalyses are based on mock data generated using a perturbative forward model,\nwith noise drawn from a Gaussian distribution with a constant power spectrum.\nThis idealized setting, where the field-level likelihood is exactly Gaussian,\nallows us to precisely quantify the information content in the nonlinear field\non large scales. We find that field-level inference accurately recovers all\ncosmological parameters in both real and redshift space, with uncertainties\nconsistent with perturbation theory expectations. We show that these error bars\nare comparable to those obtained from a joint power spectrum and bispectrum\nanalysis using the same perturbative model. Finally, we perform several tests\nusing the Gaussian field-level likelihood to fit the mock data where the true\nnoise model is non-Gaussian, and find significant biases in the inferred\ncosmological parameters. These results highlight that the success of\nfield-level inference critically depends on using the correct likelihood, which\nmay be the primary challenge for applying this method to smaller scales even in\nthe perturbative regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend field-level inference to jointly constrain the cosmological\nparameters $\\{A,\\omega_{\\rm cdm},H_0\\}$, in both real and redshift space. Our\nanalyses are based on mock data generated using a perturbative forward model,\nwith noise drawn from a Gaussian distribution with a constant power spectrum.\nThis idealized setting, where the field-level likelihood is exactly Gaussian,\nallows us to precisely quantify the information content in the nonlinear field\non large scales. We find that field-level inference accurately recovers all\ncosmological parameters in both real and redshift space, with uncertainties\nconsistent with perturbation theory expectations. We show that these error bars\nare comparable to those obtained from a joint power spectrum and bispectrum\nanalysis using the same perturbative model. Finally, we perform several tests\nusing the Gaussian field-level likelihood to fit the mock data where the true\nnoise model is non-Gaussian, and find significant biases in the inferred\ncosmological parameters. These results highlight that the success of\nfield-level inference critically depends on using the correct likelihood, which\nmay be the primary challenge for applying this method to smaller scales even in\nthe perturbative regime."
                },
                "authors": [
                    {
                        "name": "Kazuyuki Akitsu"
                    },
                    {
                        "name": "Marko Simonović"
                    },
                    {
                        "name": "Shi-Fan Chen"
                    },
                    {
                        "name": "Giovanni Cabass"
                    },
                    {
                        "name": "Matias Zaldarriaga"
                    }
                ],
                "author_detail": {
                    "name": "Matias Zaldarriaga"
                },
                "author": "Matias Zaldarriaga",
                "arxiv_comment": "50 pages, 27 figues, the code available at\n  https://github.com/kazakitsu/field-level-inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09665v1",
                "updated": "2025-09-11T17:57:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    57,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:57:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    57,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "1.8 per cent measurement of $H_0$ from Cepheids alone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1.8 per cent measurement of $H_0$ from Cepheids alone"
                },
                "summary": "One of the most pressing problems in current cosmology is the cause of the\nHubble tension. We revisit a two-rung distance ladder, composed only of Cepheid\nperiods and magnitudes, anchor distances in the Milky Way, Large Magellanic\nCloud, NGC 4258, and host galaxy redshifts. We adopt the SH0ES data for the\nmost up-to-date and carefully vetted measurements, where the Cepheid hosts were\nselected to harbour also Type Ia supernovae. We introduce two important\nimprovements: a rigorous selection modelling and a state-of-the-art density and\npeculiar velocity model using Manticore-Local, based on the Bayesian Origin\nReconstruction from Galaxies (BORG) algorithm. We infer $H_0 = 71.7 \\pm\n1.3\\,\\mathrm{km}\\,\\mathrm{s}^{-1}\\,\\mathrm{Mpc}^{-1}$, assuming the Cepheid\nhost sample was selected by estimated supernova magnitudes. Less plausible\nselection criteria shift $H_0$ by about one standard deviation. The posterior\nhas a lower central value and a 45 per cent smaller error than a previous study\nusing the same data. The result is also slightly lower than the supernova-based\nSH0ES inferred value of $H_0 = 73.2 \\pm\n0.9\\,\\mathrm{km}\\,\\mathrm{s}^{-1}\\,\\mathrm{Mpc}^{-1}$, and is in $3.3\\sigma$\ntension with the latest standard cosmological model microwave background\nresults. These results demonstrate that a measurement of $H_0$ of sufficient\nprecision to weigh in on the Hubble tension is achievable using second-rung\ndata alone, underscoring the importance of robust and accurate statistical\nmodelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most pressing problems in current cosmology is the cause of the\nHubble tension. We revisit a two-rung distance ladder, composed only of Cepheid\nperiods and magnitudes, anchor distances in the Milky Way, Large Magellanic\nCloud, NGC 4258, and host galaxy redshifts. We adopt the SH0ES data for the\nmost up-to-date and carefully vetted measurements, where the Cepheid hosts were\nselected to harbour also Type Ia supernovae. We introduce two important\nimprovements: a rigorous selection modelling and a state-of-the-art density and\npeculiar velocity model using Manticore-Local, based on the Bayesian Origin\nReconstruction from Galaxies (BORG) algorithm. We infer $H_0 = 71.7 \\pm\n1.3\\,\\mathrm{km}\\,\\mathrm{s}^{-1}\\,\\mathrm{Mpc}^{-1}$, assuming the Cepheid\nhost sample was selected by estimated supernova magnitudes. Less plausible\nselection criteria shift $H_0$ by about one standard deviation. The posterior\nhas a lower central value and a 45 per cent smaller error than a previous study\nusing the same data. The result is also slightly lower than the supernova-based\nSH0ES inferred value of $H_0 = 73.2 \\pm\n0.9\\,\\mathrm{km}\\,\\mathrm{s}^{-1}\\,\\mathrm{Mpc}^{-1}$, and is in $3.3\\sigma$\ntension with the latest standard cosmological model microwave background\nresults. These results demonstrate that a measurement of $H_0$ of sufficient\nprecision to weigh in on the Hubble tension is achievable using second-rung\ndata alone, underscoring the importance of robust and accurate statistical\nmodelling."
                },
                "authors": [
                    {
                        "name": "Richard Stiskalek"
                    },
                    {
                        "name": "Harry Desmond"
                    },
                    {
                        "name": "Eleni Tsaprazi"
                    },
                    {
                        "name": "Alan Heavens"
                    },
                    {
                        "name": "Guilhem Lavaux"
                    },
                    {
                        "name": "Stuart McAlpine"
                    },
                    {
                        "name": "Jens Jasche"
                    }
                ],
                "author_detail": {
                    "name": "Jens Jasche"
                },
                "author": "Jens Jasche",
                "arxiv_comment": "24 pages, 12 figures. To be submitted to MNRAS. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09660v1",
                "updated": "2025-09-11T17:55:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    55,
                    9,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:55:09Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    55,
                    9,
                    3,
                    254,
                    0
                ],
                "title": "Steering MoE LLMs via Expert (De)Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering MoE LLMs via Expert (De)Activation"
                },
                "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts."
                },
                "authors": [
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Hinrich Schütze"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06602v2",
                "updated": "2025-09-11T17:52:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    52,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T12:15:53Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    15,
                    53,
                    0,
                    251,
                    0
                ],
                "title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards"
                },
                "summary": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs."
                },
                "authors": [
                    {
                        "name": "Matthias Blondeel"
                    },
                    {
                        "name": "Noel Codella"
                    },
                    {
                        "name": "Sam Preston"
                    },
                    {
                        "name": "Hao Qiu"
                    },
                    {
                        "name": "Leonardo Schettini"
                    },
                    {
                        "name": "Frank Tuan"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Smitha Saligrama"
                    },
                    {
                        "name": "Mert Öz"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Matthew P. Lungren"
                    },
                    {
                        "name": "Thomas Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Osborne"
                },
                "author": "Thomas Osborne",
                "arxiv_comment": "9 pages, 1 figure; Added missing co-authors and contributors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v3",
                "updated": "2025-09-11T17:49:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    49,
                    8,
                    3,
                    254,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09650v1",
                "updated": "2025-09-11T17:41:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    41,
                    29,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:41:29Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    41,
                    29,
                    3,
                    254,
                    0
                ],
                "title": "All for One: LLMs Solve Mental Math at the Last Token With Information\n  Transferred From Other Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All for One: LLMs Solve Mental Math at the Last Token With Information\n  Transferred From Other Tokens"
                },
                "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest."
                },
                "authors": [
                    {
                        "name": "Siddarth Mamidanna"
                    },
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Yilun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhou"
                },
                "author": "Yilun Zhou",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09647v1",
                "updated": "2025-09-11T17:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    37,
                    40,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:37:40Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    37,
                    40,
                    3,
                    254,
                    0
                ],
                "title": "Reconstructing the origin of black hole mergers using sparse\n  astrophysical models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the origin of black hole mergers using sparse\n  astrophysical models"
                },
                "summary": "The astrophysical origin of binary black hole mergers discovered by LIGO and\nVirgo remains uncertain. Efforts to reconstruct the processes that lead to\nmergers typically rely on either astrophysical models with fixed parameters, or\ncontinuous analytical models that can be fit to observations. Given the\ncomplexity of astrophysical formation mechanisms, these methods typically\ncannot fully take into account model uncertainties, nor can they fully capture\nthe underlying processes. Here, we present a merger population analysis that\ncan take a discrete set of simulated model distributions as its input to\ninterpret observations. The analysis can take into account multiple formation\nscenarios as fractional contributors to the total set of observations, and can\nnaturally account for model uncertainties. We apply this technique to\ninvestigate the origin of black hole mergers observed by LIGO Virgo.\nSpecifically, we consider a model of AGN assisted black hole merger\ndistributions, exploring a range of AGN parameters along with several {{SEVN}}\npopulation synthesis models that vary in common envelope efficiency parameter\n($\\alpha$) and metallicity ($Z$). We estimate the posterior distributions for\nAGN+SEVN models using $87$ BBH detections from the $O1--O3$ observation runs.\nThe inferred total merger rate is $46.2 {Gpc}^{-3} {yr}^{-1}$, with the AGN\nsub-population contributing $21.2{Gpc}^{-3}{yr}^{-1}$ and the SEVN\nsub-population contributing $25.0 {Gpc}^{-3} {yr}^{-1}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The astrophysical origin of binary black hole mergers discovered by LIGO and\nVirgo remains uncertain. Efforts to reconstruct the processes that lead to\nmergers typically rely on either astrophysical models with fixed parameters, or\ncontinuous analytical models that can be fit to observations. Given the\ncomplexity of astrophysical formation mechanisms, these methods typically\ncannot fully take into account model uncertainties, nor can they fully capture\nthe underlying processes. Here, we present a merger population analysis that\ncan take a discrete set of simulated model distributions as its input to\ninterpret observations. The analysis can take into account multiple formation\nscenarios as fractional contributors to the total set of observations, and can\nnaturally account for model uncertainties. We apply this technique to\ninvestigate the origin of black hole mergers observed by LIGO Virgo.\nSpecifically, we consider a model of AGN assisted black hole merger\ndistributions, exploring a range of AGN parameters along with several {{SEVN}}\npopulation synthesis models that vary in common envelope efficiency parameter\n($\\alpha$) and metallicity ($Z$). We estimate the posterior distributions for\nAGN+SEVN models using $87$ BBH detections from the $O1--O3$ observation runs.\nThe inferred total merger rate is $46.2 {Gpc}^{-3} {yr}^{-1}$, with the AGN\nsub-population contributing $21.2{Gpc}^{-3}{yr}^{-1}$ and the SEVN\nsub-population contributing $25.0 {Gpc}^{-3} {yr}^{-1}$."
                },
                "authors": [
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "Giuliano Iorio"
                    },
                    {
                        "name": "Hiromichi Tagawa"
                    },
                    {
                        "name": "Daniel Wysocki"
                    },
                    {
                        "name": "Jeremiah Anglin"
                    },
                    {
                        "name": "Imre Bartos"
                    },
                    {
                        "name": "Shubhagata Bhaumik"
                    },
                    {
                        "name": "Zolt'an Haiman"
                    },
                    {
                        "name": "Michela Mapelli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "LingQin Xue"
                    }
                ],
                "author_detail": {
                    "name": "LingQin Xue"
                },
                "author": "LingQin Xue",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14032v2",
                "updated": "2025-09-11T17:25:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    25,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-18T16:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    0,
                    11,
                    4,
                    199,
                    0
                ],
                "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models"
                },
                "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale."
                },
                "authors": [
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Erika Barcelos"
                    },
                    {
                        "name": "Roger French"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "arxiv_comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15557v2",
                "updated": "2025-09-11T17:23:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    23,
                    43,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-21T14:16:56Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    16,
                    56,
                    2,
                    141,
                    0
                ],
                "title": "Modular Jump Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Jump Gaussian Processes"
                },
                "summary": "Gaussian processes (GPs) furnish accurate nonlinear predictions with\nwell-calibrated uncertainty. However, the typical GP setup has a built-in\nstationarity assumption, making it ill-suited for modeling data from processes\nwith sudden changes, or \"jumps\" in the output variable. The \"jump GP\" (JGP) was\ndeveloped for modeling data from such processes, combining local GPs and latent\n\"level\" variables under a joint inferential framework. But joint modeling can\nbe fraught with difficulty. We aim to simplify by suggesting a more modular\nsetup, eschewing joint inference but retaining the main JGP themes: (a)\nlearning optimal neighborhood sizes that locally respect manifolds of\ndiscontinuity; and (b) a new cluster-based (latent) feature to capture regions\nof distinct output levels on both sides of the manifold. We show that each of\n(a) and (b) separately leads to dramatic improvements when modeling processes\nwith jumps. In tandem (but without requiring joint inference) that benefit is\ncompounded, as illustrated on real and synthetic benchmark examples from the\nrecent literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) furnish accurate nonlinear predictions with\nwell-calibrated uncertainty. However, the typical GP setup has a built-in\nstationarity assumption, making it ill-suited for modeling data from processes\nwith sudden changes, or \"jumps\" in the output variable. The \"jump GP\" (JGP) was\ndeveloped for modeling data from such processes, combining local GPs and latent\n\"level\" variables under a joint inferential framework. But joint modeling can\nbe fraught with difficulty. We aim to simplify by suggesting a more modular\nsetup, eschewing joint inference but retaining the main JGP themes: (a)\nlearning optimal neighborhood sizes that locally respect manifolds of\ndiscontinuity; and (b) a new cluster-based (latent) feature to capture regions\nof distinct output levels on both sides of the manifold. We show that each of\n(a) and (b) separately leads to dramatic improvements when modeling processes\nwith jumps. In tandem (but without requiring joint inference) that benefit is\ncompounded, as illustrated on real and synthetic benchmark examples from the\nrecent literature."
                },
                "authors": [
                    {
                        "name": "Anna R. Flowers"
                    },
                    {
                        "name": "Christopher T. Franck"
                    },
                    {
                        "name": "Mickaël Binois"
                    },
                    {
                        "name": "Chiwoo Park"
                    },
                    {
                        "name": "Robert B. Gramacy"
                    }
                ],
                "author_detail": {
                    "name": "Robert B. Gramacy"
                },
                "author": "Robert B. Gramacy",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09631v1",
                "updated": "2025-09-11T17:16:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    16,
                    52,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    16,
                    52,
                    3,
                    254,
                    0
                ],
                "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for\n  Low-Latency Zero-Shot Text-To-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for\n  Low-Latency Zero-Shot Text-To-Speech"
                },
                "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines."
                },
                "authors": [
                    {
                        "name": "Ngoc-Son Nguyen"
                    },
                    {
                        "name": "Hieu-Nghia Huynh-Nguyen"
                    },
                    {
                        "name": "Thanh V. T. Tran"
                    },
                    {
                        "name": "Truong-Son Hy"
                    },
                    {
                        "name": "Van Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van Nguyen"
                },
                "author": "Van Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09629v1",
                "updated": "2025-09-11T17:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    15,
                    45,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:15:45Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    15,
                    45,
                    3,
                    254,
                    0
                ],
                "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems"
                },
                "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks."
                },
                "authors": [
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Shiguang Wu"
                    },
                    {
                        "name": "Lingjie Wang"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhumin Chen"
                },
                "author": "Zhumin Chen",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10576v2",
                "updated": "2025-09-11T17:11:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    11,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-11T09:42:23Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    42,
                    23,
                    4,
                    192,
                    0
                ],
                "title": "Can Large Language Models Understand As Well As Apply Patent Regulations\n  to Pass a Hands-On Patent Attorney Test?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand As Well As Apply Patent Regulations\n  to Pass a Hands-On Patent Attorney Test?"
                },
                "summary": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions."
                },
                "authors": [
                    {
                        "name": "Bhakti Khera"
                    },
                    {
                        "name": "Rezvan Alamian"
                    },
                    {
                        "name": "Pascal A. Scherz"
                    },
                    {
                        "name": "Stephan M. Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan M. Goetz"
                },
                "author": "Stephan M. Goetz",
                "arxiv_comment": "41 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10760v2",
                "updated": "2025-09-11T17:04:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    4,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2024-06-15T23:28:16Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    23,
                    28,
                    16,
                    5,
                    167,
                    0
                ],
                "title": "Joint parameter estimations for spin glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint parameter estimations for spin glasses"
                },
                "summary": "Spin glass models with quadratic-type Hamiltonians are disordered statistical\nphysics systems with competing ferromagnetic and anti-ferromagnetic spin\ninteractions. The corresponding Gibbs measures belong to the exponential family\nparametrized by (inverse) temperature $\\beta>0$ and external field\n$h\\in\\mathbb{R}$. Given a sample from these Gibbs measures, a statistically\nfundamental question is to infer the temperature and external field parameters.\nIn 2007, Chatterjee (Ann. Statist. 35 (2007), no.5, 1931-1946) first proved\nthat in the absence of external field $h=0$, the maximum pseudolikelihood\nestimator for $\\beta$ is $\\sqrt{N}$-consistent under some mild assumptions on\nthe disorder matrices. It was left open whether the same method can be used to\nestimate the temperature and external field simultaneously. In this paper,\nunder some easily verifiable conditions, we prove that the bivariate maximum\npseudolikelihood estimator is indeed jointly $\\sqrt{N}$-consistent for the\ntemperature and external field parameters. The examples cover the classical\nSherrington-Kirkpatrick model and its diluted variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin glass models with quadratic-type Hamiltonians are disordered statistical\nphysics systems with competing ferromagnetic and anti-ferromagnetic spin\ninteractions. The corresponding Gibbs measures belong to the exponential family\nparametrized by (inverse) temperature $\\beta>0$ and external field\n$h\\in\\mathbb{R}$. Given a sample from these Gibbs measures, a statistically\nfundamental question is to infer the temperature and external field parameters.\nIn 2007, Chatterjee (Ann. Statist. 35 (2007), no.5, 1931-1946) first proved\nthat in the absence of external field $h=0$, the maximum pseudolikelihood\nestimator for $\\beta$ is $\\sqrt{N}$-consistent under some mild assumptions on\nthe disorder matrices. It was left open whether the same method can be used to\nestimate the temperature and external field simultaneously. In this paper,\nunder some easily verifiable conditions, we prove that the bivariate maximum\npseudolikelihood estimator is indeed jointly $\\sqrt{N}$-consistent for the\ntemperature and external field parameters. The examples cover the classical\nSherrington-Kirkpatrick model and its diluted variants."
                },
                "authors": [
                    {
                        "name": "Wei-Kuo Chen"
                    },
                    {
                        "name": "Arnab Sen"
                    },
                    {
                        "name": "Qiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Wu"
                },
                "author": "Qiang Wu",
                "arxiv_comment": "v2: results improved by dropping the non flatness of free energy\n  condition. Fixed an issue in the proof of existence of MPLE. Minor updates on\n  the proof of concentration Lemma 2.1 and Proof of positivity of the Hessian.\n  25 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12, 62F10, 82B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00462v2",
                "updated": "2025-09-11T16:59:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    59,
                    36,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-30T11:40:11Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    11,
                    40,
                    11,
                    5,
                    242,
                    0
                ],
                "title": "AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and\n  Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and\n  Insights"
                },
                "summary": "As generative artificial intelligence (AI) tools become widely adopted, large\nlanguage models (LLMs) are increasingly involved on both sides of\ndecision-making processes, ranging from hiring to content moderation. This dual\nadoption raises a critical question: do LLMs systematically favor content that\nresembles their own outputs? Prior research in computer science has identified\nself-preference bias -- the tendency of LLMs to favor their own generated\ncontent -- but its real-world implications have not been empirically evaluated.\nWe focus on the hiring context, where job applicants often rely on LLMs to\nrefine resumes, while employers deploy them to screen those same resumes. Using\na large-scale controlled resume correspondence experiment, we find that LLMs\nconsistently prefer resumes generated by themselves over those written by\nhumans or produced by alternative models, even when content quality is\ncontrolled. The bias against human-written resumes is particularly substantial,\nwith self-preference bias ranging from 68% to 88% across major commercial and\nopen-source models. To assess labor market impact, we simulate realistic hiring\npipelines across 24 occupations. These simulations show that candidates using\nthe same LLM as the evaluator are 23% to 60% more likely to be shortlisted than\nequally qualified applicants submitting human-written resumes, with the largest\ndisadvantages observed in business-related fields such as sales and accounting.\nWe further demonstrate that this bias can be reduced by more than 50% through\nsimple interventions targeting LLMs' self-recognition capabilities. These\nfindings highlight an emerging but previously overlooked risk in AI-assisted\ndecision making and call for expanded frameworks of AI fairness that address\nnot only demographic-based disparities, but also biases in AI-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative artificial intelligence (AI) tools become widely adopted, large\nlanguage models (LLMs) are increasingly involved on both sides of\ndecision-making processes, ranging from hiring to content moderation. This dual\nadoption raises a critical question: do LLMs systematically favor content that\nresembles their own outputs? Prior research in computer science has identified\nself-preference bias -- the tendency of LLMs to favor their own generated\ncontent -- but its real-world implications have not been empirically evaluated.\nWe focus on the hiring context, where job applicants often rely on LLMs to\nrefine resumes, while employers deploy them to screen those same resumes. Using\na large-scale controlled resume correspondence experiment, we find that LLMs\nconsistently prefer resumes generated by themselves over those written by\nhumans or produced by alternative models, even when content quality is\ncontrolled. The bias against human-written resumes is particularly substantial,\nwith self-preference bias ranging from 68% to 88% across major commercial and\nopen-source models. To assess labor market impact, we simulate realistic hiring\npipelines across 24 occupations. These simulations show that candidates using\nthe same LLM as the evaluator are 23% to 60% more likely to be shortlisted than\nequally qualified applicants submitting human-written resumes, with the largest\ndisadvantages observed in business-related fields such as sales and accounting.\nWe further demonstrate that this bias can be reduced by more than 50% through\nsimple interventions targeting LLMs' self-recognition capabilities. These\nfindings highlight an emerging but previously overlooked risk in AI-assisted\ndecision making and call for expanded frameworks of AI fairness that address\nnot only demographic-based disparities, but also biases in AI-AI interactions."
                },
                "authors": [
                    {
                        "name": "Jiannan Xu"
                    },
                    {
                        "name": "Gujie Li"
                    },
                    {
                        "name": "Jane Yi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jane Yi Jiang"
                },
                "author": "Jane Yi Jiang",
                "arxiv_comment": "This paper has been accepted as a non-archival submission at EAAMO\n  2025 and AIES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09614v1",
                "updated": "2025-09-11T16:55:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    55,
                    4,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:55:04Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    55,
                    4,
                    3,
                    254,
                    0
                ],
                "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering"
                },
                "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench."
                },
                "authors": [
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "53 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09605v1",
                "updated": "2025-09-11T16:45:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    45,
                    7,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:45:07Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    45,
                    7,
                    3,
                    254,
                    0
                ],
                "title": "Multiwavelength observations of a new black-widow millisecond pulsar PSR\n  J1544-2555",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiwavelength observations of a new black-widow millisecond pulsar PSR\n  J1544-2555"
                },
                "summary": "We report the discovery of a new black-widow millisecond pulsar, PSR\nJ1544-2555, associated with the Fermi-LAT source 4FGL J1544.2-2554. Optical,\nradio, and gamma-ray observations confirmed its nature as a compact spider\nbinary system. Optical photometry from ULTRACAM revealed a \\(\\sim\\)2.7-hour\norbital period, guiding MeerKAT observations that detected \\(\\sim\\)2.4-ms radio\npulsations. Subsequent timing campaigns using the Murriyang Parkes Telescope,\nthe Effelsberg 100-m Radio Telescope, and the Nan\\c{c}ay Radio Telescope\nallowed us to obtain a preliminary timing solution, which enabled us to find\ngamma-ray pulsations. The final timing solution, spanning 16 years of Fermi-LAT\ngamma-ray data, also displays orbital period variations typical of spider\npulsars. X-ray observations from eROSITA indicate non-thermal emission, but the\nrelatively low count rate prohibits the search for X-ray pulsations. Optical\nlight curve modelling using Icarus suggests the asymmetry is best explained by\na spot model, where uneven heating creates localised temperature variations on\nthe companion. While the optical spectra we obtained are compatible with the\nphysical properties we infer for the companion star, they were not of\nsufficient signal-to-noise to allow for radial velocity measurements, thus\nlimiting constraints on the neutron star's mass. The observed bluer colour near\nthe light curve minimum suggests possible non-thermal emission from\nintra-binary shocks, supported by the presence of an X-ray source. This\ndiscovery exemplifies the proven capability of the Fermi-LAT catalogue in\nidentifying millisecond pulsar candidates and highlights the role of optical\nsurveys in detecting variable sources suitable for radio follow-up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the discovery of a new black-widow millisecond pulsar, PSR\nJ1544-2555, associated with the Fermi-LAT source 4FGL J1544.2-2554. Optical,\nradio, and gamma-ray observations confirmed its nature as a compact spider\nbinary system. Optical photometry from ULTRACAM revealed a \\(\\sim\\)2.7-hour\norbital period, guiding MeerKAT observations that detected \\(\\sim\\)2.4-ms radio\npulsations. Subsequent timing campaigns using the Murriyang Parkes Telescope,\nthe Effelsberg 100-m Radio Telescope, and the Nan\\c{c}ay Radio Telescope\nallowed us to obtain a preliminary timing solution, which enabled us to find\ngamma-ray pulsations. The final timing solution, spanning 16 years of Fermi-LAT\ngamma-ray data, also displays orbital period variations typical of spider\npulsars. X-ray observations from eROSITA indicate non-thermal emission, but the\nrelatively low count rate prohibits the search for X-ray pulsations. Optical\nlight curve modelling using Icarus suggests the asymmetry is best explained by\na spot model, where uneven heating creates localised temperature variations on\nthe companion. While the optical spectra we obtained are compatible with the\nphysical properties we infer for the companion star, they were not of\nsufficient signal-to-noise to allow for radial velocity measurements, thus\nlimiting constraints on the neutron star's mass. The observed bluer colour near\nthe light curve minimum suggests possible non-thermal emission from\nintra-binary shocks, supported by the presence of an X-ray source. This\ndiscovery exemplifies the proven capability of the Fermi-LAT catalogue in\nidentifying millisecond pulsar candidates and highlights the role of optical\nsurveys in detecting variable sources suitable for radio follow-up."
                },
                "authors": [
                    {
                        "name": "Sergio Belmonte Diaz"
                    },
                    {
                        "name": "Tinn Thingmeearkom"
                    },
                    {
                        "name": "Adipol Phosrisom"
                    },
                    {
                        "name": "Rene Breton"
                    },
                    {
                        "name": "Marta Burgay"
                    },
                    {
                        "name": "Colin Clark"
                    },
                    {
                        "name": "Lars Nieder"
                    },
                    {
                        "name": "Martin Mayer"
                    },
                    {
                        "name": "Werner Becker"
                    },
                    {
                        "name": "Ewann Barr"
                    },
                    {
                        "name": "Sarah Buchner"
                    },
                    {
                        "name": "Kaustav Kashyap Das"
                    },
                    {
                        "name": "Vik Dhillon"
                    },
                    {
                        "name": "Oliver Dodge"
                    },
                    {
                        "name": "Elizabeth Ferrara"
                    },
                    {
                        "name": "Jean-Mathias Griessmeier"
                    },
                    {
                        "name": "Ramesh Karuppusamy"
                    },
                    {
                        "name": "Mark Kennedy"
                    },
                    {
                        "name": "Michael Kramer"
                    },
                    {
                        "name": "Prajwal Padmanabh"
                    },
                    {
                        "name": "John Paice"
                    },
                    {
                        "name": "Antonio Rodriguez"
                    },
                    {
                        "name": "Ben Stappers"
                    }
                ],
                "author_detail": {
                    "name": "Ben Stappers"
                },
                "author": "Ben Stappers",
                "arxiv_comment": "Accepted for publication in Monthly Notices of the Royal Astronomical\n  Society. 16 pages. 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09602v1",
                "updated": "2025-09-11T16:42:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    42,
                    22,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:42:22Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    42,
                    22,
                    3,
                    254,
                    0
                ],
                "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death\n  Determination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death\n  Determination"
                },
                "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Yiqun T. Chen"
                    },
                    {
                        "name": "Tyler H. McCormick"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Abhirup Datta"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Datta"
                },
                "author": "Abhirup Datta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09596v1",
                "updated": "2025-09-11T16:35:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    35,
                    54,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:35:54Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    35,
                    54,
                    3,
                    254,
                    0
                ],
                "title": "How much are LLMs changing the language of academic papers after\n  ChatGPT? A multi-database and full text analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much are LLMs changing the language of academic papers after\n  ChatGPT? A multi-database and full text analysis"
                },
                "summary": "This study investigates how Large Language Models (LLMs) are influencing the\nlanguage of academic papers by tracking 12 LLM-associated terms across six\nmajor scholarly databases (Scopus, Web of Science, PubMed, PubMed Central\n(PMC), Dimensions, and OpenAlex) from 2015 to 2024. Using over 2.4 million PMC\nopen-access publications (2021-July 2025), we also analysed full texts to\nassess changes in the frequency and co-occurrence of these terms before and\nafter ChatGPT's initial public release. Across databases, delve (+1,500%),\nunderscore (+1,000%), and intricate (+700%) had the largest increases between\n2022 and 2024. Growth in LLM-term usage was much higher in STEM fields than in\nsocial sciences and arts and humanities. In PMC full texts, the proportion of\npapers using underscore six or more times increased by over 10,000% from 2022\nto 2025, followed by intricate (+5,400%) and meticulous (+2,800%). Nearly half\nof all 2024 PMC papers using any LLM term also included underscore, compared\nwith only 3%-14% of papers before ChatGPT in 2022. Papers using one LLM term\nare now much more likely to include other terms. For example, in 2024,\nunderscore strongly correlated with pivotal (0.449) and delve (0.311), compared\nwith very weak associations in 2022 (0.032 and 0.018, respectively). These\nfindings provide the first large-scale evidence based on full-text publications\nand multiple databases that some LLM-related terms are now being used much more\nfrequently and together. The rapid uptake of LLMs to support scholarly\npublishing is a welcome development reducing the language barrier to academic\npublishing for non-English speakers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how Large Language Models (LLMs) are influencing the\nlanguage of academic papers by tracking 12 LLM-associated terms across six\nmajor scholarly databases (Scopus, Web of Science, PubMed, PubMed Central\n(PMC), Dimensions, and OpenAlex) from 2015 to 2024. Using over 2.4 million PMC\nopen-access publications (2021-July 2025), we also analysed full texts to\nassess changes in the frequency and co-occurrence of these terms before and\nafter ChatGPT's initial public release. Across databases, delve (+1,500%),\nunderscore (+1,000%), and intricate (+700%) had the largest increases between\n2022 and 2024. Growth in LLM-term usage was much higher in STEM fields than in\nsocial sciences and arts and humanities. In PMC full texts, the proportion of\npapers using underscore six or more times increased by over 10,000% from 2022\nto 2025, followed by intricate (+5,400%) and meticulous (+2,800%). Nearly half\nof all 2024 PMC papers using any LLM term also included underscore, compared\nwith only 3%-14% of papers before ChatGPT in 2022. Papers using one LLM term\nare now much more likely to include other terms. For example, in 2024,\nunderscore strongly correlated with pivotal (0.449) and delve (0.311), compared\nwith very weak associations in 2022 (0.032 and 0.018, respectively). These\nfindings provide the first large-scale evidence based on full-text publications\nand multiple databases that some LLM-related terms are now being used much more\nfrequently and together. The rapid uptake of LLMs to support scholarly\npublishing is a welcome development reducing the language barrier to academic\npublishing for non-English speakers."
                },
                "authors": [
                    {
                        "name": "Kayvan Kousha"
                    },
                    {
                        "name": "Mike Thelwall"
                    }
                ],
                "author_detail": {
                    "name": "Mike Thelwall"
                },
                "author": "Mike Thelwall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09593v1",
                "updated": "2025-09-11T16:31:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    31,
                    13,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:31:13Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    31,
                    13,
                    3,
                    254,
                    0
                ],
                "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models"
                },
                "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding."
                },
                "authors": [
                    {
                        "name": "Bangzhao Shu"
                    },
                    {
                        "name": "Isha Joshi"
                    },
                    {
                        "name": "Melissa Karnaze"
                    },
                    {
                        "name": "Anh C. Pham"
                    },
                    {
                        "name": "Ishita Kakkar"
                    },
                    {
                        "name": "Sindhu Kothe"
                    },
                    {
                        "name": "Arpine Hovasapian"
                    },
                    {
                        "name": "Mai ElSherief"
                    }
                ],
                "author_detail": {
                    "name": "Mai ElSherief"
                },
                "author": "Mai ElSherief",
                "arxiv_comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08031v2",
                "updated": "2025-09-11T16:27:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    27,
                    59,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-09T15:30:40Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    30,
                    40,
                    1,
                    252,
                    0
                ],
                "title": "AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs"
                },
                "summary": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce AU-Harness, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. AU-Harness provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce AU-Harness, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. AU-Harness provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Sidharth Surapaneni"
                    },
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Jash Mehta"
                    },
                    {
                        "name": "Aman Tiwari"
                    },
                    {
                        "name": "Oluwanifemi Bamgbose"
                    },
                    {
                        "name": "Akshay Kalkunte"
                    },
                    {
                        "name": "Sai Rajeswar"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    }
                ],
                "author_detail": {
                    "name": "Sathwik Tejaswi Madhusudhan"
                },
                "author": "Sathwik Tejaswi Madhusudhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09585v1",
                "updated": "2025-09-11T16:22:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    22,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:22:20Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    22,
                    20,
                    3,
                    254,
                    0
                ],
                "title": "Causal PDE-Control Models: A Structural Framework for Dynamic Portfolio\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal PDE-Control Models: A Structural Framework for Dynamic Portfolio\n  Optimization"
                },
                "summary": "Classical portfolio models collapse under structural breaks, while modern\nmachine-learning allocators adapt flexibly but often at the cost of\ntransparency and interpretability. This paper introduces Causal PDE-Control\nModels (CPCMs), a unifying framework that integrates causal inference,\nnonlinear filtering, and forward-backward partial differential equations for\ndynamic portfolio optimization. The framework delivers three theoretical\nadvances: (i) the existence of conditional risk-neutral measures under evolving\ninformation sets; (ii) a projection-divergence duality that quantifies the\nstability cost of departing from the causal driver manifold; and (iii) causal\ncompleteness, establishing that a finite driver span can capture all systematic\npremia. Classical methods such as Markowitz, CAPM, and Black-Litterman appear\nas degenerate cases, while reinforcement learning and deep-hedging policies\nemerge as unconstrained, symmetry-breaking approximations. Empirically, CPCM\nsolvers implemented with physics-informed neural networks achieve higher Sharpe\nratios, lower turnover, and more persistent premia than both econometric and\nmachine-learning benchmarks, using a global equity panel with more than 300\ncandidate drivers. By reframing portfolio optimization around structural\ncausality and PDE control, CPCMs provide a rigorous, interpretable, and\ncomputationally tractable foundation for robust asset allocation under\nnonstationary conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical portfolio models collapse under structural breaks, while modern\nmachine-learning allocators adapt flexibly but often at the cost of\ntransparency and interpretability. This paper introduces Causal PDE-Control\nModels (CPCMs), a unifying framework that integrates causal inference,\nnonlinear filtering, and forward-backward partial differential equations for\ndynamic portfolio optimization. The framework delivers three theoretical\nadvances: (i) the existence of conditional risk-neutral measures under evolving\ninformation sets; (ii) a projection-divergence duality that quantifies the\nstability cost of departing from the causal driver manifold; and (iii) causal\ncompleteness, establishing that a finite driver span can capture all systematic\npremia. Classical methods such as Markowitz, CAPM, and Black-Litterman appear\nas degenerate cases, while reinforcement learning and deep-hedging policies\nemerge as unconstrained, symmetry-breaking approximations. Empirically, CPCM\nsolvers implemented with physics-informed neural networks achieve higher Sharpe\nratios, lower turnover, and more persistent premia than both econometric and\nmachine-learning benchmarks, using a global equity panel with more than 300\ncandidate drivers. By reframing portfolio optimization around structural\ncausality and PDE control, CPCMs provide a rigorous, interpretable, and\ncomputationally tractable foundation for robust asset allocation under\nnonstationary conditions."
                },
                "authors": [
                    {
                        "name": "Alejandro Rodriguez Dominguez"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Rodriguez Dominguez"
                },
                "author": "Alejandro Rodriguez Dominguez",
                "arxiv_comment": "54 pages, 14 pages, 14 figures. Code and data available from authors\n  upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.6; G.1.8; G.1.10; G.3; I.2.6; I.5.3; I.5.4; I.6.5; J.2; J.4; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09583v1",
                "updated": "2025-09-11T16:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    19,
                    59,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:19:59Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    19,
                    59,
                    3,
                    254,
                    0
                ],
                "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role\n  of Personality Detection in Matchmaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role\n  of Personality Detection in Matchmaking"
                },
                "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality."
                },
                "authors": [
                    {
                        "name": "Brittany Harbison"
                    },
                    {
                        "name": "Samuel Taubman"
                    },
                    {
                        "name": "Travis Taylor"
                    },
                    {
                        "name": "Ashok. K. Goel"
                    }
                ],
                "author_detail": {
                    "name": "Ashok. K. Goel"
                },
                "author": "Ashok. K. Goel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06485v2",
                "updated": "2025-09-11T15:55:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    55,
                    28,
                    3,
                    254,
                    0
                ],
                "published": "2025-06-06T19:20:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    19,
                    20,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Task Matters: Knowledge Requirements Shape LLM Responses to\n  Context-Memory Conflict",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Matters: Knowledge Requirements Shape LLM Responses to\n  Context-Memory Conflict"
                },
                "summary": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiser Sun"
                    },
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03962v2",
                "updated": "2025-09-11T15:52:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    52,
                    22,
                    3,
                    254,
                    0
                ],
                "published": "2024-09-06T01:07:29Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    7,
                    29,
                    4,
                    250,
                    0
                ],
                "title": "Average Causal Effect Estimation in DAGs with Hidden Variables: Beyond\n  Back-Door and Front-Door Criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Average Causal Effect Estimation in DAGs with Hidden Variables: Beyond\n  Back-Door and Front-Door Criteria"
                },
                "summary": "The identification theory for causal effects in directed acyclic graphs\n(DAGs) with hidden variables is well established, but methods for estimating\nand inferring functionals that extend beyond the g-formula remain\nunderdeveloped. Previous studies have introduced semiparametric estimators for\nsuch functionals in a broad class of DAGs with hidden variables. While these\nestimators exhibit desirable statistical properties such as double robustness\nin certain cases, they also face significant limitations. Notably, they\nencounter substantial computational challenges, particularly involving density\nestimation and numerical integration for continuous variables, and their\nestimates may fall outside the parameter space of the target estimand.\nAdditionally, the asymptotic properties of these estimators is underexplored,\nespecially when integrating flexible statistical and machine learning models\nfor nuisance functional estimations. This paper addresses these challenges by\nintroducing novel one-step corrected plug-in and targeted minimum loss-based\nestimators of causal effects for a class of hidden variable DAGs that go beyond\nclassical back-door and front-door criteria (known as the treatment primal\nfixability criterion in prior literature). These estimators leverage\ndata-adaptive machine learning algorithms to minimize modeling assumptions\nwhile ensuring key statistical properties including double robustness,\nefficiency, boundedness within the target parameter space, and asymptotic\nlinearity under $L^2(P)$-rate conditions for nuisance functional estimates that\nyield root-n consistent causal effect estimates. To ensure our estimation\nmethods are accessible in practice, we provide the flexCausal package in R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identification theory for causal effects in directed acyclic graphs\n(DAGs) with hidden variables is well established, but methods for estimating\nand inferring functionals that extend beyond the g-formula remain\nunderdeveloped. Previous studies have introduced semiparametric estimators for\nsuch functionals in a broad class of DAGs with hidden variables. While these\nestimators exhibit desirable statistical properties such as double robustness\nin certain cases, they also face significant limitations. Notably, they\nencounter substantial computational challenges, particularly involving density\nestimation and numerical integration for continuous variables, and their\nestimates may fall outside the parameter space of the target estimand.\nAdditionally, the asymptotic properties of these estimators is underexplored,\nespecially when integrating flexible statistical and machine learning models\nfor nuisance functional estimations. This paper addresses these challenges by\nintroducing novel one-step corrected plug-in and targeted minimum loss-based\nestimators of causal effects for a class of hidden variable DAGs that go beyond\nclassical back-door and front-door criteria (known as the treatment primal\nfixability criterion in prior literature). These estimators leverage\ndata-adaptive machine learning algorithms to minimize modeling assumptions\nwhile ensuring key statistical properties including double robustness,\nefficiency, boundedness within the target parameter space, and asymptotic\nlinearity under $L^2(P)$-rate conditions for nuisance functional estimates that\nyield root-n consistent causal effect estimates. To ensure our estimation\nmethods are accessible in practice, we provide the flexCausal package in R."
                },
                "authors": [
                    {
                        "name": "Anna Guo"
                    },
                    {
                        "name": "Razieh Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Razieh Nabi"
                },
                "author": "Razieh Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09560v1",
                "updated": "2025-09-11T15:51:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    51,
                    43,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:51:43Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    51,
                    43,
                    3,
                    254,
                    0
                ],
                "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation\n  and Asynchronous Pipeline Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Embodied AI Agents through Perception-Generation Disaggregation\n  and Asynchronous Pipeline Execution"
                },
                "summary": "Embodied AI systems operate in dynamic environments, requiring seamless\nintegration of perception and generation modules to process high-frequency\ninput and output demands. Traditional sequential computation patterns, while\neffective in ensuring accuracy, face significant limitations in achieving the\nnecessary \"thinking\" frequency for real-world applications. In this work, we\npresent Auras, an algorithm-system co-designed inference framework to optimize\nthe inference frequency of embodied AI agents. Auras disaggregates the\nperception and generation and provides controlled pipeline parallelism for them\nto achieve high and stable throughput. Faced with the data staleness problem\nthat appears when the parallelism is increased, Auras establishes a public\ncontext for perception and generation to share, thereby promising the accuracy\nof embodied agents. Experimental results show that Auras improves throughput by\n2.54x on average while achieving 102.7% of the original accuracy, demonstrating\nits efficacy in overcoming the constraints of sequential computation and\nproviding high throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI systems operate in dynamic environments, requiring seamless\nintegration of perception and generation modules to process high-frequency\ninput and output demands. Traditional sequential computation patterns, while\neffective in ensuring accuracy, face significant limitations in achieving the\nnecessary \"thinking\" frequency for real-world applications. In this work, we\npresent Auras, an algorithm-system co-designed inference framework to optimize\nthe inference frequency of embodied AI agents. Auras disaggregates the\nperception and generation and provides controlled pipeline parallelism for them\nto achieve high and stable throughput. Faced with the data staleness problem\nthat appears when the parallelism is increased, Auras establishes a public\ncontext for perception and generation to share, thereby promising the accuracy\nof embodied agents. Experimental results show that Auras improves throughput by\n2.54x on average while achieving 102.7% of the original accuracy, demonstrating\nits efficacy in overcoming the constraints of sequential computation and\nproviding high throughput."
                },
                "authors": [
                    {
                        "name": "Shulai Zhang"
                    },
                    {
                        "name": "Ao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21961v2",
                "updated": "2025-09-11T15:49:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    49,
                    39,
                    3,
                    254,
                    0
                ],
                "published": "2025-03-27T20:18:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    20,
                    18,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Gated Branching for Efficient Test-Time Reasoning"
                },
                "summary": "Test-time compute methods like beam search can significantly improve the\nreasoning capabilities and problem-solving accuracy of large language models.\nHowever, these approaches require substantially increased computational\nresources, with most computation wasted on exploring low-diversity branches\nwhere the model already exhibits high confidence. We observe that a small\nsubset of uncertain reasoning steps has a disproportionately large impact on\nfinal prediction accuracy, and branching at these points tends to yield\nhigher-quality and more diverse candidate reasoning steps. Therefore, we\nintroduce Entropy-Gated Branching: a novel inference technique that dynamically\nallocates computational resources by selectively expanding prediction sequences\nonly at points of high uncertainty. Our method leverages entropy as a gating\nmechanism to identify when branching is most beneficial, coupled with an\nexternal feedback model to rank and prune candidate branches. Empirical results\non mathematical and financial reasoning benchmarks show that this strategy\nimproves accuracy by 22.6% over standard inference while operating 37% faster\nthan conventional beam search with similar or higher performance. Our results\nshow that dynamic resource allocation during inference can substantially\nimprove both efficiency and effectiveness, offering a more scalable pathway to\nenhanced LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute methods like beam search can significantly improve the\nreasoning capabilities and problem-solving accuracy of large language models.\nHowever, these approaches require substantially increased computational\nresources, with most computation wasted on exploring low-diversity branches\nwhere the model already exhibits high confidence. We observe that a small\nsubset of uncertain reasoning steps has a disproportionately large impact on\nfinal prediction accuracy, and branching at these points tends to yield\nhigher-quality and more diverse candidate reasoning steps. Therefore, we\nintroduce Entropy-Gated Branching: a novel inference technique that dynamically\nallocates computational resources by selectively expanding prediction sequences\nonly at points of high uncertainty. Our method leverages entropy as a gating\nmechanism to identify when branching is most beneficial, coupled with an\nexternal feedback model to rank and prune candidate branches. Empirical results\non mathematical and financial reasoning benchmarks show that this strategy\nimproves accuracy by 22.6% over standard inference while operating 37% faster\nthan conventional beam search with similar or higher performance. Our results\nshow that dynamic resource allocation during inference can substantially\nimprove both efficiency and effectiveness, offering a more scalable pathway to\nenhanced LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Ethan Callanan"
                    },
                    {
                        "name": "Abdellah Ghassel"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09550v1",
                "updated": "2025-09-11T15:39:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    39,
                    59,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:39:59Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    39,
                    59,
                    3,
                    254,
                    0
                ],
                "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates"
                },
                "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel."
                },
                "authors": [
                    {
                        "name": "Harry Julia"
                    },
                    {
                        "name": "Rachel Beeson"
                    },
                    {
                        "name": "Lohith Konathala"
                    },
                    {
                        "name": "Johanna Ulin"
                    },
                    {
                        "name": "Jiameng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiameng Gao"
                },
                "author": "Jiameng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09544v1",
                "updated": "2025-09-11T15:37:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    37,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:37:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    37,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance\n  NLP (2022-2025)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance\n  NLP (2022-2025)"
                },
                "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains."
                },
                "authors": [
                    {
                        "name": "Paolo Pedinotti"
                    },
                    {
                        "name": "Peter Baumann"
                    },
                    {
                        "name": "Nathan Jessurun"
                    },
                    {
                        "name": "Leslie Barrett"
                    },
                    {
                        "name": "Enrico Santus"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Santus"
                },
                "author": "Enrico Santus",
                "arxiv_comment": "7 pages, 6 appendices, EMNLP industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16183v2",
                "updated": "2025-09-11T15:35:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    35,
                    11,
                    3,
                    254,
                    0
                ],
                "published": "2025-03-20T14:34:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    34,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog\n  Computations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog\n  Computations"
                },
                "summary": "The disparity between the computational demands of deep learning and the\ncapabilities of compute hardware is expanding drastically. Although deep\nlearning achieves remarkable performance in countless tasks, its escalating\nrequirements for computational power and energy consumption surpass the\nsustainable limits of even specialized neural processing units, including the\nApple Neural Engine and NVIDIA TensorCores. This challenge is intensified by\nthe slowdown in CMOS scaling.\n  Analog computing presents a promising alternative, offering substantial\nimprovements in energy efficiency by directly manipulating physical quantities\nsuch as current, voltage, charge, or photons. However, it is inherently\nvulnerable to manufacturing variations, nonlinearities, and noise, leading to\ndegraded prediction accuracy. One of the most effective techniques for\nenhancing robustness, Noisy Training, introduces noise during the training\nphase to reinforce the model against disturbances encountered during inference.\nAlthough highly effective, its performance degrades in real-world environments\nwhere noise characteristics fluctuate due to external factors such as\ntemperature variations and temporal drift.\n  This study underscores the necessity of Noisy Training while revealing its\nfundamental limitations in the presence of dynamic noise. To address these\nchallenges, we propose Variance-Aware Noisy Training, a novel approach that\nmitigates performance degradation by incorporating noise schedules which\nemulate the evolving noise conditions encountered during inference. Our method\nsubstantially improves model robustness, without training overhead. We\ndemonstrate a significant increase in robustness, from 79.3\\% with conventional\nNoisy Training to 97.6\\% with Variance-Aware Noisy Training on CIFAR-10 and\nfrom 32.4\\% to 99.7\\% on Tiny ImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disparity between the computational demands of deep learning and the\ncapabilities of compute hardware is expanding drastically. Although deep\nlearning achieves remarkable performance in countless tasks, its escalating\nrequirements for computational power and energy consumption surpass the\nsustainable limits of even specialized neural processing units, including the\nApple Neural Engine and NVIDIA TensorCores. This challenge is intensified by\nthe slowdown in CMOS scaling.\n  Analog computing presents a promising alternative, offering substantial\nimprovements in energy efficiency by directly manipulating physical quantities\nsuch as current, voltage, charge, or photons. However, it is inherently\nvulnerable to manufacturing variations, nonlinearities, and noise, leading to\ndegraded prediction accuracy. One of the most effective techniques for\nenhancing robustness, Noisy Training, introduces noise during the training\nphase to reinforce the model against disturbances encountered during inference.\nAlthough highly effective, its performance degrades in real-world environments\nwhere noise characteristics fluctuate due to external factors such as\ntemperature variations and temporal drift.\n  This study underscores the necessity of Noisy Training while revealing its\nfundamental limitations in the presence of dynamic noise. To address these\nchallenges, we propose Variance-Aware Noisy Training, a novel approach that\nmitigates performance degradation by incorporating noise schedules which\nemulate the evolving noise conditions encountered during inference. Our method\nsubstantially improves model robustness, without training overhead. We\ndemonstrate a significant increase in robustness, from 79.3\\% with conventional\nNoisy Training to 97.6\\% with Variance-Aware Noisy Training on CIFAR-10 and\nfrom 32.4\\% to 99.7\\% on Tiny ImageNet."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Hendrik Borras"
                    },
                    {
                        "name": "Bernhard Klein"
                    },
                    {
                        "name": "Holger Fröning"
                    }
                ],
                "author_detail": {
                    "name": "Holger Fröning"
                },
                "author": "Holger Fröning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15917v2",
                "updated": "2025-09-11T15:22:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    22,
                    50,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-22T14:02:57Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    2,
                    57,
                    1,
                    112,
                    0
                ],
                "title": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning"
                },
                "summary": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Hai Phung"
                    },
                    {
                        "name": "Hao Pham"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Vu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Vu Nguyen"
                },
                "author": "Vu Nguyen",
                "arxiv_comment": "Change the method and experimentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17287v2",
                "updated": "2025-09-11T15:22:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    22,
                    14,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-24T06:28:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    28,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "Combining Static and Dynamic Approaches for Mining and Testing\n  Constraints for RESTful API Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Static and Dynamic Approaches for Mining and Testing\n  Constraints for RESTful API Testing"
                },
                "summary": "In API testing, deriving logical constraints on API response bodies is\ncrucial in generating the test cases to cover various aspects of RESTful APIs.\nHowever, existing approaches are limited to dynamic analysis in which\nconstraints are extracted from the execution of APIs as part of the system\nunder test. The key limitation of such a dynamic approach is its\nunder-estimation in which inputs in API executions are not sufficiently diverse\nto uncover actual constraints on API response bodies. In this paper, we propose\nto combine a novel static analysis approach (in which the constraints for API\nresponse bodies are mined from API specifications), with the dynamic approach\n(which relies on API execution data). We leverage large language models (LLMs)\nto comprehend the API specifications, mine constraints for response bodies, and\ngenerate test cases. To reduce LLMs' hallucination, we apply an\nObservation-Confirmation (OC) scheme which uses initial prompts to\ncontextualize constraints. %, allowing subsequent prompts to more accurately\nconfirm their presence. Our empirical results show that~LLMs with OC prompting\nachieve high precision in constraint mining with the average of 91.2%. When\ncombining static and dynamic analysis, our tool, RBCTest , achieves a precision\nof 78.5%. RBCTest detects 107 constraints that the dynamic approach misses and\n46 more precise constraints. We also use its generated test cases to detect 21\nmismatches between the API specification and actual response data for 8\nreal-world APIs. Four of the mismatches were, in fact, reported in developers'\nforums.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In API testing, deriving logical constraints on API response bodies is\ncrucial in generating the test cases to cover various aspects of RESTful APIs.\nHowever, existing approaches are limited to dynamic analysis in which\nconstraints are extracted from the execution of APIs as part of the system\nunder test. The key limitation of such a dynamic approach is its\nunder-estimation in which inputs in API executions are not sufficiently diverse\nto uncover actual constraints on API response bodies. In this paper, we propose\nto combine a novel static analysis approach (in which the constraints for API\nresponse bodies are mined from API specifications), with the dynamic approach\n(which relies on API execution data). We leverage large language models (LLMs)\nto comprehend the API specifications, mine constraints for response bodies, and\ngenerate test cases. To reduce LLMs' hallucination, we apply an\nObservation-Confirmation (OC) scheme which uses initial prompts to\ncontextualize constraints. %, allowing subsequent prompts to more accurately\nconfirm their presence. Our empirical results show that~LLMs with OC prompting\nachieve high precision in constraint mining with the average of 91.2%. When\ncombining static and dynamic analysis, our tool, RBCTest , achieves a precision\nof 78.5%. RBCTest detects 107 constraints that the dynamic approach misses and\n46 more precise constraints. We also use its generated test cases to detect 21\nmismatches between the API specification and actual response data for 8\nreal-world APIs. Four of the mismatches were, in fact, reported in developers'\nforums."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Tri Le"
                    },
                    {
                        "name": "Vu Nguyen"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Tien N. Nguyen"
                },
                "author": "Tien N. Nguyen",
                "arxiv_comment": "Change the method and experimentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01080v2",
                "updated": "2025-09-11T15:20:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    20,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-01T16:37:55Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    37,
                    55,
                    1,
                    182,
                    0
                ],
                "title": "Development and Comparative Evaluation of Three Artificial Intelligence\n  Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A\n  7-Month Retrospective Proof-of-Concept",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Comparative Evaluation of Three Artificial Intelligence\n  Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A\n  7-Month Retrospective Proof-of-Concept"
                },
                "summary": "Emergency departments struggle with persistent triage errors, especially\nundertriage and overtriage, which are aggravated by growing patient volumes and\nstaff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP),\nURGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and\nnurse practice, using seven months of adult triage data from Roger Salengro\nHospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE\nconsistently outperformed both AI alternatives and nurse triage, achieving the\nhighest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in\npredicting hospitalization needs (GEMSA). Its robustness across structured data\nand raw transcripts highlighted the advantage of LLM architectures in\nabstracting patient information. Overall, the findings suggest that integrating\nLLM-based AI into emergency department workflows could significantly enhance\npatient safety and operational efficiency, though successful adoption will\ndepend on addressing limitations and ensuring ethical transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency departments struggle with persistent triage errors, especially\nundertriage and overtriage, which are aggravated by growing patient volumes and\nstaff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP),\nURGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and\nnurse practice, using seven months of adult triage data from Roger Salengro\nHospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE\nconsistently outperformed both AI alternatives and nurse triage, achieving the\nhighest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in\npredicting hospitalization needs (GEMSA). Its robustness across structured data\nand raw transcripts highlighted the advantage of LLM architectures in\nabstracting patient information. Overall, the findings suggest that integrating\nLLM-based AI into emergency department workflows could significantly enhance\npatient safety and operational efficiency, though successful adoption will\ndepend on addressing limitations and ensuring ethical transparency."
                },
                "authors": [
                    {
                        "name": "Edouard Lansiaux"
                    },
                    {
                        "name": "Ramy Azzouz"
                    },
                    {
                        "name": "Emmanuel Chazard"
                    },
                    {
                        "name": "Amélie Vromant"
                    },
                    {
                        "name": "Eric Wiel"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wiel"
                },
                "author": "Eric Wiel",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04867v2",
                "updated": "2025-09-11T14:52:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    52,
                    8,
                    3,
                    254,
                    0
                ],
                "published": "2025-06-05T10:38:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    38,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "LLMs for sensory-motor control: Combining in-context and iterative\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for sensory-motor control: Combining in-context and iterative\n  learning"
                },
                "summary": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. At the outset, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. The approach proves\neffective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b.\nIn most cases, it successfully identifies optimal or near-optimal solutions by\nintegrating symbolic knowledge derived through reasoning with sub-symbolic\nsensory-motor data gathered as the agent interacts with its environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. At the outset, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. The approach proves\neffective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b.\nIn most cases, it successfully identifies optimal or near-optimal solutions by\nintegrating symbolic knowledge derived through reasoning with sub-symbolic\nsensory-motor data gathered as the agent interacts with its environment."
                },
                "authors": [
                    {
                        "name": "Jônata Tyska Carvalho"
                    },
                    {
                        "name": "Stefano Nolfi"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Nolfi"
                },
                "author": "Stefano Nolfi",
                "arxiv_comment": "Article updated with results from gpt-oss:120b. 24 pages (13 pages\n  are from appendix), 6 figures, code for experiments replication and\n  supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09505v1",
                "updated": "2025-09-11T14:49:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    49,
                    50,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:49:50Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    49,
                    50,
                    3,
                    254,
                    0
                ],
                "title": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference"
                },
                "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced."
                },
                "authors": [
                    {
                        "name": "Haoran Wu"
                    },
                    {
                        "name": "Can Xiao"
                    },
                    {
                        "name": "Jiayi Nie"
                    },
                    {
                        "name": "Xuan Guo"
                    },
                    {
                        "name": "Binglei Lou"
                    },
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Przemyslaw Forys"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Timothy M. Jones"
                    },
                    {
                        "name": "Rika Antonova"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Aaron Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Zhao"
                },
                "author": "Aaron Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02348v2",
                "updated": "2025-09-11T14:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    39,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-01-04T18:04:47Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    18,
                    4,
                    47,
                    5,
                    4,
                    0
                ],
                "title": "Thinking with Many Minds: Using Large Language Models for\n  Multi-Perspective Problem-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking with Many Minds: Using Large Language Models for\n  Multi-Perspective Problem-Solving"
                },
                "summary": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution."
                },
                "authors": [
                    {
                        "name": "Sanghyun Park"
                    },
                    {
                        "name": "Boris Maciejovsky"
                    },
                    {
                        "name": "Phanish Puranam"
                    }
                ],
                "author_detail": {
                    "name": "Phanish Puranam"
                },
                "author": "Phanish Puranam",
                "arxiv_comment": "36 pages, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20999v2",
                "updated": "2025-09-11T14:36:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    36,
                    21,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-28T17:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    11,
                    26,
                    0,
                    209,
                    0
                ],
                "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning"
                },
                "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."
                },
                "authors": [
                    {
                        "name": "Yining Huang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Keke Tang"
                    },
                    {
                        "name": "Meilian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Meilian Chen"
                },
                "author": "Meilian Chen",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00039v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00039v5",
                "updated": "2025-09-11T14:34:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    52,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-29T18:36:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    36,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "An Ontology-Driven Graph RAG for Legal Norms: A Structural, Temporal,\n  and Deterministic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ontology-Driven Graph RAG for Legal Norms: A Structural, Temporal,\n  and Deterministic Approach"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces the Structure-Aware Temporal Graph\nRAG (SAT-Graph RAG), an ontology-driven framework designed to overcome these\nlimitations by explicitly modeling the formal structure and diachronic nature\nof legal norms. We ground our knowledge graph in a formal, LRMoo-inspired model\nthat distinguishes abstract legal Works from their versioned Expressions. We\nmodel temporal states as efficient aggregations that reuse the versioned\nexpressions (CTVs) of unchanged components, and we reify legislative events as\nfirst-class Action nodes to make causality explicit and queryable. This\nstructured backbone enables a unified, planner-guided query strategy that\napplies explicit policies to deterministically resolve complex requests for (i)\npoint-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable\nprovenance reconstruction. Through a case study on the Brazilian Constitution,\nwe demonstrate how this approach provides a verifiable, temporally-correct\nsubstrate for LLMs, enabling higher-order analytical capabilities while\ndrastically reducing the risk of factual errors. The result is a practical\nframework for building more trustworthy and explainable legal AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces the Structure-Aware Temporal Graph\nRAG (SAT-Graph RAG), an ontology-driven framework designed to overcome these\nlimitations by explicitly modeling the formal structure and diachronic nature\nof legal norms. We ground our knowledge graph in a formal, LRMoo-inspired model\nthat distinguishes abstract legal Works from their versioned Expressions. We\nmodel temporal states as efficient aggregations that reuse the versioned\nexpressions (CTVs) of unchanged components, and we reify legislative events as\nfirst-class Action nodes to make causality explicit and queryable. This\nstructured backbone enables a unified, planner-guided query strategy that\napplies explicit policies to deterministically resolve complex requests for (i)\npoint-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable\nprovenance reconstruction. Through a case study on the Brazilian Constitution,\nwe demonstrate how this approach provides a verifiable, temporally-correct\nsubstrate for LLMs, enabling higher-order analytical capabilities while\ndrastically reducing the risk of factual errors. The result is a practical\nframework for building more trustworthy and explainable legal AI systems."
                },
                "authors": [
                    {
                        "name": "Hudson de Martim"
                    }
                ],
                "author_detail": {
                    "name": "Hudson de Martim"
                },
                "author": "Hudson de Martim",
                "arxiv_comment": "Major revision for clarity and academic precision. Updated title and\n  abstract. Refined core terminology, contributions, related work, and shifted\n  the implementation to a conceptual architecture. Added new arguments to\n  strengthen the paper's thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00039v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00039v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08356v2",
                "updated": "2025-09-11T14:29:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    29,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2024-05-14T06:52:56Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    6,
                    52,
                    56,
                    1,
                    135,
                    0
                ],
                "title": "Information Inference Diagrams: Complementing Privacy and Security\n  Analyses Beyond Data Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Inference Diagrams: Complementing Privacy and Security\n  Analyses Beyond Data Flows"
                },
                "summary": "This work introduces Information Inference Diagrams (I2Ds), a modeling\nframework aiming to complement existing approaches for privacy and security\nanalysis of distributed systems. It is intended to support established threat\nmodeling processes. Our approach is designed to be compatible with Data Flow\nDiagrams~(DFDs), which form the basis of many established techniques and tools.\nUnlike DFDs, I2Ds represent information propagation, going beyond mere data\nflows to enable more formal reasoning in threat modeling while remaining\npractical. They define inference and sharing (flow) relations on information\nitems to model how information moves through a system. To this end, we provide\nformal definitions for information items, entities, and flows. By introducing\nclasses as a type system, our formal rules are both generic and allow\nconformance to existing vocabularies. We demonstrate the applicability of I2Ds\nthrough examples, that showcase their versatility in system analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces Information Inference Diagrams (I2Ds), a modeling\nframework aiming to complement existing approaches for privacy and security\nanalysis of distributed systems. It is intended to support established threat\nmodeling processes. Our approach is designed to be compatible with Data Flow\nDiagrams~(DFDs), which form the basis of many established techniques and tools.\nUnlike DFDs, I2Ds represent information propagation, going beyond mere data\nflows to enable more formal reasoning in threat modeling while remaining\npractical. They define inference and sharing (flow) relations on information\nitems to model how information moves through a system. To this end, we provide\nformal definitions for information items, entities, and flows. By introducing\nclasses as a type system, our formal rules are both generic and allow\nconformance to existing vocabularies. We demonstrate the applicability of I2Ds\nthrough examples, that showcase their versatility in system analysis."
                },
                "authors": [
                    {
                        "name": "Sebastian Rehms"
                    },
                    {
                        "name": "Stefan Köpsell"
                    },
                    {
                        "name": "Verena Klös"
                    },
                    {
                        "name": "Florian Tschorsch"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tschorsch"
                },
                "author": "Florian Tschorsch",
                "arxiv_comment": "24 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08105v2",
                "updated": "2025-09-11T14:14:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    14,
                    35,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-09T19:32:05Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    19,
                    32,
                    5,
                    1,
                    252,
                    0
                ],
                "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and\n  LLM Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and\n  LLM Fusion"
                },
                "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings."
                },
                "authors": [
                    {
                        "name": "Kosei Uemura"
                    },
                    {
                        "name": "David Guzmán"
                    },
                    {
                        "name": "Quang Phuoc Nguyen"
                    },
                    {
                        "name": "Jesujoba Oluwadara Alabi"
                    },
                    {
                        "name": "En-shiun Annie Lee"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    }
                ],
                "author_detail": {
                    "name": "David Ifeoluwa Adelani"
                },
                "author": "David Ifeoluwa Adelani",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00132v3",
                "updated": "2025-09-11T14:13:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    13,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2025-03-31T18:33:55Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    33,
                    55,
                    0,
                    90,
                    0
                ],
                "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B"
                },
                "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."
                },
                "authors": [
                    {
                        "name": "Aleksandra Bakalova"
                    },
                    {
                        "name": "Yana Veitsman"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09481v1",
                "updated": "2025-09-11T14:09:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    9,
                    55,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:09:55Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    9,
                    55,
                    3,
                    254,
                    0
                ],
                "title": "Spin Constraints on 4U 1630-47 via combined Continuum Fitting and\n  Reflection methods: a comparative study using Frequentist and Bayesian\n  statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin Constraints on 4U 1630-47 via combined Continuum Fitting and\n  Reflection methods: a comparative study using Frequentist and Bayesian\n  statistics"
                },
                "summary": "We present a comprehensive Bayesian spectral analysis of the black hole X-ray\nbinary 4U 1630-47 during its 2022 outburst, using simultaneous \\textit{NICER}\nand \\textit{NuSTAR} observations. Using the traditional frequentist approach,\nwe build our model combining reflection spectroscopy with continuum fitting\ntechniques and analyse the data. In the Bayesian framework, we jointly\nconstrain the black hole's spin, mass, inclination, and distance within a\nunified framework. Employing nested sampling, we capture parameter degeneracies\nand rigorously propagate both statistical and systematic uncertainties. Our\nresults yield robust and precise spin measurements from both approaches. Our\nBayesian analysis fetches spin $a_*= 0.93_{-0.04}^{+0.05}$, mass $M_{\\rm BH} =\n9.0_{-2.0}^{+2.0} \\, M_\\odot$, distance $d_{\\rm BH} = 10.5_{-1.2}^{+1.3}$~kpc,\nand inclination angle $i=53.8_{-1.3}^{+1.3}$~deg. It also demonstrates the\npower of Bayesian inference in fetching valuable insights into the complex\nphysics of black hole accretion and enabling high-confidence measurements of\nfundamental parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive Bayesian spectral analysis of the black hole X-ray\nbinary 4U 1630-47 during its 2022 outburst, using simultaneous \\textit{NICER}\nand \\textit{NuSTAR} observations. Using the traditional frequentist approach,\nwe build our model combining reflection spectroscopy with continuum fitting\ntechniques and analyse the data. In the Bayesian framework, we jointly\nconstrain the black hole's spin, mass, inclination, and distance within a\nunified framework. Employing nested sampling, we capture parameter degeneracies\nand rigorously propagate both statistical and systematic uncertainties. Our\nresults yield robust and precise spin measurements from both approaches. Our\nBayesian analysis fetches spin $a_*= 0.93_{-0.04}^{+0.05}$, mass $M_{\\rm BH} =\n9.0_{-2.0}^{+2.0} \\, M_\\odot$, distance $d_{\\rm BH} = 10.5_{-1.2}^{+1.3}$~kpc,\nand inclination angle $i=53.8_{-1.3}^{+1.3}$~deg. It also demonstrates the\npower of Bayesian inference in fetching valuable insights into the complex\nphysics of black hole accretion and enabling high-confidence measurements of\nfundamental parameters."
                },
                "authors": [
                    {
                        "name": "Debtroy Das"
                    },
                    {
                        "name": "Honghui Liu"
                    },
                    {
                        "name": "Zuobin Zhang"
                    },
                    {
                        "name": "Cosimo Bambi"
                    },
                    {
                        "name": "Jiachen Jiang"
                    },
                    {
                        "name": "Johannes Buchner"
                    },
                    {
                        "name": "Andrea Santangelo"
                    },
                    {
                        "name": "Menglei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Menglei Zhou"
                },
                "author": "Menglei Zhou",
                "arxiv_comment": "24 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00806v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00806v4",
                "updated": "2025-09-11T13:58:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    58,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2024-03-31T21:43:05Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    21,
                    43,
                    5,
                    6,
                    91,
                    0
                ],
                "title": "Algorithmic Collusion by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Collusion by Large Language Models"
                },
                "summary": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that LLM-based pricing agents quickly and autonomously\nreach supracompetitive prices and profits in oligopoly settings and that\nvariation in seemingly innocuous phrases in LLM instructions (\"prompts\") may\nsubstantially influence the degree of supracompetitive pricing. Off-path\nanalysis using novel techniques uncovers price-war concerns as contributing to\nthese phenomena. Our results extend to auction settings. Our findings uncover\nunique challenges to any future regulation of LLM-based pricing agents, and\nAI-based pricing agents more broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that LLM-based pricing agents quickly and autonomously\nreach supracompetitive prices and profits in oligopoly settings and that\nvariation in seemingly innocuous phrases in LLM instructions (\"prompts\") may\nsubstantially influence the degree of supracompetitive pricing. Off-path\nanalysis using novel techniques uncovers price-war concerns as contributing to\nthese phenomena. Our results extend to auction settings. Our findings uncover\nunique challenges to any future regulation of LLM-based pricing agents, and\nAI-based pricing agents more broadly."
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    }
                ],
                "author_detail": {
                    "name": "Ran I. Shorrer"
                },
                "author": "Ran I. Shorrer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00806v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00806v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09469v1",
                "updated": "2025-09-11T13:52:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    52,
                    47,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:52:47Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    52,
                    47,
                    3,
                    254,
                    0
                ],
                "title": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI"
                },
                "summary": "Gliomas are the most prevalent type of primary brain tumors, and their\naccurate segmentation from MRI is critical for diagnosis, treatment planning,\nand longitudinal monitoring. However, the scarcity of high-quality annotated\nimaging data in Sub-Saharan Africa (SSA) poses a significant challenge for\ndeploying advanced segmentation models in clinical workflows. This study\nintroduces a robust and computationally efficient deep learning framework\ntailored for resource-constrained settings. We leveraged a 3D Attention UNet\narchitecture augmented with residual blocks and enhanced through transfer\nlearning from pre-trained weights on the BraTS 2021 dataset. Our model was\nevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma\nsegmentation in SSA MRI data. Despite the limited data quality and quantity,\nour approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80\nfor Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding\nNon-Functional Hemisphere (SNFH). These results demonstrate the\ngeneralizability of the proposed model and its potential to support clinical\ndecision making in low-resource settings. The compact architecture,\napproximately 90 MB, and sub-minute per-volume inference time on consumer-grade\nhardware further underscore its practicality for deployment in SSA health\nsystems. This work contributes toward closing the gap in equitable AI for\nglobal health by empowering underserved regions with high-performing and\naccessible medical imaging solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gliomas are the most prevalent type of primary brain tumors, and their\naccurate segmentation from MRI is critical for diagnosis, treatment planning,\nand longitudinal monitoring. However, the scarcity of high-quality annotated\nimaging data in Sub-Saharan Africa (SSA) poses a significant challenge for\ndeploying advanced segmentation models in clinical workflows. This study\nintroduces a robust and computationally efficient deep learning framework\ntailored for resource-constrained settings. We leveraged a 3D Attention UNet\narchitecture augmented with residual blocks and enhanced through transfer\nlearning from pre-trained weights on the BraTS 2021 dataset. Our model was\nevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma\nsegmentation in SSA MRI data. Despite the limited data quality and quantity,\nour approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80\nfor Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding\nNon-Functional Hemisphere (SNFH). These results demonstrate the\ngeneralizability of the proposed model and its potential to support clinical\ndecision making in low-resource settings. The compact architecture,\napproximately 90 MB, and sub-minute per-volume inference time on consumer-grade\nhardware further underscore its practicality for deployment in SSA health\nsystems. This work contributes toward closing the gap in equitable AI for\nglobal health by empowering underserved regions with high-performing and\naccessible medical imaging solutions."
                },
                "authors": [
                    {
                        "name": "Freedmore Sidume"
                    },
                    {
                        "name": "Oumayma Soula"
                    },
                    {
                        "name": "Joseph Muthui Wacira"
                    },
                    {
                        "name": "YunFei Zhu"
                    },
                    {
                        "name": "Abbas Rabiu Muhammad"
                    },
                    {
                        "name": "Abderrazek Zeraii"
                    },
                    {
                        "name": "Oluwaseun Kalejaye"
                    },
                    {
                        "name": "Hajer Ibrahim"
                    },
                    {
                        "name": "Olfa Gaddour"
                    },
                    {
                        "name": "Brain Halubanza"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Udunna C Anazodo"
                    },
                    {
                        "name": "Confidence Raymond"
                    }
                ],
                "author_detail": {
                    "name": "Confidence Raymond"
                },
                "author": "Confidence Raymond",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20192v2",
                "updated": "2025-09-11T13:51:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    51,
                    55,
                    3,
                    254,
                    0
                ],
                "published": "2024-12-28T16:05:41Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    16,
                    5,
                    41,
                    5,
                    363,
                    0
                ],
                "title": "Physics consistent machine learning framework for inverse modeling with\n  applications to ICF capsule implosions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics consistent machine learning framework for inverse modeling with\n  applications to ICF capsule implosions"
                },
                "summary": "In high energy density physics (HEDP) and inertial confinement fusion (ICF),\npredictive modeling is complicated by uncertainty in parameters that\ncharacterize various aspects of the modeled system, such as those\ncharacterizing material properties, equation of state (EOS), opacities, and\ninitial conditions. Typically, however, these parameters are not directly\nobservable. What is observed instead is a time sequence of radiographic\nprojections using X-rays. In this work, we define a set of sparse hydrodynamic\nfeatures derived from the outgoing shock profile and outer material edge, which\ncan be obtained from radiographic measurements, to directly infer such\nparameters. Our machine learning (ML)-based methodology involves a pipeline of\ntwo architectures, a radiograph-to-features network (R2FNet) and a\nfeatures-to-parameters network (F2PNet), that are trained independently and\nlater combined to approximate a posterior distribution for the parameters from\nradiographs. We show that the estimated parameters can be used in a\nhydrodynamics code to obtain density fields and hydrodynamic shock and outer\nedge features that are consistent with the data. Finally, we demonstrate that\nfeatures resulting from an unknown EOS model can be successfully mapped onto\nparameters of a chosen analytical EOS model, implying that network predictions\nare learning physics, with a degree of invariance to the underlying choice of\nEOS model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high energy density physics (HEDP) and inertial confinement fusion (ICF),\npredictive modeling is complicated by uncertainty in parameters that\ncharacterize various aspects of the modeled system, such as those\ncharacterizing material properties, equation of state (EOS), opacities, and\ninitial conditions. Typically, however, these parameters are not directly\nobservable. What is observed instead is a time sequence of radiographic\nprojections using X-rays. In this work, we define a set of sparse hydrodynamic\nfeatures derived from the outgoing shock profile and outer material edge, which\ncan be obtained from radiographic measurements, to directly infer such\nparameters. Our machine learning (ML)-based methodology involves a pipeline of\ntwo architectures, a radiograph-to-features network (R2FNet) and a\nfeatures-to-parameters network (F2PNet), that are trained independently and\nlater combined to approximate a posterior distribution for the parameters from\nradiographs. We show that the estimated parameters can be used in a\nhydrodynamics code to obtain density fields and hydrodynamic shock and outer\nedge features that are consistent with the data. Finally, we demonstrate that\nfeatures resulting from an unknown EOS model can be successfully mapped onto\nparameters of a chosen analytical EOS model, implying that network predictions\nare learning physics, with a degree of invariance to the underlying choice of\nEOS model."
                },
                "authors": [
                    {
                        "name": "Daniel A. Serino"
                    },
                    {
                        "name": "Evan Bell"
                    },
                    {
                        "name": "Marc Klasky"
                    },
                    {
                        "name": "Ben S. Southworth"
                    },
                    {
                        "name": "Balasubramanya Nadiga"
                    },
                    {
                        "name": "Trevor Wilcox"
                    },
                    {
                        "name": "Oleg Korobkin"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Korobkin"
                },
                "author": "Oleg Korobkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09468v1",
                "updated": "2025-09-11T13:51:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    51,
                    8,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:51:08Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    51,
                    8,
                    3,
                    254,
                    0
                ],
                "title": "A Probabilistic Framework for Predicting Spatiotemporal Intensity and\n  Variability of Outdoor Thermal Comfort",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Framework for Predicting Spatiotemporal Intensity and\n  Variability of Outdoor Thermal Comfort"
                },
                "summary": "Thermal conditions in the urban canopy exhibit stochastic variability driven\nby varied radiative fluxes and turbulent wind fields, requiring probabilistic\nrather than deterministic prediction methods. This study presents a\nprobabilistic framework for predicting the spatial and temporal intensity and\nvariability of outdoor thermal comfort in tropical urban environments. The\nframework integrates ground-measured meteorological data and remote sensing\nurban morphological data to calculate Physiological Equivalent Temperature\n(PET), and applies K-means, XGBoost, and Monte Carlo simulations on PET\ntraining and inference. The prediction model achieved strong performance, with\nR2, RMSE, and SMAPE values of 0.93, 0.81 degC, and 1.34% for PET_mean, and\n0.85, 0.38 degC, and 10.44% for PET_std, respectively. A case study showed\nclear spatial heterogeneity of outdoor thermal comfort. Locations with dense\ntree canopies and vegetated surfaces displayed a normalized percentage of\nacceptable thermal comfort (NATC) up to 65%, whereas built-up zones dominated\nby impervious surfaces, such as industrial estates and high-density residential\nareas, recorded NATC below 30%. Greenery was found to mitigate both the\nintensity of heat stress and its variability, producing a stable and\ncomfortable microclimate. Daytime PET_std ranged from 4.0-4.5 degC in built-up\nareas to 1.5-2.0 degC in greenery-covered zones, while nighttime PET_std\ndecreased to 2.2-2.4 degC and 1.2-1.4 degC, respectively. These findings\nemphasize the critical role of greenery in mitigating thermal variability and\nenhancing outdoor thermal comfort, while revealing the stochastic nature of\nthermal comfort across different urban morphologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal conditions in the urban canopy exhibit stochastic variability driven\nby varied radiative fluxes and turbulent wind fields, requiring probabilistic\nrather than deterministic prediction methods. This study presents a\nprobabilistic framework for predicting the spatial and temporal intensity and\nvariability of outdoor thermal comfort in tropical urban environments. The\nframework integrates ground-measured meteorological data and remote sensing\nurban morphological data to calculate Physiological Equivalent Temperature\n(PET), and applies K-means, XGBoost, and Monte Carlo simulations on PET\ntraining and inference. The prediction model achieved strong performance, with\nR2, RMSE, and SMAPE values of 0.93, 0.81 degC, and 1.34% for PET_mean, and\n0.85, 0.38 degC, and 10.44% for PET_std, respectively. A case study showed\nclear spatial heterogeneity of outdoor thermal comfort. Locations with dense\ntree canopies and vegetated surfaces displayed a normalized percentage of\nacceptable thermal comfort (NATC) up to 65%, whereas built-up zones dominated\nby impervious surfaces, such as industrial estates and high-density residential\nareas, recorded NATC below 30%. Greenery was found to mitigate both the\nintensity of heat stress and its variability, producing a stable and\ncomfortable microclimate. Daytime PET_std ranged from 4.0-4.5 degC in built-up\nareas to 1.5-2.0 degC in greenery-covered zones, while nighttime PET_std\ndecreased to 2.2-2.4 degC and 1.2-1.4 degC, respectively. These findings\nemphasize the critical role of greenery in mitigating thermal variability and\nenhancing outdoor thermal comfort, while revealing the stochastic nature of\nthermal comfort across different urban morphologies."
                },
                "authors": [
                    {
                        "name": "Shisheng Chen"
                    },
                    {
                        "name": "Ruohan Xu"
                    },
                    {
                        "name": "Nyuk Hien Wong"
                    },
                    {
                        "name": "Shanshan Tong"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Matthaios Santamouris"
                    }
                ],
                "author_detail": {
                    "name": "Matthaios Santamouris"
                },
                "author": "Matthaios Santamouris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04275v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04275v3",
                "updated": "2025-09-11T13:50:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    50,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2024-08-08T07:20:42Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    7,
                    20,
                    42,
                    3,
                    221,
                    0
                ],
                "title": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models"
                },
                "summary": "Multimodal large language models (LLMs) empower LLMs to ingest inputs and\ngenerate outputs in multiple forms, such as text, image, and audio. However,\nthe integration of multiple modalities introduces heterogeneity in both the\nmodel and training data, creating unique systems challenges.\n  We propose DistTrain, a disaggregated training system for multimodal LLMs.\nDistTrain incorporates two novel disaggregation techniques to address model and\ndata heterogeneity, respectively. The first is disaggregated model\norchestration, which separates the training for modality encoder, LLM backbone,\nand modality generator. This allows the three components to adaptively and\nindependently orchestrate their resources and parallelism configurations. The\nsecond is disaggregated data preprocessing, which decouples data preprocessing\nfrom training. This eliminates resource contention between preprocessing and\ntraining, and enables efficient data reordering to mitigate stragglers within\nand between microbatches caused by data heterogeneity. We evaluate DistTrain\nacross different sizes of multimodal LLMs on a large-scale production cluster.\nThe experimental results show that DistTrain achieves 54.7% Model FLOPs\nUtilization (MFU) when training a 72B multimodal LLM on 1172 GPUs and\noutperforms Megatron-LM by up to 2.2x on training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) empower LLMs to ingest inputs and\ngenerate outputs in multiple forms, such as text, image, and audio. However,\nthe integration of multiple modalities introduces heterogeneity in both the\nmodel and training data, creating unique systems challenges.\n  We propose DistTrain, a disaggregated training system for multimodal LLMs.\nDistTrain incorporates two novel disaggregation techniques to address model and\ndata heterogeneity, respectively. The first is disaggregated model\norchestration, which separates the training for modality encoder, LLM backbone,\nand modality generator. This allows the three components to adaptively and\nindependently orchestrate their resources and parallelism configurations. The\nsecond is disaggregated data preprocessing, which decouples data preprocessing\nfrom training. This eliminates resource contention between preprocessing and\ntraining, and enables efficient data reordering to mitigate stragglers within\nand between microbatches caused by data heterogeneity. We evaluate DistTrain\nacross different sizes of multimodal LLMs on a large-scale production cluster.\nThe experimental results show that DistTrain achieves 54.7% Model FLOPs\nUtilization (MFU) when training a 72B multimodal LLM on 1172 GPUs and\noutperforms Megatron-LM by up to 2.2x on training throughput."
                },
                "authors": [
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "arxiv_doi": "10.1145/3718958.3750472",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3718958.3750472",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04275v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04275v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGCOMM 2025 (https://dl.acm.org/doi/10.1145/3718958.3750472)",
                "arxiv_journal_ref": "SIGCOMM'25: Proceedings of the ACM SIGCOMM 2025 Conference Pages\n  24-38",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09467v1",
                "updated": "2025-09-11T13:50:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    50,
                    23,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:50:23Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    50,
                    23,
                    3,
                    254,
                    0
                ],
                "title": "Inteligencia Artificial jurídica y el desafío de la veracidad:\n  análisis de alucinaciones, optimización de RAG y principios para una\n  integración responsable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inteligencia Artificial jurídica y el desafío de la veracidad:\n  análisis de alucinaciones, optimización de RAG y principios para una\n  integración responsable"
                },
                "summary": "This technical report analyzes the challenge of \"hallucinations\" (false\ninformation) in LLMs applied to law. It examines their causes, manifestations,\nand the effectiveness of the RAG mitigation strategy, highlighting its\nlimitations and proposing holistic optimizations. The paper explores the\nethical and regulatory implications, emphasizing human oversight as an\nirreplaceable role. It concludes that the solution lies not in incrementally\nimproving generative models, but in adopting a \"consultative\" AI paradigm that\nprioritizes veracity and traceability, acting as a tool to amplify, not\nreplace, professional judgment.\n  --\n  Este informe t\\'ecnico analiza el desaf\\'io de las \"alucinaciones\"\n(informaci\\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,\nmanifestaciones y la efectividad de la estrategia de mitigaci\\'on RAG,\nexponiendo sus limitaciones y proponiendo optimizaciones hol\\'isticas. Se\nexploran las implicaciones \\'eticas y regulatorias, enfatizando la\nsupervisi\\'on humana como un rol insustituible. El documento concluye que la\nsoluci\\'on no reside en mejorar incrementalmente los modelos generativos, sino\nen adoptar un paradigma de IA \"consultiva\" que priorice la veracidad y la\ntrazabilidad, actuando como una herramienta para amplificar, y no sustituir, el\njuicio profesional.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report analyzes the challenge of \"hallucinations\" (false\ninformation) in LLMs applied to law. It examines their causes, manifestations,\nand the effectiveness of the RAG mitigation strategy, highlighting its\nlimitations and proposing holistic optimizations. The paper explores the\nethical and regulatory implications, emphasizing human oversight as an\nirreplaceable role. It concludes that the solution lies not in incrementally\nimproving generative models, but in adopting a \"consultative\" AI paradigm that\nprioritizes veracity and traceability, acting as a tool to amplify, not\nreplace, professional judgment.\n  --\n  Este informe t\\'ecnico analiza el desaf\\'io de las \"alucinaciones\"\n(informaci\\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,\nmanifestaciones y la efectividad de la estrategia de mitigaci\\'on RAG,\nexponiendo sus limitaciones y proponiendo optimizaciones hol\\'isticas. Se\nexploran las implicaciones \\'eticas y regulatorias, enfatizando la\nsupervisi\\'on humana como un rol insustituible. El documento concluye que la\nsoluci\\'on no reside en mejorar incrementalmente los modelos generativos, sino\nen adoptar un paradigma de IA \"consultiva\" que priorice la veracidad y la\ntrazabilidad, actuando como una herramienta para amplificar, y no sustituir, el\njuicio profesional."
                },
                "authors": [
                    {
                        "name": "Alex Dantart"
                    }
                ],
                "author_detail": {
                    "name": "Alex Dantart"
                },
                "author": "Alex Dantart",
                "arxiv_comment": "in Spanish and English languages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09461v1",
                "updated": "2025-09-11T13:43:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    43,
                    30,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:43:30Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    43,
                    30,
                    3,
                    254,
                    0
                ],
                "title": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries\n  with Human Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries\n  with Human Intervention"
                },
                "summary": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world."
                },
                "authors": [
                    {
                        "name": "Ambre Assor"
                    },
                    {
                        "name": "Hyeon Jeon"
                    },
                    {
                        "name": "Sungbok Shin"
                    },
                    {
                        "name": "Jean-Daniel Fekete"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Daniel Fekete"
                },
                "author": "Jean-Daniel Fekete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09450v1",
                "updated": "2025-09-11T13:32:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    32,
                    6,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:32:06Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    32,
                    6,
                    3,
                    254,
                    0
                ],
                "title": "A Radially Resolved Magnetic Field Threading the Disk of TW Hya",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Radially Resolved Magnetic Field Threading the Disk of TW Hya"
                },
                "summary": "We present a new approach to detecting and characterizing a magnetic field in\nprotoplanetary disks through the differential broadening of unpolarized\nmolecular emission from CN. To demonstrate this technique, we apply it to new\nALMA observations of the full complement of hyperfine components from the\n$N=1-0$ transition, achieving a spatial and spectral resolution of\n${\\approx}\\,0.5^{\\prime\\prime}$ and $80~{\\rm m\\,s^{-1}}$, respectively. By\nfitting a model that incorporates the velocity structure of the disk, the\npotential non-LTE excitation of the molecule, and the Zeeman effect, we recover\na radially resolved magnetic field with a strength of ${\\sim}10~{\\rm mG}$\nbetween 60 and 120~au. The morphology of the field is also inferred through\nazimuthal variations in the line broadening, revealing a predominantly poloidal\nfield at 60~au, sharply transitioning to one within the disk plane outside of\nthe gap at 82~au. The signal-to-noise ratio of the data meant that the planar\ncomponent was unable to be decomposed into toroidal and radial components.\nLower limits on the local gas density ($n({\\rm H_2}) \\gtrsim 10^8~{\\rm\ncm^{-3}}$) from the excitation analysis of the CN emission correspond to a\nlower limit between 0.1 and 0.01 for the plasma $\\beta$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new approach to detecting and characterizing a magnetic field in\nprotoplanetary disks through the differential broadening of unpolarized\nmolecular emission from CN. To demonstrate this technique, we apply it to new\nALMA observations of the full complement of hyperfine components from the\n$N=1-0$ transition, achieving a spatial and spectral resolution of\n${\\approx}\\,0.5^{\\prime\\prime}$ and $80~{\\rm m\\,s^{-1}}$, respectively. By\nfitting a model that incorporates the velocity structure of the disk, the\npotential non-LTE excitation of the molecule, and the Zeeman effect, we recover\na radially resolved magnetic field with a strength of ${\\sim}10~{\\rm mG}$\nbetween 60 and 120~au. The morphology of the field is also inferred through\nazimuthal variations in the line broadening, revealing a predominantly poloidal\nfield at 60~au, sharply transitioning to one within the disk plane outside of\nthe gap at 82~au. The signal-to-noise ratio of the data meant that the planar\ncomponent was unable to be decomposed into toroidal and radial components.\nLower limits on the local gas density ($n({\\rm H_2}) \\gtrsim 10^8~{\\rm\ncm^{-3}}$) from the excitation analysis of the CN emission correspond to a\nlower limit between 0.1 and 0.01 for the plasma $\\beta$."
                },
                "authors": [
                    {
                        "name": "Richard Teague"
                    },
                    {
                        "name": "Boy Lankhaar"
                    },
                    {
                        "name": "Sean M. Andrews"
                    },
                    {
                        "name": "Chunhua Qi"
                    },
                    {
                        "name": "Roger R. Fu"
                    },
                    {
                        "name": "David J. Wilner"
                    },
                    {
                        "name": "John B. Biersteker"
                    },
                    {
                        "name": "Joan R. Najita"
                    }
                ],
                "author_detail": {
                    "name": "Joan R. Najita"
                },
                "author": "Joan R. Najita",
                "arxiv_comment": "19 pages, 14 figures, accepted by ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09448v1",
                "updated": "2025-09-11T13:31:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORSO: Template-Oriented Reasoning Towards General Tasks"
                },
                "summary": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales."
                },
                "authors": [
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09438v1",
                "updated": "2025-09-11T13:25:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    25,
                    40,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:25:40Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    25,
                    40,
                    3,
                    254,
                    0
                ],
                "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrACE: A Generative Approach to Better Confidence Elicitation in Large\n  Language Models"
                },
                "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation."
                },
                "authors": [
                    {
                        "name": "Zhaohan Zhang"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06174v3",
                "updated": "2025-09-11T13:10:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    10,
                    15,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-08T16:18:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    18,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "On Soft Clustering For Correlation Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Soft Clustering For Correlation Estimators"
                },
                "summary": "Properly estimating correlations between objects at different spatial scales\nnecessitates $\\mathcal{O}(n^2)$ distance calculations. For this reason, most\nwidely adopted packages for estimating correlations use clustering algorithms\nto approximate local trends. However, methods for quantifying the error\nintroduced by this clustering have been understudied. In response, we present\nan algorithm for estimating correlations that is probabilistic in the way that\nit clusters objects, enabling us to quantify the uncertainty caused by\nclustering simply through model inference. These soft clustering assignments\nenable correlation estimators that are theoretically differentiable with\nrespect to their input catalogs. Thus, we also build a theoretical framework\nfor differentiable correlation functions and describe their utility in\ncomparison to existing surrogate models. Notably, we find that repeated\nnormalization and distance function calls slow gradient calculations and that\nsparse Jacobians destabilize precision, pointing towards either approximate or\nsurrogate methods as a necessary solution to exact gradients from correlation\nfunctions. To that end, we close with a discussion of surrogate models as\nproxies for correlation functions. We provide an example that demonstrates the\nefficacy of surrogate models to enable gradient-based optimization of\nastrophysical model parameters, successfully minimizing a correlation function\noutput. Our numerical experiments cover science cases across cosmology, from\npoint spread function (PSF) modeling efforts to gravitational simulations to\ngalaxy intrinsic alignment (IA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Properly estimating correlations between objects at different spatial scales\nnecessitates $\\mathcal{O}(n^2)$ distance calculations. For this reason, most\nwidely adopted packages for estimating correlations use clustering algorithms\nto approximate local trends. However, methods for quantifying the error\nintroduced by this clustering have been understudied. In response, we present\nan algorithm for estimating correlations that is probabilistic in the way that\nit clusters objects, enabling us to quantify the uncertainty caused by\nclustering simply through model inference. These soft clustering assignments\nenable correlation estimators that are theoretically differentiable with\nrespect to their input catalogs. Thus, we also build a theoretical framework\nfor differentiable correlation functions and describe their utility in\ncomparison to existing surrogate models. Notably, we find that repeated\nnormalization and distance function calls slow gradient calculations and that\nsparse Jacobians destabilize precision, pointing towards either approximate or\nsurrogate methods as a necessary solution to exact gradients from correlation\nfunctions. To that end, we close with a discussion of surrogate models as\nproxies for correlation functions. We provide an example that demonstrates the\nefficacy of surrogate models to enable gradient-based optimization of\nastrophysical model parameters, successfully minimizing a correlation function\noutput. Our numerical experiments cover science cases across cosmology, from\npoint spread function (PSF) modeling efforts to gravitational simulations to\ngalaxy intrinsic alignment (IA)."
                },
                "authors": [
                    {
                        "name": "Edward Berman"
                    },
                    {
                        "name": "Sneh Pandya"
                    },
                    {
                        "name": "Jacqueline McCleary"
                    },
                    {
                        "name": "Marko Shuntov"
                    },
                    {
                        "name": "Caitlin Casey"
                    },
                    {
                        "name": "Nicole Drakos"
                    },
                    {
                        "name": "Andreas Faisst"
                    },
                    {
                        "name": "Steven Gillman"
                    },
                    {
                        "name": "Ghassem Gozaliasl"
                    },
                    {
                        "name": "Natalie Hogg"
                    },
                    {
                        "name": "Jeyhan Kartaltepe"
                    },
                    {
                        "name": "Anton Koekemoer"
                    },
                    {
                        "name": "Wilfried Mercier"
                    },
                    {
                        "name": "Diana Scognamiglio"
                    },
                    {
                        "name": "COSMOS-Web"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "The JWST Cosmic Origins Survey"
                    }
                ],
                "author_detail": {
                    "name": "The JWST Cosmic Origins Survey"
                },
                "author": "The JWST Cosmic Origins Survey",
                "arxiv_comment": "Published in the Open Journal of Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09424v1",
                "updated": "2025-09-11T13:04:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    4,
                    22,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:04:22Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    4,
                    22,
                    3,
                    254,
                    0
                ],
                "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ENSI: Efficient Non-Interactive Secure Inference for Large Language\n  Models"
                },
                "summary": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%."
                },
                "authors": [
                    {
                        "name": "Zhiyu He"
                    },
                    {
                        "name": "Maojiang Wang"
                    },
                    {
                        "name": "Xinwen Gao"
                    },
                    {
                        "name": "Yuchuan Luo"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shaojing Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shaojing Fu"
                },
                "author": "Shaojing Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08461v2",
                "updated": "2025-09-11T13:03:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    3,
                    4,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T10:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    7,
                    27,
                    2,
                    253,
                    0
                ],
                "title": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics."
                },
                "authors": [
                    {
                        "name": "Dikshant Sagar"
                    },
                    {
                        "name": "Kaiwen Yu"
                    },
                    {
                        "name": "Alejandro Yankelevich"
                    },
                    {
                        "name": "Jianming Bian"
                    },
                    {
                        "name": "Pierre Baldi"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Baldi"
                },
                "author": "Pierre Baldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09420v1",
                "updated": "2025-09-11T13:01:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    1,
                    55,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:01:55Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    1,
                    55,
                    3,
                    254,
                    0
                ],
                "title": "HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with\n  3D Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with\n  3D Near-Memory Processing"
                },
                "summary": "Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures\nachieve superior model performance with reduced computation costs, but at the\ncost of high memory capacity and bandwidth requirements. Near-Memory Processing\n(NMP) accelerators that stack memory directly on the compute through hybrid\nbonding have demonstrated high bandwidth with high energy efficiency, becoming\na promising architecture for MoE models. However, as NMP accelerators comprise\ndistributed memory and computation, how to map the MoE computation directly\ndetermines the LLM inference efficiency. Existing parallel mapping strategies,\nincluding Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from\neither high communication costs or unbalanced computation utilization, leading\nto inferior efficiency. The dynamic routing mechanism of MoE LLMs further\naggravates the efficiency challenges. Therefore, in this paper, we propose\nHD-MoE to automatically optimize the MoE parallel computation across an NMP\naccelerator. HD-MoE features an offline automatic hybrid parallel mapping\nalgorithm and an online dynamic scheduling strategy to reduce the communication\ncosts while maximizing the computation utilization. With extensive experimental\nresults, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to\n1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid\nTP-EP with Compute-Balanced parallelism strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures\nachieve superior model performance with reduced computation costs, but at the\ncost of high memory capacity and bandwidth requirements. Near-Memory Processing\n(NMP) accelerators that stack memory directly on the compute through hybrid\nbonding have demonstrated high bandwidth with high energy efficiency, becoming\na promising architecture for MoE models. However, as NMP accelerators comprise\ndistributed memory and computation, how to map the MoE computation directly\ndetermines the LLM inference efficiency. Existing parallel mapping strategies,\nincluding Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from\neither high communication costs or unbalanced computation utilization, leading\nto inferior efficiency. The dynamic routing mechanism of MoE LLMs further\naggravates the efficiency challenges. Therefore, in this paper, we propose\nHD-MoE to automatically optimize the MoE parallel computation across an NMP\naccelerator. HD-MoE features an offline automatic hybrid parallel mapping\nalgorithm and an online dynamic scheduling strategy to reduce the communication\ncosts while maximizing the computation utilization. With extensive experimental\nresults, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to\n1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid\nTP-EP with Compute-Balanced parallelism strategies."
                },
                "authors": [
                    {
                        "name": "Haochen Huang"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Shuangchen Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Hongzhong Zheng"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "9 pages, 15 figures, International Conference on Computer-Aided\n  Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06723v2",
                "updated": "2025-09-11T12:56:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    56,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T14:21:45Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    14,
                    21,
                    45,
                    0,
                    251,
                    0
                ],
                "title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via\n  Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via\n  Test-Time Training"
                },
                "summary": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches."
                },
                "authors": [
                    {
                        "name": "Ruicheng Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Zihao Liu"
                    },
                    {
                        "name": "Jiehui Huang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Xiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Li"
                },
                "author": "Xiu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09413v1",
                "updated": "2025-09-11T12:51:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    51,
                    34,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:51:34Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    51,
                    34,
                    3,
                    254,
                    0
                ],
                "title": "Fused Lasso Improves Accuracy of Co-occurrence Network Inference in\n  Grouped Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fused Lasso Improves Accuracy of Co-occurrence Network Inference in\n  Grouped Samples"
                },
                "summary": "Co-occurrence network inference algorithms have significantly advanced our\nunderstanding of microbiome communities. However, these algorithms typically\nanalyze microbial associations within samples collected from a single\nenvironmental niche, often capturing only static snapshots rather than dynamic\nmicrobial processes. Previous studies have commonly grouped samples from\ndifferent environmental niches together without fully considering how microbial\ncommunities adapt their associations when faced with varying ecological\nconditions. Our study addresses this limitation by explicitly investigating\nboth spatial and temporal dynamics of microbial communities. We analyzed\npublicly available microbiome abundance data across multiple locations and time\npoints, to evaluate algorithm performance in predicting microbial associations\nusing our proposed Same-All Cross-validation (SAC) framework. SAC evaluates\nalgorithms in two distinct scenarios: training and testing within the same\nenvironmental niche (Same), and training and testing on combined data from\nmultiple environmental niches (All). To overcome the limitations of\nconventional algorithms, we propose fuser, an algorithm that, while not\nentirely new in machine learning, is novel for microbiome community network\ninference. It retains subsample-specific signals while simultaneously sharing\nrelevant information across environments during training. Unlike standard\napproaches that infer a single generalized network from combined data, fuser\ngenerates distinct, environment-specific predictive networks. Our results\ndemonstrate that fuser achieves comparable predictive performance to existing\nalgorithms such as glmnet when evaluated within homogeneous environments\n(Same), and notably reduces test error compared to baseline algorithms in\ncross-environment (All) scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-occurrence network inference algorithms have significantly advanced our\nunderstanding of microbiome communities. However, these algorithms typically\nanalyze microbial associations within samples collected from a single\nenvironmental niche, often capturing only static snapshots rather than dynamic\nmicrobial processes. Previous studies have commonly grouped samples from\ndifferent environmental niches together without fully considering how microbial\ncommunities adapt their associations when faced with varying ecological\nconditions. Our study addresses this limitation by explicitly investigating\nboth spatial and temporal dynamics of microbial communities. We analyzed\npublicly available microbiome abundance data across multiple locations and time\npoints, to evaluate algorithm performance in predicting microbial associations\nusing our proposed Same-All Cross-validation (SAC) framework. SAC evaluates\nalgorithms in two distinct scenarios: training and testing within the same\nenvironmental niche (Same), and training and testing on combined data from\nmultiple environmental niches (All). To overcome the limitations of\nconventional algorithms, we propose fuser, an algorithm that, while not\nentirely new in machine learning, is novel for microbiome community network\ninference. It retains subsample-specific signals while simultaneously sharing\nrelevant information across environments during training. Unlike standard\napproaches that infer a single generalized network from combined data, fuser\ngenerates distinct, environment-specific predictive networks. Our results\ndemonstrate that fuser achieves comparable predictive performance to existing\nalgorithms such as glmnet when evaluated within homogeneous environments\n(Same), and notably reduces test error compared to baseline algorithms in\ncross-environment (All) scenarios."
                },
                "authors": [
                    {
                        "name": "Daniel Agyapong"
                    },
                    {
                        "name": "Briana H. Beatty"
                    },
                    {
                        "name": "Peter G. Kennedy"
                    },
                    {
                        "name": "Toby D. Hocking"
                    }
                ],
                "author_detail": {
                    "name": "Toby D. Hocking"
                },
                "author": "Toby D. Hocking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v7",
                "updated": "2025-09-11T12:43:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    43,
                    54,
                    3,
                    254,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,198\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,198\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04227v3",
                "updated": "2025-09-11T12:26:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    26,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2025-02-06T17:12:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks"
                },
                "summary": "Enterprise penetration-testing is often limited by high operational costs and\nthe scarcity of human expertise. This paper investigates the feasibility and\neffectiveness of using Large Language Model (LLM)-driven autonomous systems to\naddress these challenges in real-world Active Directory (AD) enterprise\nnetworks.\n  We introduce a novel prototype designed to employ LLMs to autonomously\nperform Assumed Breach penetration-testing against enterprise networks. Our\nsystem represents the first demonstration of a fully autonomous, LLM-driven\nframework capable of compromising accounts within a real-life Microsoft Active\nDirectory testbed, GOAD.\n  We perform our empirical evaluation using five LLMs, comparing reasoning to\nnon-reasoning models as well as including open-weight models. Through\nquantitative and qualitative analysis, incorporating insights from\ncybersecurity experts, we demonstrate that autonomous LLMs can effectively\nconduct Assumed Breach simulations. Key findings highlight their ability to\ndynamically adapt attack strategies, perform inter-context attacks (e.g.,\nweb-app audits, social engineering, and unstructured data analysis for\ncredentials), and generate scenario-specific attack parameters like realistic\npassword candidates. The prototype exhibits robust self-correction mechanisms,\ninstalling missing tools and rectifying invalid command generations.\n  We find that the associated costs are competitive with, and often\nsignificantly lower than, those incurred by professional human pen-testers,\nsuggesting a path toward democratizing access to essential security testing for\norganizations with budgetary constraints. However, our research also\nilluminates existing limitations, including instances of LLM ``going down\nrabbit holes'', challenges in comprehensive information transfer between\nplanning and execution modules, and critical safety concerns that necessitate\nhuman oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise penetration-testing is often limited by high operational costs and\nthe scarcity of human expertise. This paper investigates the feasibility and\neffectiveness of using Large Language Model (LLM)-driven autonomous systems to\naddress these challenges in real-world Active Directory (AD) enterprise\nnetworks.\n  We introduce a novel prototype designed to employ LLMs to autonomously\nperform Assumed Breach penetration-testing against enterprise networks. Our\nsystem represents the first demonstration of a fully autonomous, LLM-driven\nframework capable of compromising accounts within a real-life Microsoft Active\nDirectory testbed, GOAD.\n  We perform our empirical evaluation using five LLMs, comparing reasoning to\nnon-reasoning models as well as including open-weight models. Through\nquantitative and qualitative analysis, incorporating insights from\ncybersecurity experts, we demonstrate that autonomous LLMs can effectively\nconduct Assumed Breach simulations. Key findings highlight their ability to\ndynamically adapt attack strategies, perform inter-context attacks (e.g.,\nweb-app audits, social engineering, and unstructured data analysis for\ncredentials), and generate scenario-specific attack parameters like realistic\npassword candidates. The prototype exhibits robust self-correction mechanisms,\ninstalling missing tools and rectifying invalid command generations.\n  We find that the associated costs are competitive with, and often\nsignificantly lower than, those incurred by professional human pen-testers,\nsuggesting a path toward democratizing access to essential security testing for\norganizations with budgetary constraints. However, our research also\nilluminates existing limitations, including instances of LLM ``going down\nrabbit holes'', challenges in comprehensive information transfer between\nplanning and execution modules, and critical safety concerns that necessitate\nhuman oversight."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "arxiv_doi": "10.1145/3766895",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3766895",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.04227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09396v1",
                "updated": "2025-09-11T12:25:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    25,
                    41,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:25:41Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    25,
                    41,
                    3,
                    254,
                    0
                ],
                "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations"
                },
                "summary": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs."
                },
                "authors": [
                    {
                        "name": "Harry Mayne"
                    },
                    {
                        "name": "Ryan Othniel Kearns"
                    },
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Eoin Delaney"
                    },
                    {
                        "name": "Chris Russell"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02253v5",
                "updated": "2025-09-11T12:14:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    14,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-03T03:02:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    3,
                    2,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation"
                },
                "summary": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial."
                },
                "authors": [
                    {
                        "name": "Jungkoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jungkoo Kang"
                },
                "author": "Jungkoo Kang",
                "arxiv_comment": "31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09387v1",
                "updated": "2025-09-11T12:06:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    34,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:06:34Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    34,
                    3,
                    254,
                    0
                ],
                "title": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization"
                },
                "summary": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines."
                },
                "authors": [
                    {
                        "name": "Mohammed Tiouti"
                    },
                    {
                        "name": "Mohamed Bal-Ghaoui"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Bal-Ghaoui"
                },
                "author": "Mohamed Bal-Ghaoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20655v2",
                "updated": "2025-09-11T12:03:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    3,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-28T11:01:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    1,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Alignment in LVLMs with Debiased Self-Judgment"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Weilong Yan"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03613v3",
                "updated": "2025-09-11T12:00:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    0,
                    44,
                    3,
                    254,
                    0
                ],
                "published": "2024-10-04T17:14:59Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    17,
                    14,
                    59,
                    4,
                    278,
                    0
                ],
                "title": "Understanding Large Language Models in Your Pockets: Performance Study\n  on COTS Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Models in Your Pockets: Performance Study\n  on COTS Mobile Devices"
                },
                "summary": "As large language models (LLMs) increasingly integrate into every aspect of\nour work and daily lives, there are growing concerns about user privacy, which\npush the trend toward local deployment of these models. There are a number of\nlightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on\nsmartphones, providing users with greater control over their personal data. As\na rapidly emerging application, we are concerned about their performance on\ncommercial-off-the-shelf mobile devices. To fully understand the current\nlandscape of LLM deployment on mobile platforms, we conduct a comprehensive\nmeasurement study on mobile devices. We evaluate both metrics that affect user\nexperience, including token throughput, latency, and battery consumption, as\nwell as factors critical to developers, such as resource utilization, DVFS\nstrategies, and inference engines. In addition, we provide a detailed analysis\nof how these hardware capabilities and system dynamics affect on-device LLM\nperformance, which may help developers identify and address bottlenecks for\nmobile LLM applications. We also provide comprehensive comparisons across the\nmobile system-on-chips (SoCs) from major vendors, highlighting their\nperformance differences in handling LLM workloads. We hope that this study can\nprovide insights for both the development of on-device LLMs and the design for\nfuture mobile system architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate into every aspect of\nour work and daily lives, there are growing concerns about user privacy, which\npush the trend toward local deployment of these models. There are a number of\nlightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on\nsmartphones, providing users with greater control over their personal data. As\na rapidly emerging application, we are concerned about their performance on\ncommercial-off-the-shelf mobile devices. To fully understand the current\nlandscape of LLM deployment on mobile platforms, we conduct a comprehensive\nmeasurement study on mobile devices. We evaluate both metrics that affect user\nexperience, including token throughput, latency, and battery consumption, as\nwell as factors critical to developers, such as resource utilization, DVFS\nstrategies, and inference engines. In addition, we provide a detailed analysis\nof how these hardware capabilities and system dynamics affect on-device LLM\nperformance, which may help developers identify and address bottlenecks for\nmobile LLM applications. We also provide comprehensive comparisons across the\nmobile system-on-chips (SoCs) from major vendors, highlighting their\nperformance differences in handling LLM workloads. We hope that this study can\nprovide insights for both the development of on-device LLMs and the design for\nfuture mobile system architecture."
                },
                "authors": [
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Qianyi Huang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09373v1",
                "updated": "2025-09-11T11:44:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    44,
                    23,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T11:44:23Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    44,
                    23,
                    3,
                    254,
                    0
                ],
                "title": "Channel Estimation and Analog Precoding for Pixel-based\n  Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Estimation and Analog Precoding for Pixel-based\n  Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems"
                },
                "summary": "Pixel-based fluid antennas provide enhanced multiplexing gains and quicker\nradiation pattern switching than traditional designs. However, this innovation\nintroduces challenges for channel estimation and analog precoding due to the\nstate-non-separable channel response problem. This paper explores a multiuser\nMIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements\nfrom a real-world prototype. We present a sparse channel recovery framework for\nuplink channel sounding, employing an approximate separable channel response\nmodel with DNN-based antenna radiation functions. We then propose two\nlow-complexity channel estimation algorithms that leverage orthogonal matching\npursuit and variational Bayesian inference to accurately recover channel\nresponses across various scattering cluster angles. These estimations enable\nthe prediction of composite channels for all fluid antenna states, leading to\nan analog precoding scheme that optimally selects switching states for\ndifferent antennas. Our simulation results indicate that the proposed approach\nsignificantly outperforms several baseline methods, especially in high\nsignal-to-noise ratio environments with numerous users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel-based fluid antennas provide enhanced multiplexing gains and quicker\nradiation pattern switching than traditional designs. However, this innovation\nintroduces challenges for channel estimation and analog precoding due to the\nstate-non-separable channel response problem. This paper explores a multiuser\nMIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements\nfrom a real-world prototype. We present a sparse channel recovery framework for\nuplink channel sounding, employing an approximate separable channel response\nmodel with DNN-based antenna radiation functions. We then propose two\nlow-complexity channel estimation algorithms that leverage orthogonal matching\npursuit and variational Bayesian inference to accurately recover channel\nresponses across various scattering cluster angles. These estimations enable\nthe prediction of composite channels for all fluid antenna states, leading to\nan analog precoding scheme that optimally selects switching states for\ndifferent antennas. Our simulation results indicate that the proposed approach\nsignificantly outperforms several baseline methods, especially in high\nsignal-to-noise ratio environments with numerous users."
                },
                "authors": [
                    {
                        "name": "Huayan Guo"
                    },
                    {
                        "name": "Jichen Zhang"
                    },
                    {
                        "name": "Junhui Rao"
                    },
                    {
                        "name": "Ross Murch"
                    },
                    {
                        "name": "Vincent K. N. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Vincent K. N. Lau"
                },
                "author": "Vincent K. N. Lau",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09372v1",
                "updated": "2025-09-11T11:42:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    42,
                    21,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T11:42:21Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    42,
                    21,
                    3,
                    254,
                    0
                ],
                "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model"
                },
                "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Lingxiao Li"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Zirui Ge"
                    },
                    {
                        "name": "Xinyang Tong"
                    },
                    {
                        "name": "Wenxuan Song"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Pengxu Hou"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Yifan Tang"
                    },
                    {
                        "name": "Wenhui Wang"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Jianyi Liu"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09371v1",
                "updated": "2025-09-11T11:42:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    42,
                    17,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T11:42:17Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    42,
                    17,
                    3,
                    254,
                    0
                ],
                "title": "Representation-Aware Distributionally Robust Optimization: A Knowledge\n  Transfer Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation-Aware Distributionally Robust Optimization: A Knowledge\n  Transfer Framework"
                },
                "summary": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a\nnovel framework for Wasserstein distributionally robust learning that accounts\nfor predictive representations when guarding against distributional shifts.\nUnlike classical approaches that treat all feature perturbations equally, READ\nembeds a multidimensional alignment parameter into the transport cost, allowing\nthe model to differentially discourage perturbations along directions\nassociated with informative representations. This yields robustness to feature\nvariation while preserving invariant structure. Our first contribution is a\ntheoretical foundation: we show that seminorm regularizations for linear\nregression and binary classification arise as Wasserstein distributionally\nrobust objectives, thereby providing tractable reformulations of READ and\nunifying a broad class of regularized estimators under the DRO lens. Second, we\nadopt a principled procedure for selecting the Wasserstein radius using the\ntechniques of robust Wasserstein profile inference. This further enables the\nconstruction of valid, representation-aware confidence regions for model\nparameters with distinct geometric features. Finally, we analyze the geometry\nof READ estimators as the alignment parameters vary and propose an optimization\nalgorithm to estimate the projection of the global optimum onto this solution\nsurface. This procedure selects among equally robust estimators while optimally\nconstructing a representation structure. We conclude by demonstrating the\neffectiveness of our framework through extensive simulations and a real-world\nstudy, providing a powerful robust estimation grounded in learning\nrepresentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a\nnovel framework for Wasserstein distributionally robust learning that accounts\nfor predictive representations when guarding against distributional shifts.\nUnlike classical approaches that treat all feature perturbations equally, READ\nembeds a multidimensional alignment parameter into the transport cost, allowing\nthe model to differentially discourage perturbations along directions\nassociated with informative representations. This yields robustness to feature\nvariation while preserving invariant structure. Our first contribution is a\ntheoretical foundation: we show that seminorm regularizations for linear\nregression and binary classification arise as Wasserstein distributionally\nrobust objectives, thereby providing tractable reformulations of READ and\nunifying a broad class of regularized estimators under the DRO lens. Second, we\nadopt a principled procedure for selecting the Wasserstein radius using the\ntechniques of robust Wasserstein profile inference. This further enables the\nconstruction of valid, representation-aware confidence regions for model\nparameters with distinct geometric features. Finally, we analyze the geometry\nof READ estimators as the alignment parameters vary and propose an optimization\nalgorithm to estimate the projection of the global optimum onto this solution\nsurface. This procedure selects among equally robust estimators while optimally\nconstructing a representation structure. We conclude by demonstrating the\neffectiveness of our framework through extensive simulations and a real-world\nstudy, providing a powerful robust estimation grounded in learning\nrepresentation."
                },
                "authors": [
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Nian Si"
                    },
                    {
                        "name": "Molei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Molei Liu"
                },
                "author": "Molei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14425v2",
                "updated": "2025-09-11T11:39:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    39,
                    0,
                    3,
                    254,
                    0
                ],
                "published": "2024-09-22T12:52:00Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    12,
                    52,
                    0,
                    6,
                    266,
                    0
                ],
                "title": "bioSBM: a random graph model to integrate epigenomic data in chromatin\n  structure prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "bioSBM: a random graph model to integrate epigenomic data in chromatin\n  structure prediction"
                },
                "summary": "The spatial organization of chromatin within the nucleus plays a crucial role\nin gene expression and genome function. However, the quantitative relationship\nbetween this organization and nuclear biochemical processes remains under\ndebate. In this study, we present a graph-based generative model, bioSBM,\ndesigned to capture long-range chromatin interaction patterns from Hi-C data\nand, importantly, simultaneously link these patterns to biochemical features.\nApplying bioSBM to Hi-C maps of the GM12878 lymphoblastoid cell line, we\nidentified a latent structure of chromatin interactions, revealing 7 distinct\ncommunities that strongly align with known biological annotations.\nAdditionally, we infer a linear transformation that maps biochemical\nobservables, such as histone marks, to the parameters of the generative graph\nmodel, enabling accurate genome-wide predictions of chromatin contact maps on\nout-of-sample data, both within the same cell line, and on the completely\nunseen HCT116 cell line under RAD21 depletion. These findings highlight\nbioSBM's potential as a powerful tool for elucidating the relationship between\nbiochemistry and chromatin architecture and predicting long-range genome\norganization from independent biochemical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spatial organization of chromatin within the nucleus plays a crucial role\nin gene expression and genome function. However, the quantitative relationship\nbetween this organization and nuclear biochemical processes remains under\ndebate. In this study, we present a graph-based generative model, bioSBM,\ndesigned to capture long-range chromatin interaction patterns from Hi-C data\nand, importantly, simultaneously link these patterns to biochemical features.\nApplying bioSBM to Hi-C maps of the GM12878 lymphoblastoid cell line, we\nidentified a latent structure of chromatin interactions, revealing 7 distinct\ncommunities that strongly align with known biological annotations.\nAdditionally, we infer a linear transformation that maps biochemical\nobservables, such as histone marks, to the parameters of the generative graph\nmodel, enabling accurate genome-wide predictions of chromatin contact maps on\nout-of-sample data, both within the same cell line, and on the completely\nunseen HCT116 cell line under RAD21 depletion. These findings highlight\nbioSBM's potential as a powerful tool for elucidating the relationship between\nbiochemistry and chromatin architecture and predicting long-range genome\norganization from independent biochemical data."
                },
                "authors": [
                    {
                        "name": "Alex Chen Yi Zhang"
                    },
                    {
                        "name": "Angelo Rosa"
                    },
                    {
                        "name": "Guido Sanguinetti"
                    }
                ],
                "author_detail": {
                    "name": "Guido Sanguinetti"
                },
                "author": "Guido Sanguinetti",
                "arxiv_doi": "10.1103/gy1p-4256",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/gy1p-4256",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.14425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21119v3",
                "updated": "2025-09-11T11:22:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    22,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-02-28T14:55:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    55,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "Detection of the 2175Å UV Bump at z>7: Evidence for Rapid Dust\n  Evolution in a Merging Reionisation-Era Galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of the 2175Å UV Bump at z>7: Evidence for Rapid Dust\n  Evolution in a Merging Reionisation-Era Galaxy"
                },
                "summary": "Dust is a fundamental component of the interstellar medium within galaxies,\nas dust grains are highly efficient absorbers of ultraviolet (UV) and optical\nphotons. Accurately quantifying this obscuration is crucial for interpreting\ngalaxy spectral energy distributions (SEDs). The extinction curves in the Milky\nWay (MW) and Large Magellanic Cloud exhibit a strong feature known as the 2175\n{\\AA} UV bump, most often attributed to small carbonaceous dust grains. This\nfeature was recently detected in faint galaxies out to z=7.55, suggesting rapid\nformation channels. Here, we report the detection of a strong UV bump in a\nluminous Lyman-break galaxy at z_prism=7.11235, GNWY-7379420231, through\nobservations taken as part of the NIRSpec Wide GTO survey. We fit a dust\nattenuation curve that is consistent with the MW extinction curve within\n1{\\sigma}, in a galaxy just ~700 Myr after the Big Bang. From the integrated\nspectrum, we infer a young mass-weighted age (t~22-59 Myr) for this galaxy,\nhowever spatially resolved SED fitting unveils the presence of an older stellar\npopulation (t~252 Myr). Furthermore, morphological analysis provides evidence\nfor a potential merger. The underlying older stellar population suggests the\nmerging system could be pre-enriched, with the dust illuminated by a\nmerger-induced starburst. Moreover, turbulence driven by stellar feedback in\nthis bursty region may be driving polycyclic aromatic hydrocarbon formation\nthrough top-down shattering. The presence of a UV bump in GNWY-7379420231\nsolidifies growing evidence for the rapid evolution of dust properties within\nthe first billion years of cosmic time",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dust is a fundamental component of the interstellar medium within galaxies,\nas dust grains are highly efficient absorbers of ultraviolet (UV) and optical\nphotons. Accurately quantifying this obscuration is crucial for interpreting\ngalaxy spectral energy distributions (SEDs). The extinction curves in the Milky\nWay (MW) and Large Magellanic Cloud exhibit a strong feature known as the 2175\n{\\AA} UV bump, most often attributed to small carbonaceous dust grains. This\nfeature was recently detected in faint galaxies out to z=7.55, suggesting rapid\nformation channels. Here, we report the detection of a strong UV bump in a\nluminous Lyman-break galaxy at z_prism=7.11235, GNWY-7379420231, through\nobservations taken as part of the NIRSpec Wide GTO survey. We fit a dust\nattenuation curve that is consistent with the MW extinction curve within\n1{\\sigma}, in a galaxy just ~700 Myr after the Big Bang. From the integrated\nspectrum, we infer a young mass-weighted age (t~22-59 Myr) for this galaxy,\nhowever spatially resolved SED fitting unveils the presence of an older stellar\npopulation (t~252 Myr). Furthermore, morphological analysis provides evidence\nfor a potential merger. The underlying older stellar population suggests the\nmerging system could be pre-enriched, with the dust illuminated by a\nmerger-induced starburst. Moreover, turbulence driven by stellar feedback in\nthis bursty region may be driving polycyclic aromatic hydrocarbon formation\nthrough top-down shattering. The presence of a UV bump in GNWY-7379420231\nsolidifies growing evidence for the rapid evolution of dust properties within\nthe first billion years of cosmic time"
                },
                "authors": [
                    {
                        "name": "Katherine Ormerod"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Marijn Franx"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Pierluigi Rinaldi"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Tacchella"
                },
                "author": "Sandro Tacchella",
                "arxiv_doi": "10.1093/mnras/staf1228",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1228",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in MNRAS",
                "arxiv_journal_ref": "Mon Not R Astron Soc (2025) 1136-1154",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09360v1",
                "updated": "2025-09-11T11:18:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    18,
                    23,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T11:18:23Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    18,
                    23,
                    3,
                    254,
                    0
                ],
                "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments."
                },
                "authors": [
                    {
                        "name": "Channdeth Sok"
                    },
                    {
                        "name": "David Luz"
                    },
                    {
                        "name": "Yacine Haddam"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Haddam"
                },
                "author": "Yacine Haddam",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09562v3",
                "updated": "2025-09-11T11:17:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    17,
                    47,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-13T13:26:01Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    13,
                    26,
                    1,
                    6,
                    103,
                    0
                ],
                "title": "From Geometry to Observation: Gravitational Waves and the Raychaudhuri\n  Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Geometry to Observation: Gravitational Waves and the Raychaudhuri\n  Equation"
                },
                "summary": "Gravitational waves (GWs) are independent of any particular theory of\ngravity. The universality of this notion is highlighted by the Raychaudhuri\nequation (RE), which is independent of any theory of gravity and contains the\nRicci tensor $R_{\\mu\\nu}$ as a key ingredient, thereby connecting spacetime\ngeometry with matter-energy content. Under small metric perturbations,\n$R_{\\mu\\nu} \\propto \\Box h_{\\mu\\nu}$, where $h_{\\mu\\nu}$ is the perturbation,\nindicating that various gravity theories, via their corresponding $R_{\\mu\\nu}$,\nproduce different gravitational wave equations. In the framework of Einstein's\ngravity, this leads to the standard wave equation. This study analyzes a\nmodified form, {\\it GW-inspired RE}, within the homogeneous and isotropic FLRW\nbackground to investigate late-time cosmic acceleration and structure\nformation. We employ {\\it Pantheon+ SNe Ia, Hubble, and BAO} datasets to\nconstrain model parameters through Bayesian inference utilizing NUTS in {\\it\nNumPyro}. A nuisance parameter $\\mu_0$ is introduced to address residual\nsystematics. This facilitates a robust estimation of $H_0$, $\\Omega_{DE,0}$,\nand $r_d$, which addresses the resolution of the Hubble tension. We analyze the\nredshift evolution of the deceleration parameter, $q(z)$, both with and without\n$\\mu_0$, emphasizing its influence on cosmic dynamics. The GW-inspired RE is\nreformulated as a harmonic oscillator, providing insight into expansion and\ngeodesic focusing. A graphical comparison demonstrates the relationship\n$d^{GW}_L(z) = d^{EM}_L(z)$ utilizing GWOSC data. Thus, the RE in the context\nof small perturbation of the metric opens up whole new vistas of {\\it\nobservational astronomy.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) are independent of any particular theory of\ngravity. The universality of this notion is highlighted by the Raychaudhuri\nequation (RE), which is independent of any theory of gravity and contains the\nRicci tensor $R_{\\mu\\nu}$ as a key ingredient, thereby connecting spacetime\ngeometry with matter-energy content. Under small metric perturbations,\n$R_{\\mu\\nu} \\propto \\Box h_{\\mu\\nu}$, where $h_{\\mu\\nu}$ is the perturbation,\nindicating that various gravity theories, via their corresponding $R_{\\mu\\nu}$,\nproduce different gravitational wave equations. In the framework of Einstein's\ngravity, this leads to the standard wave equation. This study analyzes a\nmodified form, {\\it GW-inspired RE}, within the homogeneous and isotropic FLRW\nbackground to investigate late-time cosmic acceleration and structure\nformation. We employ {\\it Pantheon+ SNe Ia, Hubble, and BAO} datasets to\nconstrain model parameters through Bayesian inference utilizing NUTS in {\\it\nNumPyro}. A nuisance parameter $\\mu_0$ is introduced to address residual\nsystematics. This facilitates a robust estimation of $H_0$, $\\Omega_{DE,0}$,\nand $r_d$, which addresses the resolution of the Hubble tension. We analyze the\nredshift evolution of the deceleration parameter, $q(z)$, both with and without\n$\\mu_0$, emphasizing its influence on cosmic dynamics. The GW-inspired RE is\nreformulated as a harmonic oscillator, providing insight into expansion and\ngeodesic focusing. A graphical comparison demonstrates the relationship\n$d^{GW}_L(z) = d^{EM}_L(z)$ utilizing GWOSC data. Thus, the RE in the context\nof small perturbation of the metric opens up whole new vistas of {\\it\nobservational astronomy.}"
                },
                "authors": [
                    {
                        "name": "Sougata Bhunia"
                    },
                    {
                        "name": "Anubhab Dutta"
                    },
                    {
                        "name": "Debashis Gangopadhyay"
                    },
                    {
                        "name": "Goutam Manna"
                    }
                ],
                "author_detail": {
                    "name": "Goutam Manna"
                },
                "author": "Goutam Manna",
                "arxiv_comment": "19 pages, 10 figures, 6 tables, Accepted from Physics of the Dark\n  Universe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08538v2",
                "updated": "2025-09-11T11:14:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    14,
                    0,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T12:34:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    34,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models"
                },
                "summary": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos."
                },
                "authors": [
                    {
                        "name": "Garry Yang"
                    },
                    {
                        "name": "Zizhe Chen"
                    },
                    {
                        "name": "Man Hon Wong"
                    },
                    {
                        "name": "Haoyu Lei"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09352v1",
                "updated": "2025-09-11T11:07:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    7,
                    25,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T11:07:25Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    7,
                    25,
                    3,
                    254,
                    0
                ],
                "title": "Texture-aware Intrinsic Image Decomposition with Model- and\n  Learning-based Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Texture-aware Intrinsic Image Decomposition with Model- and\n  Learning-based Priors"
                },
                "summary": "This paper aims to recover the intrinsic reflectance layer and shading layer\ngiven a single image. Though this intrinsic image decomposition problem has\nbeen studied for decades, it remains a significant challenge in cases of\ncomplex scenes, i.e. spatially-varying lighting effect and rich textures. In\nthis paper, we propose a novel method for handling severe lighting and rich\ntextures in intrinsic image decomposition, which enables to produce\nhigh-quality intrinsic images for real-world images. Specifically, we observe\nthat previous learning-based methods tend to produce texture-less and\nover-smoothing intrinsic images, which can be used to infer the lighting and\ntexture information given a RGB image. In this way, we design a texture-guided\nregularization term and formulate the decomposition problem into an\noptimization framework, to separate the material textures and lighting effect.\nWe demonstrate that combining the novel texture-aware prior can produce\nsuperior results to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to recover the intrinsic reflectance layer and shading layer\ngiven a single image. Though this intrinsic image decomposition problem has\nbeen studied for decades, it remains a significant challenge in cases of\ncomplex scenes, i.e. spatially-varying lighting effect and rich textures. In\nthis paper, we propose a novel method for handling severe lighting and rich\ntextures in intrinsic image decomposition, which enables to produce\nhigh-quality intrinsic images for real-world images. Specifically, we observe\nthat previous learning-based methods tend to produce texture-less and\nover-smoothing intrinsic images, which can be used to infer the lighting and\ntexture information given a RGB image. In this way, we design a texture-guided\nregularization term and formulate the decomposition problem into an\noptimization framework, to separate the material textures and lighting effect.\nWe demonstrate that combining the novel texture-aware prior can produce\nsuperior results to existing approaches."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Zijun He"
                    },
                    {
                        "name": "Xin Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yuan"
                },
                "author": "Xin Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09341v1",
                "updated": "2025-09-11T10:54:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    54,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:54:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    54,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "Detection of colour variations from gravitational microlensing\n  observations in the quadruple quasar HE0435-1223: Implications for the\n  accretion disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of colour variations from gravitational microlensing\n  observations in the quadruple quasar HE0435-1223: Implications for the\n  accretion disk"
                },
                "summary": "We present monitoring observations of quasar microlensing in the quadruple\nquasar HE0435-1223. The microlensing-induced light curves of the quasar images\nare chromatic, i.e. they depend on the applied filter band. Comparison with\nmicrolensing simulations allows us to infer properties of the accretion disk.\nWe determine the R and V band light curves of the four images of HE0435-1223\nfrom 79 and 80 epochs respectively, taken from 2014 to 2024 at the Las Cumbres\nObservatory using difference imaging analysis. We consider difference light\ncurves to remove the intrinsic quasar variability. This reveals a prominent\nlong-term chromatic microlensing event in image B. We use microlensing light\ncurve simulations with both Gaussian and standard thin accretion disk\nbrightness profiles to analyse this signal. The particularly strong signal\nobserved in image B of HE0435-1223 makes it possible to detect a size ratio of\nthe accretion disk in the R band compared to the V band of\n$1.24^{+0.08}_{-0.20}$ and $1.42^{+0.11}_{-0.22}$ for the Gaussian and the thin\ndisk model, respectively. These values are in agreement with standard thin disk\ntheory. For the absolute size we find large disk half-light radii of around 0.7\nto 1.0 Einstein radii with an uncertainty of about 0.6 dex (depending on the\nfilter bands and the models). Finally, our calculations show that image B\nundergoes caustic crossings about once per year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present monitoring observations of quasar microlensing in the quadruple\nquasar HE0435-1223. The microlensing-induced light curves of the quasar images\nare chromatic, i.e. they depend on the applied filter band. Comparison with\nmicrolensing simulations allows us to infer properties of the accretion disk.\nWe determine the R and V band light curves of the four images of HE0435-1223\nfrom 79 and 80 epochs respectively, taken from 2014 to 2024 at the Las Cumbres\nObservatory using difference imaging analysis. We consider difference light\ncurves to remove the intrinsic quasar variability. This reveals a prominent\nlong-term chromatic microlensing event in image B. We use microlensing light\ncurve simulations with both Gaussian and standard thin accretion disk\nbrightness profiles to analyse this signal. The particularly strong signal\nobserved in image B of HE0435-1223 makes it possible to detect a size ratio of\nthe accretion disk in the R band compared to the V band of\n$1.24^{+0.08}_{-0.20}$ and $1.42^{+0.11}_{-0.22}$ for the Gaussian and the thin\ndisk model, respectively. These values are in agreement with standard thin disk\ntheory. For the absolute size we find large disk half-light radii of around 0.7\nto 1.0 Einstein radii with an uncertainty of about 0.6 dex (depending on the\nfilter bands and the models). Finally, our calculations show that image B\nundergoes caustic crossings about once per year."
                },
                "authors": [
                    {
                        "name": "Christian Sorgenfrei"
                    },
                    {
                        "name": "Robert W. Schmidt"
                    },
                    {
                        "name": "Joachim Wambsganss"
                    }
                ],
                "author_detail": {
                    "name": "Joachim Wambsganss"
                },
                "author": "Joachim Wambsganss",
                "arxiv_comment": "Accepted for publication in A&A, 9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12932v2",
                "updated": "2025-09-11T10:20:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    11,
                    3,
                    254,
                    0
                ],
                "published": "2025-02-18T15:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    14,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Culturally-Nuanced Story Generation for Reasoning in Low-Resource\n  Languages: The Case of Javanese and Sundanese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culturally-Nuanced Story Generation for Reasoning in Low-Resource\n  Languages: The Case of Javanese and Sundanese"
                },
                "summary": "Culturally grounded commonsense reasoning is underexplored in low-resource\nlanguages due to scarce data and costly native annotation. We test whether\nlarge language models (LLMs) can generate culturally nuanced narratives for\nsuch settings. Focusing on Javanese and Sundanese, we compare three data\ncreation strategies: (1) LLM-assisted stories prompted with cultural cues, (2)\nmachine translation from Indonesian benchmarks, and (3) native-written stories.\nHuman evaluation finds LLM stories match natives on cultural fidelity but lag\nin coherence and correctness. We fine-tune models on each dataset and evaluate\non a human-authored test set for classification and generation. LLM-generated\ndata yields higher downstream performance than machine-translated and\nIndonesian human-authored training data. We release a high-quality benchmark of\nculturally grounded commonsense stories in Javanese and Sundanese to support\nfuture work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culturally grounded commonsense reasoning is underexplored in low-resource\nlanguages due to scarce data and costly native annotation. We test whether\nlarge language models (LLMs) can generate culturally nuanced narratives for\nsuch settings. Focusing on Javanese and Sundanese, we compare three data\ncreation strategies: (1) LLM-assisted stories prompted with cultural cues, (2)\nmachine translation from Indonesian benchmarks, and (3) native-written stories.\nHuman evaluation finds LLM stories match natives on cultural fidelity but lag\nin coherence and correctness. We fine-tune models on each dataset and evaluate\non a human-authored test set for classification and generation. LLM-generated\ndata yields higher downstream performance than machine-translated and\nIndonesian human-authored training data. We release a high-quality benchmark of\nculturally grounded commonsense stories in Javanese and Sundanese to support\nfuture work."
                },
                "authors": [
                    {
                        "name": "Salsabila Zahirah Pranida"
                    },
                    {
                        "name": "Rifo Ahmad Genadi"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08302v2",
                "updated": "2025-09-11T10:17:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    17,
                    6,
                    3,
                    254,
                    0
                ],
                "published": "2024-11-13T02:45:21Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    2,
                    45,
                    21,
                    2,
                    318,
                    0
                ],
                "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward\n  Redistribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward\n  Redistribution"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) offers a promising approach\nto aligning large language models (LLMs) with human preferences. Typically, a\nreward model is trained or supplied to act as a proxy for humans in evaluating\ngenerated responses during the reinforcement training phase. However, current\nreward models operate as sequence-to-one models, allocating a single, sparse,\nand delayed reward to an entire output sequence. This approach may overlook the\nsignificant contributions of individual tokens toward the desired outcome. To\nthis end, we propose a more fine-grained, token-level guidance approach for RL\ntraining. Specifically, we introduce RED, a novel reward redistribition method\nthat evaluates and assigns specific credit to each token using an off-the-shelf\nreward model. Utilizing these fine-grained rewards enhances the model's\nunderstanding of language nuances, leading to more precise performance\nimprovements. Notably, our method does not require modifying the reward model\nor introducing additional training steps, thereby incurring minimal\ncomputational costs. Experimental results across diverse datasets and tasks\ndemonstrate the superiority of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) offers a promising approach\nto aligning large language models (LLMs) with human preferences. Typically, a\nreward model is trained or supplied to act as a proxy for humans in evaluating\ngenerated responses during the reinforcement training phase. However, current\nreward models operate as sequence-to-one models, allocating a single, sparse,\nand delayed reward to an entire output sequence. This approach may overlook the\nsignificant contributions of individual tokens toward the desired outcome. To\nthis end, we propose a more fine-grained, token-level guidance approach for RL\ntraining. Specifically, we introduce RED, a novel reward redistribition method\nthat evaluates and assigns specific credit to each token using an off-the-shelf\nreward model. Utilizing these fine-grained rewards enhances the model's\nunderstanding of language nuances, leading to more precise performance\nimprovements. Notably, our method does not require modifying the reward model\nor introducing additional training steps, thereby incurring minimal\ncomputational costs. Experimental results across diverse datasets and tasks\ndemonstrate the superiority of our approach."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Tai-wei Chang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09321v1",
                "updated": "2025-09-11T10:10:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    10,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:10:48Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    10,
                    48,
                    3,
                    254,
                    0
                ],
                "title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain\n  Expansion, and Metric Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain\n  Expansion, and Metric Optimization"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled the emergence of\ngeneral-purpose agents for automating end-to-end machine learning (ML)\nworkflows, including data analysis, feature engineering, model training, and\ncompetition solving. However, existing benchmarks remain limited in task\ncoverage, domain diversity, difficulty modeling, and evaluation rigor, failing\nto capture the full capabilities of such agents in realistic settings. We\npresent TAM Bench, a diverse, realistic, and structured benchmark for\nevaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three\nkey innovations: (1) A browser automation and LLM-based task acquisition system\nthat automatically collects and structures ML challenges from platforms such as\nKaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities\n(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty\nmodeling mechanism that estimates task complexity using participant counts and\nscore dispersion, enabling scalable and objective task calibration; (3) A\nmulti-dimensional evaluation framework incorporating performance, format\ncompliance, constraint adherence, and task generalization. Based on 150 curated\nAutoML tasks, we construct three benchmark subsets of different sizes -- Lite,\nMedium, and Full -- designed for varying evaluation scenarios. The Lite\nversion, with 18 tasks and balanced coverage across modalities and difficulty\nlevels, serves as a practical testbed for daily benchmarking and comparative\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled the emergence of\ngeneral-purpose agents for automating end-to-end machine learning (ML)\nworkflows, including data analysis, feature engineering, model training, and\ncompetition solving. However, existing benchmarks remain limited in task\ncoverage, domain diversity, difficulty modeling, and evaluation rigor, failing\nto capture the full capabilities of such agents in realistic settings. We\npresent TAM Bench, a diverse, realistic, and structured benchmark for\nevaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three\nkey innovations: (1) A browser automation and LLM-based task acquisition system\nthat automatically collects and structures ML challenges from platforms such as\nKaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities\n(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty\nmodeling mechanism that estimates task complexity using participant counts and\nscore dispersion, enabling scalable and objective task calibration; (3) A\nmulti-dimensional evaluation framework incorporating performance, format\ncompliance, constraint adherence, and task generalization. Based on 150 curated\nAutoML tasks, we construct three benchmark subsets of different sizes -- Lite,\nMedium, and Full -- designed for varying evaluation scenarios. The Lite\nversion, with 18 tasks and balanced coverage across modalities and difficulty\nlevels, serves as a practical testbed for daily benchmarking and comparative\nstudies."
                },
                "authors": [
                    {
                        "name": "Hangyi Jia"
                    },
                    {
                        "name": "Yuxi Qian"
                    },
                    {
                        "name": "Hanwen Tong"
                    },
                    {
                        "name": "Xinhui Wu"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Feng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wei"
                },
                "author": "Feng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09318v1",
                "updated": "2025-09-11T10:02:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    2,
                    11,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:02:11Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    2,
                    11,
                    3,
                    254,
                    0
                ],
                "title": "Efficient Transformer-Based Piano Transcription With Sparse Attention\n  Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Transformer-Based Piano Transcription With Sparse Attention\n  Mechanisms"
                },
                "summary": "This paper investigates automatic piano transcription based on\ncomputationally-efficient yet high-performant variants of the Transformer that\ncan capture longer-term dependency over the whole musical piece. Recently,\ntransformer-based sequence-to-sequence models have demonstrated excellent\nperformance in piano transcription. These models, however, fail to deal with\nthe whole piece at once due to the quadratic complexity of the self-attention\nmechanism, and music signals are thus typically processed in a sliding-window\nmanner in practice. To overcome this limitation, we propose an efficient\narchitecture with sparse attention mechanisms. Specifically, we introduce\nsliding-window self-attention mechanisms for both the encoder and decoder, and\na hybrid global-local cross-attention mechanism that attends to various spans\naccording to the MIDI token types. We also use a hierarchical pooling strategy\nbetween the encoder and decoder to further reduce computational load. Our\nexperiments on the MAESTRO dataset showed that the proposed model achieved a\nsignificant reduction in computational cost and memory usage, accelerating\ninference speed, while maintaining transcription performance comparable to the\nfull-attention baseline. This allows for training with longer audio contexts on\nthe same hardware, demonstrating the viability of sparse attention for building\nefficient and high-performance piano transcription systems. The code is\navailable at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates automatic piano transcription based on\ncomputationally-efficient yet high-performant variants of the Transformer that\ncan capture longer-term dependency over the whole musical piece. Recently,\ntransformer-based sequence-to-sequence models have demonstrated excellent\nperformance in piano transcription. These models, however, fail to deal with\nthe whole piece at once due to the quadratic complexity of the self-attention\nmechanism, and music signals are thus typically processed in a sliding-window\nmanner in practice. To overcome this limitation, we propose an efficient\narchitecture with sparse attention mechanisms. Specifically, we introduce\nsliding-window self-attention mechanisms for both the encoder and decoder, and\na hybrid global-local cross-attention mechanism that attends to various spans\naccording to the MIDI token types. We also use a hierarchical pooling strategy\nbetween the encoder and decoder to further reduce computational load. Our\nexperiments on the MAESTRO dataset showed that the proposed model achieved a\nsignificant reduction in computational cost and memory usage, accelerating\ninference speed, while maintaining transcription performance comparable to the\nfull-attention baseline. This allows for training with longer audio contexts on\nthe same hardware, demonstrating the viability of sparse attention for building\nefficient and high-performance piano transcription systems. The code is\navailable at https://github.com/WX-Wei/efficient-seq2seq-piano-trans."
                },
                "authors": [
                    {
                        "name": "Weixing Wei"
                    },
                    {
                        "name": "Kazuyoshi Yoshii"
                    }
                ],
                "author_detail": {
                    "name": "Kazuyoshi Yoshii"
                },
                "author": "Kazuyoshi Yoshii",
                "arxiv_comment": "Accepted by APSIPA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09314v1",
                "updated": "2025-09-11T10:00:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    0,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:00:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    0,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective\n  Intelligence and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective\n  Intelligence and Performance"
                },
                "summary": "Coordinated teamwork is essential in fast-paced decision-making environments\nthat require dynamic adaptation, often without an opportunity for explicit\ncommunication. Although implicit coordination has been extensively considered\nin the existing literature, the majority of work has focused on co-located,\nsynchronous teamwork (such as sports teams) or, in distributed teams, primarily\non coordination of knowledge work. However, many teams (firefighters, military,\nlaw enforcement, emergency response) must coordinate their movements in\nphysical space without the benefit of visual cues or extensive explicit\ncommunication. This paper investigates how three dimensions of spatial\ncoordination, namely exploration diversity, movement specialization, and\nadaptive spatial proximity, influence team performance in a collaborative\nonline search and rescue task where explicit communication is restricted and\nteam members rely on movement patterns to infer others' intentions and\ncoordinate actions. Our metrics capture the relational aspects of teamwork by\nmeasuring spatial proximity, distribution patterns, and alignment of movements\nwithin shared environments. We analyze data from 34 four-person teams (136\nparticipants) assigned to specialized roles in a search and rescue task.\nResults show that spatial specialization positively predicts performance, while\nadaptive spatial proximity exhibits a marginal inverted U-shaped relationship,\nsuggesting moderate levels of adaptation are optimal. Furthermore, the temporal\ndynamics of these metrics differentiate high- from low-performing teams over\ntime. These findings provide insights into implicit spatial coordination in\nrole-based teamwork and highlight the importance of balanced adaptive\nstrategies, with implications for training and AI-assisted team support\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinated teamwork is essential in fast-paced decision-making environments\nthat require dynamic adaptation, often without an opportunity for explicit\ncommunication. Although implicit coordination has been extensively considered\nin the existing literature, the majority of work has focused on co-located,\nsynchronous teamwork (such as sports teams) or, in distributed teams, primarily\non coordination of knowledge work. However, many teams (firefighters, military,\nlaw enforcement, emergency response) must coordinate their movements in\nphysical space without the benefit of visual cues or extensive explicit\ncommunication. This paper investigates how three dimensions of spatial\ncoordination, namely exploration diversity, movement specialization, and\nadaptive spatial proximity, influence team performance in a collaborative\nonline search and rescue task where explicit communication is restricted and\nteam members rely on movement patterns to infer others' intentions and\ncoordinate actions. Our metrics capture the relational aspects of teamwork by\nmeasuring spatial proximity, distribution patterns, and alignment of movements\nwithin shared environments. We analyze data from 34 four-person teams (136\nparticipants) assigned to specialized roles in a search and rescue task.\nResults show that spatial specialization positively predicts performance, while\nadaptive spatial proximity exhibits a marginal inverted U-shaped relationship,\nsuggesting moderate levels of adaptation are optimal. Furthermore, the temporal\ndynamics of these metrics differentiate high- from low-performing teams over\ntime. These findings provide insights into implicit spatial coordination in\nrole-based teamwork and highlight the importance of balanced adaptive\nstrategies, with implications for training and AI-assisted team support\nsystems."
                },
                "authors": [
                    {
                        "name": "Thuy Ngoc Nguyen"
                    },
                    {
                        "name": "Anita Williams Woolley"
                    },
                    {
                        "name": "Cleotilde Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Cleotilde Gonzalez"
                },
                "author": "Cleotilde Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09311v1",
                "updated": "2025-09-11T09:54:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    54,
                    25,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:54:25Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    54,
                    25,
                    3,
                    254,
                    0
                ],
                "title": "Image Recognition with Vision and Language Embeddings of VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Recognition with Vision and Language Embeddings of VLMs"
                },
                "summary": "Vision-language models (VLMs) have enabled strong zero-shot classification\nthrough image-text alignment. Yet, their purely visual inference capabilities\nremain under-explored. In this work, we conduct a comprehensive evaluation of\nboth language-guided and vision-only image classification with a diverse set of\ndual-encoder VLMs, including both well-established and recent models such as\nSigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the\nImageNet-1k validation set and its label-corrected variant. The key factors\naffecting accuracy are analysed, including prompt design, class diversity, the\nnumber of neighbours in k-NN, and reference set size. We show that language and\nvision offer complementary strengths, with some classes favouring textual\nprompts and others better handled by visual similarity. To exploit this\ncomplementarity, we introduce a simple, learning-free fusion method based on\nper-class precision that improves classification performance. The code is\navailable at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have enabled strong zero-shot classification\nthrough image-text alignment. Yet, their purely visual inference capabilities\nremain under-explored. In this work, we conduct a comprehensive evaluation of\nboth language-guided and vision-only image classification with a diverse set of\ndual-encoder VLMs, including both well-established and recent models such as\nSigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the\nImageNet-1k validation set and its label-corrected variant. The key factors\naffecting accuracy are analysed, including prompt design, class diversity, the\nnumber of neighbours in k-NN, and reference set size. We show that language and\nvision offer complementary strengths, with some classes favouring textual\nprompts and others better handled by visual similarity. To exploit this\ncomplementarity, we introduce a simple, learning-free fusion method based on\nper-class precision that improves classification performance. The code is\navailable at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition."
                },
                "authors": [
                    {
                        "name": "Illia Volkov"
                    },
                    {
                        "name": "Nikita Kisel"
                    },
                    {
                        "name": "Klara Janouskova"
                    },
                    {
                        "name": "Jiri Matas"
                    }
                ],
                "author_detail": {
                    "name": "Jiri Matas"
                },
                "author": "Jiri Matas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09310v1",
                "updated": "2025-09-11T09:53:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    53,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:53:20Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    53,
                    20,
                    3,
                    254,
                    0
                ],
                "title": "You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative\n  Perception"
                },
                "summary": "Collaborative perception enables vehicles to overcome individual perception\nlimitations by sharing information, allowing them to see further and through\nocclusions. In real-world scenarios, models on different vehicles are often\nheterogeneous due to manufacturer variations. Existing methods for\nheterogeneous collaborative perception address this challenge by fine-tuning\nadapters or the entire network to bridge the domain gap. However, these methods\nare impractical in real-world applications, as each new collaborator must\nundergo joint training with the ego vehicle on a dataset before inference, or\nthe ego vehicle stores models for all potential collaborators in advance.\nTherefore, we pose a new question: Can we tackle this challenge directly during\ninference, eliminating the need for joint training? To answer this, we\nintroduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel\nframework that formulates the problem as few-shot unsupervised domain\nadaptation. Unlike previous work, PHCP dynamically aligns features by\nself-training an adapter during inference, eliminating the need for labeled\ndata and joint training. Extensive experiments on the OPV2V dataset demonstrate\nthat PHCP achieves strong performance across diverse heterogeneous scenarios.\nNotably, PHCP achieves performance comparable to SOTA methods trained on the\nentire dataset while using only a small amount of unlabeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative perception enables vehicles to overcome individual perception\nlimitations by sharing information, allowing them to see further and through\nocclusions. In real-world scenarios, models on different vehicles are often\nheterogeneous due to manufacturer variations. Existing methods for\nheterogeneous collaborative perception address this challenge by fine-tuning\nadapters or the entire network to bridge the domain gap. However, these methods\nare impractical in real-world applications, as each new collaborator must\nundergo joint training with the ego vehicle on a dataset before inference, or\nthe ego vehicle stores models for all potential collaborators in advance.\nTherefore, we pose a new question: Can we tackle this challenge directly during\ninference, eliminating the need for joint training? To answer this, we\nintroduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel\nframework that formulates the problem as few-shot unsupervised domain\nadaptation. Unlike previous work, PHCP dynamically aligns features by\nself-training an adapter during inference, eliminating the need for labeled\ndata and joint training. Extensive experiments on the OPV2V dataset demonstrate\nthat PHCP achieves strong performance across diverse heterogeneous scenarios.\nNotably, PHCP achieves performance comparable to SOTA methods trained on the\nentire dataset while using only a small amount of unlabeled data."
                },
                "authors": [
                    {
                        "name": "Hao Si"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    }
                ],
                "author_detail": {
                    "name": "Manabu Tsukada"
                },
                "author": "Manabu Tsukada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09307v1",
                "updated": "2025-09-11T09:50:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    50,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:50:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    50,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization"
                },
                "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha."
                },
                "authors": [
                    {
                        "name": "Zhengzhao Lai"
                    },
                    {
                        "name": "Youbin Zheng"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Haonan Lyu"
                    },
                    {
                        "name": "Jinpu Yang"
                    },
                    {
                        "name": "Hongqing Liang"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09303v1",
                "updated": "2025-09-11T09:44:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    44,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:44:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    44,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "From scratch to silver: Creating trustworthy training data for\n  patent-SDG classification using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From scratch to silver: Creating trustworthy training data for\n  patent-SDG classification using Large Language Models"
                },
                "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale."
                },
                "authors": [
                    {
                        "name": "Grazia Sveva Ascione"
                    },
                    {
                        "name": "Nicolò Tamagnone"
                    }
                ],
                "author_detail": {
                    "name": "Nicolò Tamagnone"
                },
                "author": "Nicolò Tamagnone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07370v2",
                "updated": "2025-09-11T09:42:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    42,
                    2,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-09T03:39:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    3,
                    39,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing\n  Human-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing\n  Human-LLM Interactions"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that PersonaFuse\noffers a theoretically grounded and practical approach for developing\nsocial-emotional enhanced LLMs, marking a significant advancement toward more\nhuman-centric AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that PersonaFuse\noffers a theoretically grounded and practical approach for developing\nsocial-emotional enhanced LLMs, marking a significant advancement toward more\nhuman-centric AI systems."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Ahmed Abbasi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Abbasi"
                },
                "author": "Ahmed Abbasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v3",
                "updated": "2025-09-11T09:37:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    37,
                    46,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09292v1",
                "updated": "2025-09-11T09:29:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    29,
                    13,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:29:13Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    29,
                    13,
                    3,
                    254,
                    0
                ],
                "title": "LightAgent: Production-level Open-source Agentic AI Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightAgent: Production-level Open-source Agentic AI Framework"
                },
                "summary": "With the rapid advancement of large language models (LLMs), Multi-agent\nSystems (MAS) have achieved significant progress in various application\nscenarios. However, substantial challenges remain in designing versatile,\nrobust, and efficient platforms for agent deployment. To address these\nlimitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic\nframework, effectively resolving the trade-off between flexibility and\nsimplicity found in existing frameworks. LightAgent integrates core\nfunctionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while\nmaintaining an extremely lightweight structure. As a fully open-source\nsolution, it seamlessly integrates with mainstream chat platforms, enabling\ndevelopers to easily build self-learning agents. We have released LightAgent at\n\\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), Multi-agent\nSystems (MAS) have achieved significant progress in various application\nscenarios. However, substantial challenges remain in designing versatile,\nrobust, and efficient platforms for agent deployment. To address these\nlimitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic\nframework, effectively resolving the trade-off between flexibility and\nsimplicity found in existing frameworks. LightAgent integrates core\nfunctionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while\nmaintaining an extremely lightweight structure. As a fully open-source\nsolution, it seamlessly integrates with mainstream chat platforms, enabling\ndevelopers to easily build self-learning agents. We have released LightAgent at\n\\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}"
                },
                "authors": [
                    {
                        "name": "Weige Cai"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Jinyi Niu"
                    },
                    {
                        "name": "Ruiqi Hu"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Tenglong Wang"
                    },
                    {
                        "name": "Xiaowu Dai"
                    },
                    {
                        "name": "Weining Shen"
                    },
                    {
                        "name": "Liwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liwen Zhang"
                },
                "author": "Liwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09291v1",
                "updated": "2025-09-11T09:27:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    27,
                    37,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:27:37Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    27,
                    37,
                    3,
                    254,
                    0
                ],
                "title": "What You Code Is What We Prove: Translating BLE App Logic into Formal\n  Models with LLMs for Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You Code Is What We Prove: Translating BLE App Logic into Formal\n  Models with LLMs for Vulnerability Detection"
                },
                "summary": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains."
                },
                "authors": [
                    {
                        "name": "Biwei Yan"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Runyu Pan"
                    },
                    {
                        "name": "Jinku Li"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09290v1",
                "updated": "2025-09-11T09:25:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    25,
                    30,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:25:30Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    25,
                    30,
                    3,
                    254,
                    0
                ],
                "title": "Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in\n  Multimodal MRI with Sequences Unavailable During Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in\n  Multimodal MRI with Sequences Unavailable During Training"
                },
                "summary": "Segmentation models are important tools for the detection and analysis of\nlesions in brain MRI. Depending on the type of brain pathology that is imaged,\nMRI scanners can acquire multiple, different image modalities (contrasts). Most\nsegmentation models for multimodal brain MRI are restricted to fixed modalities\nand cannot effectively process new ones at inference. Some models generalize to\nunseen modalities but may lose discriminative modality-specific information.\nThis work aims to develop a model that can perform inference on data that\ncontain image modalities unseen during training, previously seen modalities,\nand heterogeneous combinations of both, thus allowing a user to utilize any\navailable imaging modalities. We demonstrate this is possible with a simple,\nthus practical alteration to the U-net architecture, by integrating a\nmodality-agnostic input channel or pathway, alongside modality-specific input\nchannels. To train this modality-agnostic component, we develop an image\naugmentation scheme that synthesizes artificial MRI modalities. Augmentations\ndifferentially alter the appearance of pathological and healthy brain tissue to\ncreate artificial contrasts between them while maintaining realistic anatomical\nintegrity. We evaluate the method using 8 MRI databases that include 5 types of\npathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and\nwhite matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,\nDWI, ADC and FLAIR). The results demonstrate that the approach preserves the\nability to effectively process MRI modalities encountered during training,\nwhile being able to process new, unseen modalities to improve its segmentation.\nProject code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation models are important tools for the detection and analysis of\nlesions in brain MRI. Depending on the type of brain pathology that is imaged,\nMRI scanners can acquire multiple, different image modalities (contrasts). Most\nsegmentation models for multimodal brain MRI are restricted to fixed modalities\nand cannot effectively process new ones at inference. Some models generalize to\nunseen modalities but may lose discriminative modality-specific information.\nThis work aims to develop a model that can perform inference on data that\ncontain image modalities unseen during training, previously seen modalities,\nand heterogeneous combinations of both, thus allowing a user to utilize any\navailable imaging modalities. We demonstrate this is possible with a simple,\nthus practical alteration to the U-net architecture, by integrating a\nmodality-agnostic input channel or pathway, alongside modality-specific input\nchannels. To train this modality-agnostic component, we develop an image\naugmentation scheme that synthesizes artificial MRI modalities. Augmentations\ndifferentially alter the appearance of pathological and healthy brain tissue to\ncreate artificial contrasts between them while maintaining realistic anatomical\nintegrity. We evaluate the method using 8 MRI databases that include 5 types of\npathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and\nwhite matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,\nDWI, ADC and FLAIR). The results demonstrate that the approach preserves the\nability to effectively process MRI modalities encountered during training,\nwhile being able to process new, unseen modalities to improve its segmentation.\nProject code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG"
                },
                "authors": [
                    {
                        "name": "Anthony P. Addison"
                    },
                    {
                        "name": "Felix Wagner"
                    },
                    {
                        "name": "Wentian Xu"
                    },
                    {
                        "name": "Natalie Voets"
                    },
                    {
                        "name": "Konstantinos Kamnitsas"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Kamnitsas"
                },
                "author": "Konstantinos Kamnitsas",
                "arxiv_comment": "Accepted to MICCAI 2025, for the following workshop: ML-CDS 2025:\n  Multimodal Learning and Fusion Across Scales for Clinical Decision Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09285v1",
                "updated": "2025-09-11T09:21:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    21,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:21:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    21,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "The Impact of Device Type, Data Practices, and Use Case Scenarios on\n  Privacy Concerns about Eye-tracked Augmented Reality in the United States and\n  Germany",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Device Type, Data Practices, and Use Case Scenarios on\n  Privacy Concerns about Eye-tracked Augmented Reality in the United States and\n  Germany"
                },
                "summary": "Augmented reality technology will likely be prevalent with more affordable\nhead-mounted displays. Integrating novel interaction modalities such as eye\ntrackers into head-mounted displays could lead to collecting vast amounts of\nbiometric data, which may allow inference of sensitive user attributes like\nhealth status or sexual preference, posing privacy issues. While previous works\nbroadly examined privacy concerns about augmented reality, ours is the first to\nextensively explore privacy concerns on behavioral data, particularly eye\ntracking in augmented reality. We crowdsourced four survey studies in the\nUnited States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand\nthe impact of user attributes, augmented reality devices, use cases, data\npractices, and country on privacy concerns. Our findings indicate that\nparticipants are generally concerned about privacy when they know what\ninferences can be made based on the collected data. Despite the more prominent\nuse of smartphones in daily life than augmented reality glasses, we found no\nindications of differing privacy concerns depending on the device type. In\naddition, our participants are more comfortable when a particular use case\nbenefits them and less comfortable when other humans can consume their data.\nFurthermore, participants in the United States are less concerned about their\nprivacy than those in Germany. Based on our findings, we provide several\nrecommendations to practitioners and policymakers for privacy-aware augmented\nreality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented reality technology will likely be prevalent with more affordable\nhead-mounted displays. Integrating novel interaction modalities such as eye\ntrackers into head-mounted displays could lead to collecting vast amounts of\nbiometric data, which may allow inference of sensitive user attributes like\nhealth status or sexual preference, posing privacy issues. While previous works\nbroadly examined privacy concerns about augmented reality, ours is the first to\nextensively explore privacy concerns on behavioral data, particularly eye\ntracking in augmented reality. We crowdsourced four survey studies in the\nUnited States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand\nthe impact of user attributes, augmented reality devices, use cases, data\npractices, and country on privacy concerns. Our findings indicate that\nparticipants are generally concerned about privacy when they know what\ninferences can be made based on the collected data. Despite the more prominent\nuse of smartphones in daily life than augmented reality glasses, we found no\nindications of differing privacy concerns depending on the device type. In\naddition, our participants are more comfortable when a particular use case\nbenefits them and less comfortable when other humans can consume their data.\nFurthermore, participants in the United States are less concerned about their\nprivacy than those in Germany. Based on our findings, we provide several\nrecommendations to practitioners and policymakers for privacy-aware augmented\nreality."
                },
                "authors": [
                    {
                        "name": "Efe Bozkir"
                    },
                    {
                        "name": "Babette Bühler"
                    },
                    {
                        "name": "Xiaoyuan Wu"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Lujo Bauer"
                    },
                    {
                        "name": "Lorrie Faith Cranor"
                    }
                ],
                "author_detail": {
                    "name": "Lorrie Faith Cranor"
                },
                "author": "Lorrie Faith Cranor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09284v1",
                "updated": "2025-09-11T09:18:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    18,
                    7,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:18:07Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    18,
                    7,
                    3,
                    254,
                    0
                ],
                "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning"
                },
                "summary": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures."
                },
                "authors": [
                    {
                        "name": "Bingning Huang"
                    },
                    {
                        "name": "Tu Nguyen"
                    },
                    {
                        "name": "Matthieu Zimmer"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Zimmer"
                },
                "author": "Matthieu Zimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09272v1",
                "updated": "2025-09-11T09:02:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    2,
                    15,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:02:15Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    2,
                    15,
                    3,
                    254,
                    0
                ],
                "title": "Fusing Knowledge and Language: A Comparative Study of Knowledge\n  Graph-Based Question Answering with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Knowledge and Language: A Comparative Study of Knowledge\n  Graph-Based Question Answering with LLMs"
                },
                "summary": "Knowledge graphs, a powerful tool for structuring information through\nrelational triplets, have recently become the new front-runner in enhancing\nquestion-answering systems. While traditional Retrieval Augmented Generation\n(RAG) approaches are proficient in fact-based and local context-based\nextraction from concise texts, they encounter limitations when addressing the\nthematic and holistic understanding of complex, extensive texts, requiring a\ndeeper analysis of both text and context. This paper presents a comprehensive\ntechnical comparative study of three different methodologies for constructing\nknowledge graph triplets and integrating them with Large Language Models (LLMs)\nfor question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all\nleveraging open source technologies. We evaluate the effectiveness,\nfeasibility, and adaptability of these methods by analyzing their capabilities,\nstate of development, and their impact on the performance of LLM-based question\nanswering. Experimental results indicate that while OpenIE provides the most\ncomprehensive coverage of triplets, GraphRAG demonstrates superior reasoning\nabilities among the three. We conclude with a discussion on the strengths and\nlimitations of each method and provide insights into future directions for\nimproving knowledge graph-based question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs, a powerful tool for structuring information through\nrelational triplets, have recently become the new front-runner in enhancing\nquestion-answering systems. While traditional Retrieval Augmented Generation\n(RAG) approaches are proficient in fact-based and local context-based\nextraction from concise texts, they encounter limitations when addressing the\nthematic and holistic understanding of complex, extensive texts, requiring a\ndeeper analysis of both text and context. This paper presents a comprehensive\ntechnical comparative study of three different methodologies for constructing\nknowledge graph triplets and integrating them with Large Language Models (LLMs)\nfor question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all\nleveraging open source technologies. We evaluate the effectiveness,\nfeasibility, and adaptability of these methods by analyzing their capabilities,\nstate of development, and their impact on the performance of LLM-based question\nanswering. Experimental results indicate that while OpenIE provides the most\ncomprehensive coverage of triplets, GraphRAG demonstrates superior reasoning\nabilities among the three. We conclude with a discussion on the strengths and\nlimitations of each method and provide insights into future directions for\nimproving knowledge graph-based question answering."
                },
                "authors": [
                    {
                        "name": "Vaibhav Chaudhary"
                    },
                    {
                        "name": "Neha Soni"
                    },
                    {
                        "name": "Narotam Singh"
                    },
                    {
                        "name": "Amita Kapoor"
                    }
                ],
                "author_detail": {
                    "name": "Amita Kapoor"
                },
                "author": "Amita Kapoor",
                "arxiv_comment": "46 pages, 4 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09265v1",
                "updated": "2025-09-11T08:50:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    50,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T08:50:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    50,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents"
                },
                "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/"
                },
                "authors": [
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Jiacai Liu"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Yingru Li"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Yu Yue"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Ke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Wang"
                },
                "author": "Ke Wang",
                "arxiv_comment": "ICLR 2026 Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17850v3",
                "updated": "2025-09-11T08:48:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    48,
                    37,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-25T09:57:35Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    57,
                    35,
                    0,
                    237,
                    0
                ],
                "title": "Group Expectation Policy Optimization for Heterogeneous Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Expectation Policy Optimization for Heterogeneous Reinforcement\n  Learning"
                },
                "summary": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Ruibin Zheng"
                    },
                    {
                        "name": "Zexuan Yi"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Hanyang Peng"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zike Yuan"
                    },
                    {
                        "name": "Cai Ke"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Jiangyue Yan"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Liwen Jing"
                    },
                    {
                        "name": "Jiayin Qi"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Binxing Fang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.09679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09679v1",
                "updated": "2025-09-11T17:59:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    51,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    51,
                    3,
                    254,
                    0
                ],
                "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms"
                },
                "summary": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot."
                },
                "authors": [
                    {
                        "name": "Bingxin Xu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "arxiv_comment": "Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09677v1",
                "updated": "2025-09-11T17:59:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    34,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:34Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    34,
                    3,
                    254,
                    0
                ],
                "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs"
                },
                "summary": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Akshit Sinha"
                    },
                    {
                        "name": "Arvindh Arun"
                    },
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09675v1",
                "updated": "2025-09-11T17:59:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    17,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:17Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    17,
                    3,
                    254,
                    0
                ],
                "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes."
                },
                "authors": [
                    {
                        "name": "Runpeng Dai"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Hongtu Zhu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09671v1",
                "updated": "2025-09-11T17:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    7,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    7,
                    3,
                    254,
                    0
                ],
                "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration"
                },
                "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation."
                },
                "authors": [
                    {
                        "name": "Sirui Xu"
                    },
                    {
                        "name": "Yu-Wei Chao"
                    },
                    {
                        "name": "Liuyu Bian"
                    },
                    {
                        "name": "Arsalan Mousavian"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Liang-Yan Gui"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "arxiv_comment": "CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09660v1",
                "updated": "2025-09-11T17:55:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    55,
                    9,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:55:09Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    55,
                    9,
                    3,
                    254,
                    0
                ],
                "title": "Steering MoE LLMs via Expert (De)Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering MoE LLMs via Expert (De)Activation"
                },
                "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts."
                },
                "authors": [
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Hinrich Schütze"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06602v2",
                "updated": "2025-09-11T17:52:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    52,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T12:15:53Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    15,
                    53,
                    0,
                    251,
                    0
                ],
                "title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards"
                },
                "summary": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs."
                },
                "authors": [
                    {
                        "name": "Matthias Blondeel"
                    },
                    {
                        "name": "Noel Codella"
                    },
                    {
                        "name": "Sam Preston"
                    },
                    {
                        "name": "Hao Qiu"
                    },
                    {
                        "name": "Leonardo Schettini"
                    },
                    {
                        "name": "Frank Tuan"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Smitha Saligrama"
                    },
                    {
                        "name": "Mert Öz"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Matthew P. Lungren"
                    },
                    {
                        "name": "Thomas Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Osborne"
                },
                "author": "Thomas Osborne",
                "arxiv_comment": "9 pages, 1 figure; Added missing co-authors and contributors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v3",
                "updated": "2025-09-11T17:49:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    49,
                    8,
                    3,
                    254,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09650v1",
                "updated": "2025-09-11T17:41:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    41,
                    29,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:41:29Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    41,
                    29,
                    3,
                    254,
                    0
                ],
                "title": "All for One: LLMs Solve Mental Math at the Last Token With Information\n  Transferred From Other Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All for One: LLMs Solve Mental Math at the Last Token With Information\n  Transferred From Other Tokens"
                },
                "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest."
                },
                "authors": [
                    {
                        "name": "Siddarth Mamidanna"
                    },
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Yilun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhou"
                },
                "author": "Yilun Zhou",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14032v2",
                "updated": "2025-09-11T17:25:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    25,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-18T16:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    16,
                    0,
                    11,
                    4,
                    199,
                    0
                ],
                "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models"
                },
                "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale."
                },
                "authors": [
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Erika Barcelos"
                    },
                    {
                        "name": "Roger French"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "arxiv_comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09629v1",
                "updated": "2025-09-11T17:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    15,
                    45,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T17:15:45Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    15,
                    45,
                    3,
                    254,
                    0
                ],
                "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems"
                },
                "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks."
                },
                "authors": [
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Shiguang Wu"
                    },
                    {
                        "name": "Lingjie Wang"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhumin Chen"
                },
                "author": "Zhumin Chen",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10576v2",
                "updated": "2025-09-11T17:11:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    11,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-11T09:42:23Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    42,
                    23,
                    4,
                    192,
                    0
                ],
                "title": "Can Large Language Models Understand As Well As Apply Patent Regulations\n  to Pass a Hands-On Patent Attorney Test?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand As Well As Apply Patent Regulations\n  to Pass a Hands-On Patent Attorney Test?"
                },
                "summary": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions."
                },
                "authors": [
                    {
                        "name": "Bhakti Khera"
                    },
                    {
                        "name": "Rezvan Alamian"
                    },
                    {
                        "name": "Pascal A. Scherz"
                    },
                    {
                        "name": "Stephan M. Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan M. Goetz"
                },
                "author": "Stephan M. Goetz",
                "arxiv_comment": "41 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00462v2",
                "updated": "2025-09-11T16:59:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    59,
                    36,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-30T11:40:11Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    11,
                    40,
                    11,
                    5,
                    242,
                    0
                ],
                "title": "AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and\n  Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and\n  Insights"
                },
                "summary": "As generative artificial intelligence (AI) tools become widely adopted, large\nlanguage models (LLMs) are increasingly involved on both sides of\ndecision-making processes, ranging from hiring to content moderation. This dual\nadoption raises a critical question: do LLMs systematically favor content that\nresembles their own outputs? Prior research in computer science has identified\nself-preference bias -- the tendency of LLMs to favor their own generated\ncontent -- but its real-world implications have not been empirically evaluated.\nWe focus on the hiring context, where job applicants often rely on LLMs to\nrefine resumes, while employers deploy them to screen those same resumes. Using\na large-scale controlled resume correspondence experiment, we find that LLMs\nconsistently prefer resumes generated by themselves over those written by\nhumans or produced by alternative models, even when content quality is\ncontrolled. The bias against human-written resumes is particularly substantial,\nwith self-preference bias ranging from 68% to 88% across major commercial and\nopen-source models. To assess labor market impact, we simulate realistic hiring\npipelines across 24 occupations. These simulations show that candidates using\nthe same LLM as the evaluator are 23% to 60% more likely to be shortlisted than\nequally qualified applicants submitting human-written resumes, with the largest\ndisadvantages observed in business-related fields such as sales and accounting.\nWe further demonstrate that this bias can be reduced by more than 50% through\nsimple interventions targeting LLMs' self-recognition capabilities. These\nfindings highlight an emerging but previously overlooked risk in AI-assisted\ndecision making and call for expanded frameworks of AI fairness that address\nnot only demographic-based disparities, but also biases in AI-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative artificial intelligence (AI) tools become widely adopted, large\nlanguage models (LLMs) are increasingly involved on both sides of\ndecision-making processes, ranging from hiring to content moderation. This dual\nadoption raises a critical question: do LLMs systematically favor content that\nresembles their own outputs? Prior research in computer science has identified\nself-preference bias -- the tendency of LLMs to favor their own generated\ncontent -- but its real-world implications have not been empirically evaluated.\nWe focus on the hiring context, where job applicants often rely on LLMs to\nrefine resumes, while employers deploy them to screen those same resumes. Using\na large-scale controlled resume correspondence experiment, we find that LLMs\nconsistently prefer resumes generated by themselves over those written by\nhumans or produced by alternative models, even when content quality is\ncontrolled. The bias against human-written resumes is particularly substantial,\nwith self-preference bias ranging from 68% to 88% across major commercial and\nopen-source models. To assess labor market impact, we simulate realistic hiring\npipelines across 24 occupations. These simulations show that candidates using\nthe same LLM as the evaluator are 23% to 60% more likely to be shortlisted than\nequally qualified applicants submitting human-written resumes, with the largest\ndisadvantages observed in business-related fields such as sales and accounting.\nWe further demonstrate that this bias can be reduced by more than 50% through\nsimple interventions targeting LLMs' self-recognition capabilities. These\nfindings highlight an emerging but previously overlooked risk in AI-assisted\ndecision making and call for expanded frameworks of AI fairness that address\nnot only demographic-based disparities, but also biases in AI-AI interactions."
                },
                "authors": [
                    {
                        "name": "Jiannan Xu"
                    },
                    {
                        "name": "Gujie Li"
                    },
                    {
                        "name": "Jane Yi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jane Yi Jiang"
                },
                "author": "Jane Yi Jiang",
                "arxiv_comment": "This paper has been accepted as a non-archival submission at EAAMO\n  2025 and AIES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09614v1",
                "updated": "2025-09-11T16:55:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    55,
                    4,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:55:04Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    55,
                    4,
                    3,
                    254,
                    0
                ],
                "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering"
                },
                "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench."
                },
                "authors": [
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "53 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20877v2",
                "updated": "2025-09-11T16:54:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    54,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-28T15:07:04Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    7,
                    4,
                    3,
                    240,
                    0
                ],
                "title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using\n  Multi-Modal Medical Imaging Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using\n  Multi-Modal Medical Imaging Analysis"
                },
                "summary": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms\nof cancer, with a five-year survival rate below 10% primarily due to late\ndetection. This research develops and validates a deep learning framework for\nearly PDAC detection through analysis of dual-modality imaging:\nautofluorescence and second harmonic generation (SHG). We analyzed 40 unique\npatient samples to create a specialized neural network capable of\ndistinguishing between normal, fibrotic, and cancerous tissue. Our methodology\nevaluated six distinct deep learning architectures, comparing traditional\nConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).\nThrough systematic experimentation, we identified and overcome significant\nchallenges in medical image analysis, including limited dataset size and class\nimbalance. The final optimized framework, based on a modified ResNet\narchitecture with frozen pre-trained layers and class-weighted training,\nachieved over 90% accuracy in cancer detection. This represents a significant\nimprovement over current manual analysis methods an demonstrates potential for\nclinical deployment. This work establishes a robust pipeline for automated PDAC\ndetection that can augment pathologists' capabilities while providing a\nfoundation for future expansion to other cancer types. The developed\nmethodology also offers valuable insights for applying deep learning to\nlimited-size medical imaging datasets, a common challenge in clinical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms\nof cancer, with a five-year survival rate below 10% primarily due to late\ndetection. This research develops and validates a deep learning framework for\nearly PDAC detection through analysis of dual-modality imaging:\nautofluorescence and second harmonic generation (SHG). We analyzed 40 unique\npatient samples to create a specialized neural network capable of\ndistinguishing between normal, fibrotic, and cancerous tissue. Our methodology\nevaluated six distinct deep learning architectures, comparing traditional\nConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).\nThrough systematic experimentation, we identified and overcome significant\nchallenges in medical image analysis, including limited dataset size and class\nimbalance. The final optimized framework, based on a modified ResNet\narchitecture with frozen pre-trained layers and class-weighted training,\nachieved over 90% accuracy in cancer detection. This represents a significant\nimprovement over current manual analysis methods an demonstrates potential for\nclinical deployment. This work establishes a robust pipeline for automated PDAC\ndetection that can augment pathologists' capabilities while providing a\nfoundation for future expansion to other cancer types. The developed\nmethodology also offers valuable insights for applying deep learning to\nlimited-size medical imaging datasets, a common challenge in clinical\napplications."
                },
                "authors": [
                    {
                        "name": "Dennis Slobodzian"
                    },
                    {
                        "name": "Amir Kordijazi"
                    }
                ],
                "author_detail": {
                    "name": "Amir Kordijazi"
                },
                "author": "Amir Kordijazi",
                "arxiv_comment": "21 pages, 17 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09612v1",
                "updated": "2025-09-11T16:53:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    53,
                    52,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:53:52Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    53,
                    52,
                    3,
                    254,
                    0
                ],
                "title": "Unsteady gas dynamics modeling for leakage detection in parallel\n  pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsteady gas dynamics modeling for leakage detection in parallel\n  pipelines"
                },
                "summary": "This study presents a novel analytical framework for modeling unsteady gas\ndynamics in parallel pipeline systems under leakage conditions. The proposed\nmethod introduces a time-dependent leakage mass flow rate function, which\ndynamically captures the temporal decay of leakage based on real-time inlet\npressure measurements. This functional form allows for a more physically\nconsistent and mathematically tractable representation of gas loss compared to\nconventional constant-rate or stepwise models. The pipeline system is\npartitioned into three regions relative to the leakage point, and closed-form\npressure solutions are derived using Laplace transform techniques. These\nexpressions enable direct estimation of the leakage location through inverse\npressure profiles, eliminating the need for computationally intensive iterative\nschemes. The analytical model is further validated against representative\nbenchmark scenarios, demonstrating good agreement with literature-based\nresults. A comparative analysis underscores the model's ability to localize\nleakage using minimal sensor data while preserving interpretability - an\nessential feature for deployment in industrial environments. The approach\nprovides a lightweight yet robust alternative to purely numerical or machine\nlearning-based solutions and offers potential integration into real-time\nmonitoring systems. This work contributes to the field by unifying gas dynamic\nprinciples, sensor-assisted modeling, and analytical solution strategies to\nenhance the reliability and speed of leak detection in modern gas transport\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a novel analytical framework for modeling unsteady gas\ndynamics in parallel pipeline systems under leakage conditions. The proposed\nmethod introduces a time-dependent leakage mass flow rate function, which\ndynamically captures the temporal decay of leakage based on real-time inlet\npressure measurements. This functional form allows for a more physically\nconsistent and mathematically tractable representation of gas loss compared to\nconventional constant-rate or stepwise models. The pipeline system is\npartitioned into three regions relative to the leakage point, and closed-form\npressure solutions are derived using Laplace transform techniques. These\nexpressions enable direct estimation of the leakage location through inverse\npressure profiles, eliminating the need for computationally intensive iterative\nschemes. The analytical model is further validated against representative\nbenchmark scenarios, demonstrating good agreement with literature-based\nresults. A comparative analysis underscores the model's ability to localize\nleakage using minimal sensor data while preserving interpretability - an\nessential feature for deployment in industrial environments. The approach\nprovides a lightweight yet robust alternative to purely numerical or machine\nlearning-based solutions and offers potential integration into real-time\nmonitoring systems. This work contributes to the field by unifying gas dynamic\nprinciples, sensor-assisted modeling, and analytical solution strategies to\nenhance the reliability and speed of leak detection in modern gas transport\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Ilgar G. Aliyev"
                    },
                    {
                        "name": "Konul Gafarbayli"
                    },
                    {
                        "name": "Ahad Mammadov"
                    },
                    {
                        "name": "Firangiz Mammadrazayeva"
                    }
                ],
                "author_detail": {
                    "name": "Firangiz Mammadrazayeva"
                },
                "author": "Firangiz Mammadrazayeva",
                "arxiv_doi": "10.12989/csm.2025.14.4.371",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.12989/csm.2025.14.4.371",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 3 figures",
                "arxiv_journal_ref": "Coupled Systems Mechanics, Vol. 14, No. 4 (2025) 371-393",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "76N25, 76M60, 93C20, 90B06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.6; I.2.8; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06927v2",
                "updated": "2025-09-11T16:50:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    50,
                    26,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T17:39:03Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    39,
                    3,
                    0,
                    251,
                    0
                ],
                "title": "NeedForHeat DataGear: An Open Monitoring System to Accelerate the\n  Residential Heating Transition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedForHeat DataGear: An Open Monitoring System to Accelerate the\n  Residential Heating Transition"
                },
                "summary": "We introduce NeedForHeat DataGear: an open hardware and open software data\ncollection system designed to accelerate the residential heating transition.\nNeedForHeat DataGear collects time series monitoring data in homes that have\nnot yet undergone a heating transition, enabling assessment of real-life\nthermal characteristics, heating system efficiency, and residents' comfort\nneeds. This paper outlines its architecture and functionalities, emphasizing\nits modularity, adaptability, and cost-effectiveness for field data\nacquisition. Unlike conventional domestic monitoring solutions focused on home\nautomation, direct feedback, or post-installation heat pump monitoring, it\nprioritizes time series data we deemed essential to evaluate the current\nsituation in existing homes before the heating transition. Designed for\nseamless deployment across diverse households, NeedForHeat DataGear combines\nopenness, security, and privacy with a low-cost, user-friendly approach, making\nit a valuable tool for researchers, energy professionals, and energy coaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NeedForHeat DataGear: an open hardware and open software data\ncollection system designed to accelerate the residential heating transition.\nNeedForHeat DataGear collects time series monitoring data in homes that have\nnot yet undergone a heating transition, enabling assessment of real-life\nthermal characteristics, heating system efficiency, and residents' comfort\nneeds. This paper outlines its architecture and functionalities, emphasizing\nits modularity, adaptability, and cost-effectiveness for field data\nacquisition. Unlike conventional domestic monitoring solutions focused on home\nautomation, direct feedback, or post-installation heat pump monitoring, it\nprioritizes time series data we deemed essential to evaluate the current\nsituation in existing homes before the heating transition. Designed for\nseamless deployment across diverse households, NeedForHeat DataGear combines\nopenness, security, and privacy with a low-cost, user-friendly approach, making\nit a valuable tool for researchers, energy professionals, and energy coaches."
                },
                "authors": [
                    {
                        "name": "Henri ter Hofte"
                    },
                    {
                        "name": "Nick van Ravenzwaaij"
                    }
                ],
                "author_detail": {
                    "name": "Nick van Ravenzwaaij"
                },
                "author": "Nick van Ravenzwaaij",
                "arxiv_comment": "10 pages + 3 pages appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09602v1",
                "updated": "2025-09-11T16:42:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    42,
                    22,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:42:22Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    42,
                    22,
                    3,
                    254,
                    0
                ],
                "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death\n  Determination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death\n  Determination"
                },
                "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Yiqun T. Chen"
                    },
                    {
                        "name": "Tyler H. McCormick"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Abhirup Datta"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Datta"
                },
                "author": "Abhirup Datta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09596v1",
                "updated": "2025-09-11T16:35:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    35,
                    54,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:35:54Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    35,
                    54,
                    3,
                    254,
                    0
                ],
                "title": "How much are LLMs changing the language of academic papers after\n  ChatGPT? A multi-database and full text analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much are LLMs changing the language of academic papers after\n  ChatGPT? A multi-database and full text analysis"
                },
                "summary": "This study investigates how Large Language Models (LLMs) are influencing the\nlanguage of academic papers by tracking 12 LLM-associated terms across six\nmajor scholarly databases (Scopus, Web of Science, PubMed, PubMed Central\n(PMC), Dimensions, and OpenAlex) from 2015 to 2024. Using over 2.4 million PMC\nopen-access publications (2021-July 2025), we also analysed full texts to\nassess changes in the frequency and co-occurrence of these terms before and\nafter ChatGPT's initial public release. Across databases, delve (+1,500%),\nunderscore (+1,000%), and intricate (+700%) had the largest increases between\n2022 and 2024. Growth in LLM-term usage was much higher in STEM fields than in\nsocial sciences and arts and humanities. In PMC full texts, the proportion of\npapers using underscore six or more times increased by over 10,000% from 2022\nto 2025, followed by intricate (+5,400%) and meticulous (+2,800%). Nearly half\nof all 2024 PMC papers using any LLM term also included underscore, compared\nwith only 3%-14% of papers before ChatGPT in 2022. Papers using one LLM term\nare now much more likely to include other terms. For example, in 2024,\nunderscore strongly correlated with pivotal (0.449) and delve (0.311), compared\nwith very weak associations in 2022 (0.032 and 0.018, respectively). These\nfindings provide the first large-scale evidence based on full-text publications\nand multiple databases that some LLM-related terms are now being used much more\nfrequently and together. The rapid uptake of LLMs to support scholarly\npublishing is a welcome development reducing the language barrier to academic\npublishing for non-English speakers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how Large Language Models (LLMs) are influencing the\nlanguage of academic papers by tracking 12 LLM-associated terms across six\nmajor scholarly databases (Scopus, Web of Science, PubMed, PubMed Central\n(PMC), Dimensions, and OpenAlex) from 2015 to 2024. Using over 2.4 million PMC\nopen-access publications (2021-July 2025), we also analysed full texts to\nassess changes in the frequency and co-occurrence of these terms before and\nafter ChatGPT's initial public release. Across databases, delve (+1,500%),\nunderscore (+1,000%), and intricate (+700%) had the largest increases between\n2022 and 2024. Growth in LLM-term usage was much higher in STEM fields than in\nsocial sciences and arts and humanities. In PMC full texts, the proportion of\npapers using underscore six or more times increased by over 10,000% from 2022\nto 2025, followed by intricate (+5,400%) and meticulous (+2,800%). Nearly half\nof all 2024 PMC papers using any LLM term also included underscore, compared\nwith only 3%-14% of papers before ChatGPT in 2022. Papers using one LLM term\nare now much more likely to include other terms. For example, in 2024,\nunderscore strongly correlated with pivotal (0.449) and delve (0.311), compared\nwith very weak associations in 2022 (0.032 and 0.018, respectively). These\nfindings provide the first large-scale evidence based on full-text publications\nand multiple databases that some LLM-related terms are now being used much more\nfrequently and together. The rapid uptake of LLMs to support scholarly\npublishing is a welcome development reducing the language barrier to academic\npublishing for non-English speakers."
                },
                "authors": [
                    {
                        "name": "Kayvan Kousha"
                    },
                    {
                        "name": "Mike Thelwall"
                    }
                ],
                "author_detail": {
                    "name": "Mike Thelwall"
                },
                "author": "Mike Thelwall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09594v1",
                "updated": "2025-09-11T16:34:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    34,
                    17,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:34:17Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    34,
                    17,
                    3,
                    254,
                    0
                ],
                "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjectReact: Learning Object-Relative Control for Visual Navigation"
                },
                "summary": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/"
                },
                "authors": [
                    {
                        "name": "Sourav Garg"
                    },
                    {
                        "name": "Dustin Craggs"
                    },
                    {
                        "name": "Vineeth Bhat"
                    },
                    {
                        "name": "Lachlan Mares"
                    },
                    {
                        "name": "Stefan Podgorski"
                    },
                    {
                        "name": "Madhava Krishna"
                    },
                    {
                        "name": "Feras Dayoub"
                    },
                    {
                        "name": "Ian Reid"
                    }
                ],
                "author_detail": {
                    "name": "Ian Reid"
                },
                "author": "Ian Reid",
                "arxiv_comment": "CoRL 2025; 23 pages including appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09593v1",
                "updated": "2025-09-11T16:31:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    31,
                    13,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:31:13Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    31,
                    13,
                    3,
                    254,
                    0
                ],
                "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models"
                },
                "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding."
                },
                "authors": [
                    {
                        "name": "Bangzhao Shu"
                    },
                    {
                        "name": "Isha Joshi"
                    },
                    {
                        "name": "Melissa Karnaze"
                    },
                    {
                        "name": "Anh C. Pham"
                    },
                    {
                        "name": "Ishita Kakkar"
                    },
                    {
                        "name": "Sindhu Kothe"
                    },
                    {
                        "name": "Arpine Hovasapian"
                    },
                    {
                        "name": "Mai ElSherief"
                    }
                ],
                "author_detail": {
                    "name": "Mai ElSherief"
                },
                "author": "Mai ElSherief",
                "arxiv_comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08031v2",
                "updated": "2025-09-11T16:27:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    27,
                    59,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-09T15:30:40Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    30,
                    40,
                    1,
                    252,
                    0
                ],
                "title": "AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs"
                },
                "summary": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce AU-Harness, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. AU-Harness provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce AU-Harness, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. AU-Harness provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Sidharth Surapaneni"
                    },
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Jash Mehta"
                    },
                    {
                        "name": "Aman Tiwari"
                    },
                    {
                        "name": "Oluwanifemi Bamgbose"
                    },
                    {
                        "name": "Akshay Kalkunte"
                    },
                    {
                        "name": "Sai Rajeswar"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    }
                ],
                "author_detail": {
                    "name": "Sathwik Tejaswi Madhusudhan"
                },
                "author": "Sathwik Tejaswi Madhusudhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06485v2",
                "updated": "2025-09-11T15:55:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    55,
                    28,
                    3,
                    254,
                    0
                ],
                "published": "2025-06-06T19:20:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    19,
                    20,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Task Matters: Knowledge Requirements Shape LLM Responses to\n  Context-Memory Conflict",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Matters: Knowledge Requirements Shape LLM Responses to\n  Context-Memory Conflict"
                },
                "summary": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiser Sun"
                    },
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21961v2",
                "updated": "2025-09-11T15:49:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    49,
                    39,
                    3,
                    254,
                    0
                ],
                "published": "2025-03-27T20:18:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    20,
                    18,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Gated Branching for Efficient Test-Time Reasoning"
                },
                "summary": "Test-time compute methods like beam search can significantly improve the\nreasoning capabilities and problem-solving accuracy of large language models.\nHowever, these approaches require substantially increased computational\nresources, with most computation wasted on exploring low-diversity branches\nwhere the model already exhibits high confidence. We observe that a small\nsubset of uncertain reasoning steps has a disproportionately large impact on\nfinal prediction accuracy, and branching at these points tends to yield\nhigher-quality and more diverse candidate reasoning steps. Therefore, we\nintroduce Entropy-Gated Branching: a novel inference technique that dynamically\nallocates computational resources by selectively expanding prediction sequences\nonly at points of high uncertainty. Our method leverages entropy as a gating\nmechanism to identify when branching is most beneficial, coupled with an\nexternal feedback model to rank and prune candidate branches. Empirical results\non mathematical and financial reasoning benchmarks show that this strategy\nimproves accuracy by 22.6% over standard inference while operating 37% faster\nthan conventional beam search with similar or higher performance. Our results\nshow that dynamic resource allocation during inference can substantially\nimprove both efficiency and effectiveness, offering a more scalable pathway to\nenhanced LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute methods like beam search can significantly improve the\nreasoning capabilities and problem-solving accuracy of large language models.\nHowever, these approaches require substantially increased computational\nresources, with most computation wasted on exploring low-diversity branches\nwhere the model already exhibits high confidence. We observe that a small\nsubset of uncertain reasoning steps has a disproportionately large impact on\nfinal prediction accuracy, and branching at these points tends to yield\nhigher-quality and more diverse candidate reasoning steps. Therefore, we\nintroduce Entropy-Gated Branching: a novel inference technique that dynamically\nallocates computational resources by selectively expanding prediction sequences\nonly at points of high uncertainty. Our method leverages entropy as a gating\nmechanism to identify when branching is most beneficial, coupled with an\nexternal feedback model to rank and prune candidate branches. Empirical results\non mathematical and financial reasoning benchmarks show that this strategy\nimproves accuracy by 22.6% over standard inference while operating 37% faster\nthan conventional beam search with similar or higher performance. Our results\nshow that dynamic resource allocation during inference can substantially\nimprove both efficiency and effectiveness, offering a more scalable pathway to\nenhanced LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Ethan Callanan"
                    },
                    {
                        "name": "Abdellah Ghassel"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09550v1",
                "updated": "2025-09-11T15:39:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    39,
                    59,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:39:59Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    39,
                    59,
                    3,
                    254,
                    0
                ],
                "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates"
                },
                "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel."
                },
                "authors": [
                    {
                        "name": "Harry Julia"
                    },
                    {
                        "name": "Rachel Beeson"
                    },
                    {
                        "name": "Lohith Konathala"
                    },
                    {
                        "name": "Johanna Ulin"
                    },
                    {
                        "name": "Jiameng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiameng Gao"
                },
                "author": "Jiameng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09544v1",
                "updated": "2025-09-11T15:37:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    37,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:37:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    37,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance\n  NLP (2022-2025)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance\n  NLP (2022-2025)"
                },
                "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains."
                },
                "authors": [
                    {
                        "name": "Paolo Pedinotti"
                    },
                    {
                        "name": "Peter Baumann"
                    },
                    {
                        "name": "Nathan Jessurun"
                    },
                    {
                        "name": "Leslie Barrett"
                    },
                    {
                        "name": "Enrico Santus"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Santus"
                },
                "author": "Enrico Santus",
                "arxiv_comment": "7 pages, 6 appendices, EMNLP industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15917v2",
                "updated": "2025-09-11T15:22:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    22,
                    50,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-22T14:02:57Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    2,
                    57,
                    1,
                    112,
                    0
                ],
                "title": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Test Generation from Task Description for Mobile Testing with\n  Multi-modal Reasoning"
                },
                "summary": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Android GUI testing, generating an action sequence for a task that can be\nreplayed as a test script is common. Generating sequences of actions and\nrespective test scripts from task goals described in natural language can\neliminate the need for manually writing test scripts. However, existing\napproaches based on large language models (LLM) often struggle with identifying\nthe final action, and either end prematurely or continue past the final screen.\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\nframework that iteratively determines the next action and leverages visual\nimages of screens to detect the task's completeness. The multi-modal approach\nenhances our model in two significant ways. First, this approach enables it to\navoid prematurely terminating a task when textual content alone provides\nmisleading indications of task completion. Additionally, visual input helps the\ntool avoid errors when changes in the GUI do not directly affect functionality\ntoward task completion, such as adjustments to font sizes or colors. Second,\nthe multi-modal approach also ensures the tool not progress beyond the final\nscreen, which might lack explicit textual indicators of task completion but\ncould display a visual element indicating task completion, which is common in\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\nour multi-modal framework with images and texts enables the LLM to better\ndetermine when a task is completed."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Hai Phung"
                    },
                    {
                        "name": "Hao Pham"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Vu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Vu Nguyen"
                },
                "author": "Vu Nguyen",
                "arxiv_comment": "Change the method and experimentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17287v2",
                "updated": "2025-09-11T15:22:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    22,
                    14,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-24T06:28:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    28,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "Combining Static and Dynamic Approaches for Mining and Testing\n  Constraints for RESTful API Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Static and Dynamic Approaches for Mining and Testing\n  Constraints for RESTful API Testing"
                },
                "summary": "In API testing, deriving logical constraints on API response bodies is\ncrucial in generating the test cases to cover various aspects of RESTful APIs.\nHowever, existing approaches are limited to dynamic analysis in which\nconstraints are extracted from the execution of APIs as part of the system\nunder test. The key limitation of such a dynamic approach is its\nunder-estimation in which inputs in API executions are not sufficiently diverse\nto uncover actual constraints on API response bodies. In this paper, we propose\nto combine a novel static analysis approach (in which the constraints for API\nresponse bodies are mined from API specifications), with the dynamic approach\n(which relies on API execution data). We leverage large language models (LLMs)\nto comprehend the API specifications, mine constraints for response bodies, and\ngenerate test cases. To reduce LLMs' hallucination, we apply an\nObservation-Confirmation (OC) scheme which uses initial prompts to\ncontextualize constraints. %, allowing subsequent prompts to more accurately\nconfirm their presence. Our empirical results show that~LLMs with OC prompting\nachieve high precision in constraint mining with the average of 91.2%. When\ncombining static and dynamic analysis, our tool, RBCTest , achieves a precision\nof 78.5%. RBCTest detects 107 constraints that the dynamic approach misses and\n46 more precise constraints. We also use its generated test cases to detect 21\nmismatches between the API specification and actual response data for 8\nreal-world APIs. Four of the mismatches were, in fact, reported in developers'\nforums.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In API testing, deriving logical constraints on API response bodies is\ncrucial in generating the test cases to cover various aspects of RESTful APIs.\nHowever, existing approaches are limited to dynamic analysis in which\nconstraints are extracted from the execution of APIs as part of the system\nunder test. The key limitation of such a dynamic approach is its\nunder-estimation in which inputs in API executions are not sufficiently diverse\nto uncover actual constraints on API response bodies. In this paper, we propose\nto combine a novel static analysis approach (in which the constraints for API\nresponse bodies are mined from API specifications), with the dynamic approach\n(which relies on API execution data). We leverage large language models (LLMs)\nto comprehend the API specifications, mine constraints for response bodies, and\ngenerate test cases. To reduce LLMs' hallucination, we apply an\nObservation-Confirmation (OC) scheme which uses initial prompts to\ncontextualize constraints. %, allowing subsequent prompts to more accurately\nconfirm their presence. Our empirical results show that~LLMs with OC prompting\nachieve high precision in constraint mining with the average of 91.2%. When\ncombining static and dynamic analysis, our tool, RBCTest , achieves a precision\nof 78.5%. RBCTest detects 107 constraints that the dynamic approach misses and\n46 more precise constraints. We also use its generated test cases to detect 21\nmismatches between the API specification and actual response data for 8\nreal-world APIs. Four of the mismatches were, in fact, reported in developers'\nforums."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Tri Le"
                    },
                    {
                        "name": "Vu Nguyen"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Tien N. Nguyen"
                },
                "author": "Tien N. Nguyen",
                "arxiv_comment": "Change the method and experimentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01080v2",
                "updated": "2025-09-11T15:20:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    20,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-01T16:37:55Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    37,
                    55,
                    1,
                    182,
                    0
                ],
                "title": "Development and Comparative Evaluation of Three Artificial Intelligence\n  Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A\n  7-Month Retrospective Proof-of-Concept",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Comparative Evaluation of Three Artificial Intelligence\n  Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A\n  7-Month Retrospective Proof-of-Concept"
                },
                "summary": "Emergency departments struggle with persistent triage errors, especially\nundertriage and overtriage, which are aggravated by growing patient volumes and\nstaff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP),\nURGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and\nnurse practice, using seven months of adult triage data from Roger Salengro\nHospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE\nconsistently outperformed both AI alternatives and nurse triage, achieving the\nhighest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in\npredicting hospitalization needs (GEMSA). Its robustness across structured data\nand raw transcripts highlighted the advantage of LLM architectures in\nabstracting patient information. Overall, the findings suggest that integrating\nLLM-based AI into emergency department workflows could significantly enhance\npatient safety and operational efficiency, though successful adoption will\ndepend on addressing limitations and ensuring ethical transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency departments struggle with persistent triage errors, especially\nundertriage and overtriage, which are aggravated by growing patient volumes and\nstaff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP),\nURGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and\nnurse practice, using seven months of adult triage data from Roger Salengro\nHospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE\nconsistently outperformed both AI alternatives and nurse triage, achieving the\nhighest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in\npredicting hospitalization needs (GEMSA). Its robustness across structured data\nand raw transcripts highlighted the advantage of LLM architectures in\nabstracting patient information. Overall, the findings suggest that integrating\nLLM-based AI into emergency department workflows could significantly enhance\npatient safety and operational efficiency, though successful adoption will\ndepend on addressing limitations and ensuring ethical transparency."
                },
                "authors": [
                    {
                        "name": "Edouard Lansiaux"
                    },
                    {
                        "name": "Ramy Azzouz"
                    },
                    {
                        "name": "Emmanuel Chazard"
                    },
                    {
                        "name": "Amélie Vromant"
                    },
                    {
                        "name": "Eric Wiel"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wiel"
                },
                "author": "Eric Wiel",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04867v2",
                "updated": "2025-09-11T14:52:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    52,
                    8,
                    3,
                    254,
                    0
                ],
                "published": "2025-06-05T10:38:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    38,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "LLMs for sensory-motor control: Combining in-context and iterative\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for sensory-motor control: Combining in-context and iterative\n  learning"
                },
                "summary": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. At the outset, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. The approach proves\neffective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b.\nIn most cases, it successfully identifies optimal or near-optimal solutions by\nintegrating symbolic knowledge derived through reasoning with sub-symbolic\nsensory-motor data gathered as the agent interacts with its environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. At the outset, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. The approach proves\neffective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b.\nIn most cases, it successfully identifies optimal or near-optimal solutions by\nintegrating symbolic knowledge derived through reasoning with sub-symbolic\nsensory-motor data gathered as the agent interacts with its environment."
                },
                "authors": [
                    {
                        "name": "Jônata Tyska Carvalho"
                    },
                    {
                        "name": "Stefano Nolfi"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Nolfi"
                },
                "author": "Stefano Nolfi",
                "arxiv_comment": "Article updated with results from gpt-oss:120b. 24 pages (13 pages\n  are from appendix), 6 figures, code for experiments replication and\n  supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09505v1",
                "updated": "2025-09-11T14:49:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    49,
                    50,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:49:50Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    49,
                    50,
                    3,
                    254,
                    0
                ],
                "title": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference"
                },
                "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced."
                },
                "authors": [
                    {
                        "name": "Haoran Wu"
                    },
                    {
                        "name": "Can Xiao"
                    },
                    {
                        "name": "Jiayi Nie"
                    },
                    {
                        "name": "Xuan Guo"
                    },
                    {
                        "name": "Binglei Lou"
                    },
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Przemyslaw Forys"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Timothy M. Jones"
                    },
                    {
                        "name": "Rika Antonova"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Aaron Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Zhao"
                },
                "author": "Aaron Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02348v2",
                "updated": "2025-09-11T14:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    39,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-01-04T18:04:47Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    18,
                    4,
                    47,
                    5,
                    4,
                    0
                ],
                "title": "Thinking with Many Minds: Using Large Language Models for\n  Multi-Perspective Problem-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking with Many Minds: Using Large Language Models for\n  Multi-Perspective Problem-Solving"
                },
                "summary": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution."
                },
                "authors": [
                    {
                        "name": "Sanghyun Park"
                    },
                    {
                        "name": "Boris Maciejovsky"
                    },
                    {
                        "name": "Phanish Puranam"
                    }
                ],
                "author_detail": {
                    "name": "Phanish Puranam"
                },
                "author": "Phanish Puranam",
                "arxiv_comment": "36 pages, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20999v2",
                "updated": "2025-09-11T14:36:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    36,
                    21,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-28T17:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    11,
                    26,
                    0,
                    209,
                    0
                ],
                "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning"
                },
                "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."
                },
                "authors": [
                    {
                        "name": "Yining Huang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Keke Tang"
                    },
                    {
                        "name": "Meilian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Meilian Chen"
                },
                "author": "Meilian Chen",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00039v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00039v5",
                "updated": "2025-09-11T14:34:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    52,
                    3,
                    254,
                    0
                ],
                "published": "2025-04-29T18:36:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    36,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "An Ontology-Driven Graph RAG for Legal Norms: A Structural, Temporal,\n  and Deterministic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ontology-Driven Graph RAG for Legal Norms: A Structural, Temporal,\n  and Deterministic Approach"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces the Structure-Aware Temporal Graph\nRAG (SAT-Graph RAG), an ontology-driven framework designed to overcome these\nlimitations by explicitly modeling the formal structure and diachronic nature\nof legal norms. We ground our knowledge graph in a formal, LRMoo-inspired model\nthat distinguishes abstract legal Works from their versioned Expressions. We\nmodel temporal states as efficient aggregations that reuse the versioned\nexpressions (CTVs) of unchanged components, and we reify legislative events as\nfirst-class Action nodes to make causality explicit and queryable. This\nstructured backbone enables a unified, planner-guided query strategy that\napplies explicit policies to deterministically resolve complex requests for (i)\npoint-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable\nprovenance reconstruction. Through a case study on the Brazilian Constitution,\nwe demonstrate how this approach provides a verifiable, temporally-correct\nsubstrate for LLMs, enabling higher-order analytical capabilities while\ndrastically reducing the risk of factual errors. The result is a practical\nframework for building more trustworthy and explainable legal AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces the Structure-Aware Temporal Graph\nRAG (SAT-Graph RAG), an ontology-driven framework designed to overcome these\nlimitations by explicitly modeling the formal structure and diachronic nature\nof legal norms. We ground our knowledge graph in a formal, LRMoo-inspired model\nthat distinguishes abstract legal Works from their versioned Expressions. We\nmodel temporal states as efficient aggregations that reuse the versioned\nexpressions (CTVs) of unchanged components, and we reify legislative events as\nfirst-class Action nodes to make causality explicit and queryable. This\nstructured backbone enables a unified, planner-guided query strategy that\napplies explicit policies to deterministically resolve complex requests for (i)\npoint-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable\nprovenance reconstruction. Through a case study on the Brazilian Constitution,\nwe demonstrate how this approach provides a verifiable, temporally-correct\nsubstrate for LLMs, enabling higher-order analytical capabilities while\ndrastically reducing the risk of factual errors. The result is a practical\nframework for building more trustworthy and explainable legal AI systems."
                },
                "authors": [
                    {
                        "name": "Hudson de Martim"
                    }
                ],
                "author_detail": {
                    "name": "Hudson de Martim"
                },
                "author": "Hudson de Martim",
                "arxiv_comment": "Major revision for clarity and academic precision. Updated title and\n  abstract. Refined core terminology, contributions, related work, and shifted\n  the implementation to a conceptual architecture. Added new arguments to\n  strengthen the paper's thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00039v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00039v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03700v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03700v5",
                "updated": "2025-09-11T14:28:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    28,
                    11,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-19T12:33:43Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    12,
                    33,
                    43,
                    5,
                    200,
                    0
                ],
                "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline\n  and Reinforcement Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline\n  and Reinforcement Fine-tuning"
                },
                "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."
                },
                "authors": [
                    {
                        "name": "Liujian Tang"
                    },
                    {
                        "name": "Shaokang Dong"
                    },
                    {
                        "name": "Yijia Huang"
                    },
                    {
                        "name": "Minqi Xiang"
                    },
                    {
                        "name": "Hongtao Ruan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Zhihui Cao"
                    },
                    {
                        "name": "Hailiang Pang"
                    },
                    {
                        "name": "Heng Kong"
                    },
                    {
                        "name": "He Yang"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Zhilin Gao"
                    },
                    {
                        "name": "Xingyu Liu"
                    },
                    {
                        "name": "Yingnan Fu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Kang Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Yuran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuran Wang"
                },
                "author": "Yuran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03700v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03700v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08105v2",
                "updated": "2025-09-11T14:14:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    14,
                    35,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-09T19:32:05Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    19,
                    32,
                    5,
                    1,
                    252,
                    0
                ],
                "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and\n  LLM Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and\n  LLM Fusion"
                },
                "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings."
                },
                "authors": [
                    {
                        "name": "Kosei Uemura"
                    },
                    {
                        "name": "David Guzmán"
                    },
                    {
                        "name": "Quang Phuoc Nguyen"
                    },
                    {
                        "name": "Jesujoba Oluwadara Alabi"
                    },
                    {
                        "name": "En-shiun Annie Lee"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    }
                ],
                "author_detail": {
                    "name": "David Ifeoluwa Adelani"
                },
                "author": "David Ifeoluwa Adelani",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00132v3",
                "updated": "2025-09-11T14:13:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    13,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2025-03-31T18:33:55Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    33,
                    55,
                    0,
                    90,
                    0
                ],
                "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B"
                },
                "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."
                },
                "authors": [
                    {
                        "name": "Aleksandra Bakalova"
                    },
                    {
                        "name": "Yana Veitsman"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00806v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00806v4",
                "updated": "2025-09-11T13:58:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    58,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2024-03-31T21:43:05Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    21,
                    43,
                    5,
                    6,
                    91,
                    0
                ],
                "title": "Algorithmic Collusion by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Collusion by Large Language Models"
                },
                "summary": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that LLM-based pricing agents quickly and autonomously\nreach supracompetitive prices and profits in oligopoly settings and that\nvariation in seemingly innocuous phrases in LLM instructions (\"prompts\") may\nsubstantially influence the degree of supracompetitive pricing. Off-path\nanalysis using novel techniques uncovers price-war concerns as contributing to\nthese phenomena. Our results extend to auction settings. Our findings uncover\nunique challenges to any future regulation of LLM-based pricing agents, and\nAI-based pricing agents more broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that LLM-based pricing agents quickly and autonomously\nreach supracompetitive prices and profits in oligopoly settings and that\nvariation in seemingly innocuous phrases in LLM instructions (\"prompts\") may\nsubstantially influence the degree of supracompetitive pricing. Off-path\nanalysis using novel techniques uncovers price-war concerns as contributing to\nthese phenomena. Our results extend to auction settings. Our findings uncover\nunique challenges to any future regulation of LLM-based pricing agents, and\nAI-based pricing agents more broadly."
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    }
                ],
                "author_detail": {
                    "name": "Ran I. Shorrer"
                },
                "author": "Ran I. Shorrer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00806v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00806v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09469v1",
                "updated": "2025-09-11T13:52:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    52,
                    47,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:52:47Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    52,
                    47,
                    3,
                    254,
                    0
                ],
                "title": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI"
                },
                "summary": "Gliomas are the most prevalent type of primary brain tumors, and their\naccurate segmentation from MRI is critical for diagnosis, treatment planning,\nand longitudinal monitoring. However, the scarcity of high-quality annotated\nimaging data in Sub-Saharan Africa (SSA) poses a significant challenge for\ndeploying advanced segmentation models in clinical workflows. This study\nintroduces a robust and computationally efficient deep learning framework\ntailored for resource-constrained settings. We leveraged a 3D Attention UNet\narchitecture augmented with residual blocks and enhanced through transfer\nlearning from pre-trained weights on the BraTS 2021 dataset. Our model was\nevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma\nsegmentation in SSA MRI data. Despite the limited data quality and quantity,\nour approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80\nfor Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding\nNon-Functional Hemisphere (SNFH). These results demonstrate the\ngeneralizability of the proposed model and its potential to support clinical\ndecision making in low-resource settings. The compact architecture,\napproximately 90 MB, and sub-minute per-volume inference time on consumer-grade\nhardware further underscore its practicality for deployment in SSA health\nsystems. This work contributes toward closing the gap in equitable AI for\nglobal health by empowering underserved regions with high-performing and\naccessible medical imaging solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gliomas are the most prevalent type of primary brain tumors, and their\naccurate segmentation from MRI is critical for diagnosis, treatment planning,\nand longitudinal monitoring. However, the scarcity of high-quality annotated\nimaging data in Sub-Saharan Africa (SSA) poses a significant challenge for\ndeploying advanced segmentation models in clinical workflows. This study\nintroduces a robust and computationally efficient deep learning framework\ntailored for resource-constrained settings. We leveraged a 3D Attention UNet\narchitecture augmented with residual blocks and enhanced through transfer\nlearning from pre-trained weights on the BraTS 2021 dataset. Our model was\nevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma\nsegmentation in SSA MRI data. Despite the limited data quality and quantity,\nour approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80\nfor Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding\nNon-Functional Hemisphere (SNFH). These results demonstrate the\ngeneralizability of the proposed model and its potential to support clinical\ndecision making in low-resource settings. The compact architecture,\napproximately 90 MB, and sub-minute per-volume inference time on consumer-grade\nhardware further underscore its practicality for deployment in SSA health\nsystems. This work contributes toward closing the gap in equitable AI for\nglobal health by empowering underserved regions with high-performing and\naccessible medical imaging solutions."
                },
                "authors": [
                    {
                        "name": "Freedmore Sidume"
                    },
                    {
                        "name": "Oumayma Soula"
                    },
                    {
                        "name": "Joseph Muthui Wacira"
                    },
                    {
                        "name": "YunFei Zhu"
                    },
                    {
                        "name": "Abbas Rabiu Muhammad"
                    },
                    {
                        "name": "Abderrazek Zeraii"
                    },
                    {
                        "name": "Oluwaseun Kalejaye"
                    },
                    {
                        "name": "Hajer Ibrahim"
                    },
                    {
                        "name": "Olfa Gaddour"
                    },
                    {
                        "name": "Brain Halubanza"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Udunna C Anazodo"
                    },
                    {
                        "name": "Confidence Raymond"
                    }
                ],
                "author_detail": {
                    "name": "Confidence Raymond"
                },
                "author": "Confidence Raymond",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04275v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04275v3",
                "updated": "2025-09-11T13:50:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    50,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2024-08-08T07:20:42Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    7,
                    20,
                    42,
                    3,
                    221,
                    0
                ],
                "title": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models"
                },
                "summary": "Multimodal large language models (LLMs) empower LLMs to ingest inputs and\ngenerate outputs in multiple forms, such as text, image, and audio. However,\nthe integration of multiple modalities introduces heterogeneity in both the\nmodel and training data, creating unique systems challenges.\n  We propose DistTrain, a disaggregated training system for multimodal LLMs.\nDistTrain incorporates two novel disaggregation techniques to address model and\ndata heterogeneity, respectively. The first is disaggregated model\norchestration, which separates the training for modality encoder, LLM backbone,\nand modality generator. This allows the three components to adaptively and\nindependently orchestrate their resources and parallelism configurations. The\nsecond is disaggregated data preprocessing, which decouples data preprocessing\nfrom training. This eliminates resource contention between preprocessing and\ntraining, and enables efficient data reordering to mitigate stragglers within\nand between microbatches caused by data heterogeneity. We evaluate DistTrain\nacross different sizes of multimodal LLMs on a large-scale production cluster.\nThe experimental results show that DistTrain achieves 54.7% Model FLOPs\nUtilization (MFU) when training a 72B multimodal LLM on 1172 GPUs and\noutperforms Megatron-LM by up to 2.2x on training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) empower LLMs to ingest inputs and\ngenerate outputs in multiple forms, such as text, image, and audio. However,\nthe integration of multiple modalities introduces heterogeneity in both the\nmodel and training data, creating unique systems challenges.\n  We propose DistTrain, a disaggregated training system for multimodal LLMs.\nDistTrain incorporates two novel disaggregation techniques to address model and\ndata heterogeneity, respectively. The first is disaggregated model\norchestration, which separates the training for modality encoder, LLM backbone,\nand modality generator. This allows the three components to adaptively and\nindependently orchestrate their resources and parallelism configurations. The\nsecond is disaggregated data preprocessing, which decouples data preprocessing\nfrom training. This eliminates resource contention between preprocessing and\ntraining, and enables efficient data reordering to mitigate stragglers within\nand between microbatches caused by data heterogeneity. We evaluate DistTrain\nacross different sizes of multimodal LLMs on a large-scale production cluster.\nThe experimental results show that DistTrain achieves 54.7% Model FLOPs\nUtilization (MFU) when training a 72B multimodal LLM on 1172 GPUs and\noutperforms Megatron-LM by up to 2.2x on training throughput."
                },
                "authors": [
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "arxiv_doi": "10.1145/3718958.3750472",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3718958.3750472",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04275v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04275v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGCOMM 2025 (https://dl.acm.org/doi/10.1145/3718958.3750472)",
                "arxiv_journal_ref": "SIGCOMM'25: Proceedings of the ACM SIGCOMM 2025 Conference Pages\n  24-38",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09467v1",
                "updated": "2025-09-11T13:50:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    50,
                    23,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:50:23Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    50,
                    23,
                    3,
                    254,
                    0
                ],
                "title": "Inteligencia Artificial jurídica y el desafío de la veracidad:\n  análisis de alucinaciones, optimización de RAG y principios para una\n  integración responsable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inteligencia Artificial jurídica y el desafío de la veracidad:\n  análisis de alucinaciones, optimización de RAG y principios para una\n  integración responsable"
                },
                "summary": "This technical report analyzes the challenge of \"hallucinations\" (false\ninformation) in LLMs applied to law. It examines their causes, manifestations,\nand the effectiveness of the RAG mitigation strategy, highlighting its\nlimitations and proposing holistic optimizations. The paper explores the\nethical and regulatory implications, emphasizing human oversight as an\nirreplaceable role. It concludes that the solution lies not in incrementally\nimproving generative models, but in adopting a \"consultative\" AI paradigm that\nprioritizes veracity and traceability, acting as a tool to amplify, not\nreplace, professional judgment.\n  --\n  Este informe t\\'ecnico analiza el desaf\\'io de las \"alucinaciones\"\n(informaci\\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,\nmanifestaciones y la efectividad de la estrategia de mitigaci\\'on RAG,\nexponiendo sus limitaciones y proponiendo optimizaciones hol\\'isticas. Se\nexploran las implicaciones \\'eticas y regulatorias, enfatizando la\nsupervisi\\'on humana como un rol insustituible. El documento concluye que la\nsoluci\\'on no reside en mejorar incrementalmente los modelos generativos, sino\nen adoptar un paradigma de IA \"consultiva\" que priorice la veracidad y la\ntrazabilidad, actuando como una herramienta para amplificar, y no sustituir, el\njuicio profesional.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report analyzes the challenge of \"hallucinations\" (false\ninformation) in LLMs applied to law. It examines their causes, manifestations,\nand the effectiveness of the RAG mitigation strategy, highlighting its\nlimitations and proposing holistic optimizations. The paper explores the\nethical and regulatory implications, emphasizing human oversight as an\nirreplaceable role. It concludes that the solution lies not in incrementally\nimproving generative models, but in adopting a \"consultative\" AI paradigm that\nprioritizes veracity and traceability, acting as a tool to amplify, not\nreplace, professional judgment.\n  --\n  Este informe t\\'ecnico analiza el desaf\\'io de las \"alucinaciones\"\n(informaci\\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,\nmanifestaciones y la efectividad de la estrategia de mitigaci\\'on RAG,\nexponiendo sus limitaciones y proponiendo optimizaciones hol\\'isticas. Se\nexploran las implicaciones \\'eticas y regulatorias, enfatizando la\nsupervisi\\'on humana como un rol insustituible. El documento concluye que la\nsoluci\\'on no reside en mejorar incrementalmente los modelos generativos, sino\nen adoptar un paradigma de IA \"consultiva\" que priorice la veracidad y la\ntrazabilidad, actuando como una herramienta para amplificar, y no sustituir, el\njuicio profesional."
                },
                "authors": [
                    {
                        "name": "Alex Dantart"
                    }
                ],
                "author_detail": {
                    "name": "Alex Dantart"
                },
                "author": "Alex Dantart",
                "arxiv_comment": "in Spanish and English languages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09461v1",
                "updated": "2025-09-11T13:43:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    43,
                    30,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:43:30Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    43,
                    30,
                    3,
                    254,
                    0
                ],
                "title": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries\n  with Human Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries\n  with Human Intervention"
                },
                "summary": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world."
                },
                "authors": [
                    {
                        "name": "Ambre Assor"
                    },
                    {
                        "name": "Hyeon Jeon"
                    },
                    {
                        "name": "Sungbok Shin"
                    },
                    {
                        "name": "Jean-Daniel Fekete"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Daniel Fekete"
                },
                "author": "Jean-Daniel Fekete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09448v1",
                "updated": "2025-09-11T13:31:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORSO: Template-Oriented Reasoning Towards General Tasks"
                },
                "summary": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales."
                },
                "authors": [
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09439v1",
                "updated": "2025-09-11T13:26:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    26,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:26:33Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    26,
                    33,
                    3,
                    254,
                    0
                ],
                "title": "μFork: Supporting POSIX fork Within a Single-Address-Space OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "μFork: Supporting POSIX fork Within a Single-Address-Space OS"
                },
                "summary": "Single-address-space operating systems have well-known lightweightness\nbenefits that result from their central design idea: the kernel and\napplications share a unique address space. This model makes these operating\nsystems (OSes) incompatible by design with a large class of software:\nmultiprocess POSIX applications. Indeed, the semantics of the primitive used to\ncreate POSIX processes, fork, are inextricably tied to the existence of\nmultiple address spaces.\n  Prior approaches addressing this issue trade off lightweightness,\ncompatibility and/or isolation. We propose {\\mu}Fork, a single-address-space\noperating system design supporting POSIX fork on modern hardware without\ncompromising on any of these key objectives. {\\mu}Fork emulates POSIX processes\n({\\mu}processes) and achieves fork by creating for the child a copy of the\nparent {\\mu}process' memory at a different location within a single address\nspace. This approach presents two challenges: relocating the child's absolute\nmemory references (pointers), as well as providing user/kernel and\n{\\mu}processes isolation without impacting lightweightness. We address them\nusing CHERI. We implement {\\mu}Fork and evaluate it upon three real-world\nuse-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS\nworker warm-up. {\\mu}Fork outperforms previous work and traditional monolithic\nOSes on key lightweightness metrics by an order of magnitude, e.g. it can offer\na fork-bound FaaS function throughput 24% higher than that of a monolithic OS,\nand can fork a {\\mu}process in 54{\\mu}s, 3.7x faster than a traditional fork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-address-space operating systems have well-known lightweightness\nbenefits that result from their central design idea: the kernel and\napplications share a unique address space. This model makes these operating\nsystems (OSes) incompatible by design with a large class of software:\nmultiprocess POSIX applications. Indeed, the semantics of the primitive used to\ncreate POSIX processes, fork, are inextricably tied to the existence of\nmultiple address spaces.\n  Prior approaches addressing this issue trade off lightweightness,\ncompatibility and/or isolation. We propose {\\mu}Fork, a single-address-space\noperating system design supporting POSIX fork on modern hardware without\ncompromising on any of these key objectives. {\\mu}Fork emulates POSIX processes\n({\\mu}processes) and achieves fork by creating for the child a copy of the\nparent {\\mu}process' memory at a different location within a single address\nspace. This approach presents two challenges: relocating the child's absolute\nmemory references (pointers), as well as providing user/kernel and\n{\\mu}processes isolation without impacting lightweightness. We address them\nusing CHERI. We implement {\\mu}Fork and evaluate it upon three real-world\nuse-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS\nworker warm-up. {\\mu}Fork outperforms previous work and traditional monolithic\nOSes on key lightweightness metrics by an order of magnitude, e.g. it can offer\na fork-bound FaaS function throughput 24% higher than that of a monolithic OS,\nand can fork a {\\mu}process in 54{\\mu}s, 3.7x faster than a traditional fork."
                },
                "authors": [
                    {
                        "name": "John Alistair Kressel"
                    },
                    {
                        "name": "Hugo Lefeuvre"
                    },
                    {
                        "name": "Pierre Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Olivier"
                },
                "author": "Pierre Olivier",
                "arxiv_doi": "10.1145/3731569.3764809",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764809",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to appear at SOSP 2025",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09438v1",
                "updated": "2025-09-11T13:25:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    25,
                    40,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:25:40Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    25,
                    40,
                    3,
                    254,
                    0
                ],
                "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrACE: A Generative Approach to Better Confidence Elicitation in Large\n  Language Models"
                },
                "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation."
                },
                "authors": [
                    {
                        "name": "Zhaohan Zhang"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01778v2",
                "updated": "2025-09-11T13:07:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    7,
                    26,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T21:22:51Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    21,
                    22,
                    51,
                    0,
                    244,
                    0
                ],
                "title": "Grid Transmission Evaluation for Solar Deployment and Data Center Growth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grid Transmission Evaluation for Solar Deployment and Data Center Growth"
                },
                "summary": "The rapid growth of renewable energy deployment and data center demand in the\nUnited States has intensified challenges in grid interconnection, with project\ndelays and escalating costs threatening both economic expansion and energy\nreliability. This study investigates transmission constraints using the IEEE\n39-bus New England Power System model to evaluate the simultaneous\ninterconnection of a 1 GW solar facility and a 1 GW data center load. Employing\nPSSE and Python-based automation (psspy), we conducted 1,560 load flow\nsimulations across varying siting configurations to assess branch overloads and\ntransmission line limits. Results revealed that only 14 configurations avoided\noverloads, while most scenarios highlighted recurring congestion on specific\nnetwork branches, particularly between buses 21 and 22. Optimal siting was\nidentified with the load at bus #35 and the generator at bus #39, yielding\nminimal overloads (maximum 91.1% loading). Conversely, poor siting decisions\nresulted in severe congestion with maximum branch loading above 220%. The\nfindings underscore the critical importance of optimized siting and modernized,\nautomated interconnection studies to reduce delays and costs in renewable\nintegration. This research demonstrates the potential of advanced modeling\ntools to accelerate interconnection processes, improve system reliability, and\ninform future strategies for balancing renewable energy deployment with rising\ndata center demand.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of renewable energy deployment and data center demand in the\nUnited States has intensified challenges in grid interconnection, with project\ndelays and escalating costs threatening both economic expansion and energy\nreliability. This study investigates transmission constraints using the IEEE\n39-bus New England Power System model to evaluate the simultaneous\ninterconnection of a 1 GW solar facility and a 1 GW data center load. Employing\nPSSE and Python-based automation (psspy), we conducted 1,560 load flow\nsimulations across varying siting configurations to assess branch overloads and\ntransmission line limits. Results revealed that only 14 configurations avoided\noverloads, while most scenarios highlighted recurring congestion on specific\nnetwork branches, particularly between buses 21 and 22. Optimal siting was\nidentified with the load at bus #35 and the generator at bus #39, yielding\nminimal overloads (maximum 91.1% loading). Conversely, poor siting decisions\nresulted in severe congestion with maximum branch loading above 220%. The\nfindings underscore the critical importance of optimized siting and modernized,\nautomated interconnection studies to reduce delays and costs in renewable\nintegration. This research demonstrates the potential of advanced modeling\ntools to accelerate interconnection processes, improve system reliability, and\ninform future strategies for balancing renewable energy deployment with rising\ndata center demand."
                },
                "authors": [
                    {
                        "name": "Kajal Sheth"
                    },
                    {
                        "name": "Dhvanil Patel"
                    },
                    {
                        "name": "Shyam Kareepadath Sajeev"
                    }
                ],
                "author_detail": {
                    "name": "Shyam Kareepadath Sajeev"
                },
                "author": "Shyam Kareepadath Sajeev",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09424v1",
                "updated": "2025-09-11T13:04:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    4,
                    22,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:04:22Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    4,
                    22,
                    3,
                    254,
                    0
                ],
                "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ENSI: Efficient Non-Interactive Secure Inference for Large Language\n  Models"
                },
                "summary": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%."
                },
                "authors": [
                    {
                        "name": "Zhiyu He"
                    },
                    {
                        "name": "Maojiang Wang"
                    },
                    {
                        "name": "Xinwen Gao"
                    },
                    {
                        "name": "Yuchuan Luo"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shaojing Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shaojing Fu"
                },
                "author": "Shaojing Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08461v2",
                "updated": "2025-09-11T13:03:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    3,
                    4,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T10:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    7,
                    27,
                    2,
                    253,
                    0
                ],
                "title": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics."
                },
                "authors": [
                    {
                        "name": "Dikshant Sagar"
                    },
                    {
                        "name": "Kaiwen Yu"
                    },
                    {
                        "name": "Alejandro Yankelevich"
                    },
                    {
                        "name": "Jianming Bian"
                    },
                    {
                        "name": "Pierre Baldi"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Baldi"
                },
                "author": "Pierre Baldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09420v1",
                "updated": "2025-09-11T13:01:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    1,
                    55,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T13:01:55Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    1,
                    55,
                    3,
                    254,
                    0
                ],
                "title": "HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with\n  3D Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with\n  3D Near-Memory Processing"
                },
                "summary": "Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures\nachieve superior model performance with reduced computation costs, but at the\ncost of high memory capacity and bandwidth requirements. Near-Memory Processing\n(NMP) accelerators that stack memory directly on the compute through hybrid\nbonding have demonstrated high bandwidth with high energy efficiency, becoming\na promising architecture for MoE models. However, as NMP accelerators comprise\ndistributed memory and computation, how to map the MoE computation directly\ndetermines the LLM inference efficiency. Existing parallel mapping strategies,\nincluding Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from\neither high communication costs or unbalanced computation utilization, leading\nto inferior efficiency. The dynamic routing mechanism of MoE LLMs further\naggravates the efficiency challenges. Therefore, in this paper, we propose\nHD-MoE to automatically optimize the MoE parallel computation across an NMP\naccelerator. HD-MoE features an offline automatic hybrid parallel mapping\nalgorithm and an online dynamic scheduling strategy to reduce the communication\ncosts while maximizing the computation utilization. With extensive experimental\nresults, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to\n1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid\nTP-EP with Compute-Balanced parallelism strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures\nachieve superior model performance with reduced computation costs, but at the\ncost of high memory capacity and bandwidth requirements. Near-Memory Processing\n(NMP) accelerators that stack memory directly on the compute through hybrid\nbonding have demonstrated high bandwidth with high energy efficiency, becoming\na promising architecture for MoE models. However, as NMP accelerators comprise\ndistributed memory and computation, how to map the MoE computation directly\ndetermines the LLM inference efficiency. Existing parallel mapping strategies,\nincluding Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from\neither high communication costs or unbalanced computation utilization, leading\nto inferior efficiency. The dynamic routing mechanism of MoE LLMs further\naggravates the efficiency challenges. Therefore, in this paper, we propose\nHD-MoE to automatically optimize the MoE parallel computation across an NMP\naccelerator. HD-MoE features an offline automatic hybrid parallel mapping\nalgorithm and an online dynamic scheduling strategy to reduce the communication\ncosts while maximizing the computation utilization. With extensive experimental\nresults, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to\n1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid\nTP-EP with Compute-Balanced parallelism strategies."
                },
                "authors": [
                    {
                        "name": "Haochen Huang"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Shuangchen Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Hongzhong Zheng"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "9 pages, 15 figures, International Conference on Computer-Aided\n  Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09411v1",
                "updated": "2025-09-11T12:46:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    46,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:46:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    46,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna\n  Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna\n  Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?"
                },
                "summary": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Yinghui Ye"
                    },
                    {
                        "name": "Xiaoli Chu"
                    },
                    {
                        "name": "Guangyue Lu"
                    },
                    {
                        "name": "Farshad Rostami Ghadi"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Kit Wong"
                },
                "author": "Kai-Kit Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v7",
                "updated": "2025-09-11T12:43:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    43,
                    54,
                    3,
                    254,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,198\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,198\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09397v1",
                "updated": "2025-09-11T12:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    26,
                    57,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:26:57Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    26,
                    57,
                    3,
                    254,
                    0
                ],
                "title": "Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot\n  Adaptation under Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot\n  Adaptation under Shift"
                },
                "summary": "Medical vision-language models (VLMs) offer promise for clinical decision\nsupport, yet their reliability under distribution shifts remains a major\nconcern for safe deployment. These models often learn task-agnostic\ncorrelations due to variability in imaging protocols and free-text reports,\nlimiting their generalizability and increasing the risk of failure in\nreal-world settings. We propose DRiFt, a structured feature decoupling\nframework that explicitly separates clinically relevant signals from\ntask-agnostic noise using parameter-efficient tuning (LoRA) and learnable\nprompt tokens. To enhance cross-modal alignment and reduce uncertainty, we\ncurate high-quality, clinically grounded image-text pairs by generating\ncaptions for a diverse medical dataset. Our approach improves in-distribution\nperformance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based\nmethods, while maintaining strong robustness across unseen datasets. Ablation\nstudies reveal that disentangling task-relevant features and careful alignment\nsignificantly enhance model generalization and reduce unpredictable behavior\nunder domain shift. These insights contribute toward building safer, more\ntrustworthy VLMs for clinical use. The code is available at\nhttps://github.com/rumaima/DRiFt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical vision-language models (VLMs) offer promise for clinical decision\nsupport, yet their reliability under distribution shifts remains a major\nconcern for safe deployment. These models often learn task-agnostic\ncorrelations due to variability in imaging protocols and free-text reports,\nlimiting their generalizability and increasing the risk of failure in\nreal-world settings. We propose DRiFt, a structured feature decoupling\nframework that explicitly separates clinically relevant signals from\ntask-agnostic noise using parameter-efficient tuning (LoRA) and learnable\nprompt tokens. To enhance cross-modal alignment and reduce uncertainty, we\ncurate high-quality, clinically grounded image-text pairs by generating\ncaptions for a diverse medical dataset. Our approach improves in-distribution\nperformance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based\nmethods, while maintaining strong robustness across unseen datasets. Ablation\nstudies reveal that disentangling task-relevant features and careful alignment\nsignificantly enhance model generalization and reduce unpredictable behavior\nunder domain shift. These insights contribute toward building safer, more\ntrustworthy VLMs for clinical use. The code is available at\nhttps://github.com/rumaima/DRiFt."
                },
                "authors": [
                    {
                        "name": "Umaima Rahman"
                    },
                    {
                        "name": "Raza Imam"
                    },
                    {
                        "name": "Mohammad Yaqub"
                    },
                    {
                        "name": "Dwarikanath Mahapatra"
                    }
                ],
                "author_detail": {
                    "name": "Dwarikanath Mahapatra"
                },
                "author": "Dwarikanath Mahapatra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04227v3",
                "updated": "2025-09-11T12:26:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    26,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2025-02-06T17:12:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks"
                },
                "summary": "Enterprise penetration-testing is often limited by high operational costs and\nthe scarcity of human expertise. This paper investigates the feasibility and\neffectiveness of using Large Language Model (LLM)-driven autonomous systems to\naddress these challenges in real-world Active Directory (AD) enterprise\nnetworks.\n  We introduce a novel prototype designed to employ LLMs to autonomously\nperform Assumed Breach penetration-testing against enterprise networks. Our\nsystem represents the first demonstration of a fully autonomous, LLM-driven\nframework capable of compromising accounts within a real-life Microsoft Active\nDirectory testbed, GOAD.\n  We perform our empirical evaluation using five LLMs, comparing reasoning to\nnon-reasoning models as well as including open-weight models. Through\nquantitative and qualitative analysis, incorporating insights from\ncybersecurity experts, we demonstrate that autonomous LLMs can effectively\nconduct Assumed Breach simulations. Key findings highlight their ability to\ndynamically adapt attack strategies, perform inter-context attacks (e.g.,\nweb-app audits, social engineering, and unstructured data analysis for\ncredentials), and generate scenario-specific attack parameters like realistic\npassword candidates. The prototype exhibits robust self-correction mechanisms,\ninstalling missing tools and rectifying invalid command generations.\n  We find that the associated costs are competitive with, and often\nsignificantly lower than, those incurred by professional human pen-testers,\nsuggesting a path toward democratizing access to essential security testing for\norganizations with budgetary constraints. However, our research also\nilluminates existing limitations, including instances of LLM ``going down\nrabbit holes'', challenges in comprehensive information transfer between\nplanning and execution modules, and critical safety concerns that necessitate\nhuman oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise penetration-testing is often limited by high operational costs and\nthe scarcity of human expertise. This paper investigates the feasibility and\neffectiveness of using Large Language Model (LLM)-driven autonomous systems to\naddress these challenges in real-world Active Directory (AD) enterprise\nnetworks.\n  We introduce a novel prototype designed to employ LLMs to autonomously\nperform Assumed Breach penetration-testing against enterprise networks. Our\nsystem represents the first demonstration of a fully autonomous, LLM-driven\nframework capable of compromising accounts within a real-life Microsoft Active\nDirectory testbed, GOAD.\n  We perform our empirical evaluation using five LLMs, comparing reasoning to\nnon-reasoning models as well as including open-weight models. Through\nquantitative and qualitative analysis, incorporating insights from\ncybersecurity experts, we demonstrate that autonomous LLMs can effectively\nconduct Assumed Breach simulations. Key findings highlight their ability to\ndynamically adapt attack strategies, perform inter-context attacks (e.g.,\nweb-app audits, social engineering, and unstructured data analysis for\ncredentials), and generate scenario-specific attack parameters like realistic\npassword candidates. The prototype exhibits robust self-correction mechanisms,\ninstalling missing tools and rectifying invalid command generations.\n  We find that the associated costs are competitive with, and often\nsignificantly lower than, those incurred by professional human pen-testers,\nsuggesting a path toward democratizing access to essential security testing for\norganizations with budgetary constraints. However, our research also\nilluminates existing limitations, including instances of LLM ``going down\nrabbit holes'', challenges in comprehensive information transfer between\nplanning and execution modules, and critical safety concerns that necessitate\nhuman oversight."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "arxiv_doi": "10.1145/3766895",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3766895",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.04227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09396v1",
                "updated": "2025-09-11T12:25:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    25,
                    41,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:25:41Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    25,
                    41,
                    3,
                    254,
                    0
                ],
                "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations"
                },
                "summary": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs."
                },
                "authors": [
                    {
                        "name": "Harry Mayne"
                    },
                    {
                        "name": "Ryan Othniel Kearns"
                    },
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Eoin Delaney"
                    },
                    {
                        "name": "Chris Russell"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02253v5",
                "updated": "2025-09-11T12:14:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    14,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-03T03:02:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    3,
                    2,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation"
                },
                "summary": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial."
                },
                "authors": [
                    {
                        "name": "Jungkoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jungkoo Kang"
                },
                "author": "Jungkoo Kang",
                "arxiv_comment": "31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09387v1",
                "updated": "2025-09-11T12:06:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    34,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T12:06:34Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    34,
                    3,
                    254,
                    0
                ],
                "title": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization"
                },
                "summary": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines."
                },
                "authors": [
                    {
                        "name": "Mohammed Tiouti"
                    },
                    {
                        "name": "Mohamed Bal-Ghaoui"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Bal-Ghaoui"
                },
                "author": "Mohamed Bal-Ghaoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20655v2",
                "updated": "2025-09-11T12:03:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    3,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-28T11:01:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    1,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Alignment in LVLMs with Debiased Self-Judgment"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Weilong Yan"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03613v3",
                "updated": "2025-09-11T12:00:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    0,
                    44,
                    3,
                    254,
                    0
                ],
                "published": "2024-10-04T17:14:59Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    17,
                    14,
                    59,
                    4,
                    278,
                    0
                ],
                "title": "Understanding Large Language Models in Your Pockets: Performance Study\n  on COTS Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Models in Your Pockets: Performance Study\n  on COTS Mobile Devices"
                },
                "summary": "As large language models (LLMs) increasingly integrate into every aspect of\nour work and daily lives, there are growing concerns about user privacy, which\npush the trend toward local deployment of these models. There are a number of\nlightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on\nsmartphones, providing users with greater control over their personal data. As\na rapidly emerging application, we are concerned about their performance on\ncommercial-off-the-shelf mobile devices. To fully understand the current\nlandscape of LLM deployment on mobile platforms, we conduct a comprehensive\nmeasurement study on mobile devices. We evaluate both metrics that affect user\nexperience, including token throughput, latency, and battery consumption, as\nwell as factors critical to developers, such as resource utilization, DVFS\nstrategies, and inference engines. In addition, we provide a detailed analysis\nof how these hardware capabilities and system dynamics affect on-device LLM\nperformance, which may help developers identify and address bottlenecks for\nmobile LLM applications. We also provide comprehensive comparisons across the\nmobile system-on-chips (SoCs) from major vendors, highlighting their\nperformance differences in handling LLM workloads. We hope that this study can\nprovide insights for both the development of on-device LLMs and the design for\nfuture mobile system architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate into every aspect of\nour work and daily lives, there are growing concerns about user privacy, which\npush the trend toward local deployment of these models. There are a number of\nlightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on\nsmartphones, providing users with greater control over their personal data. As\na rapidly emerging application, we are concerned about their performance on\ncommercial-off-the-shelf mobile devices. To fully understand the current\nlandscape of LLM deployment on mobile platforms, we conduct a comprehensive\nmeasurement study on mobile devices. We evaluate both metrics that affect user\nexperience, including token throughput, latency, and battery consumption, as\nwell as factors critical to developers, such as resource utilization, DVFS\nstrategies, and inference engines. In addition, we provide a detailed analysis\nof how these hardware capabilities and system dynamics affect on-device LLM\nperformance, which may help developers identify and address bottlenecks for\nmobile LLM applications. We also provide comprehensive comparisons across the\nmobile system-on-chips (SoCs) from major vendors, highlighting their\nperformance differences in handling LLM workloads. We hope that this study can\nprovide insights for both the development of on-device LLMs and the design for\nfuture mobile system architecture."
                },
                "authors": [
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Qianyi Huang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09360v1",
                "updated": "2025-09-11T11:18:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    18,
                    23,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T11:18:23Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    18,
                    23,
                    3,
                    254,
                    0
                ],
                "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments."
                },
                "authors": [
                    {
                        "name": "Channdeth Sok"
                    },
                    {
                        "name": "David Luz"
                    },
                    {
                        "name": "Yacine Haddam"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Haddam"
                },
                "author": "Yacine Haddam",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08538v2",
                "updated": "2025-09-11T11:14:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    14,
                    0,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T12:34:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    34,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models"
                },
                "summary": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos."
                },
                "authors": [
                    {
                        "name": "Garry Yang"
                    },
                    {
                        "name": "Zizhe Chen"
                    },
                    {
                        "name": "Man Hon Wong"
                    },
                    {
                        "name": "Haoyu Lei"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09343v1",
                "updated": "2025-09-11T10:57:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    57,
                    52,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:57:52Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    57,
                    52,
                    3,
                    254,
                    0
                ],
                "title": "Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN\n  Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN\n  Deployments"
                },
                "summary": "Open Radio Access Network (O-RAN) architecture provides an intrinsic\ncapability to exploit key performance monitoring (KPM) within Radio\nIntelligence Controller (RIC) to derive network optimisation through xApps.\nThese xApps can leverage KPM knowledge to dynamically switch on/off the\nassociated RUs where such a function is supported over the E2 interface.\nSeveral existing studies employ artificial intelligence (AI)/Machine Learning\n(ML) based approaches to realise such dynamic sleeping for increased energy\nefficiency (EE). Nevertheless, most of these approaches rely upon offloading\nuser equipment (UE) to carve out a sleeping opportunity. Such an approach\ninherently creates load imbalance across the network. Such load imbalance may\nimpact the throughput performance of offloaded UEs as they might be allocated a\nlower number of physical resource blocks (PRBs). Maintaining the same PRB\nallocation while addressing the EE at the network level is a challenging task.\nTo that end, in this article, we present a comprehensive ML-based framework for\njoint optimisation of load balancing and EE for ORAN deployments. We formulate\nthe problem as a multi-class classification system that predictively evaluates\npotential RU configurations before optimising the EE, mapping network\nconditions to three load balance categories (Well Balanced, Moderately\nBalanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,\nAggressive) accommodates different operational priorities between energy\nsavings and performance assurance. Experimental evaluation using 4.26 million\nreal network measurements from simulations demonstrates that our Random Forest\nmodel achieves 98.3% F1-macro performance, representing 195% improvement over\ntraditional baseline strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Radio Access Network (O-RAN) architecture provides an intrinsic\ncapability to exploit key performance monitoring (KPM) within Radio\nIntelligence Controller (RIC) to derive network optimisation through xApps.\nThese xApps can leverage KPM knowledge to dynamically switch on/off the\nassociated RUs where such a function is supported over the E2 interface.\nSeveral existing studies employ artificial intelligence (AI)/Machine Learning\n(ML) based approaches to realise such dynamic sleeping for increased energy\nefficiency (EE). Nevertheless, most of these approaches rely upon offloading\nuser equipment (UE) to carve out a sleeping opportunity. Such an approach\ninherently creates load imbalance across the network. Such load imbalance may\nimpact the throughput performance of offloaded UEs as they might be allocated a\nlower number of physical resource blocks (PRBs). Maintaining the same PRB\nallocation while addressing the EE at the network level is a challenging task.\nTo that end, in this article, we present a comprehensive ML-based framework for\njoint optimisation of load balancing and EE for ORAN deployments. We formulate\nthe problem as a multi-class classification system that predictively evaluates\npotential RU configurations before optimising the EE, mapping network\nconditions to three load balance categories (Well Balanced, Moderately\nBalanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,\nAggressive) accommodates different operational priorities between energy\nsavings and performance assurance. Experimental evaluation using 4.26 million\nreal network measurements from simulations demonstrates that our Random Forest\nmodel achieves 98.3% F1-macro performance, representing 195% improvement over\ntraditional baseline strategies."
                },
                "authors": [
                    {
                        "name": "Mohammed M. H. Qazzaz"
                    },
                    {
                        "name": "Abdelaziz Salama"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed A. R. Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed A. R. Zaidi"
                },
                "author": "Syed A. R. Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12932v2",
                "updated": "2025-09-11T10:20:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    11,
                    3,
                    254,
                    0
                ],
                "published": "2025-02-18T15:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    14,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Culturally-Nuanced Story Generation for Reasoning in Low-Resource\n  Languages: The Case of Javanese and Sundanese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culturally-Nuanced Story Generation for Reasoning in Low-Resource\n  Languages: The Case of Javanese and Sundanese"
                },
                "summary": "Culturally grounded commonsense reasoning is underexplored in low-resource\nlanguages due to scarce data and costly native annotation. We test whether\nlarge language models (LLMs) can generate culturally nuanced narratives for\nsuch settings. Focusing on Javanese and Sundanese, we compare three data\ncreation strategies: (1) LLM-assisted stories prompted with cultural cues, (2)\nmachine translation from Indonesian benchmarks, and (3) native-written stories.\nHuman evaluation finds LLM stories match natives on cultural fidelity but lag\nin coherence and correctness. We fine-tune models on each dataset and evaluate\non a human-authored test set for classification and generation. LLM-generated\ndata yields higher downstream performance than machine-translated and\nIndonesian human-authored training data. We release a high-quality benchmark of\nculturally grounded commonsense stories in Javanese and Sundanese to support\nfuture work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culturally grounded commonsense reasoning is underexplored in low-resource\nlanguages due to scarce data and costly native annotation. We test whether\nlarge language models (LLMs) can generate culturally nuanced narratives for\nsuch settings. Focusing on Javanese and Sundanese, we compare three data\ncreation strategies: (1) LLM-assisted stories prompted with cultural cues, (2)\nmachine translation from Indonesian benchmarks, and (3) native-written stories.\nHuman evaluation finds LLM stories match natives on cultural fidelity but lag\nin coherence and correctness. We fine-tune models on each dataset and evaluate\non a human-authored test set for classification and generation. LLM-generated\ndata yields higher downstream performance than machine-translated and\nIndonesian human-authored training data. We release a high-quality benchmark of\nculturally grounded commonsense stories in Javanese and Sundanese to support\nfuture work."
                },
                "authors": [
                    {
                        "name": "Salsabila Zahirah Pranida"
                    },
                    {
                        "name": "Rifo Ahmad Genadi"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08302v2",
                "updated": "2025-09-11T10:17:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    17,
                    6,
                    3,
                    254,
                    0
                ],
                "published": "2024-11-13T02:45:21Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    2,
                    45,
                    21,
                    2,
                    318,
                    0
                ],
                "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward\n  Redistribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward\n  Redistribution"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) offers a promising approach\nto aligning large language models (LLMs) with human preferences. Typically, a\nreward model is trained or supplied to act as a proxy for humans in evaluating\ngenerated responses during the reinforcement training phase. However, current\nreward models operate as sequence-to-one models, allocating a single, sparse,\nand delayed reward to an entire output sequence. This approach may overlook the\nsignificant contributions of individual tokens toward the desired outcome. To\nthis end, we propose a more fine-grained, token-level guidance approach for RL\ntraining. Specifically, we introduce RED, a novel reward redistribition method\nthat evaluates and assigns specific credit to each token using an off-the-shelf\nreward model. Utilizing these fine-grained rewards enhances the model's\nunderstanding of language nuances, leading to more precise performance\nimprovements. Notably, our method does not require modifying the reward model\nor introducing additional training steps, thereby incurring minimal\ncomputational costs. Experimental results across diverse datasets and tasks\ndemonstrate the superiority of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) offers a promising approach\nto aligning large language models (LLMs) with human preferences. Typically, a\nreward model is trained or supplied to act as a proxy for humans in evaluating\ngenerated responses during the reinforcement training phase. However, current\nreward models operate as sequence-to-one models, allocating a single, sparse,\nand delayed reward to an entire output sequence. This approach may overlook the\nsignificant contributions of individual tokens toward the desired outcome. To\nthis end, we propose a more fine-grained, token-level guidance approach for RL\ntraining. Specifically, we introduce RED, a novel reward redistribition method\nthat evaluates and assigns specific credit to each token using an off-the-shelf\nreward model. Utilizing these fine-grained rewards enhances the model's\nunderstanding of language nuances, leading to more precise performance\nimprovements. Notably, our method does not require modifying the reward model\nor introducing additional training steps, thereby incurring minimal\ncomputational costs. Experimental results across diverse datasets and tasks\ndemonstrate the superiority of our approach."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Tai-wei Chang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09322v1",
                "updated": "2025-09-11T10:12:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    12,
                    56,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:12:56Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    12,
                    56,
                    3,
                    254,
                    0
                ],
                "title": "ORCA: Unveiling Obscure Containers In The Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORCA: Unveiling Obscure Containers In The Wild"
                },
                "summary": "Modern software development increasingly depends on open-source libraries and\nthird-party components, which are often encapsulated into containerized\nenvironments. While improving the development and deployment of applications,\nthis approach introduces security risks, particularly when outdated or\nvulnerable components are inadvertently included in production environments.\nSoftware Composition Analysis (SCA) is a critical process that helps identify\nand manage packages and dependencies inside a container. However, unintentional\nmodifications to the container filesystem can lead to incomplete container\nimages, which compromise the reliability of SCA tools. In this paper, we\nexamine the limitations of both cloud-based and open-source SCA tools when\nfaced with such obscure images. An analysis of 600 popular containers revealed\nthat obscure containers exist in well-known registries and trusted images and\nthat many tools fail to analyze such containers. To mitigate these issues, we\npropose an obscuration-resilient methodology for container analysis and\nintroduce ORCA (Obscuration-Resilient Container Analyzer), its open-source\nimplementation. We reported our findings to all vendors using their appropriate\nchannels. Our results demonstrate that ORCA effectively detects the content of\nobscure containers and achieves a median 40% improvement in file coverage\ncompared to Docker Scout and Syft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software development increasingly depends on open-source libraries and\nthird-party components, which are often encapsulated into containerized\nenvironments. While improving the development and deployment of applications,\nthis approach introduces security risks, particularly when outdated or\nvulnerable components are inadvertently included in production environments.\nSoftware Composition Analysis (SCA) is a critical process that helps identify\nand manage packages and dependencies inside a container. However, unintentional\nmodifications to the container filesystem can lead to incomplete container\nimages, which compromise the reliability of SCA tools. In this paper, we\nexamine the limitations of both cloud-based and open-source SCA tools when\nfaced with such obscure images. An analysis of 600 popular containers revealed\nthat obscure containers exist in well-known registries and trusted images and\nthat many tools fail to analyze such containers. To mitigate these issues, we\npropose an obscuration-resilient methodology for container analysis and\nintroduce ORCA (Obscuration-Resilient Container Analyzer), its open-source\nimplementation. We reported our findings to all vendors using their appropriate\nchannels. Our results demonstrate that ORCA effectively detects the content of\nobscure containers and achieves a median 40% improvement in file coverage\ncompared to Docker Scout and Syft."
                },
                "authors": [
                    {
                        "name": "Jacopo Bufalino"
                    },
                    {
                        "name": "Agathe Blaise"
                    },
                    {
                        "name": "Stefano Secci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Secci"
                },
                "author": "Stefano Secci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09321v1",
                "updated": "2025-09-11T10:10:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    10,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T10:10:48Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    10,
                    48,
                    3,
                    254,
                    0
                ],
                "title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain\n  Expansion, and Metric Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain\n  Expansion, and Metric Optimization"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled the emergence of\ngeneral-purpose agents for automating end-to-end machine learning (ML)\nworkflows, including data analysis, feature engineering, model training, and\ncompetition solving. However, existing benchmarks remain limited in task\ncoverage, domain diversity, difficulty modeling, and evaluation rigor, failing\nto capture the full capabilities of such agents in realistic settings. We\npresent TAM Bench, a diverse, realistic, and structured benchmark for\nevaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three\nkey innovations: (1) A browser automation and LLM-based task acquisition system\nthat automatically collects and structures ML challenges from platforms such as\nKaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities\n(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty\nmodeling mechanism that estimates task complexity using participant counts and\nscore dispersion, enabling scalable and objective task calibration; (3) A\nmulti-dimensional evaluation framework incorporating performance, format\ncompliance, constraint adherence, and task generalization. Based on 150 curated\nAutoML tasks, we construct three benchmark subsets of different sizes -- Lite,\nMedium, and Full -- designed for varying evaluation scenarios. The Lite\nversion, with 18 tasks and balanced coverage across modalities and difficulty\nlevels, serves as a practical testbed for daily benchmarking and comparative\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled the emergence of\ngeneral-purpose agents for automating end-to-end machine learning (ML)\nworkflows, including data analysis, feature engineering, model training, and\ncompetition solving. However, existing benchmarks remain limited in task\ncoverage, domain diversity, difficulty modeling, and evaluation rigor, failing\nto capture the full capabilities of such agents in realistic settings. We\npresent TAM Bench, a diverse, realistic, and structured benchmark for\nevaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three\nkey innovations: (1) A browser automation and LLM-based task acquisition system\nthat automatically collects and structures ML challenges from platforms such as\nKaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities\n(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty\nmodeling mechanism that estimates task complexity using participant counts and\nscore dispersion, enabling scalable and objective task calibration; (3) A\nmulti-dimensional evaluation framework incorporating performance, format\ncompliance, constraint adherence, and task generalization. Based on 150 curated\nAutoML tasks, we construct three benchmark subsets of different sizes -- Lite,\nMedium, and Full -- designed for varying evaluation scenarios. The Lite\nversion, with 18 tasks and balanced coverage across modalities and difficulty\nlevels, serves as a practical testbed for daily benchmarking and comparative\nstudies."
                },
                "authors": [
                    {
                        "name": "Hangyi Jia"
                    },
                    {
                        "name": "Yuxi Qian"
                    },
                    {
                        "name": "Hanwen Tong"
                    },
                    {
                        "name": "Xinhui Wu"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Feng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wei"
                },
                "author": "Feng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09313v1",
                "updated": "2025-09-11T09:58:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    58,
                    43,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:58:43Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    58,
                    43,
                    3,
                    254,
                    0
                ],
                "title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on\n  Open & Industry Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on\n  Open & Industry Data"
                },
                "summary": "Deep learning solutions for vulnerability detection proposed in academic\nresearch are not always accessible to developers, and their applicability in\nindustrial settings is rarely addressed. Transferring such technologies from\nacademia to industry presents challenges related to trustworthiness, legacy\nsystems, limited digital literacy, and the gap between academic and industrial\nexpertise. For deep learning in particular, performance and integration into\nexisting workflows are additional concerns. In this work, we first evaluate the\nperformance of CodeBERT for detecting vulnerable functions in industrial and\nopen-source software. We analyse its cross-domain generalisation when\nfine-tuned on open-source data and tested on industrial data, and vice versa,\nalso exploring strategies for handling class imbalance. Based on these results,\nwe develop AI-DO(Automating vulnerability detection Integration for Developers'\nOperations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated\nrecommender system that uses fine-tuned CodeBERT to detect and localise\nvulnerabilities during code review without disrupting workflows. Finally, we\nassess the tool's perceived usefulness through a survey with the company's IT\nprofessionals. Our results show that models trained on industrial data detect\nvulnerabilities accurately within the same domain but lose performance on\nopen-source code, while a deep learner fine-tuned on open data, with\nappropriate undersampling techniques, improves the detection of\nvulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning solutions for vulnerability detection proposed in academic\nresearch are not always accessible to developers, and their applicability in\nindustrial settings is rarely addressed. Transferring such technologies from\nacademia to industry presents challenges related to trustworthiness, legacy\nsystems, limited digital literacy, and the gap between academic and industrial\nexpertise. For deep learning in particular, performance and integration into\nexisting workflows are additional concerns. In this work, we first evaluate the\nperformance of CodeBERT for detecting vulnerable functions in industrial and\nopen-source software. We analyse its cross-domain generalisation when\nfine-tuned on open-source data and tested on industrial data, and vice versa,\nalso exploring strategies for handling class imbalance. Based on these results,\nwe develop AI-DO(Automating vulnerability detection Integration for Developers'\nOperations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated\nrecommender system that uses fine-tuned CodeBERT to detect and localise\nvulnerabilities during code review without disrupting workflows. Finally, we\nassess the tool's perceived usefulness through a survey with the company's IT\nprofessionals. Our results show that models trained on industrial data detect\nvulnerabilities accurately within the same domain but lose performance on\nopen-source code, while a deep learner fine-tuned on open data, with\nappropriate undersampling techniques, improves the detection of\nvulnerabilities."
                },
                "authors": [
                    {
                        "name": "Moritz Mock"
                    },
                    {
                        "name": "Thomas Forrer"
                    },
                    {
                        "name": "Barbara Russo"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Russo"
                },
                "author": "Barbara Russo",
                "arxiv_comment": "Accepted to the 26th International Conference on Product-Focused\n  Software Process Improvement (PROFES 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09307v1",
                "updated": "2025-09-11T09:50:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    50,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:50:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    50,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization"
                },
                "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha."
                },
                "authors": [
                    {
                        "name": "Zhengzhao Lai"
                    },
                    {
                        "name": "Youbin Zheng"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Haonan Lyu"
                    },
                    {
                        "name": "Jinpu Yang"
                    },
                    {
                        "name": "Hongqing Liang"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09306v1",
                "updated": "2025-09-11T09:48:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    48,
                    9,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:48:09Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    48,
                    9,
                    3,
                    254,
                    0
                ],
                "title": "Listening for \"You\": Enhancing Speech Image Retrieval via Target Speaker\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Listening for \"You\": Enhancing Speech Image Retrieval via Target Speaker\n  Extraction"
                },
                "summary": "Image retrieval using spoken language cues has emerged as a promising\ndirection in multimodal perception, yet leveraging speech in multi-speaker\nscenarios remains challenging. We propose a novel Target Speaker Speech-Image\nRetrieval task and a framework that learns the relationship between images and\nmulti-speaker speech signals in the presence of a target speaker. Our method\nintegrates pre-trained self-supervised audio encoders with vision models via\ntarget speaker-aware contrastive learning, conditioned on a Target Speaker\nExtraction and Retrieval module. This enables the system to extract spoken\ncommands from the target speaker and align them with corresponding images.\nExperiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantly\noutperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3\nspeaker scenarios, respectively - substantial improvements over single speaker\nbaselines and state-of-the-art models. Our approach demonstrates potential for\nreal-world deployment in assistive robotics and multimodal interaction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image retrieval using spoken language cues has emerged as a promising\ndirection in multimodal perception, yet leveraging speech in multi-speaker\nscenarios remains challenging. We propose a novel Target Speaker Speech-Image\nRetrieval task and a framework that learns the relationship between images and\nmulti-speaker speech signals in the presence of a target speaker. Our method\nintegrates pre-trained self-supervised audio encoders with vision models via\ntarget speaker-aware contrastive learning, conditioned on a Target Speaker\nExtraction and Retrieval module. This enables the system to extract spoken\ncommands from the target speaker and align them with corresponding images.\nExperiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantly\noutperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3\nspeaker scenarios, respectively - substantial improvements over single speaker\nbaselines and state-of-the-art models. Our approach demonstrates potential for\nreal-world deployment in assistive robotics and multimodal interaction systems."
                },
                "authors": [
                    {
                        "name": "Wenhao Yang"
                    },
                    {
                        "name": "Jianguo Wei"
                    },
                    {
                        "name": "Wenhuan Lu"
                    },
                    {
                        "name": "Xinyue Song"
                    },
                    {
                        "name": "Xianghu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xianghu Yue"
                },
                "author": "Xianghu Yue",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09303v1",
                "updated": "2025-09-11T09:44:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    44,
                    16,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:44:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    44,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "From scratch to silver: Creating trustworthy training data for\n  patent-SDG classification using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From scratch to silver: Creating trustworthy training data for\n  patent-SDG classification using Large Language Models"
                },
                "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale."
                },
                "authors": [
                    {
                        "name": "Grazia Sveva Ascione"
                    },
                    {
                        "name": "Nicolò Tamagnone"
                    }
                ],
                "author_detail": {
                    "name": "Nicolò Tamagnone"
                },
                "author": "Nicolò Tamagnone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09299v1",
                "updated": "2025-09-11T09:43:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    43,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:43:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    43,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Towards Efficient and Secure Cloud Control Systems: Advances,\n  Challenges, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient and Secure Cloud Control Systems: Advances,\n  Challenges, and Future Directions"
                },
                "summary": "Networked Control Systems (NCSs) have been instrumental in realizing fully\nconnected and responsive intelligent environments within the context of\nreal-time virtual control and management. However, traditional NCSs face\nconsiderable challenges in handling the vast amounts of data generated by\nlarge-scale control applications, particularly in terms of data acquisition,\nstorage, and computational processing. To address these challenges, the\nemergence of cloud computing and advancements in control theory have empowered\nthe new paradigm known as Cloud Control Systems (CCSs). Recently, CCSs have\nreceived substantial attention from industries for their potential properties,\nsuch as large-scale data management, complex computations, and data-centric\noptimized decisions. This study presents an extensive review of recent progress\nin CCSs spanning over multiple studies published between 2012 and 2025.\nSpecifically, the focus is on providing a taxonomy of the current findings in\nCCS research, encompassing various perspectives, such as its efficient\nimplementations in industrial automation, security and privacy considerations,\nand cloud-based control techniques. Each category is examined in depth through\nselected state-of-the-art analyses of different approaches and contrasting\nmethodologies. Furthermore, we discuss future directions aimed at designing\nmore efficient and practical CCSs. The insights gained from this study can help\nresearchers, practitioners, and decision-makers in their domain for effective\nCCS design and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networked Control Systems (NCSs) have been instrumental in realizing fully\nconnected and responsive intelligent environments within the context of\nreal-time virtual control and management. However, traditional NCSs face\nconsiderable challenges in handling the vast amounts of data generated by\nlarge-scale control applications, particularly in terms of data acquisition,\nstorage, and computational processing. To address these challenges, the\nemergence of cloud computing and advancements in control theory have empowered\nthe new paradigm known as Cloud Control Systems (CCSs). Recently, CCSs have\nreceived substantial attention from industries for their potential properties,\nsuch as large-scale data management, complex computations, and data-centric\noptimized decisions. This study presents an extensive review of recent progress\nin CCSs spanning over multiple studies published between 2012 and 2025.\nSpecifically, the focus is on providing a taxonomy of the current findings in\nCCS research, encompassing various perspectives, such as its efficient\nimplementations in industrial automation, security and privacy considerations,\nand cloud-based control techniques. Each category is examined in depth through\nselected state-of-the-art analyses of different approaches and contrasting\nmethodologies. Furthermore, we discuss future directions aimed at designing\nmore efficient and practical CCSs. The insights gained from this study can help\nresearchers, practitioners, and decision-makers in their domain for effective\nCCS design and deployment."
                },
                "authors": [
                    {
                        "name": "Yasir Ali"
                    },
                    {
                        "name": "Tayyab Manzoor"
                    },
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Asif Ali"
                    },
                    {
                        "name": "Yuanqing Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yuanqing Xia"
                },
                "author": "Yuanqing Xia",
                "arxiv_comment": "42 pages, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07370v2",
                "updated": "2025-09-11T09:42:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    42,
                    2,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-09T03:39:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    3,
                    39,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing\n  Human-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing\n  Human-LLM Interactions"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that PersonaFuse\noffers a theoretically grounded and practical approach for developing\nsocial-emotional enhanced LLMs, marking a significant advancement toward more\nhuman-centric AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that PersonaFuse\noffers a theoretically grounded and practical approach for developing\nsocial-emotional enhanced LLMs, marking a significant advancement toward more\nhuman-centric AI systems."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Ahmed Abbasi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Abbasi"
                },
                "author": "Ahmed Abbasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v3",
                "updated": "2025-09-11T09:37:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    37,
                    46,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09292v1",
                "updated": "2025-09-11T09:29:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    29,
                    13,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:29:13Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    29,
                    13,
                    3,
                    254,
                    0
                ],
                "title": "LightAgent: Production-level Open-source Agentic AI Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightAgent: Production-level Open-source Agentic AI Framework"
                },
                "summary": "With the rapid advancement of large language models (LLMs), Multi-agent\nSystems (MAS) have achieved significant progress in various application\nscenarios. However, substantial challenges remain in designing versatile,\nrobust, and efficient platforms for agent deployment. To address these\nlimitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic\nframework, effectively resolving the trade-off between flexibility and\nsimplicity found in existing frameworks. LightAgent integrates core\nfunctionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while\nmaintaining an extremely lightweight structure. As a fully open-source\nsolution, it seamlessly integrates with mainstream chat platforms, enabling\ndevelopers to easily build self-learning agents. We have released LightAgent at\n\\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), Multi-agent\nSystems (MAS) have achieved significant progress in various application\nscenarios. However, substantial challenges remain in designing versatile,\nrobust, and efficient platforms for agent deployment. To address these\nlimitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic\nframework, effectively resolving the trade-off between flexibility and\nsimplicity found in existing frameworks. LightAgent integrates core\nfunctionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while\nmaintaining an extremely lightweight structure. As a fully open-source\nsolution, it seamlessly integrates with mainstream chat platforms, enabling\ndevelopers to easily build self-learning agents. We have released LightAgent at\n\\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}"
                },
                "authors": [
                    {
                        "name": "Weige Cai"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Jinyi Niu"
                    },
                    {
                        "name": "Ruiqi Hu"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Tenglong Wang"
                    },
                    {
                        "name": "Xiaowu Dai"
                    },
                    {
                        "name": "Weining Shen"
                    },
                    {
                        "name": "Liwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liwen Zhang"
                },
                "author": "Liwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09291v1",
                "updated": "2025-09-11T09:27:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    27,
                    37,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:27:37Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    27,
                    37,
                    3,
                    254,
                    0
                ],
                "title": "What You Code Is What We Prove: Translating BLE App Logic into Formal\n  Models with LLMs for Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You Code Is What We Prove: Translating BLE App Logic into Formal\n  Models with LLMs for Vulnerability Detection"
                },
                "summary": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains."
                },
                "authors": [
                    {
                        "name": "Biwei Yan"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Runyu Pan"
                    },
                    {
                        "name": "Jinku Li"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09284v1",
                "updated": "2025-09-11T09:18:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    18,
                    7,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:18:07Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    18,
                    7,
                    3,
                    254,
                    0
                ],
                "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning"
                },
                "summary": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures."
                },
                "authors": [
                    {
                        "name": "Bingning Huang"
                    },
                    {
                        "name": "Tu Nguyen"
                    },
                    {
                        "name": "Matthieu Zimmer"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Zimmer"
                },
                "author": "Matthieu Zimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09283v1",
                "updated": "2025-09-11T09:17:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    17,
                    50,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:17:50Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    17,
                    50,
                    3,
                    254,
                    0
                ],
                "title": "RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant\n  Estimator Networks under Visual Collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant\n  Estimator Networks under Visual Collapse"
                },
                "summary": "Vision-based locomotion in outdoor environments presents significant\nchallenges for quadruped robots. Accurate environmental prediction and\neffective handling of depth sensor noise during real-world deployment remain\ndifficult, severely restricting the outdoor applications of such algorithms. To\naddress these deployment challenges in vision-based motion control, this letter\nproposes the Redundant Estimator Network (RENet) framework. The framework\nemploys a dual-estimator architecture that ensures robust motion performance\nwhile maintaining deployment stability during onboard vision failures. Through\nan online estimator adaptation, our method enables seamless transitions between\nestimation modules when handling visual perception uncertainties. Experimental\nvalidation on a real-world robot demonstrates the framework's effectiveness in\ncomplex outdoor environments, showing particular advantages in scenarios with\ndegraded visual perception. This framework demonstrates its potential as a\npractical solution for reliable robotic deployment in challenging field\nconditions. Project website: https://RENet-Loco.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-based locomotion in outdoor environments presents significant\nchallenges for quadruped robots. Accurate environmental prediction and\neffective handling of depth sensor noise during real-world deployment remain\ndifficult, severely restricting the outdoor applications of such algorithms. To\naddress these deployment challenges in vision-based motion control, this letter\nproposes the Redundant Estimator Network (RENet) framework. The framework\nemploys a dual-estimator architecture that ensures robust motion performance\nwhile maintaining deployment stability during onboard vision failures. Through\nan online estimator adaptation, our method enables seamless transitions between\nestimation modules when handling visual perception uncertainties. Experimental\nvalidation on a real-world robot demonstrates the framework's effectiveness in\ncomplex outdoor environments, showing particular advantages in scenarios with\ndegraded visual perception. This framework demonstrates its potential as a\npractical solution for reliable robotic deployment in challenging field\nconditions. Project website: https://RENet-Loco.github.io/"
                },
                "authors": [
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Quancheng Qian"
                    },
                    {
                        "name": "Taixian Hou"
                    },
                    {
                        "name": "Peng Zhai"
                    },
                    {
                        "name": "Xiaoyi Wei"
                    },
                    {
                        "name": "Kangmai Hu"
                    },
                    {
                        "name": "Jiafu Yi"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_doi": "10.1109/LRA.2025.3608633",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3608633",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for IEEE Robotics and Automation Letters (RA-L)",
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters (2025)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09272v1",
                "updated": "2025-09-11T09:02:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    2,
                    15,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T09:02:15Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    2,
                    15,
                    3,
                    254,
                    0
                ],
                "title": "Fusing Knowledge and Language: A Comparative Study of Knowledge\n  Graph-Based Question Answering with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Knowledge and Language: A Comparative Study of Knowledge\n  Graph-Based Question Answering with LLMs"
                },
                "summary": "Knowledge graphs, a powerful tool for structuring information through\nrelational triplets, have recently become the new front-runner in enhancing\nquestion-answering systems. While traditional Retrieval Augmented Generation\n(RAG) approaches are proficient in fact-based and local context-based\nextraction from concise texts, they encounter limitations when addressing the\nthematic and holistic understanding of complex, extensive texts, requiring a\ndeeper analysis of both text and context. This paper presents a comprehensive\ntechnical comparative study of three different methodologies for constructing\nknowledge graph triplets and integrating them with Large Language Models (LLMs)\nfor question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all\nleveraging open source technologies. We evaluate the effectiveness,\nfeasibility, and adaptability of these methods by analyzing their capabilities,\nstate of development, and their impact on the performance of LLM-based question\nanswering. Experimental results indicate that while OpenIE provides the most\ncomprehensive coverage of triplets, GraphRAG demonstrates superior reasoning\nabilities among the three. We conclude with a discussion on the strengths and\nlimitations of each method and provide insights into future directions for\nimproving knowledge graph-based question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs, a powerful tool for structuring information through\nrelational triplets, have recently become the new front-runner in enhancing\nquestion-answering systems. While traditional Retrieval Augmented Generation\n(RAG) approaches are proficient in fact-based and local context-based\nextraction from concise texts, they encounter limitations when addressing the\nthematic and holistic understanding of complex, extensive texts, requiring a\ndeeper analysis of both text and context. This paper presents a comprehensive\ntechnical comparative study of three different methodologies for constructing\nknowledge graph triplets and integrating them with Large Language Models (LLMs)\nfor question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all\nleveraging open source technologies. We evaluate the effectiveness,\nfeasibility, and adaptability of these methods by analyzing their capabilities,\nstate of development, and their impact on the performance of LLM-based question\nanswering. Experimental results indicate that while OpenIE provides the most\ncomprehensive coverage of triplets, GraphRAG demonstrates superior reasoning\nabilities among the three. We conclude with a discussion on the strengths and\nlimitations of each method and provide insights into future directions for\nimproving knowledge graph-based question answering."
                },
                "authors": [
                    {
                        "name": "Vaibhav Chaudhary"
                    },
                    {
                        "name": "Neha Soni"
                    },
                    {
                        "name": "Narotam Singh"
                    },
                    {
                        "name": "Amita Kapoor"
                    }
                ],
                "author_detail": {
                    "name": "Amita Kapoor"
                },
                "author": "Amita Kapoor",
                "arxiv_comment": "46 pages, 4 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09267v1",
                "updated": "2025-09-11T08:53:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    53,
                    37,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T08:53:37Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    53,
                    37,
                    3,
                    254,
                    0
                ],
                "title": "Unified Start, Personalized End: Progressive Pruning for Efficient 3D\n  Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Start, Personalized End: Progressive Pruning for Efficient 3D\n  Medical Image Segmentation"
                },
                "summary": "3D medical image segmentation often faces heavy resource and time\nconsumption, limiting its scalability and rapid deployment in clinical\nenvironments. Existing efficient segmentation models are typically static and\nmanually designed prior to training, which restricts their adaptability across\ndiverse tasks and makes it difficult to balance performance with resource\nefficiency. In this paper, we propose PSP-Seg, a progressive pruning framework\nthat enables dynamic and efficient 3D segmentation. PSP-Seg begins with a\nredundant model and iteratively prunes redundant modules through a combination\nof block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on\nfive public datasets, benchmarking it against seven state-of-the-art models and\nsix efficient segmentation models. Results demonstrate that the lightweight\nvariant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU\nmemory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%\nacross all datasets. These findings underscore PSP-Seg's potential as a\ncost-effective yet high-performing alternative for widespread clinical\napplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D medical image segmentation often faces heavy resource and time\nconsumption, limiting its scalability and rapid deployment in clinical\nenvironments. Existing efficient segmentation models are typically static and\nmanually designed prior to training, which restricts their adaptability across\ndiverse tasks and makes it difficult to balance performance with resource\nefficiency. In this paper, we propose PSP-Seg, a progressive pruning framework\nthat enables dynamic and efficient 3D segmentation. PSP-Seg begins with a\nredundant model and iteratively prunes redundant modules through a combination\nof block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on\nfive public datasets, benchmarking it against seven state-of-the-art models and\nsix efficient segmentation models. Results demonstrate that the lightweight\nvariant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU\nmemory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%\nacross all datasets. These findings underscore PSP-Seg's potential as a\ncost-effective yet high-performing alternative for widespread clinical\napplication."
                },
                "authors": [
                    {
                        "name": "Linhao Li"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09265v1",
                "updated": "2025-09-11T08:50:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    50,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T08:50:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    50,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents"
                },
                "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/"
                },
                "authors": [
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Jiacai Liu"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Yingru Li"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Yu Yue"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Ke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Wang"
                },
                "author": "Ke Wang",
                "arxiv_comment": "ICLR 2026 Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17850v3",
                "updated": "2025-09-11T08:48:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    48,
                    37,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-25T09:57:35Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    9,
                    57,
                    35,
                    0,
                    237,
                    0
                ],
                "title": "Group Expectation Policy Optimization for Heterogeneous Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Expectation Policy Optimization for Heterogeneous Reinforcement\n  Learning"
                },
                "summary": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As single-center computing approaches power constraints, decentralized\ntraining is becoming essential. Reinforcement Learning (RL) post-training\nenhances Large Language Models (LLMs) but faces challenges in heterogeneous\ndistributed environments due to its tightly-coupled sampling-learning\nalternation. We propose HeteroRL, an asynchronous RL architecture that\ndecouples rollout sampling from parameter learning, enabling robust deployment\nacross geographically distributed nodes under network delays. We identify that\nlatency-induced KL divergence causes importance sampling failure due to high\nvariance. To address this, we propose Group Expectation Policy Optimization\n(GEPO), which reduces importance weight variance through a refined sampling\nmechanism. Theoretically, GEPO achieves exponential variance reduction.\nExperiments show it maintains superior stability over methods like GRPO, with\nless than 3% performance degradation under 1800-second delays, demonstrating\nstrong potential for decentralized RL in heterogeneous networks."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Ruibin Zheng"
                    },
                    {
                        "name": "Zexuan Yi"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Hanyang Peng"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zike Yuan"
                    },
                    {
                        "name": "Cai Ke"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Jiangyue Yan"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Liwen Jing"
                    },
                    {
                        "name": "Jiayin Qi"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Binxing Fang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09245v1",
                "updated": "2025-09-11T08:27:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    27,
                    54,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T08:27:54Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    27,
                    54,
                    3,
                    254,
                    0
                ],
                "title": "Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and\n  Inference-Time Value-Guided Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and\n  Inference-Time Value-Guided Search"
                },
                "summary": "Large language models (LLMs) have shown great promise in automating data\nscience workflows, but existing models still struggle with multi-step reasoning\nand tool use, which limits their effectiveness on complex data analysis tasks.\nTo address this, we propose a scalable pipeline that extracts high-quality,\ntool-based data analysis tasks and their executable multi-step solutions from\nreal-world Jupyter notebooks and associated data files. Using this pipeline, we\nintroduce NbQA, a large-scale dataset of standardized task-solution pairs that\nreflect authentic tool-use patterns in practical data science scenarios. To\nfurther enhance multi-step reasoning, we present Jupiter, a framework that\nformulates data analysis as a search problem and applies Monte Carlo Tree\nSearch (MCTS) to generate diverse solution trajectories for value model\nlearning. During inference, Jupiter combines the value model and node visit\ncounts to efficiently collect executable multi-step plans with minimal search\nsteps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on\nNbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,\nrespectively-matching or surpassing GPT-4o and advanced agent frameworks.\nFurther evaluations demonstrate improved generalization and stronger tool-use\nreasoning across diverse multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great promise in automating data\nscience workflows, but existing models still struggle with multi-step reasoning\nand tool use, which limits their effectiveness on complex data analysis tasks.\nTo address this, we propose a scalable pipeline that extracts high-quality,\ntool-based data analysis tasks and their executable multi-step solutions from\nreal-world Jupyter notebooks and associated data files. Using this pipeline, we\nintroduce NbQA, a large-scale dataset of standardized task-solution pairs that\nreflect authentic tool-use patterns in practical data science scenarios. To\nfurther enhance multi-step reasoning, we present Jupiter, a framework that\nformulates data analysis as a search problem and applies Monte Carlo Tree\nSearch (MCTS) to generate diverse solution trajectories for value model\nlearning. During inference, Jupiter combines the value model and node visit\ncounts to efficiently collect executable multi-step plans with minimal search\nsteps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on\nNbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,\nrespectively-matching or surpassing GPT-4o and advanced agent frameworks.\nFurther evaluations demonstrate improved generalization and stronger tool-use\nreasoning across diverse multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shuocheng Li"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Silin Du"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09504v3",
                "updated": "2025-09-11T08:17:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    17,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2024-10-12T11:45:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    11,
                    45,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "Bayesian Transfer Learning for Artificially Intelligent Geospatial\n  Systems: A Predictive Stacking Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Transfer Learning for Artificially Intelligent Geospatial\n  Systems: A Predictive Stacking Approach"
                },
                "summary": "Building artificially intelligent geospatial systems requires rapid delivery\nof spatial data analysis on massive scales with minimal human intervention.\nDepending upon their intended use, data analysis can also involve model\nassessment and uncertainty quantification. This article devises transfer\nlearning frameworks for deployment in artificially intelligent systems, where a\nmassive data set is split into smaller data sets that stream into the\nanalytical framework to propagate learning and assimilate inference for the\nentire data set. Specifically, we introduce Bayesian predictive stacking for\nmultivariate spatial data and demonstrate rapid and automated analysis of\nmassive data sets. Furthermore, inference is delivered without human\nintervention without excessively demanding hardware settings. We illustrate the\neffectiveness of our approach through extensive simulation experiments and in\nproducing inference from massive dataset on vegetation index that are\nindistinguishable from traditional (and more expensive) statistical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building artificially intelligent geospatial systems requires rapid delivery\nof spatial data analysis on massive scales with minimal human intervention.\nDepending upon their intended use, data analysis can also involve model\nassessment and uncertainty quantification. This article devises transfer\nlearning frameworks for deployment in artificially intelligent systems, where a\nmassive data set is split into smaller data sets that stream into the\nanalytical framework to propagate learning and assimilate inference for the\nentire data set. Specifically, we introduce Bayesian predictive stacking for\nmultivariate spatial data and demonstrate rapid and automated analysis of\nmassive data sets. Furthermore, inference is delivered without human\nintervention without excessively demanding hardware settings. We illustrate the\neffectiveness of our approach through extensive simulation experiments and in\nproducing inference from massive dataset on vegetation index that are\nindistinguishable from traditional (and more expensive) statistical approaches."
                },
                "authors": [
                    {
                        "name": "Luca Presicce"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09234v1",
                "updated": "2025-09-11T08:12:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    12,
                    38,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T08:12:38Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    12,
                    38,
                    3,
                    254,
                    0
                ],
                "title": "Agentic LLMs for Question Answering over Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLMs for Question Answering over Tabular Data"
                },
                "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA."
                },
                "authors": [
                    {
                        "name": "Rishit Tyagi"
                    },
                    {
                        "name": "Mohit Gupta"
                    },
                    {
                        "name": "Rahul Bouri"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Bouri"
                },
                "author": "Rahul Bouri",
                "arxiv_comment": "Accepted at ACL workshop SemEval 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09229v1",
                "updated": "2025-09-11T08:06:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    6,
                    2,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T08:06:02Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    6,
                    2,
                    3,
                    254,
                    0
                ],
                "title": "Reading Between the Lines: Classifying Resume Seniority with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading Between the Lines: Classifying Resume Seniority with Large\n  Language Models"
                },
                "summary": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt"
                },
                "authors": [
                    {
                        "name": "Matan Cohen"
                    },
                    {
                        "name": "Shira Shani"
                    },
                    {
                        "name": "Eden Menahem"
                    },
                    {
                        "name": "Yehudit Aperstein"
                    },
                    {
                        "name": "Alexander Apartsin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Apartsin"
                },
                "author": "Alexander Apartsin",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16505v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16505v3",
                "updated": "2025-09-11T08:05:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    8,
                    5,
                    33,
                    3,
                    254,
                    0
                ],
                "published": "2025-03-13T08:13:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    13,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic\n  Simulation of Discussions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Evaluation of Online Facilitation Strategies via Synthetic\n  Simulation of Discussions"
                },
                "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose design principles\nbased on existing methodologies for synthetic discussion generation. Based on\nthese principles, we propose a simple, generalizable, LLM-driven methodology to\nprototype the development of LLM facilitators by generating synthetic data\nwithout human involvement, and which surpasses current baselines. We use our\nmethodology to test whether current Social Science strategies for facilitation\ncan improve the performance of LLM facilitators. We find that, while LLM\nfacilitators significantly improve synthetic discussions, there is no evidence\nthat the application of these strategies leads to further improvements in\ndiscussion quality. In an effort to aid research in the field of facilitation,\nwe release a large, publicly available dataset containing LLM-generated and\nLLM-annotated discussions using multiple open-source models. This dataset can\nbe used for LLM facilitator finetuning as well as behavioral analysis of\ncurrent out-of-the-box LLMs in the task. We also release an open-source python\nframework that efficiently implements our methodology at great scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose design principles\nbased on existing methodologies for synthetic discussion generation. Based on\nthese principles, we propose a simple, generalizable, LLM-driven methodology to\nprototype the development of LLM facilitators by generating synthetic data\nwithout human involvement, and which surpasses current baselines. We use our\nmethodology to test whether current Social Science strategies for facilitation\ncan improve the performance of LLM facilitators. We find that, while LLM\nfacilitators significantly improve synthetic discussions, there is no evidence\nthat the application of these strategies leads to further improvements in\ndiscussion quality. In an effort to aid research in the field of facilitation,\nwe release a large, publicly available dataset containing LLM-generated and\nLLM-annotated discussions using multiple open-source models. This dataset can\nbe used for LLM facilitator finetuning as well as behavioral analysis of\ncurrent out-of-the-box LLMs in the task. We also release an open-source python\nframework that efficiently implements our methodology at great scale."
                },
                "authors": [
                    {
                        "name": "Dimitris Tsirmpas"
                    },
                    {
                        "name": "Ion Androutsopoulos"
                    },
                    {
                        "name": "John Pavlopoulos"
                    }
                ],
                "author_detail": {
                    "name": "John Pavlopoulos"
                },
                "author": "John Pavlopoulos",
                "arxiv_comment": "15 pages, 3 tables, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16505v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16505v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09226v1",
                "updated": "2025-09-11T07:59:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    59,
                    30,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T07:59:30Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    59,
                    30,
                    3,
                    254,
                    0
                ],
                "title": "Constructing a Question-Answering Simulator through the Distillation of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing a Question-Answering Simulator through the Distillation of\n  LLMs"
                },
                "summary": "The question-answering (QA) simulator is a model that mimics real student\nlearning behaviors and predicts their correctness of their responses to\nquestions. QA simulators enable educational recommender systems (ERS) to\ncollect large amounts of training data without interacting with real students,\nthereby preventing harmful recommendations made by an undertrained ERS from\nundermining actual student learning. Given the QA history, there are two\ncategories of solutions to predict the correctness, conducting the simulation:\n(1) LLM-free methods, which apply a traditional sequential model to transfer\nthe QA history into a vector representation first, and make predictions based\non the representation; (2) LLM-based methods, which leverage the domain\nknowledge and reasoning capability of LLM to enhence the prediction. LLM-free\nmethods offer fast inference but generally yield suboptimal performance. In\ncontrast, most LLM-based methods achieve better results, but at the cost of\nslower inference speed and higher GPU memory consumption. In this paper, we\npropose a method named LLM Distillation based Simulator (LDSim), which distills\ndomain knowledge and reasoning capability from an LLM to better assist\nprediction, thereby improving simulation performance. Extensive experiments\ndemonstrate that our LDSim achieves strong results on both the simulation task\nand the knowledge tracing (KT) task. Our code is publicly available at\nhttps://anonymous.4open.science/r/LDSim-05A9.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question-answering (QA) simulator is a model that mimics real student\nlearning behaviors and predicts their correctness of their responses to\nquestions. QA simulators enable educational recommender systems (ERS) to\ncollect large amounts of training data without interacting with real students,\nthereby preventing harmful recommendations made by an undertrained ERS from\nundermining actual student learning. Given the QA history, there are two\ncategories of solutions to predict the correctness, conducting the simulation:\n(1) LLM-free methods, which apply a traditional sequential model to transfer\nthe QA history into a vector representation first, and make predictions based\non the representation; (2) LLM-based methods, which leverage the domain\nknowledge and reasoning capability of LLM to enhence the prediction. LLM-free\nmethods offer fast inference but generally yield suboptimal performance. In\ncontrast, most LLM-based methods achieve better results, but at the cost of\nslower inference speed and higher GPU memory consumption. In this paper, we\npropose a method named LLM Distillation based Simulator (LDSim), which distills\ndomain knowledge and reasoning capability from an LLM to better assist\nprediction, thereby improving simulation performance. Extensive experiments\ndemonstrate that our LDSim achieves strong results on both the simulation task\nand the knowledge tracing (KT) task. Our code is publicly available at\nhttps://anonymous.4open.science/r/LDSim-05A9."
                },
                "authors": [
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Ting Long"
                    },
                    {
                        "name": "Jing Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Fu"
                },
                "author": "Jing Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09215v1",
                "updated": "2025-09-11T07:46:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    46,
                    0,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T07:46:00Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    46,
                    0,
                    3,
                    254,
                    0
                ],
                "title": "Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges,\n  and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges,\n  and Solutions"
                },
                "summary": "Large language models (LLMs)-empowered autonomous agents are transforming\nboth digital and physical environments by enabling adaptive, multi-agent\ncollaboration. While these agents offer significant opportunities across\ndomains such as finance, healthcare, and smart manufacturing, their\nunpredictable behaviors and heterogeneous capabilities pose substantial\ngovernance and accountability challenges. In this paper, we propose a\nblockchain-enabled layered architecture for regulatory agent collaboration,\ncomprising an agent layer, a blockchain data layer, and a regulatory\napplication layer. Within this framework, we design three key modules: (i) an\nagent behavior tracing and arbitration module for automated accountability,\n(ii) a dynamic reputation evaluation module for trust assessment in\ncollaborative scenarios, and (iii) a malicious behavior forecasting module for\nearly detection of adversarial activities. Our approach establishes a\nsystematic foundation for trustworthy, resilient, and scalable regulatory\nmechanisms in large-scale agent ecosystems. Finally, we discuss the future\nresearch directions for blockchain-enabled regulatory frameworks in multi-agent\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs)-empowered autonomous agents are transforming\nboth digital and physical environments by enabling adaptive, multi-agent\ncollaboration. While these agents offer significant opportunities across\ndomains such as finance, healthcare, and smart manufacturing, their\nunpredictable behaviors and heterogeneous capabilities pose substantial\ngovernance and accountability challenges. In this paper, we propose a\nblockchain-enabled layered architecture for regulatory agent collaboration,\ncomprising an agent layer, a blockchain data layer, and a regulatory\napplication layer. Within this framework, we design three key modules: (i) an\nagent behavior tracing and arbitration module for automated accountability,\n(ii) a dynamic reputation evaluation module for trust assessment in\ncollaborative scenarios, and (iii) a malicious behavior forecasting module for\nearly detection of adversarial activities. Our approach establishes a\nsystematic foundation for trustworthy, resilient, and scalable regulatory\nmechanisms in large-scale agent ecosystems. Finally, we discuss the future\nresearch directions for blockchain-enabled regulatory frameworks in multi-agent\nsystems."
                },
                "authors": [
                    {
                        "name": "Qinnan Hu"
                    },
                    {
                        "name": "Yuntao Wang"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Linkang Du"
                    }
                ],
                "author_detail": {
                    "name": "Linkang Du"
                },
                "author": "Linkang Du",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04077v2",
                "updated": "2025-09-11T07:34:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    34,
                    6,
                    3,
                    254,
                    0
                ],
                "published": "2025-06-04T15:42:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    42,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions"
                },
                "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information."
                },
                "authors": [
                    {
                        "name": "Chung-Chun Wang"
                    },
                    {
                        "name": "Jhen-Ke Lin"
                    },
                    {
                        "name": "Hao-Chien Lu"
                    },
                    {
                        "name": "Hong-Yun Lin"
                    },
                    {
                        "name": "Berlin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Berlin Chen"
                },
                "author": "Berlin Chen",
                "arxiv_comment": "submitted to the ISCA SLaTE-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09207v1",
                "updated": "2025-09-11T07:30:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    30,
                    44,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T07:30:44Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    30,
                    44,
                    3,
                    254,
                    0
                ],
                "title": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for\n  Automated Penetration Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for\n  Automated Penetration Testing"
                },
                "summary": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting."
                },
                "authors": [
                    {
                        "name": "Wuyuao Mai"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Jiarun Dai"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19813v2",
                "updated": "2025-09-11T07:29:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    29,
                    17,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T11:55:40Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    11,
                    55,
                    40,
                    2,
                    239,
                    0
                ],
                "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables"
                },
                "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Changzai Pan"
                    },
                    {
                        "name": "Kaiwen Wei"
                    },
                    {
                        "name": "Sishi Xiong"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Jiaxin Peng"
                    },
                    {
                        "name": "Xiaoyan Gu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wenhan Chang"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Jiang Zhong"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09198v1",
                "updated": "2025-09-11T07:13:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    13,
                    44,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T07:13:44Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    13,
                    44,
                    3,
                    254,
                    0
                ],
                "title": "GmSLM : Generative Marmoset Spoken Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GmSLM : Generative Marmoset Spoken Language Modeling"
                },
                "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM."
                },
                "authors": [
                    {
                        "name": "Talia Sternberg"
                    },
                    {
                        "name": "Michael London"
                    },
                    {
                        "name": "David Omer"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09194v1",
                "updated": "2025-09-11T07:10:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    10,
                    25,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T07:10:25Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    10,
                    25,
                    3,
                    254,
                    0
                ],
                "title": "On Integrating Large Language Models and Scenario-Based Programming for\n  Improving Software Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Integrating Large Language Models and Scenario-Based Programming for\n  Improving Software Reliability"
                },
                "summary": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper."
                },
                "authors": [
                    {
                        "name": "Ayelet Berzack"
                    },
                    {
                        "name": "Guy Katz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Katz"
                },
                "author": "Guy Katz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09193v1",
                "updated": "2025-09-11T07:09:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    9,
                    30,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T07:09:30Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    7,
                    9,
                    30,
                    3,
                    254,
                    0
                ],
                "title": "AI Reasoning for Wireless Communications and Networking: A Survey and\n  Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Reasoning for Wireless Communications and Networking: A Survey and\n  Perspectives"
                },
                "summary": "Artificial Intelligence (AI) techniques play a pivotal role in optimizing\nwireless communication networks. However, traditional deep learning approaches\noften act as closed boxes, lacking the structured reasoning abilities needed to\ntackle complex, multi-step decision problems. This survey provides a\ncomprehensive review and outlook of reasoning-enabled AI in wireless\ncommunication networks, with a focus on Large Language Models (LLMs) and other\nadvanced reasoning paradigms. In particular, LLM-based agents can combine\nreasoning with long-term planning, memory, tool utilization, and autonomous\ncross-layer control to dynamically optimize network operations with minimal\nhuman intervention. We begin by outlining the evolution of intelligent wireless\nnetworking and the limitations of conventional AI methods. We then introduce\nemerging AI reasoning techniques. Furthermore, we establish a classification\nsystem applicable to wireless network tasks. We also present a layer-by-layer\nexamination for AI reasoning, covering the physical, data link, network,\ntransport, and application layers. For each part, we identify key challenges\nand illustrate how AI reasoning methods can improve AI-based wireless\ncommunication performance. Finally, we discuss key research directions for AI\nreasoning toward future wireless communication networks. By combining insights\nfrom both communications and AI, this survey aims to chart a path for\nintegrating reasoning techniques into the next-generation wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) techniques play a pivotal role in optimizing\nwireless communication networks. However, traditional deep learning approaches\noften act as closed boxes, lacking the structured reasoning abilities needed to\ntackle complex, multi-step decision problems. This survey provides a\ncomprehensive review and outlook of reasoning-enabled AI in wireless\ncommunication networks, with a focus on Large Language Models (LLMs) and other\nadvanced reasoning paradigms. In particular, LLM-based agents can combine\nreasoning with long-term planning, memory, tool utilization, and autonomous\ncross-layer control to dynamically optimize network operations with minimal\nhuman intervention. We begin by outlining the evolution of intelligent wireless\nnetworking and the limitations of conventional AI methods. We then introduce\nemerging AI reasoning techniques. Furthermore, we establish a classification\nsystem applicable to wireless network tasks. We also present a layer-by-layer\nexamination for AI reasoning, covering the physical, data link, network,\ntransport, and application layers. For each part, we identify key challenges\nand illustrate how AI reasoning methods can improve AI-based wireless\ncommunication performance. Finally, we discuss key research directions for AI\nreasoning toward future wireless communication networks. By combining insights\nfrom both communications and AI, this survey aims to chart a path for\nintegrating reasoning techniques into the next-generation wireless networks."
                },
                "authors": [
                    {
                        "name": "Haoxiang Luo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Yanhui Bian"
                    },
                    {
                        "name": "Wenjiao Feng"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00827v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00827v5",
                "updated": "2025-09-11T06:44:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    44,
                    5,
                    3,
                    254,
                    0
                ],
                "published": "2024-10-29T07:15:56Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    15,
                    56,
                    1,
                    303,
                    0
                ],
                "title": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models\n  Using Themselves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models\n  Using Themselves"
                },
                "summary": "As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLJailbreakBench, a\nsafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark\nresults on 11 recently released VLMs reveal significant gaps in safety\nalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o\nand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger\ndefenses.VLJailbreakBench is publicly available at\nhttps://roywang021.github.io/VLJailbreakBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLJailbreakBench, a\nsafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark\nresults on 11 recently released VLMs reveal significant gaps in safety\nalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o\nand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger\ndefenses.VLJailbreakBench is publicly available at\nhttps://roywang021.github.io/VLJailbreakBench."
                },
                "authors": [
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Xiaosen Wang"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00827v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00827v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09177v1",
                "updated": "2025-09-11T06:27:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    27,
                    10,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T06:27:10Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    27,
                    10,
                    3,
                    254,
                    0
                ],
                "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL"
                },
                "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping\ndirectly in the importance-sampling (IS) weight space. We revisit\nsequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping\nis transplanted to sequences: a fixed clip range systematically reweights short\nvs. long responses, distorting the effective objective. Theoretically, we\nformalize length fairness via a Length Reweighting Error (LRE) and prove that\nsmall LRE yields a directional cosine guarantee between the clipped and true\nupdates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the\nsequence log-IS ratio with a band that applies a KL-corrected drift term and\nscales as $\\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,\nstabilizes training, and outperforms all baselines across multiple evaluation\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping\ndirectly in the importance-sampling (IS) weight space. We revisit\nsequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping\nis transplanted to sequences: a fixed clip range systematically reweights short\nvs. long responses, distorting the effective objective. Theoretically, we\nformalize length fairness via a Length Reweighting Error (LRE) and prove that\nsmall LRE yields a directional cosine guarantee between the clipped and true\nupdates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the\nsequence log-IS ratio with a band that applies a KL-corrected drift term and\nscales as $\\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,\nstabilizes training, and outperforms all baselines across multiple evaluation\ndatasets."
                },
                "authors": [
                    {
                        "name": "Hanyi Mao"
                    },
                    {
                        "name": "Quanjia Xiao"
                    },
                    {
                        "name": "Lei Pang"
                    },
                    {
                        "name": "Haixiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Haixiao Liu"
                },
                "author": "Haixiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09174v1",
                "updated": "2025-09-11T06:17:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    17,
                    59,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T06:17:59Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    17,
                    59,
                    3,
                    254,
                    0
                ],
                "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs"
                },
                "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "Zhanchen Dai"
                    },
                    {
                        "name": "Xiangnan Ma"
                    },
                    {
                        "name": "Kaiqi Kou"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17434v2",
                "updated": "2025-09-11T06:17:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    17,
                    45,
                    3,
                    254,
                    0
                ],
                "published": "2025-02-24T18:59:50Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    59,
                    50,
                    0,
                    55,
                    0
                ],
                "title": "V-HOP: Visuo-Haptic 6D Object Pose Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-HOP: Visuo-Haptic 6D Object Pose Tracking"
                },
                "summary": "Humans naturally integrate vision and haptics for robust object perception\nduring manipulation. The loss of either modality significantly degrades\nperformance. Inspired by this multisensory integration, prior object pose\nestimation research has attempted to combine visual and haptic/tactile\nfeedback. Although these works demonstrate improvements in controlled\nenvironments or synthetic datasets, they often underperform vision-only\napproaches in real-world settings due to poor generalization across diverse\ngrippers, sensor layouts, or sim-to-real environments. Furthermore, they\ntypically estimate the object pose for each frame independently, resulting in\nless coherent tracking over sequences in real-world deployments. To address\nthese limitations, we introduce a novel unified haptic representation that\neffectively handles multiple gripper embodiments. Building on this\nrepresentation, we introduce a new visuo-haptic transformer-based object pose\ntracker that seamlessly integrates visual and haptic input. We validate our\nframework in our dataset and the Feelsight dataset, demonstrating significant\nperformance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and\nsensor types (both taxel-based and vision-based tactile sensors). In real-world\nexperiments, we demonstrate that our approach outperforms state-of-the-art\nvisual trackers by a large margin. We further show that we can achieve precise\nmanipulation tasks by incorporating our real-time object tracking result into\nmotion plans, underscoring the advantages of visuo-haptic perception. Project\nwebsite: https://ivl.cs.brown.edu/research/v-hop",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally integrate vision and haptics for robust object perception\nduring manipulation. The loss of either modality significantly degrades\nperformance. Inspired by this multisensory integration, prior object pose\nestimation research has attempted to combine visual and haptic/tactile\nfeedback. Although these works demonstrate improvements in controlled\nenvironments or synthetic datasets, they often underperform vision-only\napproaches in real-world settings due to poor generalization across diverse\ngrippers, sensor layouts, or sim-to-real environments. Furthermore, they\ntypically estimate the object pose for each frame independently, resulting in\nless coherent tracking over sequences in real-world deployments. To address\nthese limitations, we introduce a novel unified haptic representation that\neffectively handles multiple gripper embodiments. Building on this\nrepresentation, we introduce a new visuo-haptic transformer-based object pose\ntracker that seamlessly integrates visual and haptic input. We validate our\nframework in our dataset and the Feelsight dataset, demonstrating significant\nperformance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and\nsensor types (both taxel-based and vision-based tactile sensors). In real-world\nexperiments, we demonstrate that our approach outperforms state-of-the-art\nvisual trackers by a large margin. We further show that we can achieve precise\nmanipulation tasks by incorporating our real-time object tracking result into\nmotion plans, underscoring the advantages of visuo-haptic perception. Project\nwebsite: https://ivl.cs.brown.edu/research/v-hop"
                },
                "authors": [
                    {
                        "name": "Hongyu Li"
                    },
                    {
                        "name": "Mingxi Jia"
                    },
                    {
                        "name": "Tuluhan Akbulut"
                    },
                    {
                        "name": "Yu Xiang"
                    },
                    {
                        "name": "George Konidaris"
                    },
                    {
                        "name": "Srinath Sridhar"
                    }
                ],
                "author_detail": {
                    "name": "Srinath Sridhar"
                },
                "author": "Srinath Sridhar",
                "arxiv_doi": "10.15607/RSS.2025.XXI.037",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.15607/RSS.2025.XXI.037",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.17434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by RSS 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09168v1",
                "updated": "2025-09-11T06:05:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    5,
                    35,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T06:05:35Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    5,
                    35,
                    3,
                    254,
                    0
                ],
                "title": "Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in\n  Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in\n  Semantic Communication"
                },
                "summary": "Large-scale transformer models have emerged as a powerful tool for semantic\ncommunication systems, enabling edge devices to extract rich representations\nfor robust inference across noisy wireless channels. However, their substantial\ncomputational demands remain a major barrier to practical deployment in\nresource-constrained 6G networks. In this paper, we present a training-free\nframework for adaptive token merging in pretrained vision transformers to\njointly reduce inference time and transmission resource usage. We formulate the\nselection of per-layer merging proportions as a multi-objective optimization\nproblem to balance accuracy and computational cost. We employ Gaussian\nprocess-based Bayesian optimization to construct a Pareto frontier of optimal\nconfigurations, enabling flexible runtime adaptation to dynamic application\nrequirements and channel conditions. Extensive experiments demonstrate that our\nmethod consistently outperforms other baselines and achieves significant\nreductions in floating-point operations while maintaining competitive accuracy\nacross a wide range of signal-to-noise ratio (SNR) conditions. Additional\nresults highlight the effectiveness of adaptive policies that adjust merging\naggressiveness in response to channel quality, providing a practical mechanism\nto trade off latency and semantic fidelity on demand. These findings establish\na scalable and efficient approach for deploying transformer-based semantic\ncommunication in future edge intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale transformer models have emerged as a powerful tool for semantic\ncommunication systems, enabling edge devices to extract rich representations\nfor robust inference across noisy wireless channels. However, their substantial\ncomputational demands remain a major barrier to practical deployment in\nresource-constrained 6G networks. In this paper, we present a training-free\nframework for adaptive token merging in pretrained vision transformers to\njointly reduce inference time and transmission resource usage. We formulate the\nselection of per-layer merging proportions as a multi-objective optimization\nproblem to balance accuracy and computational cost. We employ Gaussian\nprocess-based Bayesian optimization to construct a Pareto frontier of optimal\nconfigurations, enabling flexible runtime adaptation to dynamic application\nrequirements and channel conditions. Extensive experiments demonstrate that our\nmethod consistently outperforms other baselines and achieves significant\nreductions in floating-point operations while maintaining competitive accuracy\nacross a wide range of signal-to-noise ratio (SNR) conditions. Additional\nresults highlight the effectiveness of adaptive policies that adjust merging\naggressiveness in response to channel quality, providing a practical mechanism\nto trade off latency and semantic fidelity on demand. These findings establish\na scalable and efficient approach for deploying transformer-based semantic\ncommunication in future edge intelligence systems."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Hatem Abou-Zeid"
                    },
                    {
                        "name": "Mehdi Bennis"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Bennis"
                },
                "author": "Mehdi Bennis",
                "arxiv_comment": "To appear in IEEE Globecom 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06435v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06435v3",
                "updated": "2025-09-11T06:00:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    0,
                    29,
                    3,
                    254,
                    0
                ],
                "published": "2024-12-09T12:21:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Simulating Human-like Daily Activities with Desire-driven Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-like Daily Activities with Desire-driven Autonomy"
                },
                "summary": "Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06435v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06435v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]