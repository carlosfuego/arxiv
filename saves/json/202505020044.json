[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxc√©"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v1",
                "updated": "2025-04-29T00:58:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "Jo√£o Oliveira"
                    },
                    {
                        "name": "Jo√£o Gon√ßalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Ho√üfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max L√ºbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2405.20774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20774v3",
                "updated": "2025-04-30T17:59:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    59,
                    57,
                    2,
                    120,
                    0
                ],
                "published": "2024-05-27T17:59:43Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    59,
                    43,
                    0,
                    148,
                    0
                ],
                "title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against\n  Embodied LLM-based Decision-Making Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against\n  Embodied LLM-based Decision-Making Systems"
                },
                "summary": "Large Language Models (LLMs) have shown significant promise in real-world\ndecision-making tasks for embodied artificial intelligence, especially when\nfine-tuned to leverage their inherent common sense and reasoning abilities\nwhile being tailored to specific applications. However, this fine-tuning\nprocess introduces considerable safety and security vulnerabilities, especially\nin safety-critical cyber-physical systems. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-based Decision-making\nsystems (BALD) in embodied AI, systematically exploring the attack surfaces and\ntrigger mechanisms. Specifically, we propose three distinct attack mechanisms:\nword injection, scenario manipulation, and knowledge injection, targeting\nvarious components in the LLM-based decision-making pipeline. We perform\nextensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in\nautonomous driving and home robot tasks, demonstrating the effectiveness and\nstealthiness of our backdoor triggers across various attack channels, with\ncases like vehicles accelerating toward obstacles and robots placing knives on\nbeds. Our word and knowledge injection attacks achieve nearly 100% success rate\nacross multiple models and datasets while requiring only limited access to the\nsystem. Our scenario manipulation attack yields success rates exceeding 65%,\nreaching up to 90%, and does not require any runtime system intrusion. We also\nassess the robustness of these attacks against defenses, revealing their\nresilience. Our findings highlight critical security vulnerabilities in\nembodied LLM systems and emphasize the urgent need for safeguarding these\nsystems to mitigate potential risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant promise in real-world\ndecision-making tasks for embodied artificial intelligence, especially when\nfine-tuned to leverage their inherent common sense and reasoning abilities\nwhile being tailored to specific applications. However, this fine-tuning\nprocess introduces considerable safety and security vulnerabilities, especially\nin safety-critical cyber-physical systems. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-based Decision-making\nsystems (BALD) in embodied AI, systematically exploring the attack surfaces and\ntrigger mechanisms. Specifically, we propose three distinct attack mechanisms:\nword injection, scenario manipulation, and knowledge injection, targeting\nvarious components in the LLM-based decision-making pipeline. We perform\nextensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in\nautonomous driving and home robot tasks, demonstrating the effectiveness and\nstealthiness of our backdoor triggers across various attack channels, with\ncases like vehicles accelerating toward obstacles and robots placing knives on\nbeds. Our word and knowledge injection attacks achieve nearly 100% success rate\nacross multiple models and datasets while requiring only limited access to the\nsystem. Our scenario manipulation attack yields success rates exceeding 65%,\nreaching up to 90%, and does not require any runtime system intrusion. We also\nassess the robustness of these attacks against defenses, revealing their\nresilience. Our findings highlight critical security vulnerabilities in\nembodied LLM systems and emphasize the urgent need for safeguarding these\nsystems to mitigate potential risks."
                },
                "authors": [
                    {
                        "name": "Ruochen Jiao"
                    },
                    {
                        "name": "Shaoyuan Xie"
                    },
                    {
                        "name": "Justin Yue"
                    },
                    {
                        "name": "Takami Sato"
                    },
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Qi Alfred Chen"
                    },
                    {
                        "name": "Qi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhu"
                },
                "author": "Qi Zhu",
                "arxiv_comment": "Accepted paper at ICLR 2025, 31 pages, including main paper,\n  references, and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21851v1",
                "updated": "2025-04-30T17:58:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    58,
                    6,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:58:06Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    58,
                    6,
                    2,
                    120,
                    0
                ],
                "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and\n  Structured Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and\n  Structured Assessments"
                },
                "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability."
                },
                "authors": [
                    {
                        "name": "Sichang Tu"
                    },
                    {
                        "name": "Abigail Powers"
                    },
                    {
                        "name": "Stephen Doogan"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21844v1",
                "updated": "2025-04-30T17:53:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    53,
                    8,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    53,
                    8,
                    2,
                    120,
                    0
                ],
                "title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks"
                },
                "summary": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme."
                },
                "authors": [
                    {
                        "name": "William Sutcliffe"
                    },
                    {
                        "name": "Marta Calvi"
                    },
                    {
                        "name": "Simone Capelli"
                    },
                    {
                        "name": "Jonas Eschle"
                    },
                    {
                        "name": "Juli√°n Garc√≠a Pardi√±as"
                    },
                    {
                        "name": "Abhijit Mathad"
                    },
                    {
                        "name": "Azusa Uzuki"
                    },
                    {
                        "name": "Nicola Serra"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Serra"
                },
                "author": "Nicola Serra",
                "arxiv_comment": "21 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21840v1",
                "updated": "2025-04-30T17:50:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    50,
                    47,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:50:47Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    50,
                    47,
                    2,
                    120,
                    0
                ],
                "title": "Parameter Inference of Black Hole Images using Deep Learning in\n  Visibility Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Inference of Black Hole Images using Deep Learning in\n  Visibility Space"
                },
                "summary": "Using very long baseline interferometry, the Event Horizon Telescope (EHT)\ncollaboration has resolved the shadows of two supermassive black holes. Model\ncomparison is traditionally performed in image space, where imaging algorithms\nintroduce uncertainties in the recovered structure. Here, we develop a deep\nlearning framework to perform parameter inference in visibility space, directly\nusing the data measured by the interferometer without introducing potential\nerrors and biases from image reconstruction. First, we train and validate our\nframework on synthetic data derived from general relativistic\nmagnetohydrodynamics (GRMHD) simulations that vary in magnetic field state,\nspin, and $R_\\mathrm{high}$. Applying these models to the real data obtained\nduring the 2017 EHT campaign, and only considering total intensity, we do not\nderive meaningful constraints on either of these parameters. At present, our\nmethod is limited both by theoretical uncertainties in the GRMHD simulations\nand variation between snapshots of the same underlying physical model. However,\nwe demonstrate that spin and $R_\\mathrm{high}$ could be recovered using this\nframework through continuous monitoring of our sources, which mitigates\nvariations due to turbulence. In future work, we anticipate that including\nspectral or polarimetric information will greatly improve the performance of\nthis framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using very long baseline interferometry, the Event Horizon Telescope (EHT)\ncollaboration has resolved the shadows of two supermassive black holes. Model\ncomparison is traditionally performed in image space, where imaging algorithms\nintroduce uncertainties in the recovered structure. Here, we develop a deep\nlearning framework to perform parameter inference in visibility space, directly\nusing the data measured by the interferometer without introducing potential\nerrors and biases from image reconstruction. First, we train and validate our\nframework on synthetic data derived from general relativistic\nmagnetohydrodynamics (GRMHD) simulations that vary in magnetic field state,\nspin, and $R_\\mathrm{high}$. Applying these models to the real data obtained\nduring the 2017 EHT campaign, and only considering total intensity, we do not\nderive meaningful constraints on either of these parameters. At present, our\nmethod is limited both by theoretical uncertainties in the GRMHD simulations\nand variation between snapshots of the same underlying physical model. However,\nwe demonstrate that spin and $R_\\mathrm{high}$ could be recovered using this\nframework through continuous monitoring of our sources, which mitigates\nvariations due to turbulence. In future work, we anticipate that including\nspectral or polarimetric information will greatly improve the performance of\nthis framework."
                },
                "authors": [
                    {
                        "name": "Franc O"
                    },
                    {
                        "name": "Pavlos Protopapas"
                    },
                    {
                        "name": "Dominic W. Pesce"
                    },
                    {
                        "name": "Angelo Ricarte"
                    },
                    {
                        "name": "Sheperd S. Doeleman"
                    },
                    {
                        "name": "Cecilia Garraffo"
                    },
                    {
                        "name": "Lindy Blackburn"
                    },
                    {
                        "name": "Mauricio Santillana"
                    }
                ],
                "author_detail": {
                    "name": "Mauricio Santillana"
                },
                "author": "Mauricio Santillana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21831v1",
                "updated": "2025-04-30T17:37:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    37,
                    55,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:37:55Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    37,
                    55,
                    2,
                    120,
                    0
                ],
                "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video\n  Summarization"
                },
                "summary": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research."
                },
                "authors": [
                    {
                        "name": "Anas Anwarul Haq Khan"
                    },
                    {
                        "name": "Utkarsh Verma"
                    },
                    {
                        "name": "Prateek Chanda"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16214v2",
                "updated": "2025-04-30T17:29:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    29,
                    28,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-22T19:01:28Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    19,
                    1,
                    28,
                    1,
                    112,
                    0
                ],
                "title": "Hexcute: A Tile-based Programming Language with Automatic Layout and\n  Task-Mapping Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexcute: A Tile-based Programming Language with Automatic Layout and\n  Task-Mapping Synthesis"
                },
                "summary": "Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation."
                },
                "authors": [
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yaoyao Ding"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_comment": "17 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21812v1",
                "updated": "2025-04-30T17:15:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    15,
                    55,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:15:55Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    15,
                    55,
                    2,
                    120,
                    0
                ],
                "title": "Easily Computed Marginal Likelihoods for Multivariate Mixture Models\n  Using the THAMES Estimator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easily Computed Marginal Likelihoods for Multivariate Mixture Models\n  Using the THAMES Estimator"
                },
                "summary": "We present a new version of the truncated harmonic mean estimator (THAMES)\nfor univariate or multivariate mixture models. The estimator computes the\nmarginal likelihood from Markov chain Monte Carlo (MCMC) samples, is\nconsistent, asymptotically normal and of finite variance. In addition, it is\ninvariant to label switching, does not require posterior samples from hidden\nallocation vectors, and is easily approximated, even for an arbitrarily high\nnumber of components. Its computational efficiency is based on an\nasymptotically optimal ordering of the parameter space, which can in turn be\nused to provide useful visualisations. We test it in simulation settings where\nthe true marginal likelihood is available analytically. It performs well\nagainst state-of-the-art competitors, even in multivariate settings with a high\nnumber of components. We demonstrate its utility for inference and model\nselection on univariate and multivariate data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new version of the truncated harmonic mean estimator (THAMES)\nfor univariate or multivariate mixture models. The estimator computes the\nmarginal likelihood from Markov chain Monte Carlo (MCMC) samples, is\nconsistent, asymptotically normal and of finite variance. In addition, it is\ninvariant to label switching, does not require posterior samples from hidden\nallocation vectors, and is easily approximated, even for an arbitrarily high\nnumber of components. Its computational efficiency is based on an\nasymptotically optimal ordering of the parameter space, which can in turn be\nused to provide useful visualisations. We test it in simulation settings where\nthe true marginal likelihood is available analytically. It performs well\nagainst state-of-the-art competitors, even in multivariate settings with a high\nnumber of components. We demonstrate its utility for inference and model\nselection on univariate and multivariate data sets."
                },
                "authors": [
                    {
                        "name": "Martin Metodiev"
                    },
                    {
                        "name": "Nicholas J. Irons"
                    },
                    {
                        "name": "Marie Perrot-Dock√®s"
                    },
                    {
                        "name": "Pierre Latouche"
                    },
                    {
                        "name": "Adrian E. Raftery"
                    }
                ],
                "author_detail": {
                    "name": "Adrian E. Raftery"
                },
                "author": "Adrian E. Raftery",
                "arxiv_comment": "36 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20152v2",
                "updated": "2025-04-30T17:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    7,
                    7,
                    2,
                    120,
                    0
                ],
                "published": "2024-05-30T15:27:56Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    27,
                    56,
                    3,
                    151,
                    0
                ],
                "title": "Uncovering Bias in Large Vision-Language Models at Scale with\n  Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Bias in Large Vision-Language Models at Scale with\n  Counterfactuals"
                },
                "summary": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images,\nproducing over 57 million responses from popular models. Our multi-dimensional\nbias evaluation framework reveals that social attributes such as perceived\nrace, gender, and physical characteristics depicted in images can significantly\ninfluence the generation of toxic content, competency-associated words, harmful\nstereotypes, and numerical ratings of individuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images,\nproducing over 57 million responses from popular models. Our multi-dimensional\nbias evaluation framework reveals that social attributes such as perceived\nrace, gender, and physical characteristics depicted in images can significantly\ninfluence the generation of toxic content, competency-associated words, harmful\nstereotypes, and numerical ratings of individuals."
                },
                "authors": [
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "Accepted to NAACL 2025 main track (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21803v1",
                "updated": "2025-04-30T17:02:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    2,
                    6,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:02:06Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    2,
                    6,
                    2,
                    120,
                    0
                ],
                "title": "An Empirical Study on the Effectiveness of Large Language Models for\n  Binary Code Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Effectiveness of Large Language Models for\n  Binary Code Understanding"
                },
                "summary": "Binary code analysis plays a pivotal role in the field of software security\nand is widely used in tasks such as software maintenance, malware detection,\nsoftware vulnerability discovery, patch analysis, etc. However, unlike source\ncode, reverse engineers face significant challenges in understanding binary\ncode due to the lack of intuitive semantic information. Although traditional\nreverse tools can convert binary code into C-like pseudo code, the lack of code\ncomments and symbolic information such as function names still makes code\nunderstanding difficult. In recent years, two groups of techniques have shown\npromising prospects: (1) Deep learning-based techniques have demonstrated\ncompetitive results in tasks related to binary code understanding, furthermore,\n(2) Large Language Models (LLMs) have been extensively pre-trained at the\nsource-code level for tasks such as code understanding and generation. This has\nleft participants wondering about the capabilities of LLMs in binary code\nunderstanding. To this end, this work proposes a benchmark to evaluate the\neffectiveness of LLMs in real-world reverse engineering scenarios, which covers\ntwo key binary code understanding tasks, i.e., function name recovery and\nbinary code summarization. To more comprehensively evaluate, we include\nbinaries with multiple target architectures as well as different optimization\noptions. We gain valuable insights into the capabilities and limitations\nthrough extensive empirical studies of popular LLMs using our benchmark. Our\nevaluations reveal that existing LLMs can understand binary code to a certain\nextent, thereby improving the efficiency of binary code analysis. Our results\nhighlight the great potential of the LLMs in advancing the field of binary code\nunderstanding, and provide new directions for binary code analysis techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary code analysis plays a pivotal role in the field of software security\nand is widely used in tasks such as software maintenance, malware detection,\nsoftware vulnerability discovery, patch analysis, etc. However, unlike source\ncode, reverse engineers face significant challenges in understanding binary\ncode due to the lack of intuitive semantic information. Although traditional\nreverse tools can convert binary code into C-like pseudo code, the lack of code\ncomments and symbolic information such as function names still makes code\nunderstanding difficult. In recent years, two groups of techniques have shown\npromising prospects: (1) Deep learning-based techniques have demonstrated\ncompetitive results in tasks related to binary code understanding, furthermore,\n(2) Large Language Models (LLMs) have been extensively pre-trained at the\nsource-code level for tasks such as code understanding and generation. This has\nleft participants wondering about the capabilities of LLMs in binary code\nunderstanding. To this end, this work proposes a benchmark to evaluate the\neffectiveness of LLMs in real-world reverse engineering scenarios, which covers\ntwo key binary code understanding tasks, i.e., function name recovery and\nbinary code summarization. To more comprehensively evaluate, we include\nbinaries with multiple target architectures as well as different optimization\noptions. We gain valuable insights into the capabilities and limitations\nthrough extensive empirical studies of popular LLMs using our benchmark. Our\nevaluations reveal that existing LLMs can understand binary code to a certain\nextent, thereby improving the efficiency of binary code analysis. Our results\nhighlight the great potential of the LLMs in advancing the field of binary code\nunderstanding, and provide new directions for binary code analysis techniques."
                },
                "authors": [
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Zhenkan Fu"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Gangyang Li"
                    },
                    {
                        "name": "Li Hu"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19254v2",
                "updated": "2025-04-30T16:49:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    49,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-27T14:24:45Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    24,
                    45,
                    6,
                    117,
                    0
                ],
                "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers"
                },
                "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Singh Chauhan"
                },
                "author": "Mohit Singh Chauhan",
                "arxiv_comment": "UQLM repository: https://github.com/cvs-health/uqlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01907v2",
                "updated": "2025-04-30T16:47:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    47,
                    42,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-03T17:22:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    22,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "QCD Phase Diagram and Astrophysical Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCD Phase Diagram and Astrophysical Implications"
                },
                "summary": "I make a brief review about the QCD phases and the equation of state inferred\nfrom the neutron star data. Along the temperature axis at low baryon density,\nthe QCD phase transition is a smooth crossover, and it is a natural extension\nof our imagination to postulate a similar crossover along the density axis at\nlow temperature. Even without phase transitions, the inferred thermodynamic\nproperties of neutron star matter turn out to be highly nontrivial already at\ntwice of the nuclear saturation density. I also give some discussions about the\nsubstantiation of quark matter by means of the gravitational wave signals\nincluding the multi-messenger prospect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I make a brief review about the QCD phases and the equation of state inferred\nfrom the neutron star data. Along the temperature axis at low baryon density,\nthe QCD phase transition is a smooth crossover, and it is a natural extension\nof our imagination to postulate a similar crossover along the density axis at\nlow temperature. Even without phase transitions, the inferred thermodynamic\nproperties of neutron star matter turn out to be highly nontrivial already at\ntwice of the nuclear saturation density. I also give some discussions about the\nsubstantiation of quark matter by means of the gravitational wave signals\nincluding the multi-messenger prospect."
                },
                "authors": [
                    {
                        "name": "Kenji Fukushima"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Fukushima"
                },
                "author": "Kenji Fukushima",
                "arxiv_comment": "9 pages, 7 figures; some important references omitted due to the page\n  limit added; a proceedings contribution to Aspects of Criticality II,\n  Wroclaw, Poland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16949v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16949v3",
                "updated": "2025-04-30T16:35:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    35,
                    50,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-24T08:21:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    8,
                    21,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "SparseTransX: Efficient Training of Translation-Based Knowledge Graph\n  Embeddings Using Sparse Matrix Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTransX: Efficient Training of Translation-Based Knowledge Graph\n  Embeddings Using Sparse Matrix Operations"
                },
                "summary": "Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can be extended to accelerate other translation-based (such as\nTransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,\netc.) models as well. An implementation of the SpTransX framework is publicly\navailable as a Python package in https://github.com/HipGraph/SpTransX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can be extended to accelerate other translation-based (such as\nTransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,\netc.) models as well. An implementation of the SpTransX framework is publicly\navailable as a Python package in https://github.com/HipGraph/SpTransX."
                },
                "authors": [
                    {
                        "name": "Md Saidul Hoque Anik"
                    },
                    {
                        "name": "Ariful Azad"
                    }
                ],
                "author_detail": {
                    "name": "Ariful Azad"
                },
                "author": "Ariful Azad",
                "arxiv_comment": "15 pages, 9 figures, 9 tables. MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16949v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16949v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14258v3",
                "updated": "2025-04-30T16:23:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    23,
                    54,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-18T13:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System"
                },
                "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Baoqing Yue"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21773v1",
                "updated": "2025-04-30T16:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T16:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness"
                },
                "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Sandeep Polisetty"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    }
                ],
                "author_detail": {
                    "name": "May Fung"
                },
                "author": "May Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21770v1",
                "updated": "2025-04-30T16:15:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    15,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T16:15:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    15,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL\n  Bugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL\n  Bugs"
                },
                "summary": "While static analysis is useful in detecting early-stage hardware security\nbugs, its efficacy is limited because it requires information to form checks\nand is often unable to explain the security impact of a detected vulnerability.\nLarge Language Models can be useful in filling these gaps by identifying\nrelevant assets, removing false violations flagged by static analysis tools,\nand explaining the reported violations. LASHED combines the two approaches\n(LLMs and Static Analysis) to overcome each other's limitations for hardware\nsecurity bug detection. We investigate our approach on four open-source SoCs\nfor five Common Weakness Enumerations (CWEs) and present strategies for\nimprovement with better prompt engineering. We find that 87.5% of instances\nflagged by our recommended scheme are plausible CWEs. In-context learning and\nasking the model to 'think again' improves LASHED's precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While static analysis is useful in detecting early-stage hardware security\nbugs, its efficacy is limited because it requires information to form checks\nand is often unable to explain the security impact of a detected vulnerability.\nLarge Language Models can be useful in filling these gaps by identifying\nrelevant assets, removing false violations flagged by static analysis tools,\nand explaining the reported violations. LASHED combines the two approaches\n(LLMs and Static Analysis) to overcome each other's limitations for hardware\nsecurity bug detection. We investigate our approach on four open-source SoCs\nfor five Common Weakness Enumerations (CWEs) and present strategies for\nimprovement with better prompt engineering. We find that 87.5% of instances\nflagged by our recommended scheme are plausible CWEs. In-context learning and\nasking the model to 'think again' improves LASHED's precision."
                },
                "authors": [
                    {
                        "name": "Baleegh Ahmad"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Benjamin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Tan"
                },
                "author": "Benjamin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21769v1",
                "updated": "2025-04-30T16:14:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    14,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T16:14:25Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    14,
                    25,
                    2,
                    120,
                    0
                ],
                "title": "LLM-based Interactive Imitation Learning for Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Interactive Imitation Learning for Robotic Manipulation"
                },
                "summary": "Recent advancements in machine learning provide methods to train autonomous\nagents capable of handling the increasing complexity of sequential\ndecision-making in robotics. Imitation Learning (IL) is a prominent approach,\nwhere agents learn to control robots based on human demonstrations. However, IL\ncommonly suffers from violating the independent and identically distributed\n(i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL)\nachieves improved performance by allowing agents to learn from interactive\nfeedback from human teachers. Despite these improvements, both approaches come\nwith significant costs due to the necessity of human involvement. Leveraging\nthe emergent capabilities of Large Language Models (LLMs) in reasoning and\ngenerating human-like responses, we introduce LLM-iTeach -- a novel IIL\nframework that utilizes an LLM as an interactive teacher to enhance agent\nperformance while alleviating the dependence on human resources. Firstly,\nLLM-iTeach uses a hierarchical prompting strategy that guides the LLM in\ngenerating a policy in Python code. Then, with a designed similarity-based\nfeedback mechanism, LLM-iTeach provides corrective and evaluative feedback\ninteractively during the agent's training. We evaluate LLM-iTeach against\nbaseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a\nstate-of-the-art IIL method using a human teacher, on various robotic\nmanipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the\nsuccess rate and achieves or even outscores that of CEILing, highlighting the\npotential of LLMs as cost-effective, human-like teachers in interactive\nlearning environments. We further demonstrate the method's potential for\ngeneralization by evaluating it on additional tasks. The code and prompts are\nprovided at: https://github.com/Tubicor/LLM-iTeach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning provide methods to train autonomous\nagents capable of handling the increasing complexity of sequential\ndecision-making in robotics. Imitation Learning (IL) is a prominent approach,\nwhere agents learn to control robots based on human demonstrations. However, IL\ncommonly suffers from violating the independent and identically distributed\n(i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL)\nachieves improved performance by allowing agents to learn from interactive\nfeedback from human teachers. Despite these improvements, both approaches come\nwith significant costs due to the necessity of human involvement. Leveraging\nthe emergent capabilities of Large Language Models (LLMs) in reasoning and\ngenerating human-like responses, we introduce LLM-iTeach -- a novel IIL\nframework that utilizes an LLM as an interactive teacher to enhance agent\nperformance while alleviating the dependence on human resources. Firstly,\nLLM-iTeach uses a hierarchical prompting strategy that guides the LLM in\ngenerating a policy in Python code. Then, with a designed similarity-based\nfeedback mechanism, LLM-iTeach provides corrective and evaluative feedback\ninteractively during the agent's training. We evaluate LLM-iTeach against\nbaseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a\nstate-of-the-art IIL method using a human teacher, on various robotic\nmanipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the\nsuccess rate and achieves or even outscores that of CEILing, highlighting the\npotential of LLMs as cost-effective, human-like teachers in interactive\nlearning environments. We further demonstrate the method's potential for\ngeneralization by evaluating it on additional tasks. The code and prompts are\nprovided at: https://github.com/Tubicor/LLM-iTeach."
                },
                "authors": [
                    {
                        "name": "Jonas Werner"
                    },
                    {
                        "name": "Kun Chu"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "To be published in IJCNN 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21751v1",
                "updated": "2025-04-30T15:45:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    45,
                    28,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:45:28Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    45,
                    28,
                    2,
                    120,
                    0
                ],
                "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code\n  Generation"
                },
                "summary": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Dongsheng Ma"
                    },
                    {
                        "name": "Yongan Yu"
                    },
                    {
                        "name": "Rui Ling"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16570v2",
                "updated": "2025-04-30T15:44:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    44,
                    22,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-23T09:48:08Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    48,
                    8,
                    2,
                    113,
                    0
                ],
                "title": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using\n  Unsupervised Backbones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using\n  Unsupervised Backbones"
                },
                "summary": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we consistently outperform a baseline based on an SOTA\nunsupervised object detector under the same label- and training-free setting.\nAdditionally, we achieve competitive results -- and in some cases surpass --\ntraining-free methods that rely on supervised backbones, non-training-free\nunsupervised methods, as well as several fully supervised SOTA approaches. This\ndemonstrates that label- and training-free CAC can be both scalable and\neffective. Code: https://lorebianchi98.github.io/CountingDINO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we consistently outperform a baseline based on an SOTA\nunsupervised object detector under the same label- and training-free setting.\nAdditionally, we achieve competitive results -- and in some cases surpass --\ntraining-free methods that rely on supervised backbones, non-training-free\nunsupervised methods, as well as several fully supervised SOTA approaches. This\ndemonstrates that label- and training-free CAC can be both scalable and\neffective. Code: https://lorebianchi98.github.io/CountingDINO/."
                },
                "authors": [
                    {
                        "name": "Giacomo Pacini"
                    },
                    {
                        "name": "Lorenzo Bianchi"
                    },
                    {
                        "name": "Luca Ciampi"
                    },
                    {
                        "name": "Nicola Messina"
                    },
                    {
                        "name": "Giuseppe Amato"
                    },
                    {
                        "name": "Fabrizio Falchi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Falchi"
                },
                "author": "Fabrizio Falchi",
                "arxiv_comment": "13 pages, 2 figures, 2 tables. Project website:\n  https://lorebianchi98.github.io/CountingDINO/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07825v2",
                "updated": "2025-04-30T15:32:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    32,
                    19,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-10T11:23:18Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    23,
                    18,
                    3,
                    284,
                    0
                ],
                "title": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models"
                },
                "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at https://github.com/RUCAIBox/MAET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at https://github.com/RUCAIBox/MAET."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "17 Pages. Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21735v1",
                "updated": "2025-04-30T15:31:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    31,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    31,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy\n  Training"
                },
                "summary": "Massage therapy training emphasizes hands-on techniques and effective\ntherapist--patient communication. However, many educational programs struggle\nto provide realistic practice scenarios. To address this problem, we propose\nTheraQuest, a gamified, web-based simulation platform that employs large\nlanguage models (LLMs) to generate diverse virtual patients with varying\nsymptoms and cultural backgrounds. Through interactive dialogue, anatomical\ndecision-making, and immediate assessment, trainees develop both diagnostic\nreasoning and empathetic communication skills in a low-risk environment. Unlike\nexclusively VR-based solutions, TheraQuest remains accessible via standard web\nbrowsers, mitigating the cost and discomfort associated with extended headset\nuse. Preliminary testing suggests that integrating LLM-driven virtual patients\nwith real-time skill metrics can enhance trainee engagement and help bridge the\ngap between theoretical knowledge and clinical proficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massage therapy training emphasizes hands-on techniques and effective\ntherapist--patient communication. However, many educational programs struggle\nto provide realistic practice scenarios. To address this problem, we propose\nTheraQuest, a gamified, web-based simulation platform that employs large\nlanguage models (LLMs) to generate diverse virtual patients with varying\nsymptoms and cultural backgrounds. Through interactive dialogue, anatomical\ndecision-making, and immediate assessment, trainees develop both diagnostic\nreasoning and empathetic communication skills in a low-risk environment. Unlike\nexclusively VR-based solutions, TheraQuest remains accessible via standard web\nbrowsers, mitigating the cost and discomfort associated with extended headset\nuse. Preliminary testing suggests that integrating LLM-driven virtual patients\nwith real-time skill metrics can enhance trainee engagement and help bridge the\ngap between theoretical knowledge and clinical proficiency."
                },
                "authors": [
                    {
                        "name": "Shengqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shengqian Wang"
                },
                "author": "Shengqian Wang",
                "arxiv_comment": "8 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07723v3",
                "updated": "2025-04-30T15:11:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    11,
                    38,
                    2,
                    120,
                    0
                ],
                "published": "2024-06-24T03:58:11Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    58,
                    11,
                    0,
                    176,
                    0
                ],
                "title": "Lossless data compression by large models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless data compression by large models"
                },
                "summary": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression. We have previously shown all\nunderstanding or learning are compression, under reasonable assumptions. Large\nlanguage models (LLMs) understand data better than ever before. Can they help\nus to compress data? The LLMs may be seen to approximate the uncomputable\nSolomonoff induction. Therefore, under this new uncomputable paradigm, we\npresent LMCompress. LMCompress shatters all previous lossless compression\nalgorithms, doubling the lossless compression ratios of JPEG-XL for images,\nFLAC for audios, and H.264 for videos, and quadrupling the compression ratio of\nbz2 for texts. The better a large model understands the data, the better\nLMCompress compresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression. We have previously shown all\nunderstanding or learning are compression, under reasonable assumptions. Large\nlanguage models (LLMs) understand data better than ever before. Can they help\nus to compress data? The LLMs may be seen to approximate the uncomputable\nSolomonoff induction. Therefore, under this new uncomputable paradigm, we\npresent LMCompress. LMCompress shatters all previous lossless compression\nalgorithms, doubling the lossless compression ratios of JPEG-XL for images,\nFLAC for audios, and H.264 for videos, and quadrupling the compression ratio of\nbz2 for texts. The better a large model understands the data, the better\nLMCompress compresses."
                },
                "authors": [
                    {
                        "name": "Ziguang Li"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Xuliang Wang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Cole Wyeth"
                    },
                    {
                        "name": "Dongbo Bu"
                    },
                    {
                        "name": "Quan Yu"
                    },
                    {
                        "name": "Wen Gao"
                    },
                    {
                        "name": "Xingwu Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_doi": "10.1038/s42256-025-01033-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-025-01033-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published by Nature Machine Intelligence at\n  https://www.nature.com/articles/s42256-025-01033-7",
                "arxiv_journal_ref": "Nature Machine Intelligence, 2025, May 1",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21725v1",
                "updated": "2025-04-30T15:11:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    11,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:11:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    11,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "New {\\em ab initio} constrained extended Skyrme equations of state for\n  simulations of neutron stars, supernovae and binary mergers: I. Subsaturation\n  density domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New {\\em ab initio} constrained extended Skyrme equations of state for\n  simulations of neutron stars, supernovae and binary mergers: I. Subsaturation\n  density domain"
                },
                "summary": "In numerical simulations of core-collapse supernova and binary neutron stars\nmergers information about the energetics and composition of matter is\nimplemented via external tables covering the huge ranges of thermodynamic\nconditions explored during the astrophysical evolution. More than 120 general\npurpose equation of state tables have been contributed so far. Not all of them\ncomply with current constraints from theoretical and experimental nuclear\nphysics and astrophysical observations of neutron stars. Systematic\ninvestigations of the role that dense matter properties play in the evolution\nof these astrophysical phenomena require that more equation of state tables are\nprovided. We build a set of general purpose equation of state tables. At zero\ntemperature, they comply with all currently accepted constraints, including ab\ninitio chiral effective field theory calculations of pure neutron and symmetric\nnuclear matter. This set is designed to explore a wide variety of the behaviors\nof the effective masses as functions of density, which is reflected into a wide\nrange of thermal behaviors. We employ Brussels extended Skyrme interactions\ngenerated by means of Bayesian inference techniques. An extended nuclear\nstatistical equilibrium model is developed for modeling sub-saturated\ninhomogeneous nuclear matter. We study the properties of sub-saturated\ninhomogeneous nuclear matter over wide ranges of density, temperature and\nproton fraction. We analyze the mechanisms of transition to homogeneous matter\nand estimate the transition density. Our key results include the presence of a\nthink $^{14}$He layer in the inner crusts of (neo-)neutron stars, significant\nabundance of other exotic isotopes of H and He in warm and neutron rich matter\nand a detailed study of the thermodynamic stability of cold stellar matter. The\nequation of state tables will be publicly available in the Compose online\ndatabase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerical simulations of core-collapse supernova and binary neutron stars\nmergers information about the energetics and composition of matter is\nimplemented via external tables covering the huge ranges of thermodynamic\nconditions explored during the astrophysical evolution. More than 120 general\npurpose equation of state tables have been contributed so far. Not all of them\ncomply with current constraints from theoretical and experimental nuclear\nphysics and astrophysical observations of neutron stars. Systematic\ninvestigations of the role that dense matter properties play in the evolution\nof these astrophysical phenomena require that more equation of state tables are\nprovided. We build a set of general purpose equation of state tables. At zero\ntemperature, they comply with all currently accepted constraints, including ab\ninitio chiral effective field theory calculations of pure neutron and symmetric\nnuclear matter. This set is designed to explore a wide variety of the behaviors\nof the effective masses as functions of density, which is reflected into a wide\nrange of thermal behaviors. We employ Brussels extended Skyrme interactions\ngenerated by means of Bayesian inference techniques. An extended nuclear\nstatistical equilibrium model is developed for modeling sub-saturated\ninhomogeneous nuclear matter. We study the properties of sub-saturated\ninhomogeneous nuclear matter over wide ranges of density, temperature and\nproton fraction. We analyze the mechanisms of transition to homogeneous matter\nand estimate the transition density. Our key results include the presence of a\nthink $^{14}$He layer in the inner crusts of (neo-)neutron stars, significant\nabundance of other exotic isotopes of H and He in warm and neutron rich matter\nand a detailed study of the thermodynamic stability of cold stellar matter. The\nequation of state tables will be publicly available in the Compose online\ndatabase."
                },
                "authors": [
                    {
                        "name": "Adriana R. Raduta"
                    },
                    {
                        "name": "Mikhail V. Beznogov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail V. Beznogov"
                },
                "author": "Mikhail V. Beznogov",
                "arxiv_comment": "20 pages, 12 figures, 2 tables; submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20002v2",
                "updated": "2025-04-30T15:06:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    6,
                    16,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-25T18:39:48Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    18,
                    39,
                    48,
                    1,
                    84,
                    0
                ],
                "title": "Predictions for new physics in the CMB damping tail",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictions for new physics in the CMB damping tail"
                },
                "summary": "Ever since the Planck satellite measured the the cosmic microwave background\n(CMB) down to arcminute angular scales, the mismatch between the CMB-inferred\nvalue of the Hubble constant and the value inferred from the distance ladder\n(i.e., the Hubble tension) has been a growing concern and is currently at the\n$\\sim 6 \\sigma$ level. There are a handful of proposed mechanisms operating in\nthe early universe which have shown some promise in resolving the Hubble\ntension. These mechanisms are expected to leave a measurable impact on the\nsmallest scale CMB anisotropy, deep in the damping tail. Using current CMB\ndata, baryonic acoustic oscillation data, and the luminosities of Type Ia\nsupernovae as a baseline, we compute the predicted small-scale CMB power\nspectra for a characteristic set of these models. We find that near-future CMB\ndata should be able to distinguish some but not all of the investigated models\nfrom the core cosmological model, $\\Lambda$CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ever since the Planck satellite measured the the cosmic microwave background\n(CMB) down to arcminute angular scales, the mismatch between the CMB-inferred\nvalue of the Hubble constant and the value inferred from the distance ladder\n(i.e., the Hubble tension) has been a growing concern and is currently at the\n$\\sim 6 \\sigma$ level. There are a handful of proposed mechanisms operating in\nthe early universe which have shown some promise in resolving the Hubble\ntension. These mechanisms are expected to leave a measurable impact on the\nsmallest scale CMB anisotropy, deep in the damping tail. Using current CMB\ndata, baryonic acoustic oscillation data, and the luminosities of Type Ia\nsupernovae as a baseline, we compute the predicted small-scale CMB power\nspectra for a characteristic set of these models. We find that near-future CMB\ndata should be able to distinguish some but not all of the investigated models\nfrom the core cosmological model, $\\Lambda$CDM."
                },
                "authors": [
                    {
                        "name": "Tristan L. Smith"
                    },
                    {
                        "name": "Nils Sch√∂neberg"
                    }
                ],
                "author_detail": {
                    "name": "Nils Sch√∂neberg"
                },
                "author": "Nils Sch√∂neberg",
                "arxiv_comment": "12 pages, 8 figures, references added, expanded discussion of CMB\n  lensing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16658v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16658v3",
                "updated": "2025-04-30T15:05:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    5,
                    27,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-22T03:19:16Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    19,
                    16,
                    1,
                    296,
                    0
                ],
                "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent"
                },
                "summary": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions."
                },
                "authors": [
                    {
                        "name": "Janghoon Ock"
                    },
                    {
                        "name": "Tirtha Vinchurkar"
                    },
                    {
                        "name": "Yayati Jadhav"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16658v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04785v2",
                "updated": "2025-04-30T15:04:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    4,
                    55,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-27T14:02:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    2,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice"
                },
                "summary": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jos√© Siqueira de Cerqueira"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Rebekah Rousi"
                    },
                    {
                        "name": "Nannan Xi"
                    },
                    {
                        "name": "Juho Hamari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02140v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02140v3",
                "updated": "2025-04-30T15:01:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    1,
                    1,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-03T01:52:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    1,
                    52,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "A Formal Framework for Understanding Length Generalization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Formal Framework for Understanding Length Generalization in\n  Transformers"
                },
                "summary": "A major challenge for transformers is generalizing to sequences longer than\nthose observed during training. While previous works have empirically shown\nthat transformers can either succeed or fail at length generalization depending\non the task, theoretical understanding of this phenomenon remains limited. In\nthis work, we introduce a rigorous theoretical framework to analyze length\ngeneralization in causal transformers with learnable absolute positional\nencodings. In particular, we characterize those functions that are identifiable\nin the limit from sufficiently long inputs with absolute positional encodings\nunder an idealized inference scheme using a norm-based regularizer. This\nenables us to prove the possibility of length generalization for a rich family\nof problems. We experimentally validate the theory as a predictor of success\nand failure of length generalization across a range of algorithmic and formal\nlanguage tasks. Our theory not only explains a broad set of empirical\nobservations but also opens the way to provably predicting length\ngeneralization capabilities in transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge for transformers is generalizing to sequences longer than\nthose observed during training. While previous works have empirically shown\nthat transformers can either succeed or fail at length generalization depending\non the task, theoretical understanding of this phenomenon remains limited. In\nthis work, we introduce a rigorous theoretical framework to analyze length\ngeneralization in causal transformers with learnable absolute positional\nencodings. In particular, we characterize those functions that are identifiable\nin the limit from sufficiently long inputs with absolute positional encodings\nunder an idealized inference scheme using a norm-based regularizer. This\nenables us to prove the possibility of length generalization for a rich family\nof problems. We experimentally validate the theory as a predictor of success\nand failure of length generalization across a range of algorithmic and formal\nlanguage tasks. Our theory not only explains a broad set of empirical\nobservations but also opens the way to provably predicting length\ngeneralization capabilities in transformers."
                },
                "authors": [
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Andy Yang"
                    },
                    {
                        "name": "Satwik Bhattamishra"
                    },
                    {
                        "name": "Yash Sarrof"
                    },
                    {
                        "name": "Andreas Krebs"
                    },
                    {
                        "name": "Hattie Zhou"
                    },
                    {
                        "name": "Preetum Nakkiran"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "arxiv_comment": "85 pages, 9 figures, 11 tables. Accepted for publication at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02140v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02140v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21716v1",
                "updated": "2025-04-30T15:00:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    0,
                    20,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:00:20Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    0,
                    20,
                    2,
                    120,
                    0
                ],
                "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in\n  Household Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in\n  Household Robotics"
                },
                "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr."
                },
                "authors": [
                    {
                        "name": "Marc Glocker"
                    },
                    {
                        "name": "Peter H√∂nig"
                    },
                    {
                        "name": "Matthias Hirschmanner"
                    },
                    {
                        "name": "Markus Vincze"
                    }
                ],
                "author_detail": {
                    "name": "Markus Vincze"
                },
                "author": "Markus Vincze",
                "arxiv_comment": "Accepted at Austrian Robotics Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18964v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18964v4",
                "updated": "2025-04-30T14:53:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    53,
                    16,
                    2,
                    120,
                    0
                ],
                "published": "2023-10-29T10:07:32Z",
                "published_parsed": [
                    2023,
                    10,
                    29,
                    10,
                    7,
                    32,
                    6,
                    302,
                    0
                ],
                "title": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection"
                },
                "summary": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. While our research\ndemonstrates the potential of large language models (LLMs) for hate speech\ndetection, several limitations remain, particularly regarding the validity and\nthe reproducibility of the results. We conclude with an exhaustive discussion\nof the challenges we faced in our experimentation and offer recommended best\npractices for future scholars designing benchmarking experiments of this kind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. While our research\ndemonstrates the potential of large language models (LLMs) for hate speech\ndetection, several limitations remain, particularly regarding the validity and\nthe reproducibility of the results. We conclude with an exhaustive discussion\nof the challenges we faced in our experimentation and offer recommended best\npractices for future scholars designing benchmarking experiments of this kind."
                },
                "authors": [
                    {
                        "name": "Ahmad Nasir"
                    },
                    {
                        "name": "Aadish Sharma"
                    },
                    {
                        "name": "Kokil Jaidka"
                    },
                    {
                        "name": "Saifuddin Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Saifuddin Ahmed"
                },
                "author": "Saifuddin Ahmed",
                "arxiv_comment": "18 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18964v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18964v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10342v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10342v3",
                "updated": "2025-04-30T14:45:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    45,
                    1,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-14T15:50:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    50,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge"
                },
                "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."
                },
                "authors": [
                    {
                        "name": "Yueqi Song"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yibo Kong"
                    },
                    {
                        "name": "Zecheng Li"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "arxiv_comment": "56 pages, 43 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10342v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10342v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21700v1",
                "updated": "2025-04-30T14:44:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs"
                },
                "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Vignesh Kumar Kembu"
                    },
                    {
                        "name": "Antonino Nocera"
                    },
                    {
                        "name": "Vinod P"
                    }
                ],
                "author_detail": {
                    "name": "Vinod P"
                },
                "author": "Vinod P",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21688v1",
                "updated": "2025-04-30T14:23:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    23,
                    50,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:23:50Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    23,
                    50,
                    2,
                    120,
                    0
                ],
                "title": "Assessing Racial Disparities in Healthcare Expenditures Using Causal\n  Path-Specific Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Racial Disparities in Healthcare Expenditures Using Causal\n  Path-Specific Effects"
                },
                "summary": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\nemploys causal and counterfactual path-specific effects to quantify how various\nfactors, including socioeconomic status, insurance access, health behaviors,\nand health status, mediate these disparities. Using data from the Medical\nExpenditures Panel Survey, we estimate how expenditures would differ under\ncounterfactual scenarios in which the values of specific mediators were aligned\nacross racial groups along selected causal pathways. A key challenge in this\nanalysis is ensuring robustness against model misspecification while addressing\nthe zero-inflation and right-skewness of healthcare expenditures. For reliable\ninference, we derive asymptotically linear estimators by integrating influence\nfunction-based techniques with flexible machine learning methods, including\nsuper learners and a two-part model tailored to the zero-inflated, right-skewed\nnature of healthcare expenditures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\nemploys causal and counterfactual path-specific effects to quantify how various\nfactors, including socioeconomic status, insurance access, health behaviors,\nand health status, mediate these disparities. Using data from the Medical\nExpenditures Panel Survey, we estimate how expenditures would differ under\ncounterfactual scenarios in which the values of specific mediators were aligned\nacross racial groups along selected causal pathways. A key challenge in this\nanalysis is ensuring robustness against model misspecification while addressing\nthe zero-inflation and right-skewness of healthcare expenditures. For reliable\ninference, we derive asymptotically linear estimators by integrating influence\nfunction-based techniques with flexible machine learning methods, including\nsuper learners and a two-part model tailored to the zero-inflated, right-skewed\nnature of healthcare expenditures."
                },
                "authors": [
                    {
                        "name": "Xiaxian Ou"
                    },
                    {
                        "name": "Xinwei He"
                    },
                    {
                        "name": "David Benkeser"
                    },
                    {
                        "name": "Razieh Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Razieh Nabi"
                },
                "author": "Razieh Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21680v1",
                "updated": "2025-04-30T14:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    18,
                    11,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:18:11Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    18,
                    11,
                    2,
                    120,
                    0
                ],
                "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate\n  Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hoist with His Own Petard: Inducing Guardrails to Facilitate\n  Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions."
                },
                "authors": [
                    {
                        "name": "Pan Suo"
                    },
                    {
                        "name": "Yu-Ming Shang"
                    },
                    {
                        "name": "San-Chuan Guo"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang",
                "arxiv_comment": "11 pages, 6 figures. This work will be submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21668v1",
                "updated": "2025-04-30T14:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    10,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    10,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security."
                },
                "authors": [
                    {
                        "name": "Baolei Zhang"
                    },
                    {
                        "name": "Haoran Xin"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Zhuqing Liu"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "arxiv_comment": "Accepted by The Web Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20828v2",
                "updated": "2025-04-30T14:08:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    8,
                    38,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T14:51:26Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    51,
                    26,
                    1,
                    119,
                    0
                ],
                "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs."
                },
                "authors": [
                    {
                        "name": "Azam Ikram"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Sameh Elnikety"
                    },
                    {
                        "name": "Saurabh Bagchi"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Bagchi"
                },
                "author": "Saurabh Bagchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21659v1",
                "updated": "2025-04-30T14:01:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    1,
                    45,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:01:45Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    1,
                    45,
                    2,
                    120,
                    0
                ],
                "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization"
                },
                "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1"
                },
                "authors": [
                    {
                        "name": "Haotian Luo"
                    },
                    {
                        "name": "Haiying He"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Jinluan Yang"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Li Shen"
                    }
                ],
                "author_detail": {
                    "name": "Li Shen"
                },
                "author": "Li Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13722v2",
                "updated": "2025-04-30T13:59:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    59,
                    34,
                    2,
                    120,
                    0
                ],
                "published": "2024-12-18T11:03:26Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    3,
                    26,
                    2,
                    353,
                    0
                ],
                "title": "Data-driven Discovery of Biophysical T Cell Receptor Co-specificity\n  Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Discovery of Biophysical T Cell Receptor Co-specificity\n  Rules"
                },
                "summary": "The biophysical interactions between the T cell receptor (TCR) and its\nligands determine the specificity of the cellular immune response. However, the\nimmense diversity of receptors and ligands has made it challenging to discover\ngeneralizable rules across the distinct binding affinity landscapes created by\ndifferent ligands. Here, we present an optimization framework for discovering\nbiophysical rules that predict whether TCRs share specificity to a ligand.\nApplying this framework to TCRs associated with a collection of SARS-CoV-2\npeptides we systematically characterize how co-specificity depends on the type\nand position of amino-acid differences between receptors. We also demonstrate\nthat the inferred rules generalize to ligands highly dissimilar to any seen\nduring training. Our analysis reveals that matching of steric properties\nbetween substituted amino acids is more important for receptor co-specificity\nred than the hydrophobic properties that prominently determine evolutionary\nsubstitutability. Our analysis also quantifies the substantial importance of\npositions not in direct contact with the peptide for specificity. These\nfindings highlight the potential for data-driven approaches to uncover the\nmolecular mechanisms underpinning the specificity of adaptive immune responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The biophysical interactions between the T cell receptor (TCR) and its\nligands determine the specificity of the cellular immune response. However, the\nimmense diversity of receptors and ligands has made it challenging to discover\ngeneralizable rules across the distinct binding affinity landscapes created by\ndifferent ligands. Here, we present an optimization framework for discovering\nbiophysical rules that predict whether TCRs share specificity to a ligand.\nApplying this framework to TCRs associated with a collection of SARS-CoV-2\npeptides we systematically characterize how co-specificity depends on the type\nand position of amino-acid differences between receptors. We also demonstrate\nthat the inferred rules generalize to ligands highly dissimilar to any seen\nduring training. Our analysis reveals that matching of steric properties\nbetween substituted amino acids is more important for receptor co-specificity\nred than the hydrophobic properties that prominently determine evolutionary\nsubstitutability. Our analysis also quantifies the substantial importance of\npositions not in direct contact with the peptide for specificity. These\nfindings highlight the potential for data-driven approaches to uncover the\nmolecular mechanisms underpinning the specificity of adaptive immune responses."
                },
                "authors": [
                    {
                        "name": "Andrew G. T. Pyo"
                    },
                    {
                        "name": "Yuta Nagano"
                    },
                    {
                        "name": "Martina Milighetti"
                    },
                    {
                        "name": "James Henderson"
                    },
                    {
                        "name": "Curtis G. Callan Jr."
                    },
                    {
                        "name": "Benny Chain"
                    },
                    {
                        "name": "Ned S. Wingreen"
                    },
                    {
                        "name": "Andreas Tiffeau-Mayer"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Tiffeau-Mayer"
                },
                "author": "Andreas Tiffeau-Mayer",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02891v2",
                "updated": "2025-04-30T13:55:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    55,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-26T22:34:44Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    22,
                    34,
                    44,
                    2,
                    57,
                    0
                ],
                "title": "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies"
                },
                "summary": "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Shaibal Saha"
                    },
                    {
                        "name": "Lanyu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lanyu Xu"
                },
                "author": "Lanyu Xu",
                "arxiv_comment": "Accepted in Neurocomputing, Elsevier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21498v3",
                "updated": "2025-04-30T13:50:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    50,
                    0,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-28T20:06:28Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    20,
                    6,
                    28,
                    0,
                    302,
                    0
                ],
                "title": "Bayesian Nonparametric Models for Multiple Raters: a General Statistical\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonparametric Models for Multiple Raters: a General Statistical\n  Framework"
                },
                "summary": "Rating procedure is crucial in many applied fields (e.g., educational,\nclinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a\nsubject (e.g., student, doctor) on a rating scale. Given raters variability,\nseveral statistical methods have been proposed for assessing and improving the\nquality of ratings. Model estimation in the presence of heterogeneity has been\none of the recent challenges in this research line. Consequently, several\nmethods have been proposed to address this issue under a parametric multilevel\nmodelling framework, in which strong distributional assumptions are made. We\npropose a more flexible model under the Bayesian nonparametric (BNP) framework,\nin which most of those assumptions are relaxed. By eliciting hierarchical\ndiscrete nonparametric priors, the model accommodates clusters among raters and\nsubjects, naturally accounts for heterogeneity, and improves estimates\naccuracy. We propose a general BNP heteroscedastic framework to analyse\ncontinuous and coarse rating data and possible latent differences among\nsubjects and raters. The estimated densities are used to make inferences about\nthe rating process and the quality of the ratings. By exploiting a\nstick-breaking representation of the Dirichlet Process, a general class of\nIntraclass Correlation Coefficient (ICC) indices might be derived for these\nmodels. Our method allows us to independently identify latent similarities\nbetween subjects and raters and can be applied in precise education to improve\npersonalised teaching programs or interventions. Theoretical results about the\nICC are provided together with computational strategies. Simulations and a\nreal-world application are presented, and possible future directions are\ndiscussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rating procedure is crucial in many applied fields (e.g., educational,\nclinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a\nsubject (e.g., student, doctor) on a rating scale. Given raters variability,\nseveral statistical methods have been proposed for assessing and improving the\nquality of ratings. Model estimation in the presence of heterogeneity has been\none of the recent challenges in this research line. Consequently, several\nmethods have been proposed to address this issue under a parametric multilevel\nmodelling framework, in which strong distributional assumptions are made. We\npropose a more flexible model under the Bayesian nonparametric (BNP) framework,\nin which most of those assumptions are relaxed. By eliciting hierarchical\ndiscrete nonparametric priors, the model accommodates clusters among raters and\nsubjects, naturally accounts for heterogeneity, and improves estimates\naccuracy. We propose a general BNP heteroscedastic framework to analyse\ncontinuous and coarse rating data and possible latent differences among\nsubjects and raters. The estimated densities are used to make inferences about\nthe rating process and the quality of the ratings. By exploiting a\nstick-breaking representation of the Dirichlet Process, a general class of\nIntraclass Correlation Coefficient (ICC) indices might be derived for these\nmodels. Our method allows us to independently identify latent similarities\nbetween subjects and raters and can be applied in precise education to improve\npersonalised teaching programs or interventions. Theoretical results about the\nICC are provided together with computational strategies. Simulations and a\nreal-world application are presented, and possible future directions are\ndiscussed."
                },
                "authors": [
                    {
                        "name": "Giuseppe Mignemi"
                    },
                    {
                        "name": "Ioanna Manolopoulou"
                    }
                ],
                "author_detail": {
                    "name": "Ioanna Manolopoulou"
                },
                "author": "Ioanna Manolopoulou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21633v1",
                "updated": "2025-04-30T13:34:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    34,
                    14,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:34:14Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    34,
                    14,
                    2,
                    120,
                    0
                ],
                "title": "Convergence rate for Nearest Neighbour matching: geometry of the domain\n  and higher-order regularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convergence rate for Nearest Neighbour matching: geometry of the domain\n  and higher-order regularity"
                },
                "summary": "Estimating some mathematical expectations from partially observed data and in\nparticular missing outcomes is a central problem encountered in numerous fields\nsuch as transfer learning, counterfactual analysis or causal inference.\nMatching estimators, estimators based on k-nearest neighbours, are widely used\nin this context. It is known that the variance of such estimators can converge\nto zero at a parametric rate, but their bias can have a slower rate when the\ndimension of the covariates is larger than 2. This makes analysis of this bias\nparticularly important. In this paper, we provide higher order properties of\nthe bias. In contrast to the existing literature related to this problem, we do\nnot assume that the support of the target distribution of the covariates is\nstrictly included in that of the source, and we analyse two geometric\nconditions on the support that avoid such boundary bias problems. We show that\nthese conditions are much more general than the usual convex support\nassumption, leading to an improvement of existing results. Furthermore, we show\nthat the matching estimator studied by Abadie and Imbens (2006) for the average\ntreatment effect can be asymptotically efficient when the dimension of the\ncovariates is less than 4, a result only known in dimension 1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating some mathematical expectations from partially observed data and in\nparticular missing outcomes is a central problem encountered in numerous fields\nsuch as transfer learning, counterfactual analysis or causal inference.\nMatching estimators, estimators based on k-nearest neighbours, are widely used\nin this context. It is known that the variance of such estimators can converge\nto zero at a parametric rate, but their bias can have a slower rate when the\ndimension of the covariates is larger than 2. This makes analysis of this bias\nparticularly important. In this paper, we provide higher order properties of\nthe bias. In contrast to the existing literature related to this problem, we do\nnot assume that the support of the target distribution of the covariates is\nstrictly included in that of the source, and we analyse two geometric\nconditions on the support that avoid such boundary bias problems. We show that\nthese conditions are much more general than the usual convex support\nassumption, leading to an improvement of existing results. Furthermore, we show\nthat the matching estimator studied by Abadie and Imbens (2006) for the average\ntreatment effect can be asymptotically efficient when the dimension of the\ncovariates is less than 4, a result only known in dimension 1."
                },
                "authors": [
                    {
                        "name": "Simon Viel"
                    },
                    {
                        "name": "Lionel Truquet"
                    },
                    {
                        "name": "Ikko Yamane"
                    }
                ],
                "author_detail": {
                    "name": "Ikko Yamane"
                },
                "author": "Ikko Yamane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21625v1",
                "updated": "2025-04-30T13:28:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn\n  Instruction-Following Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn\n  Instruction-Following Ability"
                },
                "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications."
                },
                "authors": [
                    {
                        "name": "Jiaming Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Wang"
                },
                "author": "Jiaming Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15551v2",
                "updated": "2025-04-30T13:26:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    26,
                    38,
                    2,
                    120,
                    0
                ],
                "published": "2024-09-23T21:07:06Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    21,
                    7,
                    6,
                    0,
                    267,
                    0
                ],
                "title": "Revise, Reason, and Recognize: LLM-Based Emotion Recognition via\n  Emotion-Specific Prompts and ASR Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revise, Reason, and Recognize: LLM-Based Emotion Recognition via\n  Emotion-Specific Prompts and ASR Error Correction"
                },
                "summary": "Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains."
                },
                "authors": [
                    {
                        "name": "Yuanchao Li"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Catherine Lai"
                    }
                ],
                "author_detail": {
                    "name": "Catherine Lai"
                },
                "author": "Catherine Lai",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20114v2",
                "updated": "2025-04-30T13:15:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    15,
                    49,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-28T01:56:31Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    1,
                    56,
                    31,
                    0,
                    118,
                    0
                ],
                "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering"
                },
                "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG."
                },
                "authors": [
                    {
                        "name": "Zhonghao Li"
                    },
                    {
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "name": "Jinghuai Ou"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15585v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15585v5",
                "updated": "2025-04-30T13:14:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    14,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2024-02-23T19:52:09Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    19,
                    52,
                    9,
                    4,
                    54,
                    0
                ],
                "title": "Inference for Regression with Variables Generated by AI or Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Regression with Variables Generated by AI or Machine\n  Learning"
                },
                "summary": "Researchers now routinely use AI or other machine learning methods to\nestimate latent variables of economic interest, then plug-in the estimates as\ncovariates in a regression. We show both theoretically and empirically that\nnaively treating AI/ML-generated variables as \"data\" leads to biased estimates\nand invalid inference. To restore valid inference, we propose two methods: (1)\nan explicit bias correction with bias-corrected confidence intervals, and (2)\njoint estimation of the regression parameters and latent variables. We\nillustrate these ideas through applications involving label imputation,\ndimensionality reduction, and index construction via classification and\naggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Researchers now routinely use AI or other machine learning methods to\nestimate latent variables of economic interest, then plug-in the estimates as\ncovariates in a regression. We show both theoretically and empirically that\nnaively treating AI/ML-generated variables as \"data\" leads to biased estimates\nand invalid inference. To restore valid inference, we propose two methods: (1)\nan explicit bias correction with bias-corrected confidence intervals, and (2)\njoint estimation of the regression parameters and latent variables. We\nillustrate these ideas through applications involving label imputation,\ndimensionality reduction, and index construction via classification and\naggregation."
                },
                "authors": [
                    {
                        "name": "Laura Battaglia"
                    },
                    {
                        "name": "Timothy Christensen"
                    },
                    {
                        "name": "Stephen Hansen"
                    },
                    {
                        "name": "Szymon Sacher"
                    }
                ],
                "author_detail": {
                    "name": "Szymon Sacher"
                },
                "author": "Szymon Sacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15585v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15585v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18428v2",
                "updated": "2025-04-30T13:10:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    10,
                    37,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-25T15:39:04Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"
                },
                "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Chenshu Sun"
                    },
                    {
                        "name": "Feitong Sun"
                    },
                    {
                        "name": "Jiran Zhang"
                    },
                    {
                        "name": "Junxuan Wu"
                    },
                    {
                        "name": "Qiqian Cang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21605v1",
                "updated": "2025-04-30T13:06:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    6,
                    40,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:06:40Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    6,
                    40,
                    2,
                    120,
                    0
                ],
                "title": "RDF-Based Structured Quality Assessment Representation of Multilingual\n  LLM Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDF-Based Structured Quality Assessment Representation of Multilingual\n  LLM Evaluations"
                },
                "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy."
                },
                "authors": [
                    {
                        "name": "Jonas Gwozdz"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21604v1",
                "updated": "2025-04-30T13:03:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    3,
                    17,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:03:17Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    3,
                    17,
                    2,
                    120,
                    0
                ],
                "title": "Robust Misinformation Detection by Visiting Potential Commonsense\n  Conflict",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Misinformation Detection by Visiting Potential Commonsense\n  Conflict"
                },
                "summary": "The development of Internet technology has led to an increased prevalence of\nmisinformation, causing severe negative effects across diverse domains. To\nmitigate this challenge, Misinformation Detection (MD), aiming to detect online\nmisinformation automatically, emerges as a rapidly growing research topic in\nthe community. In this paper, we propose a novel plug-and-play augmentation\nmethod for the MD task, namely Misinformation Detection with Potential\nCommonsense Conflict (MD-PCC). We take inspiration from the prior studies\nindicating that fake articles are more likely to involve commonsense conflict.\nAccordingly, we construct commonsense expressions for articles, serving to\nexpress potential commonsense conflicts inferred by the difference between\nextracted commonsense triplet and golden ones inferred by the well-established\ncommonsense reasoning tool COMET. These expressions are then specified for each\narticle as augmentation. Any specific MD methods can be then trained on those\ncommonsense-augmented articles. Besides, we also collect a novel\ncommonsense-oriented dataset named CoMis, whose all fake articles are caused by\ncommonsense conflict. We integrate MD-PCC with various existing MD backbones\nand compare them across both 4 public benchmark datasets and CoMis. Empirical\nresults demonstrate that MD-PCC can consistently outperform the existing MD\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Internet technology has led to an increased prevalence of\nmisinformation, causing severe negative effects across diverse domains. To\nmitigate this challenge, Misinformation Detection (MD), aiming to detect online\nmisinformation automatically, emerges as a rapidly growing research topic in\nthe community. In this paper, we propose a novel plug-and-play augmentation\nmethod for the MD task, namely Misinformation Detection with Potential\nCommonsense Conflict (MD-PCC). We take inspiration from the prior studies\nindicating that fake articles are more likely to involve commonsense conflict.\nAccordingly, we construct commonsense expressions for articles, serving to\nexpress potential commonsense conflicts inferred by the difference between\nextracted commonsense triplet and golden ones inferred by the well-established\ncommonsense reasoning tool COMET. These expressions are then specified for each\narticle as augmentation. Any specific MD methods can be then trained on those\ncommonsense-augmented articles. Besides, we also collect a novel\ncommonsense-oriented dataset named CoMis, whose all fake articles are caused by\ncommonsense conflict. We integrate MD-PCC with various existing MD backbones\nand compare them across both 4 public benchmark datasets and CoMis. Empirical\nresults demonstrate that MD-PCC can consistently outperform the existing MD\nbaselines."
                },
                "authors": [
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Ximing Li"
                    },
                    {
                        "name": "Changchun Li"
                    },
                    {
                        "name": "Bingrui Zhao"
                    },
                    {
                        "name": "Bo Fu"
                    },
                    {
                        "name": "Renchu Guan"
                    },
                    {
                        "name": "Shengsheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shengsheng Wang"
                },
                "author": "Shengsheng Wang",
                "arxiv_comment": "11 pages, 2 figures. Accepted by IJCAI 2025. Code:\n  https://github.com/wangbing1416/MD-PCC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21598v1",
                "updated": "2025-04-30T12:58:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    58,
                    30,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:58:30Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    58,
                    30,
                    2,
                    120,
                    0
                ],
                "title": "Cascade Detector Analysis and Application to Biomedical Microscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade Detector Analysis and Application to Biomedical Microscopy"
                },
                "summary": "As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains."
                },
                "authors": [
                    {
                        "name": "Thomas L. Athey"
                    },
                    {
                        "name": "Shashata Sawmya"
                    },
                    {
                        "name": "Nir Shavit"
                    }
                ],
                "author_detail": {
                    "name": "Nir Shavit"
                },
                "author": "Nir Shavit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21596v1",
                "updated": "2025-04-30T12:53:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    53,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:53:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    53,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for\n  Online Task and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Pre-trained Large Language Models with Refined Prompting for\n  Online Task and Motion Planning"
                },
                "summary": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution."
                },
                "authors": [
                    {
                        "name": "Huihui Guo"
                    },
                    {
                        "name": "Huilong Pi"
                    },
                    {
                        "name": "Yunchuan Qin"
                    },
                    {
                        "name": "Zhuo Tang"
                    },
                    {
                        "name": "Kenli Li"
                    }
                ],
                "author_detail": {
                    "name": "Kenli Li"
                },
                "author": "Kenli Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21595v1",
                "updated": "2025-04-30T12:53:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    53,
                    8,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    53,
                    8,
                    2,
                    120,
                    0
                ],
                "title": "Real-time Program Evaluation using Anytime-valid Rank Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Program Evaluation using Anytime-valid Rank Tests"
                },
                "summary": "Counterfactual mean estimators such as difference-in-differences and\nsynthetic control have grown into workhorse tools for program evaluation.\nInference for these estimators is well-developed in settings where all\npost-treatment data is available at the time of analysis. However, in settings\nwhere data arrives sequentially, these tests do not permit real-time inference,\nas they require a pre-specified sample size T. We introduce real-time inference\nfor program evaluation through anytime-valid rank tests. Our methodology relies\non interpreting the absence of a treatment effect as exchangeability of the\ntreatment estimates. We then convert these treatment estimates into sequential\nranks, and construct optimal finite-sample valid sequential tests for\nexchangeability. We illustrate our methods in the context of\ndifference-in-differences and synthetic control. In simulations, they control\nsize even under mild exchangeability violations. While our methods suffer\nslight power loss at T, they allow for early rejection (before T) and preserve\nthe ability to reject later (after T).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual mean estimators such as difference-in-differences and\nsynthetic control have grown into workhorse tools for program evaluation.\nInference for these estimators is well-developed in settings where all\npost-treatment data is available at the time of analysis. However, in settings\nwhere data arrives sequentially, these tests do not permit real-time inference,\nas they require a pre-specified sample size T. We introduce real-time inference\nfor program evaluation through anytime-valid rank tests. Our methodology relies\non interpreting the absence of a treatment effect as exchangeability of the\ntreatment estimates. We then convert these treatment estimates into sequential\nranks, and construct optimal finite-sample valid sequential tests for\nexchangeability. We illustrate our methods in the context of\ndifference-in-differences and synthetic control. In simulations, they control\nsize even under mild exchangeability violations. While our methods suffer\nslight power loss at T, they allow for early rejection (before T) and preserve\nthe ability to reject later (after T)."
                },
                "authors": [
                    {
                        "name": "Sam van Meer"
                    },
                    {
                        "name": "Nick W. Koning"
                    }
                ],
                "author_detail": {
                    "name": "Nick W. Koning"
                },
                "author": "Nick W. Koning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21589v1",
                "updated": "2025-04-30T12:47:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    47,
                    9,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:47:09Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    47,
                    9,
                    2,
                    120,
                    0
                ],
                "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for\n  Automated Subject Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for\n  Automated Subject Indexing"
                },
                "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts."
                },
                "authors": [
                    {
                        "name": "Lisa Kluge"
                    },
                    {
                        "name": "Maximilian K√§hler"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian K√§hler"
                },
                "author": "Maximilian K√§hler",
                "arxiv_comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21584v1",
                "updated": "2025-04-30T12:44:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    44,
                    26,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:44:26Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    44,
                    26,
                    2,
                    120,
                    0
                ],
                "title": "An Aldous-Hoover type representation for row exchangeable arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Aldous-Hoover type representation for row exchangeable arrays"
                },
                "summary": "In an array of random variables, each row can be regarded as a single,\nsequence-valued random variable. In this way, the array is seen as a sequence\nof sequences. Such an array is said to be row exchangeable if each row is an\nexchangeable sequence, and the entire array, viewed as a sequence of sequences,\nis exchangeable. We give a representation theorem, analogous to those of Aldous\nand Hoover, which characterizes row exchangeable arrays. We then use this\nrepresentation theorem to address the problem of performing Bayesian inference\non row exchangeable arrays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an array of random variables, each row can be regarded as a single,\nsequence-valued random variable. In this way, the array is seen as a sequence\nof sequences. Such an array is said to be row exchangeable if each row is an\nexchangeable sequence, and the entire array, viewed as a sequence of sequences,\nis exchangeable. We give a representation theorem, analogous to those of Aldous\nand Hoover, which characterizes row exchangeable arrays. We then use this\nrepresentation theorem to address the problem of performing Bayesian inference\non row exchangeable arrays."
                },
                "authors": [
                    {
                        "name": "Evan Donald"
                    },
                    {
                        "name": "Jason Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Jason Swanson"
                },
                "author": "Jason Swanson",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G09 (Primary) 62E10, 60G25, 62M20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21582v1",
                "updated": "2025-04-30T12:41:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    41,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    41,
                    51,
                    2,
                    120,
                    0
                ],
                "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large\n  Language Model Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large\n  Language Model Framework"
                },
                "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation."
                },
                "authors": [
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Xiangning Yu"
                    },
                    {
                        "name": "Zhiyu Zhao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "27 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21572v1",
                "updated": "2025-04-30T12:21:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    21,
                    50,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:21:50Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    21,
                    50,
                    2,
                    120,
                    0
                ],
                "title": "Powerful randomization tests for subgroup analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful randomization tests for subgroup analysis"
                },
                "summary": "Randomization tests are widely used to generate valid $p$-values for testing\nsharp null hypotheses in finite-population causal inference. This article\nextends their application to subgroup analysis. We show that directly testing\nsubgroup null hypotheses may lack power due to small subgroup sizes.\nIncorporating an estimator of the conditional average treatment effect (CATE)\ncan substantially improve power but requires splitting the treatment variables\nbetween estimation and testing to preserve finite-sample validity. To this end,\nwe propose BaR-learner, a Bayesian extension of the popular method R-learner\nfor CATE estimation. BaR-learner imputes the treatment variables reserved for\nrandomization tests, reducing information loss due to sample-splitting.\nFurthermore, we show that the treatment variables most informative for training\nBaR-learner are different from those most valuable for increasing test power.\nMotivated by this insight, we introduce AdaSplit, a sample-splitting procedure\nthat adaptively allocates units between estimation and testing. Simulation\nstudies demonstrate that our method yields more powerful randomization tests\nthan baselines that omit CATE estimation or rely on random sample-splitting. We\nalso apply our method to a blood pressure intervention trial, identifying\npatient subgroups with significant treatment effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization tests are widely used to generate valid $p$-values for testing\nsharp null hypotheses in finite-population causal inference. This article\nextends their application to subgroup analysis. We show that directly testing\nsubgroup null hypotheses may lack power due to small subgroup sizes.\nIncorporating an estimator of the conditional average treatment effect (CATE)\ncan substantially improve power but requires splitting the treatment variables\nbetween estimation and testing to preserve finite-sample validity. To this end,\nwe propose BaR-learner, a Bayesian extension of the popular method R-learner\nfor CATE estimation. BaR-learner imputes the treatment variables reserved for\nrandomization tests, reducing information loss due to sample-splitting.\nFurthermore, we show that the treatment variables most informative for training\nBaR-learner are different from those most valuable for increasing test power.\nMotivated by this insight, we introduce AdaSplit, a sample-splitting procedure\nthat adaptively allocates units between estimation and testing. Simulation\nstudies demonstrate that our method yields more powerful randomization tests\nthan baselines that omit CATE estimation or rely on random sample-splitting. We\nalso apply our method to a blood pressure intervention trial, identifying\npatient subgroups with significant treatment effects."
                },
                "authors": [
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Zijun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Zijun Gao"
                },
                "author": "Zijun Gao",
                "arxiv_comment": "33 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21568v1",
                "updated": "2025-04-30T12:14:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    14,
                    48,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:14:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    14,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and\n  Bayesian Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and\n  Bayesian Networks"
                },
                "summary": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts."
                },
                "authors": [
                    {
                        "name": "Shui-jin Rong"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Da-qing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Da-qing Zhang"
                },
                "author": "Da-qing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21562v1",
                "updated": "2025-04-30T12:06:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    6,
                    56,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:06:56Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    6,
                    56,
                    2,
                    120,
                    0
                ],
                "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes"
                },
                "summary": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule."
                },
                "authors": [
                    {
                        "name": "Henry John Krumb"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08165v2",
                "updated": "2025-04-30T12:02:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    2,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2024-11-12T20:15:58Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    20,
                    15,
                    58,
                    1,
                    317,
                    0
                ],
                "title": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for\n  Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for\n  Knowledge Graph Completion"
                },
                "summary": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets."
                },
                "authors": [
                    {
                        "name": "Muzhi Li"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "Accepted by NAACL2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21840v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21840v5",
                "updated": "2025-04-30T12:00:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    0,
                    36,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-29T08:07:22Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    7,
                    22,
                    1,
                    303,
                    0
                ],
                "title": "Optimized Homomorphic Permutation From New Permutation Decomposition\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Homomorphic Permutation From New Permutation Decomposition\n  Techniques"
                },
                "summary": "Homomorphic permutation is fundamental to privacy-preserving computations\nbased on batch-encoding homomorphic encryption. It underpins nearly all\nhomomorphic matrix operations and predominantly influences their complexity.\nPermutation decomposition as a potential approach to optimize this critical\ncomponent remains underexplored. In this paper, we propose novel decomposition\ntechniques to optimize homomorphic permutations, advancing homomorphic\nencryption-based privacy-preserving computations.\n  We start by defining an ideal decomposition form for permutations and propose\nan algorithm searching depth-1 ideal decompositions. Based on this, we prove\nthe full-depth ideal decomposability of permutations used in specific\nhomomorphic matrix transposition (HMT) and multiplication (HMM) algorithms,\nallowing them to achieve asymptotic improvement in speed and rotation key\nreduction. As a demonstration of applicability, substituting the HMM components\nin the best-known inference framework of encrypted neural networks with our\nenhanced version shows up to $7.9\\times$ reduction in latency.\n  We further devise a new method for computing arbitrary homomorphic\npermutations, specifically those with weak structures that cannot be ideally\ndecomposed. We design a network structure that deviates from the conventional\nscope of decomposition and outperforms the state-of-the-art technique with a\nspeed-up of up to $1.69\\times$ under a minimal rotation key requirement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homomorphic permutation is fundamental to privacy-preserving computations\nbased on batch-encoding homomorphic encryption. It underpins nearly all\nhomomorphic matrix operations and predominantly influences their complexity.\nPermutation decomposition as a potential approach to optimize this critical\ncomponent remains underexplored. In this paper, we propose novel decomposition\ntechniques to optimize homomorphic permutations, advancing homomorphic\nencryption-based privacy-preserving computations.\n  We start by defining an ideal decomposition form for permutations and propose\nan algorithm searching depth-1 ideal decompositions. Based on this, we prove\nthe full-depth ideal decomposability of permutations used in specific\nhomomorphic matrix transposition (HMT) and multiplication (HMM) algorithms,\nallowing them to achieve asymptotic improvement in speed and rotation key\nreduction. As a demonstration of applicability, substituting the HMM components\nin the best-known inference framework of encrypted neural networks with our\nenhanced version shows up to $7.9\\times$ reduction in latency.\n  We further devise a new method for computing arbitrary homomorphic\npermutations, specifically those with weak structures that cannot be ideally\ndecomposed. We design a network structure that deviates from the conventional\nscope of decomposition and outperforms the state-of-the-art technique with a\nspeed-up of up to $1.69\\times$ under a minimal rotation key requirement."
                },
                "authors": [
                    {
                        "name": "Xirong Ma"
                    },
                    {
                        "name": "Junling Fang"
                    },
                    {
                        "name": "Chunpeng Ge"
                    },
                    {
                        "name": "Dung Hoang Duong"
                    },
                    {
                        "name": "Yali Jiang"
                    },
                    {
                        "name": "Yanbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Li"
                },
                "author": "Yanbin Li",
                "arxiv_comment": "Submission on 30/04/2025, large amount of context updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21840v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21840v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21553v1",
                "updated": "2025-04-30T11:52:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    52,
                    18,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:52:18Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    52,
                    18,
                    2,
                    120,
                    0
                ],
                "title": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision\n  Quantization Strategy for LLaMA-based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision\n  Quantization Strategy for LLaMA-based Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21543v1",
                "updated": "2025-04-30T11:37:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    37,
                    22,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:37:22Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    37,
                    22,
                    2,
                    120,
                    0
                ],
                "title": "CryptoUNets: Applying Convolutional Networks to Encrypted Data for\n  Biomedical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoUNets: Applying Convolutional Networks to Encrypted Data for\n  Biomedical Image Segmentation"
                },
                "summary": "In this manuscript, we demonstrate the feasibility of a privacy-preserving\nU-Net deep learning inference framework, namely, homomorphic encryption-based\nU-Net inference. That is, U-Net inference can be performed solely using\nhomomorphic encryption techniques. To our knowledge, this is the first work to\nachieve support perform implement enable U-Net inference entirely based on\nhomomorphic encryption ?.\n  The primary technical challenge lies in data encoding. To address this, we\nemploy a flexible encoding scheme, termed Double Volley Revolver, which enables\neffective support for skip connections and upsampling operations within the\nU-Net architecture.\n  We adopt a tailored HE-friendly U-Net design incorporating square activation\nfunctions, mean pooling layers, and transposed convolution layers (implemented\nas ConvTranspose2d in PyTorch) with a kernel size of 2 and stride of 2. After\ntraining the model in plaintext, we deploy the resulting parameters using the\nHEAAN homomorphic encryption library to perform encrypted U-Net inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this manuscript, we demonstrate the feasibility of a privacy-preserving\nU-Net deep learning inference framework, namely, homomorphic encryption-based\nU-Net inference. That is, U-Net inference can be performed solely using\nhomomorphic encryption techniques. To our knowledge, this is the first work to\nachieve support perform implement enable U-Net inference entirely based on\nhomomorphic encryption ?.\n  The primary technical challenge lies in data encoding. To address this, we\nemploy a flexible encoding scheme, termed Double Volley Revolver, which enables\neffective support for skip connections and upsampling operations within the\nU-Net architecture.\n  We adopt a tailored HE-friendly U-Net design incorporating square activation\nfunctions, mean pooling layers, and transposed convolution layers (implemented\nas ConvTranspose2d in PyTorch) with a kernel size of 2 and stride of 2. After\ntraining the model in plaintext, we deploy the resulting parameters using the\nHEAAN homomorphic encryption library to perform encrypted U-Net inference."
                },
                "authors": [
                    {
                        "name": "John Chiang"
                    }
                ],
                "author_detail": {
                    "name": "John Chiang"
                },
                "author": "John Chiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21538v1",
                "updated": "2025-04-30T11:35:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    35,
                    20,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:35:20Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    35,
                    20,
                    2,
                    120,
                    0
                ],
                "title": "Coyote v2: Raising the Level of Abstraction for Data Center FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coyote v2: Raising the Level of Abstraction for Data Center FPGAs"
                },
                "summary": "In the trend towards hardware specialization, FPGAs play a dual role as\naccelerators for offloading, e.g., network virtualization, and as a vehicle for\nprototyping and exploring hardware designs. While FPGAs offer versatility and\nperformance, integrating them in larger systems remains challenging. Thus,\nrecent efforts have focused on raising the level of abstraction through better\ninterfaces and high-level programming languages. Yet, there is still quite some\nroom for improvement. In this paper, we present Coyote v2, an open source FPGA\nshell built with a novel, three-layer hierarchical design supporting dynamic\npartial reconfiguration of services and user logic, with a unified logic\ninterface, and high-level software abstractions such as support for\nmultithreading and multitenancy. Experimental results indicate Coyote v2\nreduces synthesis times between 15% and 20% and run-time reconfiguration times\nby an order of magnitude, when compared to existing systems. We also\ndemonstrate the advantages of Coyote v2 by deploying several realistic\napplications, including HyperLogLog cardinality estimation, AES encryption, and\nneural network inference. Finally, Coyote v2 places a great deal of emphasis on\nintegration with real systems through reusable and reconfigurable services,\nincluding a fully RoCE v2-compliant networking stack, a shared virtual memory\nmodel with the host, and a DMA engine between FPGAs and GPUs. We demonstrate\nthese features by, e.g., seamlessly deploying an FPGA-accelerated neural\nnetwork from Python.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the trend towards hardware specialization, FPGAs play a dual role as\naccelerators for offloading, e.g., network virtualization, and as a vehicle for\nprototyping and exploring hardware designs. While FPGAs offer versatility and\nperformance, integrating them in larger systems remains challenging. Thus,\nrecent efforts have focused on raising the level of abstraction through better\ninterfaces and high-level programming languages. Yet, there is still quite some\nroom for improvement. In this paper, we present Coyote v2, an open source FPGA\nshell built with a novel, three-layer hierarchical design supporting dynamic\npartial reconfiguration of services and user logic, with a unified logic\ninterface, and high-level software abstractions such as support for\nmultithreading and multitenancy. Experimental results indicate Coyote v2\nreduces synthesis times between 15% and 20% and run-time reconfiguration times\nby an order of magnitude, when compared to existing systems. We also\ndemonstrate the advantages of Coyote v2 by deploying several realistic\napplications, including HyperLogLog cardinality estimation, AES encryption, and\nneural network inference. Finally, Coyote v2 places a great deal of emphasis on\nintegration with real systems through reusable and reconfigurable services,\nincluding a fully RoCE v2-compliant networking stack, a shared virtual memory\nmodel with the host, and a DMA engine between FPGAs and GPUs. We demonstrate\nthese features by, e.g., seamlessly deploying an FPGA-accelerated neural\nnetwork from Python."
                },
                "authors": [
                    {
                        "name": "Benjamin Ramhorst"
                    },
                    {
                        "name": "Dario Korolija"
                    },
                    {
                        "name": "Maximilian Jakob Heer"
                    },
                    {
                        "name": "Jonas Dann"
                    },
                    {
                        "name": "Luhao Liu"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21526v1",
                "updated": "2025-04-30T11:19:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    19,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:19:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    19,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "Physics-Informed Priors Improve Gravitational-Wave Constraints on\n  Neutron-Star Matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Priors Improve Gravitational-Wave Constraints on\n  Neutron-Star Matter"
                },
                "summary": "Gravitational-wave astronomy shows great promise in determining nuclear\nphysics in a regime not accessible to terrestrial experiments. We introduce\nphysics-informed priors constrained by nuclear theory and perturbative Quantum\nChromodynamics calculations, as well as astrophysical measurements of\nneutron-star masses and radii. When these priors are used in gravitational-wave\nastrophysical inference, we show a significant improvement on nuclear equation\nof state constraints. Applying these to the first observed gravitational-wave\nbinary neutron-star merger GW170817, the constraints on the radius of a\n$1.4\\,M_\\odot$ neutron star improve from $R_{1.4} ={12.54^{+1.05}_{-1.54}} \\,\n{\\rm km}$ to $R_{1.4} = 12.11^{+0.91}_{-1.11} \\,{\\rm km}$ and those on the\ntidal deformability from $\\tilde{\\Lambda}_{1.186} < 720$ to\n$\\tilde{\\Lambda}_{1.186} = 384^{+306}_{-158}$ ($90\\%$ confidence intervals) at\nthe events measured chirp mass $\\mathcal{M}=1.186\\,M_\\odot$. We also show these\npriors can be used to perform model selection between binary neutron star and\nneutron star-black hole mergers; in the case of GW190425, the results provide\nonly marginal evidence with a Bayes factor $\\mathcal{BF}=1.33$ in favour of the\nbinary neutron star merger hypothesis. Given their ability to improve the\nastrophysical inference of binary mergers involving neutron stars, we advocate\nfor these physics-informed priors to be used as standard in the literature and\nprovide open-source code for reproducibility and adaptation of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave astronomy shows great promise in determining nuclear\nphysics in a regime not accessible to terrestrial experiments. We introduce\nphysics-informed priors constrained by nuclear theory and perturbative Quantum\nChromodynamics calculations, as well as astrophysical measurements of\nneutron-star masses and radii. When these priors are used in gravitational-wave\nastrophysical inference, we show a significant improvement on nuclear equation\nof state constraints. Applying these to the first observed gravitational-wave\nbinary neutron-star merger GW170817, the constraints on the radius of a\n$1.4\\,M_\\odot$ neutron star improve from $R_{1.4} ={12.54^{+1.05}_{-1.54}} \\,\n{\\rm km}$ to $R_{1.4} = 12.11^{+0.91}_{-1.11} \\,{\\rm km}$ and those on the\ntidal deformability from $\\tilde{\\Lambda}_{1.186} < 720$ to\n$\\tilde{\\Lambda}_{1.186} = 384^{+306}_{-158}$ ($90\\%$ confidence intervals) at\nthe events measured chirp mass $\\mathcal{M}=1.186\\,M_\\odot$. We also show these\npriors can be used to perform model selection between binary neutron star and\nneutron star-black hole mergers; in the case of GW190425, the results provide\nonly marginal evidence with a Bayes factor $\\mathcal{BF}=1.33$ in favour of the\nbinary neutron star merger hypothesis. Given their ability to improve the\nastrophysical inference of binary mergers involving neutron stars, we advocate\nfor these physics-informed priors to be used as standard in the literature and\nprovide open-source code for reproducibility and adaptation of the method."
                },
                "authors": [
                    {
                        "name": "Spencer J. Magnall"
                    },
                    {
                        "name": "Christian Ecker"
                    },
                    {
                        "name": "Luciano Rezzolla"
                    },
                    {
                        "name": "Paul D. Lasky"
                    },
                    {
                        "name": "Simon R. Goode"
                    }
                ],
                "author_detail": {
                    "name": "Simon R. Goode"
                },
                "author": "Simon R. Goode",
                "arxiv_comment": "9 pages, 5 figures. Submitted to ApjL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21522v1",
                "updated": "2025-04-30T11:16:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    16,
                    21,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:16:21Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    16,
                    21,
                    2,
                    120,
                    0
                ],
                "title": "The Principles of Probability: From Formal Logic to Measure Theory to\n  the Principle of Indifference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Principles of Probability: From Formal Logic to Measure Theory to\n  the Principle of Indifference"
                },
                "summary": "In this work, we develop a formal system of inductive logic. It uses an\ninfinitary language that allows for countable conjunctions and disjunctions. It\nis based on a set of nine syntactic rules of inductive inference, and contains\nclassical first-order logic as a special case. We also provide natural,\nprobabilistic semantics, and prove both $\\sigma$-compactness and completeness.\n  We show that the whole of measure-theoretic probability theory is embedded in\nthis system of inductive logic. The semantic models of inductive logic are\nprobability measures on sets of structures. (Structures are the semantic models\nof finitary, deductive logic.) Moreover, any probability space, together with a\nset of its random variables, can be mapped to such a model in a way that gives\neach outcome, event, and random variable a logical interpretation. This\nembedding, however, is proper. There are scenarios that are expressible in this\nsystem of logic which cannot be formulated in a measure-theoretic probability\nmodel.\n  The principle of indifference is an idea originating with Laplace. It says,\nroughly, that if we are \"equally ignorant\" about two possibilities, then we\nshould assign them the same probability. The principle of indifference has no\nrigorous formulation in probability. It exists only as a heuristic. Moreover,\nits use has a problematic history and is prone to apparent paradoxes. Within\ninductive logic, however, we formulate it rigorously and illustrate its use\nthrough a number of examples.\n  Many of the ideas in inductive logic have counterparts in measure theory. The\nprinciple of indifference, however, does not. Its formulation requires the\nstructure of inductive logic, both its syntactic structure and the semantic\nstructures embedded in its models. As such, it exemplifies the fact that\ninductive logic is a strictly broader theory of probability than any that is\nbased on measure theory alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we develop a formal system of inductive logic. It uses an\ninfinitary language that allows for countable conjunctions and disjunctions. It\nis based on a set of nine syntactic rules of inductive inference, and contains\nclassical first-order logic as a special case. We also provide natural,\nprobabilistic semantics, and prove both $\\sigma$-compactness and completeness.\n  We show that the whole of measure-theoretic probability theory is embedded in\nthis system of inductive logic. The semantic models of inductive logic are\nprobability measures on sets of structures. (Structures are the semantic models\nof finitary, deductive logic.) Moreover, any probability space, together with a\nset of its random variables, can be mapped to such a model in a way that gives\neach outcome, event, and random variable a logical interpretation. This\nembedding, however, is proper. There are scenarios that are expressible in this\nsystem of logic which cannot be formulated in a measure-theoretic probability\nmodel.\n  The principle of indifference is an idea originating with Laplace. It says,\nroughly, that if we are \"equally ignorant\" about two possibilities, then we\nshould assign them the same probability. The principle of indifference has no\nrigorous formulation in probability. It exists only as a heuristic. Moreover,\nits use has a problematic history and is prone to apparent paradoxes. Within\ninductive logic, however, we formulate it rigorously and illustrate its use\nthrough a number of examples.\n  Many of the ideas in inductive logic have counterparts in measure theory. The\nprinciple of indifference, however, does not. Its formulation requires the\nstructure of inductive logic, both its syntactic structure and the semantic\nstructures embedded in its models. As such, it exemplifies the fact that\ninductive logic is a strictly broader theory of probability than any that is\nbased on measure theory alone."
                },
                "authors": [
                    {
                        "name": "Jason Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Jason Swanson"
                },
                "author": "Jason Swanson",
                "arxiv_comment": "240 pages. To appear in Lecture Notes in Mathematics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60A05 (Primary) 03B48 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21517v1",
                "updated": "2025-04-30T11:13:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    13,
                    16,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:13:16Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    13,
                    16,
                    2,
                    120,
                    0
                ],
                "title": "A Bayesian approach to sharing information on sensitivity of a\n  Multi-Cancer Early Detection test across and within tumour types and stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian approach to sharing information on sensitivity of a\n  Multi-Cancer Early Detection test across and within tumour types and stages"
                },
                "summary": "The Galleri (R) (GRAIL) multi-cancer early detection test measures\ncirculating tumour DNA (ctDNA) to predict the presence of more than 50\ndifferent cancers, from a blood test. If sensitivity of the test to detect\nearly-stage cancers is high, using it as part of a screening programme may lead\nto better cancer outcomes, but available evidence indicates there is\nheterogeneity in sensitivity between cancer types and stages. We describe a\nframework for sharing evidence on test sensitivity between cancer types and/or\nstages, examining whether models with different sharing assumptions are\nsupported by the evidence and considering how further data could be used to\nstrengthen inference. Bayesian hierarchical models were fitted, and the impact\nof information sharing in increasing precision of the estimates of test\nsensitivity for different cancer types and stages was examined. Assumptions on\nsharing were informed by evidence from a review of the literature on the\ndeterminants of ctDNA shedding and its detection in a blood test. Support was\nstrongest for the assumption that sensitivity can be shared only across stage 4\nfor all cancer types. There was also support for the assumption that\nsensitivities can be shared across cancer types for each stage, if cancer types\nexpected to have low sensitivity are excluded which increased precision of\nearly-stage cancer sensitivity estimates and was considered the most\nappropriate model. High heterogeneity limited improvements in precision. For\nfuture research, elicitation of expert opinion could inform more realistic\nsharing assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Galleri (R) (GRAIL) multi-cancer early detection test measures\ncirculating tumour DNA (ctDNA) to predict the presence of more than 50\ndifferent cancers, from a blood test. If sensitivity of the test to detect\nearly-stage cancers is high, using it as part of a screening programme may lead\nto better cancer outcomes, but available evidence indicates there is\nheterogeneity in sensitivity between cancer types and stages. We describe a\nframework for sharing evidence on test sensitivity between cancer types and/or\nstages, examining whether models with different sharing assumptions are\nsupported by the evidence and considering how further data could be used to\nstrengthen inference. Bayesian hierarchical models were fitted, and the impact\nof information sharing in increasing precision of the estimates of test\nsensitivity for different cancer types and stages was examined. Assumptions on\nsharing were informed by evidence from a review of the literature on the\ndeterminants of ctDNA shedding and its detection in a blood test. Support was\nstrongest for the assumption that sensitivity can be shared only across stage 4\nfor all cancer types. There was also support for the assumption that\nsensitivities can be shared across cancer types for each stage, if cancer types\nexpected to have low sensitivity are excluded which increased precision of\nearly-stage cancer sensitivity estimates and was considered the most\nappropriate model. High heterogeneity limited improvements in precision. For\nfuture research, elicitation of expert opinion could inform more realistic\nsharing assumptions."
                },
                "authors": [
                    {
                        "name": "Sofia Dias"
                    },
                    {
                        "name": "Yiwen Liu"
                    },
                    {
                        "name": "Stephen Palmer"
                    },
                    {
                        "name": "Marta O Soares"
                    }
                ],
                "author_detail": {
                    "name": "Marta O Soares"
                },
                "author": "Marta O Soares",
                "arxiv_comment": "link to code:\n  https://github.com/MCED-Galleri-HealthEconomicEval-Program/BayesianModelTestSens-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20189v2",
                "updated": "2025-04-30T10:44:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    44,
                    57,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-28T18:43:07Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    18,
                    43,
                    7,
                    0,
                    118,
                    0
                ],
                "title": "Cosmos: A Cost Model for Serverless Workflows in the 3D Compute\n  Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmos: A Cost Model for Serverless Workflows in the 3D Compute\n  Continuum"
                },
                "summary": "Due to the high scalability, infrastructure management, and pay-per-use\npricing model, serverless computing has been adopted in a wide range of\napplications such as real-time data processing, IoT, and AI-related workflows.\nHowever, deploying serverless functions across dynamic and heterogeneous\nenvironments such as the 3D (Edge-Cloud-Space) Continuum introduces additional\ncomplexity. Each layer of the 3D Continuum shows different performance\ncapabilities and costs according to workload characteristics. Cloud services\nalone often show significant differences in performance and pricing for similar\nfunctions, further complicating cost management. Additionally, serverless\nworkflows consist of functions with diverse characteristics, requiring a\ngranular understanding of performance and cost trade-offs across different\ninfrastructure layers to be able to address them individually. In this paper,\nwe present Cosmos, a cost- and a performance-cost-tradeoff model for serverless\nworkflows that identifies key factors that affect cost changes across different\nworkloads and cloud providers. We present a case study analyzing the main\ndrivers that influence the costs of serverless workflows. We demonstrate how to\nclassify the costs of serverless workflows in leading cloud providers AWS and\nGCP. Our results show that for data-intensive functions, data transfer and\nstate management costs contribute to up to 75% of the costs in AWS and 52% in\nGCP. For compute-intensive functions such as AI inference, the cost results\nshow that BaaS services are the largest cost driver, reaching up to 83% in AWS\nand 97% in GCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high scalability, infrastructure management, and pay-per-use\npricing model, serverless computing has been adopted in a wide range of\napplications such as real-time data processing, IoT, and AI-related workflows.\nHowever, deploying serverless functions across dynamic and heterogeneous\nenvironments such as the 3D (Edge-Cloud-Space) Continuum introduces additional\ncomplexity. Each layer of the 3D Continuum shows different performance\ncapabilities and costs according to workload characteristics. Cloud services\nalone often show significant differences in performance and pricing for similar\nfunctions, further complicating cost management. Additionally, serverless\nworkflows consist of functions with diverse characteristics, requiring a\ngranular understanding of performance and cost trade-offs across different\ninfrastructure layers to be able to address them individually. In this paper,\nwe present Cosmos, a cost- and a performance-cost-tradeoff model for serverless\nworkflows that identifies key factors that affect cost changes across different\nworkloads and cloud providers. We present a case study analyzing the main\ndrivers that influence the costs of serverless workflows. We demonstrate how to\nclassify the costs of serverless workflows in leading cloud providers AWS and\nGCP. Our results show that for data-intensive functions, data transfer and\nstate management costs contribute to up to 75% of the costs in AWS and 52% in\nGCP. For compute-intensive functions such as AI inference, the cost results\nshow that BaaS services are the largest cost driver, reaching up to 83% in AWS\nand 97% in GCP."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Sebastian Gollhofer-Berger"
                    },
                    {
                        "name": "Thomas Pusztai"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_journal_ref": "IEEE International Conference on Smart Computing (SmartComp 2025),\n  June 16--19, 2025, Cork, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02367v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02367v6",
                "updated": "2025-04-30T10:31:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    31,
                    29,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-03T10:25:23Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    25,
                    23,
                    3,
                    277,
                    0
                ],
                "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration"
                },
                "summary": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of $O(N^2)$,\ncompared to $O(N)$ for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of $O(N^2)$,\ncompared to $O(N)$ for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Jia wei"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Pengle Zhang"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02367v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02367v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10133v2",
                "updated": "2025-04-30T10:25:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    25,
                    22,
                    2,
                    120,
                    0
                ],
                "published": "2024-12-13T13:30:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    30,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects"
                },
                "summary": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "PUBLISHED AT ISSTA 2025",
                "arxiv_journal_ref": "ISSTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09948v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09948v3",
                "updated": "2025-05-01T01:58:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    1,
                    58,
                    55,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-14T07:18:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    18,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for\n  Arbitrary Chinese Dishes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for\n  Arbitrary Chinese Dishes"
                },
                "summary": "Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods."
                },
                "authors": [
                    {
                        "name": "Huijie Liu"
                    },
                    {
                        "name": "Bingcan Wang"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Xiaoming Wei"
                    },
                    {
                        "name": "Guoliang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Kang"
                },
                "author": "Guoliang Kang",
                "arxiv_comment": "10 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09948v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09948v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20462v2",
                "updated": "2025-04-30T10:20:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    20,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T06:50:48Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    50,
                    48,
                    1,
                    119,
                    0
                ],
                "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data"
                },
                "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Mingyi Li"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Mengbai Xiao"
                    },
                    {
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02304v4",
                "updated": "2025-04-30T10:18:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    18,
                    40,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-04T13:18:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    18,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb"
                },
                "summary": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power."
                },
                "authors": [
                    {
                        "name": "Fotis I. Giasemis"
                    },
                    {
                        "name": "Vladimir Lonƒçar"
                    },
                    {
                        "name": "Bertrand Granado"
                    },
                    {
                        "name": "Vladimir Vava Gligorov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Vava Gligorov"
                },
                "author": "Vladimir Vava Gligorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21487v1",
                "updated": "2025-04-30T10:12:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    12,
                    48,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T10:12:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    12,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling\n  for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling\n  for Image Restoration"
                },
                "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver."
                },
                "authors": [
                    {
                        "name": "Hebaixu Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Haonan Guo"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jiayi Ma"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19258v2",
                "updated": "2025-04-30T10:06:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    6,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-27T14:39:26Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    39,
                    26,
                    6,
                    117,
                    0
                ],
                "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via\n  Adaptive Radial Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via\n  Adaptive Radial Fusion"
                },
                "summary": "LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components. First, a\ncross-modal visibility mask that identifies maximal observable regions from\nboth modalities to guide feature learning. Second, an adaptive radial fusion\nmodule that dynamically consolidates radial features into discriminative global\ndescriptors. Extensive experiments on the KITTI and KITTI-360 datasets\ndemonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold\nfor top-1 retrieved matches, along with 12x faster inference speed compared to\nthe state-of-the-art approach. Code and datasets will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components. First, a\ncross-modal visibility mask that identifies maximal observable regions from\nboth modalities to guide feature learning. Second, an adaptive radial fusion\nmodule that dynamically consolidates radial features into discriminative global\ndescriptors. Extensive experiments on the KITTI and KITTI-360 datasets\ndemonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold\nfor top-1 retrieved matches, along with 12x faster inference speed compared to\nthe state-of-the-art approach. Code and datasets will be publicly available."
                },
                "authors": [
                    {
                        "name": "Shuhao Kang"
                    },
                    {
                        "name": "Martin Y. Liao"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Olaf Wysocki"
                    },
                    {
                        "name": "Boris Jutzi"
                    },
                    {
                        "name": "Daniel Cremers"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cremers"
                },
                "author": "Daniel Cremers",
                "arxiv_comment": "Technical report. 15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19509v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19509v3",
                "updated": "2025-04-30T09:42:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    42,
                    0,
                    2,
                    120,
                    0
                ],
                "published": "2024-11-29T07:01:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    1,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis"
                },
                "summary": "Recent advances in diffusion models have endowed talking head synthesis with\nsubtle expressions and vivid head movements, but have also led to slow\ninference speed and insufficient control over generated results. To address\nthese issues, we propose Ditto, a diffusion-based talking head framework that\nenables fine-grained controls and real-time inference. Specifically, we utilize\nan off-the-shelf motion extractor and devise a diffusion transformer to\ngenerate representations in a specific motion space. We optimize the model\narchitecture and training strategy to address the issues in generating motion\nrepresentations, including insufficient disentanglement between motion and\nidentity, and large internal discrepancies within the representation. Besides,\nwe employ diverse conditional signals while establishing a mapping between\nmotion representation and facial semantics, enabling control over the\ngeneration process and correction of the results. Moreover, we jointly optimize\nthe holistic framework to enable streaming processing, real-time inference, and\nlow first-frame delay, offering functionalities crucial for interactive\napplications such as AI assistants. Extensive experimental results demonstrate\nthat Ditto generates compelling talking head videos and exhibits superiority in\nboth controllability and real-time performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have endowed talking head synthesis with\nsubtle expressions and vivid head movements, but have also led to slow\ninference speed and insufficient control over generated results. To address\nthese issues, we propose Ditto, a diffusion-based talking head framework that\nenables fine-grained controls and real-time inference. Specifically, we utilize\nan off-the-shelf motion extractor and devise a diffusion transformer to\ngenerate representations in a specific motion space. We optimize the model\narchitecture and training strategy to address the issues in generating motion\nrepresentations, including insufficient disentanglement between motion and\nidentity, and large internal discrepancies within the representation. Besides,\nwe employ diverse conditional signals while establishing a mapping between\nmotion representation and facial semantics, enabling control over the\ngeneration process and correction of the results. Moreover, we jointly optimize\nthe holistic framework to enable streaming processing, real-time inference, and\nlow first-frame delay, offering functionalities crucial for interactive\napplications such as AI assistants. Extensive experimental results demonstrate\nthat Ditto generates compelling talking head videos and exhibits superiority in\nboth controllability and real-time performance."
                },
                "authors": [
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Ruobing Zheng"
                    },
                    {
                        "name": "Minghui Yang"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "arxiv_comment": "Project Page: https://digital-avatar.github.io/ai/Ditto/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19509v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19509v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21463v1",
                "updated": "2025-04-30T09:38:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    38,
                    17,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T09:38:17Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    38,
                    17,
                    2,
                    120,
                    0
                ],
                "title": "RWKV-X: A Linear Complexity Hybrid Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWKV-X: A Linear Complexity Hybrid Language Model"
                },
                "summary": "In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that\ncombines the efficiency of RWKV for short-range modeling with a sparse\nattention mechanism designed to capture long-range context. Unlike previous\nhybrid approaches that rely on full attention layers and retain quadratic\ncomplexity, RWKV-X achieves linear-time complexity in training and\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\nperformance on short-context tasks. These results highlight RWKV-X as a\nscalable and efficient backbone for general-purpose language modeling, capable\nof decoding sequences up to 1 million tokens with stable speed and memory\nusage. To facilitate further research and analysis, we have made the\ncheckpoints and the associated code publicly accessible at:\nhttps://github.com/howard-hou/RWKV-X.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that\ncombines the efficiency of RWKV for short-range modeling with a sparse\nattention mechanism designed to capture long-range context. Unlike previous\nhybrid approaches that rely on full attention layers and retain quadratic\ncomplexity, RWKV-X achieves linear-time complexity in training and\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\nperformance on short-context tasks. These results highlight RWKV-X as a\nscalable and efficient backbone for general-purpose language modeling, capable\nof decoding sequences up to 1 million tokens with stable speed and memory\nusage. To facilitate further research and analysis, we have made the\ncheckpoints and the associated code publicly accessible at:\nhttps://github.com/howard-hou/RWKV-X."
                },
                "authors": [
                    {
                        "name": "Haowen Hou"
                    },
                    {
                        "name": "Zhiyi Huang"
                    },
                    {
                        "name": "Kaifeng Tan"
                    },
                    {
                        "name": "Rongchang Lu"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01713v2",
                "updated": "2025-04-30T09:32:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    32,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-03T16:25:58Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    16,
                    25,
                    58,
                    0,
                    62,
                    0
                ],
                "title": "SAGE: A Framework of Precise Retrieval for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: A Framework of Precise Retrieval for RAG"
                },
                "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Jinyang Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Su"
                },
                "author": "Jinyang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.11564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.11564v3",
                "updated": "2025-04-30T09:22:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    22,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2023-01-27T07:00:54Z",
                "published_parsed": [
                    2023,
                    1,
                    27,
                    7,
                    0,
                    54,
                    4,
                    27,
                    0
                ],
                "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance\n  Grounding"
                },
                "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape"
                },
                "authors": [
                    {
                        "name": "Yaoxian Song"
                    },
                    {
                        "name": "Penglei Sun"
                    },
                    {
                        "name": "Piaopiao Jin"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Tiefeng Li"
                    },
                    {
                        "name": "Jason Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jason Gu"
                },
                "author": "Jason Gu",
                "arxiv_comment": "15 pages, 8 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.11564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.11564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21447v1",
                "updated": "2025-04-30T09:07:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    7,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T09:07:10Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    7,
                    10,
                    2,
                    120,
                    0
                ],
                "title": "Rethinking Visual Layer Selection in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Layer Selection in Multimodal LLMs"
                },
                "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs."
                },
                "authors": [
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Junyan Lin"
                    },
                    {
                        "name": "Xinhao Chen"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Jianfeng Dong"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "8 pages, 4 figures, submitted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17827v3",
                "updated": "2025-04-30T08:52:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    52,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-24T03:09:04Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    9,
                    4,
                    3,
                    114,
                    0
                ],
                "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution Meets Diffusion: Efficient Neural Architecture Generation"
                },
                "summary": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Bingye Zhou"
                    },
                    {
                        "name": "Caiyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Caiyang Yu"
                },
                "author": "Caiyang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21436v1",
                "updated": "2025-04-30T08:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    51,
                    6,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:51:06Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    51,
                    6,
                    2,
                    120,
                    0
                ],
                "title": "Whispers of Data: Unveiling Label Distributions in Federated Learning\n  Through Virtual Client Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whispers of Data: Unveiling Label Distributions in Federated Learning\n  Through Virtual Client Simulation"
                },
                "summary": "Federated Learning enables collaborative training of a global model across\nmultiple geographically dispersed clients without the need for data sharing.\nHowever, it is susceptible to inference attacks, particularly label inference\nattacks.\n  Existing studies on label distribution inference exhibits sensitive to the\nspecific settings of the victim client and typically underperforms under\ndefensive strategies. In this study, we propose a novel label distribution\ninference attack that is stable and adaptable to various scenarios.\nSpecifically, we estimate the size of the victim client's dataset and construct\nseveral virtual clients tailored to the victim client. We then quantify the\ntemporal generalization of each class label for the virtual clients and utilize\nthe variation in temporal generalization to train an inference model that\npredicts the label distribution proportions of the victim client.\n  We validate our approach on multiple datasets, including MNIST,\nFashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of\nour method compared to state-of-the-art techniques. Furthermore, our attack\nremains effective even under differential privacy defense mechanisms,\nunderscoring its potential for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning enables collaborative training of a global model across\nmultiple geographically dispersed clients without the need for data sharing.\nHowever, it is susceptible to inference attacks, particularly label inference\nattacks.\n  Existing studies on label distribution inference exhibits sensitive to the\nspecific settings of the victim client and typically underperforms under\ndefensive strategies. In this study, we propose a novel label distribution\ninference attack that is stable and adaptable to various scenarios.\nSpecifically, we estimate the size of the victim client's dataset and construct\nseveral virtual clients tailored to the victim client. We then quantify the\ntemporal generalization of each class label for the virtual clients and utilize\nthe variation in temporal generalization to train an inference model that\npredicts the label distribution proportions of the victim client.\n  We validate our approach on multiple datasets, including MNIST,\nFashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of\nour method compared to state-of-the-art techniques. Furthermore, our attack\nremains effective even under differential privacy defense mechanisms,\nunderscoring its potential for real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhixuan Ma"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Junxiang Huang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21432v1",
                "updated": "2025-04-30T08:40:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    40,
                    47,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:40:47Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    40,
                    47,
                    2,
                    120,
                    0
                ],
                "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs"
                },
                "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy."
                },
                "authors": [
                    {
                        "name": "Pranav Saxena"
                    },
                    {
                        "name": "Nishant Raghuvanshi"
                    },
                    {
                        "name": "Neena Goveas"
                    }
                ],
                "author_detail": {
                    "name": "Neena Goveas"
                },
                "author": "Neena Goveas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21414v1",
                "updated": "2025-04-30T08:16:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    16,
                    33,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:16:33Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    16,
                    33,
                    2,
                    120,
                    0
                ],
                "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without\n  Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting In-Domain Few-Shot Segmentation to New Domains without\n  Retraining"
                },
                "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks."
                },
                "authors": [
                    {
                        "name": "Qi Fan"
                    },
                    {
                        "name": "Kaiqi Liu"
                    },
                    {
                        "name": "Nian Liu"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Wenbin Li"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21409v1",
                "updated": "2025-04-30T08:09:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    9,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:09:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    9,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Towards Intelligent Edge Sensing for ISCC Network: Joint Multi-Tier DNN\n  Partitioning and Beamforming Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Intelligent Edge Sensing for ISCC Network: Joint Multi-Tier DNN\n  Partitioning and Beamforming Design"
                },
                "summary": "The combination of Integrated Sensing and Communication (ISAC) and Mobile\nEdge Computing (MEC) enables devices to simultaneously sense the environment\nand offload data to the base stations (BS) for intelligent processing, thereby\nreducing local computational burdens. However, transmitting raw sensing data\nfrom ISAC devices to the BS often incurs substantial fronthaul overhead and\nlatency. This paper investigates a three-tier collaborative inference framework\nenabled by Integrated Sensing, Communication, and Computing (ISCC), where cloud\nservers, MEC servers, and ISAC devices cooperatively execute different segments\nof a pre-trained deep neural network (DNN) for intelligent sensing. By\noffloading intermediate DNN features, the proposed framework can significantly\nreduce fronthaul transmission load. Furthermore, multiple-input multiple-output\n(MIMO) technology is employed to enhance both sensing quality and offloading\nefficiency. To minimize the overall sensing task inference latency across all\nISAC devices, we jointly optimize the DNN partitioning strategy, ISAC\nbeamforming, and computational resource allocation at the MEC servers and\ndevices, subject to sensing beampattern constraints. We also propose an\nefficient two-layer optimization algorithm. In the inner layer, we derive\nclosed-form solutions for computational resource allocation using the\nKarush-Kuhn-Tucker conditions. Moreover, we design the ISAC beamforming vectors\nvia an iterative method based on the majorization-minimization and weighted\nminimum mean square error techniques. In the outer layer, we develop a\ncross-entropy based probabilistic learning algorithm to determine an optimal\nDNN partitioning strategy. Simulation results demonstrate that the proposed\nframework substantially outperforms existing two-tier schemes in inference\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of Integrated Sensing and Communication (ISAC) and Mobile\nEdge Computing (MEC) enables devices to simultaneously sense the environment\nand offload data to the base stations (BS) for intelligent processing, thereby\nreducing local computational burdens. However, transmitting raw sensing data\nfrom ISAC devices to the BS often incurs substantial fronthaul overhead and\nlatency. This paper investigates a three-tier collaborative inference framework\nenabled by Integrated Sensing, Communication, and Computing (ISCC), where cloud\nservers, MEC servers, and ISAC devices cooperatively execute different segments\nof a pre-trained deep neural network (DNN) for intelligent sensing. By\noffloading intermediate DNN features, the proposed framework can significantly\nreduce fronthaul transmission load. Furthermore, multiple-input multiple-output\n(MIMO) technology is employed to enhance both sensing quality and offloading\nefficiency. To minimize the overall sensing task inference latency across all\nISAC devices, we jointly optimize the DNN partitioning strategy, ISAC\nbeamforming, and computational resource allocation at the MEC servers and\ndevices, subject to sensing beampattern constraints. We also propose an\nefficient two-layer optimization algorithm. In the inner layer, we derive\nclosed-form solutions for computational resource allocation using the\nKarush-Kuhn-Tucker conditions. Moreover, we design the ISAC beamforming vectors\nvia an iterative method based on the majorization-minimization and weighted\nminimum mean square error techniques. In the outer layer, we develop a\ncross-entropy based probabilistic learning algorithm to determine an optimal\nDNN partitioning strategy. Simulation results demonstrate that the proposed\nframework substantially outperforms existing two-tier schemes in inference\nlatency."
                },
                "authors": [
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Zesong Fei"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Weijie Yuan"
                    },
                    {
                        "name": "Yuanhao Li"
                    },
                    {
                        "name": "Cheng Hu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "13 pages, 9 figures, submitted to IEEE journal for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01338v2",
                "updated": "2025-04-30T08:08:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    49,
                    2,
                    120,
                    0
                ],
                "published": "2024-02-02T11:47:56Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    11,
                    47,
                    56,
                    4,
                    33,
                    0
                ],
                "title": "Inferring the Langevin Equation with Uncertainty via Bayesian Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the Langevin Equation with Uncertainty via Bayesian Neural\n  Networks"
                },
                "summary": "Pervasive across diverse domains, stochastic systems exhibit fluctuations in\nprocesses ranging from molecular dynamics to climate phenomena. The Langevin\nequation has served as a common mathematical model for studying such systems,\nenabling predictions of their temporal evolution and analyses of thermodynamic\nquantities, including absorbed heat, work done on the system, and entropy\nproduction. However, inferring the Langevin equation from observed trajectories\nis a challenging problem, and assessing the uncertainty associated with the\ninferred equation has yet to be accomplished. In this study, we present a\ncomprehensive framework that employs Bayesian neural networks for inferring\nLangevin equations in both overdamped and underdamped regimes. Our framework\nfirst provides the drift force and diffusion matrix separately and then\ncombines them to construct the Langevin equation. By providing a distribution\nof predictions instead of a single value, our approach allows us to assess\nprediction uncertainties, which can help prevent potential misunderstandings\nand erroneous decisions about the system. We demonstrate the effectiveness of\nour framework in inferring Langevin equations for various scenarios including a\nneuron model and microscopic engine, highlighting its versatility and potential\nimpact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pervasive across diverse domains, stochastic systems exhibit fluctuations in\nprocesses ranging from molecular dynamics to climate phenomena. The Langevin\nequation has served as a common mathematical model for studying such systems,\nenabling predictions of their temporal evolution and analyses of thermodynamic\nquantities, including absorbed heat, work done on the system, and entropy\nproduction. However, inferring the Langevin equation from observed trajectories\nis a challenging problem, and assessing the uncertainty associated with the\ninferred equation has yet to be accomplished. In this study, we present a\ncomprehensive framework that employs Bayesian neural networks for inferring\nLangevin equations in both overdamped and underdamped regimes. Our framework\nfirst provides the drift force and diffusion matrix separately and then\ncombines them to construct the Langevin equation. By providing a distribution\nof predictions instead of a single value, our approach allows us to assess\nprediction uncertainties, which can help prevent potential misunderstandings\nand erroneous decisions about the system. We demonstrate the effectiveness of\nour framework in inferring Langevin equations for various scenarios including a\nneuron model and microscopic engine, highlighting its versatility and potential\nimpact."
                },
                "authors": [
                    {
                        "name": "Youngkyoung Bae"
                    },
                    {
                        "name": "Seungwoong Ha"
                    },
                    {
                        "name": "Hawoong Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Hawoong Jeong"
                },
                "author": "Hawoong Jeong",
                "arxiv_doi": "10.1016/j.chaos.2025.116440",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.chaos.2025.116440",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.01338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "34 pages, 17 figures",
                "arxiv_journal_ref": "Chaos, Solitons & Fractals 197 (2025) 116440",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06048v2",
                "updated": "2025-04-30T08:05:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    5,
                    13,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-08T13:47:07Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    47,
                    7,
                    1,
                    98,
                    0
                ],
                "title": "Trust-Region Twisted Policy Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust-Region Twisted Policy Improvement"
                },
                "summary": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains."
                },
                "authors": [
                    {
                        "name": "Joery A. de Vries"
                    },
                    {
                        "name": "Jinke He"
                    },
                    {
                        "name": "Yaniv Oren"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs T. J. Spaan"
                },
                "author": "Matthijs T. J. Spaan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04178v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04178v3",
                "updated": "2025-04-30T08:01:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    1,
                    26,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-05T13:48:33Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    13,
                    48,
                    33,
                    5,
                    95,
                    0
                ],
                "title": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender"
                },
                "summary": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yuegang Sun"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "arxiv_comment": "Accepted by SIGIR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04178v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04178v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21400v1",
                "updated": "2025-04-30T07:55:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    55,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    55,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "Who Gets the Callback? Generative AI and Gender Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Gets the Callback? Generative AI and Gender Bias"
                },
                "summary": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms."
                },
                "authors": [
                    {
                        "name": "Sugat Chaturvedi"
                    },
                    {
                        "name": "Rochana Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Rochana Chaturvedi"
                },
                "author": "Rochana Chaturvedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21398v1",
                "updated": "2025-04-30T07:54:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    54,
                    4,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:54:04Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    54,
                    4,
                    2,
                    120,
                    0
                ],
                "title": "In a Few Words: Comparing Weak Supervision and LLMs for Short Query\n  Intent Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a Few Words: Comparing Weak Supervision and LLMs for Short Query\n  Intent Classification"
                },
                "summary": "User intent classification is an important task in information retrieval.\nPreviously, user intents were classified manually and automatically; the latter\nhelped to avoid hand labelling of large datasets. Recent studies explored\nwhether LLMs can reliably determine user intent. However, researchers have\nrecognized the limitations of using generative LLMs for classification tasks.\nIn this study, we empirically compare user intent classification into\ninformational, navigational, and transactional categories, using weak\nsupervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and\nLLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for\nfine-tuning, comparing their performance to an established baseline classifier\ntrained using weak supervision (ORCAS-I). Our results indicate that while LLMs\noutperform weak supervision in recall, they continue to struggle with\nprecision, which shows the need for improved methods to balance both metrics\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User intent classification is an important task in information retrieval.\nPreviously, user intents were classified manually and automatically; the latter\nhelped to avoid hand labelling of large datasets. Recent studies explored\nwhether LLMs can reliably determine user intent. However, researchers have\nrecognized the limitations of using generative LLMs for classification tasks.\nIn this study, we empirically compare user intent classification into\ninformational, navigational, and transactional categories, using weak\nsupervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and\nLLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for\nfine-tuning, comparing their performance to an established baseline classifier\ntrained using weak supervision (ORCAS-I). Our results indicate that while LLMs\noutperform weak supervision in recall, they continue to struggle with\nprecision, which shows the need for improved methods to balance both metrics\neffectively."
                },
                "authors": [
                    {
                        "name": "Daria Alexander"
                    },
                    {
                        "name": "Arjen P. de Vries"
                    }
                ],
                "author_detail": {
                    "name": "Arjen P. de Vries"
                },
                "author": "Arjen P. de Vries",
                "arxiv_doi": "10.1145/3726302.3730213",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730213",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.21398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted at International ACM SIGIR Conference on Research and\n  Development in Information Retrieval (SIGIR '25), July 13--18, 2025, Padua,\n  Italy",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21390v1",
                "updated": "2025-04-30T07:44:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    44,
                    17,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:44:17Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    44,
                    17,
                    2,
                    120,
                    0
                ],
                "title": "Statistical process discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical process discovery"
                },
                "summary": "Stochastic process discovery is concerned with deriving a model capable of\nreproducing the stochastic character of observed executions of a given process,\nstored in a log. This leads to an optimisation problem in which the model's\nparameter space is searched for, driven by the resemblance between the log's\nand the model's stochastic languages. The bottleneck of such optimisation\nproblem lay in the determination of the model's stochastic language which\nexisting approaches deal with through, hardly scalable, exact computation\napproaches. In this paper we introduce a novel framework in which we combine a\nsimulation-based Bayesian parameter inference scheme, used to search for the\n``optimal'' instance of a stochastic model, with an expressive statistical\nmodel checking engine, used (during inference) to approximate the language of\nthe considered model's instance. Because of its simulation-based nature, the\npayoff is that, the runtime for discovering of the optimal instance of a model\ncan be easily traded in for accuracy, hence allowing to treat large models\nwhich would result in a prohibitive runtime with non-simulation based\nalternatives. We validate our approach on several popular event logs concerning\nreal-life systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic process discovery is concerned with deriving a model capable of\nreproducing the stochastic character of observed executions of a given process,\nstored in a log. This leads to an optimisation problem in which the model's\nparameter space is searched for, driven by the resemblance between the log's\nand the model's stochastic languages. The bottleneck of such optimisation\nproblem lay in the determination of the model's stochastic language which\nexisting approaches deal with through, hardly scalable, exact computation\napproaches. In this paper we introduce a novel framework in which we combine a\nsimulation-based Bayesian parameter inference scheme, used to search for the\n``optimal'' instance of a stochastic model, with an expressive statistical\nmodel checking engine, used (during inference) to approximate the language of\nthe considered model's instance. Because of its simulation-based nature, the\npayoff is that, the runtime for discovering of the optimal instance of a model\ncan be easily traded in for accuracy, hence allowing to treat large models\nwhich would result in a prohibitive runtime with non-simulation based\nalternatives. We validate our approach on several popular event logs concerning\nreal-life systems."
                },
                "authors": [
                    {
                        "name": "Pierre Cry"
                    },
                    {
                        "name": "Paolo Ballarini"
                    },
                    {
                        "name": "Andr√°s Horv√°th"
                    },
                    {
                        "name": "Pascale Le Gall"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Le Gall"
                },
                "author": "Pascale Le Gall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04843v3",
                "updated": "2025-04-30T07:43:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    43,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-07T08:56:16Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    56,
                    16,
                    0,
                    97,
                    0
                ],
                "title": "Data Augmentation as Free Lunch: Exploring the Test-Time Augmentation\n  for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Augmentation as Free Lunch: Exploring the Test-Time Augmentation\n  for Sequential Recommendation"
                },
                "summary": "Data augmentation has become a promising method of mitigating data sparsity\nin sequential recommendation. Existing methods generate new yet effective data\nduring model training to improve performance. However, deploying them requires\nretraining, architecture modification, or introducing additional learnable\nparameters. The above steps are time-consuming and costly for well-trained\nmodels, especially when the model scale becomes large. In this work, we explore\nthe test-time augmentation (TTA) for sequential recommendation, which augments\nthe inputs during the model inference and then aggregates the model's\npredictions for augmented data to improve final accuracy. It avoids significant\ntime and cost overhead from loss calculation and backward propagation. We first\nexperimentally disclose the potential of existing augmentation operators for\nTTA and find that the Mask and Substitute consistently achieve better\nperformance. Further analysis reveals that these two operators are effective\nbecause they retain the original sequential pattern while adding appropriate\nperturbations. Meanwhile, we argue that these two operators still face\ntime-consuming item selection or interference information from mask tokens.\nBased on the analysis and limitations, we present TNoise and TMask. The former\ninjects uniform noise into the original representation, avoiding the\ncomputational overhead of item selection. The latter blocks mask token from\nparticipating in model calculations or directly removes interactions that\nshould have been replaced with mask tokens. Comprehensive experiments\ndemonstrate the effectiveness, efficiency, and generalizability of our method.\nWe provide an anonymous implementation at https://github.com/KingGugu/TTA4SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation has become a promising method of mitigating data sparsity\nin sequential recommendation. Existing methods generate new yet effective data\nduring model training to improve performance. However, deploying them requires\nretraining, architecture modification, or introducing additional learnable\nparameters. The above steps are time-consuming and costly for well-trained\nmodels, especially when the model scale becomes large. In this work, we explore\nthe test-time augmentation (TTA) for sequential recommendation, which augments\nthe inputs during the model inference and then aggregates the model's\npredictions for augmented data to improve final accuracy. It avoids significant\ntime and cost overhead from loss calculation and backward propagation. We first\nexperimentally disclose the potential of existing augmentation operators for\nTTA and find that the Mask and Substitute consistently achieve better\nperformance. Further analysis reveals that these two operators are effective\nbecause they retain the original sequential pattern while adding appropriate\nperturbations. Meanwhile, we argue that these two operators still face\ntime-consuming item selection or interference information from mask tokens.\nBased on the analysis and limitations, we present TNoise and TMask. The former\ninjects uniform noise into the original representation, avoiding the\ncomputational overhead of item selection. The latter blocks mask token from\nparticipating in model calculations or directly removes interactions that\nshould have been replaced with mask tokens. Comprehensive experiments\ndemonstrate the effectiveness, efficiency, and generalizability of our method.\nWe provide an anonymous implementation at https://github.com/KingGugu/TTA4SR."
                },
                "authors": [
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Minhan Huang"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "arxiv_comment": "Accepted by SIGIR 2025 Full Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21380v1",
                "updated": "2025-04-30T07:28:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    28,
                    11,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:28:11Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    28,
                    11,
                    2,
                    120,
                    0
                ],
                "title": "Sparse-to-Sparse Training of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-to-Sparse Training of Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs."
                },
                "authors": [
                    {
                        "name": "In√™s Cardoso Oliveira"
                    },
                    {
                        "name": "Decebal Constantin Mocanu"
                    },
                    {
                        "name": "Luis A. Leiva"
                    }
                ],
                "author_detail": {
                    "name": "Luis A. Leiva"
                },
                "author": "Luis A. Leiva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21377v1",
                "updated": "2025-04-30T07:25:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    25,
                    14,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:25:14Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    25,
                    14,
                    2,
                    120,
                    0
                ],
                "title": "Physics-informed Gaussian Processes for Model Predictive Control of\n  Nonlinear Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed Gaussian Processes for Model Predictive Control of\n  Nonlinear Systems"
                },
                "summary": "Recently, a novel linear model predictive control algorithm based on a\nphysics-informed Gaussian Process has been introduced, whose realizations\nstrictly follow a system of underlying linear ordinary differential equations\nwith constant coefficients. The control task is formulated as an inference\nproblem by conditioning the Gaussian process prior on the setpoints and\nincorporating pointwise soft-constraints as further virtual setpoints. We apply\nthis method to systems of nonlinear differential equations, obtaining a local\napproximation through the linearization around an equilibrium point. In the\ncase of an asymptotically stable equilibrium point convergence is given through\nthe Bayesian inference schema of the Gaussian Process. Results for this are\ndemonstrated in a numerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a novel linear model predictive control algorithm based on a\nphysics-informed Gaussian Process has been introduced, whose realizations\nstrictly follow a system of underlying linear ordinary differential equations\nwith constant coefficients. The control task is formulated as an inference\nproblem by conditioning the Gaussian process prior on the setpoints and\nincorporating pointwise soft-constraints as further virtual setpoints. We apply\nthis method to systems of nonlinear differential equations, obtaining a local\napproximation through the linearization around an equilibrium point. In the\ncase of an asymptotically stable equilibrium point convergence is given through\nthe Bayesian inference schema of the Gaussian Process. Results for this are\ndemonstrated in a numerical example."
                },
                "authors": [
                    {
                        "name": "Adrian Lepp"
                    },
                    {
                        "name": "J√∂rn Tebbe"
                    },
                    {
                        "name": "Andreas Besginow"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Besginow"
                },
                "author": "Andreas Besginow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21372v1",
                "updated": "2025-04-30T07:10:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    10,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:10:10Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    10,
                    10,
                    2,
                    120,
                    0
                ],
                "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction"
                },
                "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features."
                },
                "authors": [
                    {
                        "name": "M√°t√© Gedeon"
                    }
                ],
                "author_detail": {
                    "name": "M√°t√© Gedeon"
                },
                "author": "M√°t√© Gedeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21370v1",
                "updated": "2025-04-30T07:04:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    4,
                    19,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:04:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    4,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length\n  for Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length\n  for Efficient Reasoning"
                },
                "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong\nperformance on reasoning-intensive tasks through extended Chain-of-Thought\n(CoT) prompting. While longer reasoning traces can facilitate a more thorough\nexploration of solution paths for complex problems, researchers have observed\nthat these models often \"overthink\", leading to inefficient inference. In this\npaper, we introduce ShorterBetter, a simple yet effective reinforcement\nlearning methed that enables reasoning language models to discover their own\noptimal CoT lengths without human intervention. By sampling multiple outputs\nper problem and defining the Sample Optimal Length (SOL) as the shortest\ncorrect response among all the outputs, our method dynamically guides the model\ntoward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B\nmodel, ShorterBetter achieves up to an 80% reduction in output length on both\nin-domain and out-of-domain reasoning tasks while maintaining accuracy. Our\nanalysis shows that overly long reasoning traces often reflect loss of\nreasoning direction, and thus suggests that the extended CoT produced by\nreasoning models is highly compressible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong\nperformance on reasoning-intensive tasks through extended Chain-of-Thought\n(CoT) prompting. While longer reasoning traces can facilitate a more thorough\nexploration of solution paths for complex problems, researchers have observed\nthat these models often \"overthink\", leading to inefficient inference. In this\npaper, we introduce ShorterBetter, a simple yet effective reinforcement\nlearning methed that enables reasoning language models to discover their own\noptimal CoT lengths without human intervention. By sampling multiple outputs\nper problem and defining the Sample Optimal Length (SOL) as the shortest\ncorrect response among all the outputs, our method dynamically guides the model\ntoward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B\nmodel, ShorterBetter achieves up to an 80% reduction in output length on both\nin-domain and out-of-domain reasoning tasks while maintaining accuracy. Our\nanalysis shows that overly long reasoning traces often reflect loss of\nreasoning direction, and thus suggests that the extended CoT produced by\nreasoning models is highly compressible."
                },
                "authors": [
                    {
                        "name": "Jingyang Yi"
                    },
                    {
                        "name": "Jiazheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiazheng Wang"
                },
                "author": "Jiazheng Wang",
                "arxiv_comment": "An appendix will be uploaded soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21361v1",
                "updated": "2025-04-30T06:48:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    48,
                    18,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:48:18Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    48,
                    18,
                    2,
                    120,
                    0
                ],
                "title": "Zero-Shot Super-Resolution from Unstructured Data Using a\n  Transformer-Based Neural Operator for Urban Micrometeorology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Super-Resolution from Unstructured Data Using a\n  Transformer-Based Neural Operator for Urban Micrometeorology"
                },
                "summary": "This study demonstrates that a transformer-based neural operator (TNO) can\nperform zero-shot super-resolution of two-dimensional temperature fields near\nthe ground in urban areas. During training, super-resolution is performed from\na horizontal resolution of 100 m to 20 m, while during testing, it is performed\nfrom 100 m to a finer resolution of 5 m. This setting is referred to as\nzero-shot, since no data with the target 5 m resolution are included in the\ntraining dataset. The 20 m and 5 m resolution data were independently obtained\nby dynamically downscaling the 100 m data using a physics-based\nmicrometeorology model that resolves buildings. Compared to a convolutional\nneural network, the TNO more accurately reproduces temperature distributions at\n5 m resolution and reduces test errors by approximately 33%. Furthermore, the\nTNO successfully performs zero-shot super-resolution even when trained with\nunstructured data, in which grid points are randomly arranged. These results\nsuggest that the TNO recognizes building shapes independently of grid point\nlocations and adaptively infers the temperature fields induced by buildings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates that a transformer-based neural operator (TNO) can\nperform zero-shot super-resolution of two-dimensional temperature fields near\nthe ground in urban areas. During training, super-resolution is performed from\na horizontal resolution of 100 m to 20 m, while during testing, it is performed\nfrom 100 m to a finer resolution of 5 m. This setting is referred to as\nzero-shot, since no data with the target 5 m resolution are included in the\ntraining dataset. The 20 m and 5 m resolution data were independently obtained\nby dynamically downscaling the 100 m data using a physics-based\nmicrometeorology model that resolves buildings. Compared to a convolutional\nneural network, the TNO more accurately reproduces temperature distributions at\n5 m resolution and reduces test errors by approximately 33%. Furthermore, the\nTNO successfully performs zero-shot super-resolution even when trained with\nunstructured data, in which grid points are randomly arranged. These results\nsuggest that the TNO recognizes building shapes independently of grid point\nlocations and adaptively infers the temperature fields induced by buildings."
                },
                "authors": [
                    {
                        "name": "Yuki Yasuda"
                    },
                    {
                        "name": "Ryo Onishi"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Onishi"
                },
                "author": "Ryo Onishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21360v1",
                "updated": "2025-04-30T06:44:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    44,
                    14,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:44:14Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    44,
                    14,
                    2,
                    120,
                    0
                ],
                "title": "ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality"
                },
                "summary": "While augmented reality (AR) enables new ways to play, tell stories, and\nexplore ideas rooted in the physical world, authoring personalized AR content\nremains difficult for non-experts, often requiring professional tools and time.\nPrior systems have explored AI-driven XR design but typically rely on\nmanually-defined environments and fixed asset libraries, limiting creative\nflexibility and real-world relevance. We introduce ImaginateAR, a mobile\nAI-assisted AR authoring system that aims to let anyone build anything,\nanywhere -- simply by speaking their imagination. ImaginateAR is powered by\ncustom pipelines for offline scene understanding, fast 3D asset generation, and\nLLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\"\n(P7) and iteratively refine the scene using both AI and manual tools. Our\ntechnical evaluation shows that ImaginateAR produces more accurate outdoor\nscene graphs and generates 3D meshes faster than prior methods. A three-part\nuser study (N=20) revealed preferred roles for AI in authoring, what and how\nusers create in free-form use, and design implications for future AR authoring\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While augmented reality (AR) enables new ways to play, tell stories, and\nexplore ideas rooted in the physical world, authoring personalized AR content\nremains difficult for non-experts, often requiring professional tools and time.\nPrior systems have explored AI-driven XR design but typically rely on\nmanually-defined environments and fixed asset libraries, limiting creative\nflexibility and real-world relevance. We introduce ImaginateAR, a mobile\nAI-assisted AR authoring system that aims to let anyone build anything,\nanywhere -- simply by speaking their imagination. ImaginateAR is powered by\ncustom pipelines for offline scene understanding, fast 3D asset generation, and\nLLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\"\n(P7) and iteratively refine the scene using both AI and manual tools. Our\ntechnical evaluation shows that ImaginateAR produces more accurate outdoor\nscene graphs and generates 3D meshes faster than prior methods. A three-part\nuser study (N=20) revealed preferred roles for AI in authoring, what and how\nusers create in free-form use, and design implications for future AR authoring\ntools."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Filippo Aleotti"
                    },
                    {
                        "name": "Diego Mazala"
                    },
                    {
                        "name": "Guillermo Garcia-Hernando"
                    },
                    {
                        "name": "Sara Vicente"
                    },
                    {
                        "name": "Oliver James Johnston"
                    },
                    {
                        "name": "Isabel Kraus-Liang"
                    },
                    {
                        "name": "Jakub Powierza"
                    },
                    {
                        "name": "Donghoon Shin"
                    },
                    {
                        "name": "Jon E. Froehlich"
                    },
                    {
                        "name": "Gabriel Brostow"
                    },
                    {
                        "name": "Jessica Van Brummelen"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Van Brummelen"
                },
                "author": "Jessica Van Brummelen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12256v2",
                "updated": "2025-04-30T06:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    42,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2024-11-19T06:10:22Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    10,
                    22,
                    1,
                    324,
                    0
                ],
                "title": "Restructuring Tractable Probabilistic Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Restructuring Tractable Probabilistic Circuits"
                },
                "summary": "Probabilistic circuits (PCs) are a unifying representation for probabilistic\nmodels that support tractable inference. Numerous applications of PCs like\ncontrollable text generation depend on the ability to efficiently multiply two\ncircuits. Existing multiplication algorithms require that the circuits respect\nthe same structure, i.e. variable scopes decomposes according to the same\nvtree. In this work, we propose and study the task of restructuring\nstructured(-decomposable) PCs, that is, transforming a structured PC such that\nit conforms to a target vtree. We propose a generic approach for this problem\nand show that it leads to novel polynomial-time algorithms for multiplying\ncircuits respecting different vtrees, as well as a practical depth-reduction\nalgorithm that preserves structured decomposibility. Our work opens up new\navenues for tractable PC inference, suggesting the possibility of training with\nless restrictive PC structures while enabling efficient inference by changing\ntheir structures at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic circuits (PCs) are a unifying representation for probabilistic\nmodels that support tractable inference. Numerous applications of PCs like\ncontrollable text generation depend on the ability to efficiently multiply two\ncircuits. Existing multiplication algorithms require that the circuits respect\nthe same structure, i.e. variable scopes decomposes according to the same\nvtree. In this work, we propose and study the task of restructuring\nstructured(-decomposable) PCs, that is, transforming a structured PC such that\nit conforms to a target vtree. We propose a generic approach for this problem\nand show that it leads to novel polynomial-time algorithms for multiplying\ncircuits respecting different vtrees, as well as a practical depth-reduction\nalgorithm that preserves structured decomposibility. Our work opens up new\navenues for tractable PC inference, suggesting the possibility of training with\nless restrictive PC structures while enabling efficient inference by changing\ntheir structures at inference time."
                },
                "authors": [
                    {
                        "name": "Honghua Zhang"
                    },
                    {
                        "name": "Benjie Wang"
                    },
                    {
                        "name": "Marcelo Arenas"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04168v4",
                "updated": "2025-04-30T06:36:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    36,
                    8,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-05T14:14:08Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    14,
                    14,
                    8,
                    5,
                    279,
                    0
                ],
                "title": "R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust\n  Task-Oriented Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust\n  Task-Oriented Communications"
                },
                "summary": "Collaborative perception enhances sensing in multirobot and vehicular\nnetworks by fusing information from multiple agents, improving perception\naccuracy and sensing range. However, mobility and non-rigid sensor mounts\nintroduce extrinsic calibration errors, necessitating online calibration,\nfurther complicated by limited overlap in sensing regions. Moreover,\nmaintaining fresh information is crucial for timely and accurate sensing. To\naddress calibration errors and ensure timely and accurate perception, we\npropose a robust task-oriented communication strategy to optimize online\nself-calibration and efficient feature sharing for Real-time Adaptive\nCollaborative Perception (R-ACP). Specifically, we first formulate an Age of\nPerceived Targets (AoPT) minimization problem to capture data timeliness of\nmulti-view streaming. Then, in the calibration phase, we introduce a\nchannel-aware self-calibration technique based on reidentification (Re-ID),\nwhich adaptively compresses key features according to channel capacities,\neffectively addressing calibration issues via spatial and temporal cross-camera\ncorrelations. In the streaming phase, we tackle the trade-off between bandwidth\nand inference accuracy by leveraging an Information Bottleneck (IB) based\nencoding method to adjust video compression rates based on task relevance,\nthereby reducing communication overhead and latency. Finally, we design a\npriority-aware network to filter corrupted features to mitigate performance\ndegradation from packet corruption. Extensive studies demonstrate that our\nframework outperforms five baselines, improving multiple object detection\naccuracy (MODA) by 25.49% and reducing communication costs by 51.36% under\nseverely poor channel conditions. Code will be made publicly available:\ngithub.com/fangzr/R-ACP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative perception enhances sensing in multirobot and vehicular\nnetworks by fusing information from multiple agents, improving perception\naccuracy and sensing range. However, mobility and non-rigid sensor mounts\nintroduce extrinsic calibration errors, necessitating online calibration,\nfurther complicated by limited overlap in sensing regions. Moreover,\nmaintaining fresh information is crucial for timely and accurate sensing. To\naddress calibration errors and ensure timely and accurate perception, we\npropose a robust task-oriented communication strategy to optimize online\nself-calibration and efficient feature sharing for Real-time Adaptive\nCollaborative Perception (R-ACP). Specifically, we first formulate an Age of\nPerceived Targets (AoPT) minimization problem to capture data timeliness of\nmulti-view streaming. Then, in the calibration phase, we introduce a\nchannel-aware self-calibration technique based on reidentification (Re-ID),\nwhich adaptively compresses key features according to channel capacities,\neffectively addressing calibration issues via spatial and temporal cross-camera\ncorrelations. In the streaming phase, we tackle the trade-off between bandwidth\nand inference accuracy by leveraging an Information Bottleneck (IB) based\nencoding method to adjust video compression rates based on task relevance,\nthereby reducing communication overhead and latency. Finally, we design a\npriority-aware network to filter corrupted features to mitigate performance\ndegradation from packet corruption. Extensive studies demonstrate that our\nframework outperforms five baselines, improving multiple object detection\naccuracy (MODA) by 25.49% and reducing communication costs by 51.36% under\nseverely poor channel conditions. Code will be made publicly available:\ngithub.com/fangzr/R-ACP."
                },
                "authors": [
                    {
                        "name": "Zhengru Fang"
                    },
                    {
                        "name": "Jingjing Wang"
                    },
                    {
                        "name": "Yanan Ma"
                    },
                    {
                        "name": "Yihang Tao"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "arxiv_comment": "This work has been accepted by IEEE JSAC. The code will be publicly\n  available at: github.com/fangzr/R-ACP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21356v1",
                "updated": "2025-04-30T06:30:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    30,
                    48,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:30:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    30,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing"
                },
                "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."
                },
                "authors": [
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Zhongjie Duan"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21353v1",
                "updated": "2025-04-30T06:19:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    19,
                    37,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:19:37Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    19,
                    37,
                    2,
                    120,
                    0
                ],
                "title": "Generative QoE Modeling: A Lightweight Approach for Telecom Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative QoE Modeling: A Lightweight Approach for Telecom Networks"
                },
                "summary": "Quality of Experience (QoE) prediction plays a crucial role in optimizing\nresource management and enhancing user satisfaction across both\ntelecommunication and OTT services. While recent advances predominantly rely on\ndeep learning models, this study introduces a lightweight generative modeling\nframework that balances computational efficiency, interpretability, and\npredictive accuracy. By validating the use of Vector Quantization (VQ) as a\npreprocessing technique, continuous network features are effectively\ntransformed into discrete categorical symbols, enabling integration with a\nHidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline\nenhances the model's capacity to capture dynamic QoE patterns while supporting\nprobabilistic inference on new and unseen data. Experimental results on\npublicly available time-series datasets incorporating both objective indicators\nand subjective QoE scores demonstrate the viability of this approach in\nreal-time and resource-constrained environments, where inference latency is\nalso critical. The framework offers a scalable alternative to complex deep\nlearning methods, particularly in scenarios with limited computational\nresources or where latency constraints are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality of Experience (QoE) prediction plays a crucial role in optimizing\nresource management and enhancing user satisfaction across both\ntelecommunication and OTT services. While recent advances predominantly rely on\ndeep learning models, this study introduces a lightweight generative modeling\nframework that balances computational efficiency, interpretability, and\npredictive accuracy. By validating the use of Vector Quantization (VQ) as a\npreprocessing technique, continuous network features are effectively\ntransformed into discrete categorical symbols, enabling integration with a\nHidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline\nenhances the model's capacity to capture dynamic QoE patterns while supporting\nprobabilistic inference on new and unseen data. Experimental results on\npublicly available time-series datasets incorporating both objective indicators\nand subjective QoE scores demonstrate the viability of this approach in\nreal-time and resource-constrained environments, where inference latency is\nalso critical. The framework offers a scalable alternative to complex deep\nlearning methods, particularly in scenarios with limited computational\nresources or where latency constraints are critical."
                },
                "authors": [
                    {
                        "name": "Vinti Nayar"
                    },
                    {
                        "name": "Kanica Sachdev"
                    },
                    {
                        "name": "Brejesh Lall"
                    }
                ],
                "author_detail": {
                    "name": "Brejesh Lall"
                },
                "author": "Brejesh Lall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21344v1",
                "updated": "2025-04-30T06:11:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    11,
                    34,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:11:34Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    11,
                    34,
                    2,
                    120,
                    0
                ],
                "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early\n  Lung Cancer Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early\n  Lung Cancer Detection"
                },
                "summary": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings."
                },
                "authors": [
                    {
                        "name": "Luoting Zhuang"
                    },
                    {
                        "name": "Seyed Mohammad Hossein Tabatabaei"
                    },
                    {
                        "name": "Ramin Salehi-Rad"
                    },
                    {
                        "name": "Linh M. Tran"
                    },
                    {
                        "name": "Denise R. Aberle"
                    },
                    {
                        "name": "Ashley E. Prosper"
                    },
                    {
                        "name": "William Hsu"
                    }
                ],
                "author_detail": {
                    "name": "William Hsu"
                },
                "author": "William Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2004.12571v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2004.12571v5",
                "updated": "2025-04-30T06:09:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    9,
                    54,
                    2,
                    120,
                    0
                ],
                "published": "2020-04-27T03:45:48Z",
                "published_parsed": [
                    2020,
                    4,
                    27,
                    3,
                    45,
                    48,
                    0,
                    118,
                    0
                ],
                "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Defenses against GAN-Based Feature Inference Attacks in\n  Federated Learning"
                },
                "summary": "Federated learning (FL) is a decentralized model training framework that aims\nto merge isolated data islands while maintaining data privacy. However, recent\nstudies have revealed that Generative Adversarial Network (GAN) based attacks\ncan be employed in FL to learn the distribution of private datasets and\nreconstruct recognizable images. In this paper, we exploit defenses against\nGAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to manipulate the visual features of private training images to\nmake them indistinguishable to human eyes even restored by attackers.\nSpecifically, Anti-GAN projects the private dataset onto a GAN's generator and\ncombines the generated fake images with the actual images to create the\ntraining dataset, which is then used for federated model training. The\nexperimental results demonstrate that Anti-GAN is effective in preventing\nattackers from learning the distribution of private images while causing\nminimal harm to the accuracy of the federated model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a decentralized model training framework that aims\nto merge isolated data islands while maintaining data privacy. However, recent\nstudies have revealed that Generative Adversarial Network (GAN) based attacks\ncan be employed in FL to learn the distribution of private datasets and\nreconstruct recognizable images. In this paper, we exploit defenses against\nGAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to manipulate the visual features of private training images to\nmake them indistinguishable to human eyes even restored by attackers.\nSpecifically, Anti-GAN projects the private dataset onto a GAN's generator and\ncombines the generated fake images with the actual images to create the\ntraining dataset, which is then used for federated model training. The\nexperimental results demonstrate that Anti-GAN is effective in preventing\nattackers from learning the distribution of private images while causing\nminimal harm to the accuracy of the federated model."
                },
                "authors": [
                    {
                        "name": "Xinjian Luo"
                    },
                    {
                        "name": "Xianglong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Zhang"
                },
                "author": "Xianglong Zhang",
                "arxiv_doi": "10.1145/3719350",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719350",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2004.12571v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2004.12571v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ACM Transactions on Knowledge Discovery from Data\n  (TKDD), 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2405.20774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20774v3",
                "updated": "2025-04-30T17:59:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    59,
                    57,
                    2,
                    120,
                    0
                ],
                "published": "2024-05-27T17:59:43Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    59,
                    43,
                    0,
                    148,
                    0
                ],
                "title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against\n  Embodied LLM-based Decision-Making Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against\n  Embodied LLM-based Decision-Making Systems"
                },
                "summary": "Large Language Models (LLMs) have shown significant promise in real-world\ndecision-making tasks for embodied artificial intelligence, especially when\nfine-tuned to leverage their inherent common sense and reasoning abilities\nwhile being tailored to specific applications. However, this fine-tuning\nprocess introduces considerable safety and security vulnerabilities, especially\nin safety-critical cyber-physical systems. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-based Decision-making\nsystems (BALD) in embodied AI, systematically exploring the attack surfaces and\ntrigger mechanisms. Specifically, we propose three distinct attack mechanisms:\nword injection, scenario manipulation, and knowledge injection, targeting\nvarious components in the LLM-based decision-making pipeline. We perform\nextensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in\nautonomous driving and home robot tasks, demonstrating the effectiveness and\nstealthiness of our backdoor triggers across various attack channels, with\ncases like vehicles accelerating toward obstacles and robots placing knives on\nbeds. Our word and knowledge injection attacks achieve nearly 100% success rate\nacross multiple models and datasets while requiring only limited access to the\nsystem. Our scenario manipulation attack yields success rates exceeding 65%,\nreaching up to 90%, and does not require any runtime system intrusion. We also\nassess the robustness of these attacks against defenses, revealing their\nresilience. Our findings highlight critical security vulnerabilities in\nembodied LLM systems and emphasize the urgent need for safeguarding these\nsystems to mitigate potential risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant promise in real-world\ndecision-making tasks for embodied artificial intelligence, especially when\nfine-tuned to leverage their inherent common sense and reasoning abilities\nwhile being tailored to specific applications. However, this fine-tuning\nprocess introduces considerable safety and security vulnerabilities, especially\nin safety-critical cyber-physical systems. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-based Decision-making\nsystems (BALD) in embodied AI, systematically exploring the attack surfaces and\ntrigger mechanisms. Specifically, we propose three distinct attack mechanisms:\nword injection, scenario manipulation, and knowledge injection, targeting\nvarious components in the LLM-based decision-making pipeline. We perform\nextensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in\nautonomous driving and home robot tasks, demonstrating the effectiveness and\nstealthiness of our backdoor triggers across various attack channels, with\ncases like vehicles accelerating toward obstacles and robots placing knives on\nbeds. Our word and knowledge injection attacks achieve nearly 100% success rate\nacross multiple models and datasets while requiring only limited access to the\nsystem. Our scenario manipulation attack yields success rates exceeding 65%,\nreaching up to 90%, and does not require any runtime system intrusion. We also\nassess the robustness of these attacks against defenses, revealing their\nresilience. Our findings highlight critical security vulnerabilities in\nembodied LLM systems and emphasize the urgent need for safeguarding these\nsystems to mitigate potential risks."
                },
                "authors": [
                    {
                        "name": "Ruochen Jiao"
                    },
                    {
                        "name": "Shaoyuan Xie"
                    },
                    {
                        "name": "Justin Yue"
                    },
                    {
                        "name": "Takami Sato"
                    },
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Qi Alfred Chen"
                    },
                    {
                        "name": "Qi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhu"
                },
                "author": "Qi Zhu",
                "arxiv_comment": "Accepted paper at ICLR 2025, 31 pages, including main paper,\n  references, and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21851v1",
                "updated": "2025-04-30T17:58:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    58,
                    6,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:58:06Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    58,
                    6,
                    2,
                    120,
                    0
                ],
                "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and\n  Structured Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and\n  Structured Assessments"
                },
                "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability."
                },
                "authors": [
                    {
                        "name": "Sichang Tu"
                    },
                    {
                        "name": "Abigail Powers"
                    },
                    {
                        "name": "Stephen Doogan"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21848v1",
                "updated": "2025-04-30T17:55:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    55,
                    48,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:55:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    55,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "Characterizing AI Agents for Alignment and Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing AI Agents for Alignment and Governance"
                },
                "summary": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals."
                },
                "authors": [
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Iason Gabriel"
                    }
                ],
                "author_detail": {
                    "name": "Iason Gabriel"
                },
                "author": "Iason Gabriel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21841v1",
                "updated": "2025-04-30T17:51:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    51,
                    20,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:51:20Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    51,
                    20,
                    2,
                    120,
                    0
                ],
                "title": "Neuro-Symbolic Generation of Explanations for Robot Policies with\n  Weighted Signal Temporal Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Generation of Explanations for Robot Policies with\n  Weighted Signal Temporal Logic"
                },
                "summary": "Neural network-based policies have demonstrated success in many robotic\napplications, but often lack human-explanability, which poses challenges in\nsafety-critical deployments. To address this, we propose a neuro-symbolic\nexplanation framework that generates a weighted signal temporal logic (wSTL)\nspecification to describe a robot policy in a interpretable form. Existing\nmethods typically produce explanations that are verbose and inconsistent, which\nhinders explainability, and loose, which do not give meaningful insights into\nthe underlying policy. We address these issues by introducing a simplification\nprocess consisting of predicate filtering, regularization, and iterative\npruning. We also introduce three novel explainability evaluation metrics --\nconciseness, consistency, and strictness -- to assess explanation quality\nbeyond conventional classification metrics. Our method is validated in three\nsimulated robotic environments, where it outperforms baselines in generating\nconcise, consistent, and strict wSTL explanations without sacrificing\nclassification accuracy. This work bridges policy learning with formal methods,\ncontributing to safer and more transparent decision-making in robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network-based policies have demonstrated success in many robotic\napplications, but often lack human-explanability, which poses challenges in\nsafety-critical deployments. To address this, we propose a neuro-symbolic\nexplanation framework that generates a weighted signal temporal logic (wSTL)\nspecification to describe a robot policy in a interpretable form. Existing\nmethods typically produce explanations that are verbose and inconsistent, which\nhinders explainability, and loose, which do not give meaningful insights into\nthe underlying policy. We address these issues by introducing a simplification\nprocess consisting of predicate filtering, regularization, and iterative\npruning. We also introduce three novel explainability evaluation metrics --\nconciseness, consistency, and strictness -- to assess explanation quality\nbeyond conventional classification metrics. Our method is validated in three\nsimulated robotic environments, where it outperforms baselines in generating\nconcise, consistent, and strict wSTL explanations without sacrificing\nclassification accuracy. This work bridges policy learning with formal methods,\ncontributing to safer and more transparent decision-making in robotics."
                },
                "authors": [
                    {
                        "name": "Mikihisa Yuasa"
                    },
                    {
                        "name": "Ramavarapu S. Sreenivas"
                    },
                    {
                        "name": "Huy T. Tran"
                    }
                ],
                "author_detail": {
                    "name": "Huy T. Tran"
                },
                "author": "Huy T. Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07836v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07836v5",
                "updated": "2025-04-30T17:22:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    22,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-10T11:52:07Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    52,
                    7,
                    3,
                    284,
                    0
                ],
                "title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities"
                },
                "summary": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies."
                },
                "authors": [
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Mircea Lica"
                    },
                    {
                        "name": "Zarif Ikram"
                    },
                    {
                        "name": "Akihiro Nakano"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Aniket Rajiv Didolkar"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Justin Dauwels"
                    }
                ],
                "author_detail": {
                    "name": "Justin Dauwels"
                },
                "author": "Justin Dauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07836v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07836v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20152v2",
                "updated": "2025-04-30T17:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    7,
                    7,
                    2,
                    120,
                    0
                ],
                "published": "2024-05-30T15:27:56Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    27,
                    56,
                    3,
                    151,
                    0
                ],
                "title": "Uncovering Bias in Large Vision-Language Models at Scale with\n  Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Bias in Large Vision-Language Models at Scale with\n  Counterfactuals"
                },
                "summary": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images,\nproducing over 57 million responses from popular models. Our multi-dimensional\nbias evaluation framework reveals that social attributes such as perceived\nrace, gender, and physical characteristics depicted in images can significantly\ninfluence the generation of toxic content, competency-associated words, harmful\nstereotypes, and numerical ratings of individuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images,\nproducing over 57 million responses from popular models. Our multi-dimensional\nbias evaluation framework reveals that social attributes such as perceived\nrace, gender, and physical characteristics depicted in images can significantly\ninfluence the generation of toxic content, competency-associated words, harmful\nstereotypes, and numerical ratings of individuals."
                },
                "authors": [
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "Accepted to NAACL 2025 main track (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21803v1",
                "updated": "2025-04-30T17:02:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    2,
                    6,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T17:02:06Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    2,
                    6,
                    2,
                    120,
                    0
                ],
                "title": "An Empirical Study on the Effectiveness of Large Language Models for\n  Binary Code Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Effectiveness of Large Language Models for\n  Binary Code Understanding"
                },
                "summary": "Binary code analysis plays a pivotal role in the field of software security\nand is widely used in tasks such as software maintenance, malware detection,\nsoftware vulnerability discovery, patch analysis, etc. However, unlike source\ncode, reverse engineers face significant challenges in understanding binary\ncode due to the lack of intuitive semantic information. Although traditional\nreverse tools can convert binary code into C-like pseudo code, the lack of code\ncomments and symbolic information such as function names still makes code\nunderstanding difficult. In recent years, two groups of techniques have shown\npromising prospects: (1) Deep learning-based techniques have demonstrated\ncompetitive results in tasks related to binary code understanding, furthermore,\n(2) Large Language Models (LLMs) have been extensively pre-trained at the\nsource-code level for tasks such as code understanding and generation. This has\nleft participants wondering about the capabilities of LLMs in binary code\nunderstanding. To this end, this work proposes a benchmark to evaluate the\neffectiveness of LLMs in real-world reverse engineering scenarios, which covers\ntwo key binary code understanding tasks, i.e., function name recovery and\nbinary code summarization. To more comprehensively evaluate, we include\nbinaries with multiple target architectures as well as different optimization\noptions. We gain valuable insights into the capabilities and limitations\nthrough extensive empirical studies of popular LLMs using our benchmark. Our\nevaluations reveal that existing LLMs can understand binary code to a certain\nextent, thereby improving the efficiency of binary code analysis. Our results\nhighlight the great potential of the LLMs in advancing the field of binary code\nunderstanding, and provide new directions for binary code analysis techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary code analysis plays a pivotal role in the field of software security\nand is widely used in tasks such as software maintenance, malware detection,\nsoftware vulnerability discovery, patch analysis, etc. However, unlike source\ncode, reverse engineers face significant challenges in understanding binary\ncode due to the lack of intuitive semantic information. Although traditional\nreverse tools can convert binary code into C-like pseudo code, the lack of code\ncomments and symbolic information such as function names still makes code\nunderstanding difficult. In recent years, two groups of techniques have shown\npromising prospects: (1) Deep learning-based techniques have demonstrated\ncompetitive results in tasks related to binary code understanding, furthermore,\n(2) Large Language Models (LLMs) have been extensively pre-trained at the\nsource-code level for tasks such as code understanding and generation. This has\nleft participants wondering about the capabilities of LLMs in binary code\nunderstanding. To this end, this work proposes a benchmark to evaluate the\neffectiveness of LLMs in real-world reverse engineering scenarios, which covers\ntwo key binary code understanding tasks, i.e., function name recovery and\nbinary code summarization. To more comprehensively evaluate, we include\nbinaries with multiple target architectures as well as different optimization\noptions. We gain valuable insights into the capabilities and limitations\nthrough extensive empirical studies of popular LLMs using our benchmark. Our\nevaluations reveal that existing LLMs can understand binary code to a certain\nextent, thereby improving the efficiency of binary code analysis. Our results\nhighlight the great potential of the LLMs in advancing the field of binary code\nunderstanding, and provide new directions for binary code analysis techniques."
                },
                "authors": [
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Zhenkan Fu"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Gangyang Li"
                    },
                    {
                        "name": "Li Hu"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19254v2",
                "updated": "2025-04-30T16:49:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    49,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-27T14:24:45Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    24,
                    45,
                    6,
                    117,
                    0
                ],
                "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers"
                },
                "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Singh Chauhan"
                },
                "author": "Mohit Singh Chauhan",
                "arxiv_comment": "UQLM repository: https://github.com/cvs-health/uqlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14258v3",
                "updated": "2025-04-30T16:23:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    23,
                    54,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-18T13:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System"
                },
                "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Baoqing Yue"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21773v1",
                "updated": "2025-04-30T16:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T16:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness"
                },
                "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Sandeep Polisetty"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    }
                ],
                "author_detail": {
                    "name": "May Fung"
                },
                "author": "May Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21770v1",
                "updated": "2025-04-30T16:15:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    15,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T16:15:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    15,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL\n  Bugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL\n  Bugs"
                },
                "summary": "While static analysis is useful in detecting early-stage hardware security\nbugs, its efficacy is limited because it requires information to form checks\nand is often unable to explain the security impact of a detected vulnerability.\nLarge Language Models can be useful in filling these gaps by identifying\nrelevant assets, removing false violations flagged by static analysis tools,\nand explaining the reported violations. LASHED combines the two approaches\n(LLMs and Static Analysis) to overcome each other's limitations for hardware\nsecurity bug detection. We investigate our approach on four open-source SoCs\nfor five Common Weakness Enumerations (CWEs) and present strategies for\nimprovement with better prompt engineering. We find that 87.5% of instances\nflagged by our recommended scheme are plausible CWEs. In-context learning and\nasking the model to 'think again' improves LASHED's precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While static analysis is useful in detecting early-stage hardware security\nbugs, its efficacy is limited because it requires information to form checks\nand is often unable to explain the security impact of a detected vulnerability.\nLarge Language Models can be useful in filling these gaps by identifying\nrelevant assets, removing false violations flagged by static analysis tools,\nand explaining the reported violations. LASHED combines the two approaches\n(LLMs and Static Analysis) to overcome each other's limitations for hardware\nsecurity bug detection. We investigate our approach on four open-source SoCs\nfor five Common Weakness Enumerations (CWEs) and present strategies for\nimprovement with better prompt engineering. We find that 87.5% of instances\nflagged by our recommended scheme are plausible CWEs. In-context learning and\nasking the model to 'think again' improves LASHED's precision."
                },
                "authors": [
                    {
                        "name": "Baleegh Ahmad"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Benjamin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Tan"
                },
                "author": "Benjamin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21769v1",
                "updated": "2025-04-30T16:14:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    14,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T16:14:25Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    14,
                    25,
                    2,
                    120,
                    0
                ],
                "title": "LLM-based Interactive Imitation Learning for Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Interactive Imitation Learning for Robotic Manipulation"
                },
                "summary": "Recent advancements in machine learning provide methods to train autonomous\nagents capable of handling the increasing complexity of sequential\ndecision-making in robotics. Imitation Learning (IL) is a prominent approach,\nwhere agents learn to control robots based on human demonstrations. However, IL\ncommonly suffers from violating the independent and identically distributed\n(i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL)\nachieves improved performance by allowing agents to learn from interactive\nfeedback from human teachers. Despite these improvements, both approaches come\nwith significant costs due to the necessity of human involvement. Leveraging\nthe emergent capabilities of Large Language Models (LLMs) in reasoning and\ngenerating human-like responses, we introduce LLM-iTeach -- a novel IIL\nframework that utilizes an LLM as an interactive teacher to enhance agent\nperformance while alleviating the dependence on human resources. Firstly,\nLLM-iTeach uses a hierarchical prompting strategy that guides the LLM in\ngenerating a policy in Python code. Then, with a designed similarity-based\nfeedback mechanism, LLM-iTeach provides corrective and evaluative feedback\ninteractively during the agent's training. We evaluate LLM-iTeach against\nbaseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a\nstate-of-the-art IIL method using a human teacher, on various robotic\nmanipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the\nsuccess rate and achieves or even outscores that of CEILing, highlighting the\npotential of LLMs as cost-effective, human-like teachers in interactive\nlearning environments. We further demonstrate the method's potential for\ngeneralization by evaluating it on additional tasks. The code and prompts are\nprovided at: https://github.com/Tubicor/LLM-iTeach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning provide methods to train autonomous\nagents capable of handling the increasing complexity of sequential\ndecision-making in robotics. Imitation Learning (IL) is a prominent approach,\nwhere agents learn to control robots based on human demonstrations. However, IL\ncommonly suffers from violating the independent and identically distributed\n(i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL)\nachieves improved performance by allowing agents to learn from interactive\nfeedback from human teachers. Despite these improvements, both approaches come\nwith significant costs due to the necessity of human involvement. Leveraging\nthe emergent capabilities of Large Language Models (LLMs) in reasoning and\ngenerating human-like responses, we introduce LLM-iTeach -- a novel IIL\nframework that utilizes an LLM as an interactive teacher to enhance agent\nperformance while alleviating the dependence on human resources. Firstly,\nLLM-iTeach uses a hierarchical prompting strategy that guides the LLM in\ngenerating a policy in Python code. Then, with a designed similarity-based\nfeedback mechanism, LLM-iTeach provides corrective and evaluative feedback\ninteractively during the agent's training. We evaluate LLM-iTeach against\nbaseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a\nstate-of-the-art IIL method using a human teacher, on various robotic\nmanipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the\nsuccess rate and achieves or even outscores that of CEILing, highlighting the\npotential of LLMs as cost-effective, human-like teachers in interactive\nlearning environments. We further demonstrate the method's potential for\ngeneralization by evaluating it on additional tasks. The code and prompts are\nprovided at: https://github.com/Tubicor/LLM-iTeach."
                },
                "authors": [
                    {
                        "name": "Jonas Werner"
                    },
                    {
                        "name": "Kun Chu"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "To be published in IJCNN 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21759v1",
                "updated": "2025-04-30T15:59:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    59,
                    35,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:59:35Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    59,
                    35,
                    2,
                    120,
                    0
                ],
                "title": "Smart Environmental Monitoring of Marine Pollution using Edge AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Environmental Monitoring of Marine Pollution using Edge AI"
                },
                "summary": "Oil spill incidents pose severe threats to marine ecosystems and coastal\nenvironments, necessitating rapid detection and monitoring capabilities to\nmitigate environmental damage. In this paper, we demonstrate how artificial\nintelligence, despite the inherent high computational and memory requirements,\ncan be efficiently integrated into marine pollution monitoring systems. More\nprecisely, we propose a drone-based smart monitoring system leveraging a\ncompressed deep learning U-Net architecture for oil spill detection and\nthickness estimation. Compared to the standard U-Net architecture, the number\nof convolution blocks and channels per block are modified. The new model is\nthen trained on synthetic radar data to accurately predict thick oil slick\nthickness up to 10 mm. Results show that our optimized Tiny U-Net achieves\nsuperior performance with an Intersection over Union (IoU) metric of\napproximately 79%, while simultaneously reducing the model size by a factor of\n$\\sim$269x compared to the state-of-the-art. This significant model compression\nenables efficient edge computing deployment on field-programmable gate array\n(FPGA) hardware integrated directly into the drone platform. Hardware\nimplementation demonstrates near real-time thickness estimation capabilities\nwith a run-time power consumption of approximately 2.2 watts. Our findings\nhighlight the increasing potential of smart monitoring technologies and\nefficient edge computing for operational characterization in marine\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oil spill incidents pose severe threats to marine ecosystems and coastal\nenvironments, necessitating rapid detection and monitoring capabilities to\nmitigate environmental damage. In this paper, we demonstrate how artificial\nintelligence, despite the inherent high computational and memory requirements,\ncan be efficiently integrated into marine pollution monitoring systems. More\nprecisely, we propose a drone-based smart monitoring system leveraging a\ncompressed deep learning U-Net architecture for oil spill detection and\nthickness estimation. Compared to the standard U-Net architecture, the number\nof convolution blocks and channels per block are modified. The new model is\nthen trained on synthetic radar data to accurately predict thick oil slick\nthickness up to 10 mm. Results show that our optimized Tiny U-Net achieves\nsuperior performance with an Intersection over Union (IoU) metric of\napproximately 79%, while simultaneously reducing the model size by a factor of\n$\\sim$269x compared to the state-of-the-art. This significant model compression\nenables efficient edge computing deployment on field-programmable gate array\n(FPGA) hardware integrated directly into the drone platform. Hardware\nimplementation demonstrates near real-time thickness estimation capabilities\nwith a run-time power consumption of approximately 2.2 watts. Our findings\nhighlight the increasing potential of smart monitoring technologies and\nefficient edge computing for operational characterization in marine\nenvironments."
                },
                "authors": [
                    {
                        "name": "Mohamed Moursi"
                    },
                    {
                        "name": "Norbert Wehn"
                    },
                    {
                        "name": "Bilal Hammoud"
                    }
                ],
                "author_detail": {
                    "name": "Bilal Hammoud"
                },
                "author": "Bilal Hammoud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21751v1",
                "updated": "2025-04-30T15:45:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    45,
                    28,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:45:28Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    45,
                    28,
                    2,
                    120,
                    0
                ],
                "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code\n  Generation"
                },
                "summary": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Dongsheng Ma"
                    },
                    {
                        "name": "Yongan Yu"
                    },
                    {
                        "name": "Rui Ling"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21746v1",
                "updated": "2025-04-30T15:40:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    40,
                    49,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:40:49Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    40,
                    49,
                    2,
                    120,
                    0
                ],
                "title": "Laser injection locking and nanophotonic spectral translation of\n  electro-optic frequency combs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser injection locking and nanophotonic spectral translation of\n  electro-optic frequency combs"
                },
                "summary": "High-resolution electro-optic frequency combs (EO combs) consisting of\nthousands to millions of comb teeth across a bandwidth between 1 GHz to 500 GHz\nare powerful tools for atomic, molecular, and cavity-based spectroscopy,\nincluding in the context of deployable quantum sensors. However, achieving\nsufficiently high signal-to-noise ratio (SNR) EO combs for use across the broad\nrange of wavelengths required in the aforementioned applications is hindered by\nthe corresponding unavailability of relevant components such as\nnarrow-linewidth lasers, electro-optic phase modulators with adequate optical\npower handling, and low-noise optical amplifiers. Here, we address the latter\ntwo points by showing that optical injection locking of commercial Fabry-Perot\n(FP) laser diodes can help enable high SNR EO combs. We injection lock the FP\nlaser diode to more than 10^6 comb teeth at injected comb powers as low as 1 nW\nand produce a high SNR replica of the EO comb. In comparison to a commercial\nsemiconductor optical amplifier, injection locking achieves approximately 100x\ngreater SNR for the same input power (when <1 microwatt) and equal SNR for >\n35x lower input power. Such low-power injection locking is of particular\nrelevance in conjunction with nanophotonic spectral translation, which extends\nthe range of wavelengths available for EO combs. We show that the usable\nwavelength range of an EO comb produced by photo-induced second harmonic\ngeneration of an EO comb in a silicon nitride resonator is significantly\nincreased when combined with optical injection locking. Our results demonstrate\nthat optical injection locking provides a versatile and high-performance\napproach to addressing many different scenarios in which EO comb SNR would be\notherwise limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution electro-optic frequency combs (EO combs) consisting of\nthousands to millions of comb teeth across a bandwidth between 1 GHz to 500 GHz\nare powerful tools for atomic, molecular, and cavity-based spectroscopy,\nincluding in the context of deployable quantum sensors. However, achieving\nsufficiently high signal-to-noise ratio (SNR) EO combs for use across the broad\nrange of wavelengths required in the aforementioned applications is hindered by\nthe corresponding unavailability of relevant components such as\nnarrow-linewidth lasers, electro-optic phase modulators with adequate optical\npower handling, and low-noise optical amplifiers. Here, we address the latter\ntwo points by showing that optical injection locking of commercial Fabry-Perot\n(FP) laser diodes can help enable high SNR EO combs. We injection lock the FP\nlaser diode to more than 10^6 comb teeth at injected comb powers as low as 1 nW\nand produce a high SNR replica of the EO comb. In comparison to a commercial\nsemiconductor optical amplifier, injection locking achieves approximately 100x\ngreater SNR for the same input power (when <1 microwatt) and equal SNR for >\n35x lower input power. Such low-power injection locking is of particular\nrelevance in conjunction with nanophotonic spectral translation, which extends\nthe range of wavelengths available for EO combs. We show that the usable\nwavelength range of an EO comb produced by photo-induced second harmonic\ngeneration of an EO comb in a silicon nitride resonator is significantly\nincreased when combined with optical injection locking. Our results demonstrate\nthat optical injection locking provides a versatile and high-performance\napproach to addressing many different scenarios in which EO comb SNR would be\notherwise limited."
                },
                "authors": [
                    {
                        "name": "Roy Zektzer"
                    },
                    {
                        "name": "Ashish Chanana"
                    },
                    {
                        "name": "Xiyuan Lu"
                    },
                    {
                        "name": "David A. Long"
                    },
                    {
                        "name": "Kartik Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Srinivasan"
                },
                "author": "Kartik Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07825v2",
                "updated": "2025-04-30T15:32:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    32,
                    19,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-10T11:23:18Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    23,
                    18,
                    3,
                    284,
                    0
                ],
                "title": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models"
                },
                "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at https://github.com/RUCAIBox/MAET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at https://github.com/RUCAIBox/MAET."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "17 Pages. Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21735v1",
                "updated": "2025-04-30T15:31:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    31,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    31,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy\n  Training"
                },
                "summary": "Massage therapy training emphasizes hands-on techniques and effective\ntherapist--patient communication. However, many educational programs struggle\nto provide realistic practice scenarios. To address this problem, we propose\nTheraQuest, a gamified, web-based simulation platform that employs large\nlanguage models (LLMs) to generate diverse virtual patients with varying\nsymptoms and cultural backgrounds. Through interactive dialogue, anatomical\ndecision-making, and immediate assessment, trainees develop both diagnostic\nreasoning and empathetic communication skills in a low-risk environment. Unlike\nexclusively VR-based solutions, TheraQuest remains accessible via standard web\nbrowsers, mitigating the cost and discomfort associated with extended headset\nuse. Preliminary testing suggests that integrating LLM-driven virtual patients\nwith real-time skill metrics can enhance trainee engagement and help bridge the\ngap between theoretical knowledge and clinical proficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massage therapy training emphasizes hands-on techniques and effective\ntherapist--patient communication. However, many educational programs struggle\nto provide realistic practice scenarios. To address this problem, we propose\nTheraQuest, a gamified, web-based simulation platform that employs large\nlanguage models (LLMs) to generate diverse virtual patients with varying\nsymptoms and cultural backgrounds. Through interactive dialogue, anatomical\ndecision-making, and immediate assessment, trainees develop both diagnostic\nreasoning and empathetic communication skills in a low-risk environment. Unlike\nexclusively VR-based solutions, TheraQuest remains accessible via standard web\nbrowsers, mitigating the cost and discomfort associated with extended headset\nuse. Preliminary testing suggests that integrating LLM-driven virtual patients\nwith real-time skill metrics can enhance trainee engagement and help bridge the\ngap between theoretical knowledge and clinical proficiency."
                },
                "authors": [
                    {
                        "name": "Shengqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shengqian Wang"
                },
                "author": "Shengqian Wang",
                "arxiv_comment": "8 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07723v3",
                "updated": "2025-04-30T15:11:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    11,
                    38,
                    2,
                    120,
                    0
                ],
                "published": "2024-06-24T03:58:11Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    58,
                    11,
                    0,
                    176,
                    0
                ],
                "title": "Lossless data compression by large models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless data compression by large models"
                },
                "summary": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression. We have previously shown all\nunderstanding or learning are compression, under reasonable assumptions. Large\nlanguage models (LLMs) understand data better than ever before. Can they help\nus to compress data? The LLMs may be seen to approximate the uncomputable\nSolomonoff induction. Therefore, under this new uncomputable paradigm, we\npresent LMCompress. LMCompress shatters all previous lossless compression\nalgorithms, doubling the lossless compression ratios of JPEG-XL for images,\nFLAC for audios, and H.264 for videos, and quadrupling the compression ratio of\nbz2 for texts. The better a large model understands the data, the better\nLMCompress compresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression. We have previously shown all\nunderstanding or learning are compression, under reasonable assumptions. Large\nlanguage models (LLMs) understand data better than ever before. Can they help\nus to compress data? The LLMs may be seen to approximate the uncomputable\nSolomonoff induction. Therefore, under this new uncomputable paradigm, we\npresent LMCompress. LMCompress shatters all previous lossless compression\nalgorithms, doubling the lossless compression ratios of JPEG-XL for images,\nFLAC for audios, and H.264 for videos, and quadrupling the compression ratio of\nbz2 for texts. The better a large model understands the data, the better\nLMCompress compresses."
                },
                "authors": [
                    {
                        "name": "Ziguang Li"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Xuliang Wang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Cole Wyeth"
                    },
                    {
                        "name": "Dongbo Bu"
                    },
                    {
                        "name": "Quan Yu"
                    },
                    {
                        "name": "Wen Gao"
                    },
                    {
                        "name": "Xingwu Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_doi": "10.1038/s42256-025-01033-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-025-01033-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published by Nature Machine Intelligence at\n  https://www.nature.com/articles/s42256-025-01033-7",
                "arxiv_journal_ref": "Nature Machine Intelligence, 2025, May 1",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16658v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16658v3",
                "updated": "2025-04-30T15:05:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    5,
                    27,
                    2,
                    120,
                    0
                ],
                "published": "2024-10-22T03:19:16Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    19,
                    16,
                    1,
                    296,
                    0
                ],
                "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent"
                },
                "summary": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions."
                },
                "authors": [
                    {
                        "name": "Janghoon Ock"
                    },
                    {
                        "name": "Tirtha Vinchurkar"
                    },
                    {
                        "name": "Yayati Jadhav"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16658v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04785v2",
                "updated": "2025-04-30T15:04:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    4,
                    55,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-27T14:02:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    2,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice"
                },
                "summary": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jos√© Siqueira de Cerqueira"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Rebekah Rousi"
                    },
                    {
                        "name": "Nannan Xi"
                    },
                    {
                        "name": "Juho Hamari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21716v1",
                "updated": "2025-04-30T15:00:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    0,
                    20,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T15:00:20Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    0,
                    20,
                    2,
                    120,
                    0
                ],
                "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in\n  Household Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in\n  Household Robotics"
                },
                "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr."
                },
                "authors": [
                    {
                        "name": "Marc Glocker"
                    },
                    {
                        "name": "Peter H√∂nig"
                    },
                    {
                        "name": "Matthias Hirschmanner"
                    },
                    {
                        "name": "Markus Vincze"
                    }
                ],
                "author_detail": {
                    "name": "Markus Vincze"
                },
                "author": "Markus Vincze",
                "arxiv_comment": "Accepted at Austrian Robotics Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18964v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18964v4",
                "updated": "2025-04-30T14:53:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    53,
                    16,
                    2,
                    120,
                    0
                ],
                "published": "2023-10-29T10:07:32Z",
                "published_parsed": [
                    2023,
                    10,
                    29,
                    10,
                    7,
                    32,
                    6,
                    302,
                    0
                ],
                "title": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection"
                },
                "summary": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. While our research\ndemonstrates the potential of large language models (LLMs) for hate speech\ndetection, several limitations remain, particularly regarding the validity and\nthe reproducibility of the results. We conclude with an exhaustive discussion\nof the challenges we faced in our experimentation and offer recommended best\npractices for future scholars designing benchmarking experiments of this kind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. While our research\ndemonstrates the potential of large language models (LLMs) for hate speech\ndetection, several limitations remain, particularly regarding the validity and\nthe reproducibility of the results. We conclude with an exhaustive discussion\nof the challenges we faced in our experimentation and offer recommended best\npractices for future scholars designing benchmarking experiments of this kind."
                },
                "authors": [
                    {
                        "name": "Ahmad Nasir"
                    },
                    {
                        "name": "Aadish Sharma"
                    },
                    {
                        "name": "Kokil Jaidka"
                    },
                    {
                        "name": "Saifuddin Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Saifuddin Ahmed"
                },
                "author": "Saifuddin Ahmed",
                "arxiv_comment": "18 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18964v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18964v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21700v1",
                "updated": "2025-04-30T14:44:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs"
                },
                "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Vignesh Kumar Kembu"
                    },
                    {
                        "name": "Antonino Nocera"
                    },
                    {
                        "name": "Vinod P"
                    }
                ],
                "author_detail": {
                    "name": "Vinod P"
                },
                "author": "Vinod P",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21680v1",
                "updated": "2025-04-30T14:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    18,
                    11,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:18:11Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    18,
                    11,
                    2,
                    120,
                    0
                ],
                "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate\n  Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hoist with His Own Petard: Inducing Guardrails to Facilitate\n  Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions."
                },
                "authors": [
                    {
                        "name": "Pan Suo"
                    },
                    {
                        "name": "Yu-Ming Shang"
                    },
                    {
                        "name": "San-Chuan Guo"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang",
                "arxiv_comment": "11 pages, 6 figures. This work will be submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21668v1",
                "updated": "2025-04-30T14:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    10,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T14:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    10,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security."
                },
                "authors": [
                    {
                        "name": "Baolei Zhang"
                    },
                    {
                        "name": "Haoran Xin"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Zhuqing Liu"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "arxiv_comment": "Accepted by The Web Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20828v2",
                "updated": "2025-04-30T14:08:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    8,
                    38,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T14:51:26Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    51,
                    26,
                    1,
                    119,
                    0
                ],
                "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs."
                },
                "authors": [
                    {
                        "name": "Azam Ikram"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Sameh Elnikety"
                    },
                    {
                        "name": "Saurabh Bagchi"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Bagchi"
                },
                "author": "Saurabh Bagchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02891v2",
                "updated": "2025-04-30T13:55:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    55,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-26T22:34:44Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    22,
                    34,
                    44,
                    2,
                    57,
                    0
                ],
                "title": "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies"
                },
                "summary": "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Shaibal Saha"
                    },
                    {
                        "name": "Lanyu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lanyu Xu"
                },
                "author": "Lanyu Xu",
                "arxiv_comment": "Accepted in Neurocomputing, Elsevier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21625v1",
                "updated": "2025-04-30T13:28:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn\n  Instruction-Following Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn\n  Instruction-Following Ability"
                },
                "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications."
                },
                "authors": [
                    {
                        "name": "Jiaming Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Wang"
                },
                "author": "Jiaming Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15551v2",
                "updated": "2025-04-30T13:26:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    26,
                    38,
                    2,
                    120,
                    0
                ],
                "published": "2024-09-23T21:07:06Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    21,
                    7,
                    6,
                    0,
                    267,
                    0
                ],
                "title": "Revise, Reason, and Recognize: LLM-Based Emotion Recognition via\n  Emotion-Specific Prompts and ASR Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revise, Reason, and Recognize: LLM-Based Emotion Recognition via\n  Emotion-Specific Prompts and ASR Error Correction"
                },
                "summary": "Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains."
                },
                "authors": [
                    {
                        "name": "Yuanchao Li"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Catherine Lai"
                    }
                ],
                "author_detail": {
                    "name": "Catherine Lai"
                },
                "author": "Catherine Lai",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20114v2",
                "updated": "2025-04-30T13:15:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    15,
                    49,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-28T01:56:31Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    1,
                    56,
                    31,
                    0,
                    118,
                    0
                ],
                "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering"
                },
                "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG."
                },
                "authors": [
                    {
                        "name": "Zhonghao Li"
                    },
                    {
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "name": "Jinghuai Ou"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21614v1",
                "updated": "2025-04-30T13:10:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    10,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:10:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    10,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary\n  Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary\n  Data Selection"
                },
                "summary": "With an ever-increasing availability of data, it has become more and more\nchallenging to select and label appropriate samples for the training of machine\nlearning models. It is especially difficult to detect long-tail classes of\ninterest in large amounts of unlabeled data. This holds especially true for\nIntelligent Transportation Systems (ITS), where vehicle fleets and roadside\nperception systems generate an abundance of raw data. While industrial,\nproprietary data engines for such iterative data selection and model training\nprocesses exist, researchers and the open-source community suffer from a lack\nof an openly available system. We present the Mcity Data Engine, which provides\nmodules for the complete data-based development cycle, beginning at the data\nacquisition phase and ending at the model deployment stage. The Mcity Data\nEngine focuses on rare and novel classes through an open-vocabulary data\nselection process. All code is publicly available on GitHub under an MIT\nlicense: https://github.com/mcity/mcity_data_engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With an ever-increasing availability of data, it has become more and more\nchallenging to select and label appropriate samples for the training of machine\nlearning models. It is especially difficult to detect long-tail classes of\ninterest in large amounts of unlabeled data. This holds especially true for\nIntelligent Transportation Systems (ITS), where vehicle fleets and roadside\nperception systems generate an abundance of raw data. While industrial,\nproprietary data engines for such iterative data selection and model training\nprocesses exist, researchers and the open-source community suffer from a lack\nof an openly available system. We present the Mcity Data Engine, which provides\nmodules for the complete data-based development cycle, beginning at the data\nacquisition phase and ending at the model deployment stage. The Mcity Data\nEngine focuses on rare and novel classes through an open-vocabulary data\nselection process. All code is publicly available on GitHub under an MIT\nlicense: https://github.com/mcity/mcity_data_engine"
                },
                "authors": [
                    {
                        "name": "Daniel Bogdoll"
                    },
                    {
                        "name": "Rajanikant Patnaik Ananta"
                    },
                    {
                        "name": "Abeyankar Giridharan"
                    },
                    {
                        "name": "Isabel Moore"
                    },
                    {
                        "name": "Gregory Stevens"
                    },
                    {
                        "name": "Henry X. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Henry X. Liu"
                },
                "author": "Henry X. Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18428v2",
                "updated": "2025-04-30T13:10:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    10,
                    37,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-25T15:39:04Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"
                },
                "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Chenshu Sun"
                    },
                    {
                        "name": "Feitong Sun"
                    },
                    {
                        "name": "Jiran Zhang"
                    },
                    {
                        "name": "Junxuan Wu"
                    },
                    {
                        "name": "Qiqian Cang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21605v1",
                "updated": "2025-04-30T13:06:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    6,
                    40,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T13:06:40Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    6,
                    40,
                    2,
                    120,
                    0
                ],
                "title": "RDF-Based Structured Quality Assessment Representation of Multilingual\n  LLM Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDF-Based Structured Quality Assessment Representation of Multilingual\n  LLM Evaluations"
                },
                "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy."
                },
                "authors": [
                    {
                        "name": "Jonas Gwozdz"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21596v1",
                "updated": "2025-04-30T12:53:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    53,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:53:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    53,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for\n  Online Task and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Pre-trained Large Language Models with Refined Prompting for\n  Online Task and Motion Planning"
                },
                "summary": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution."
                },
                "authors": [
                    {
                        "name": "Huihui Guo"
                    },
                    {
                        "name": "Huilong Pi"
                    },
                    {
                        "name": "Yunchuan Qin"
                    },
                    {
                        "name": "Zhuo Tang"
                    },
                    {
                        "name": "Kenli Li"
                    }
                ],
                "author_detail": {
                    "name": "Kenli Li"
                },
                "author": "Kenli Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21589v1",
                "updated": "2025-04-30T12:47:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    47,
                    9,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:47:09Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    47,
                    9,
                    2,
                    120,
                    0
                ],
                "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for\n  Automated Subject Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for\n  Automated Subject Indexing"
                },
                "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts."
                },
                "authors": [
                    {
                        "name": "Lisa Kluge"
                    },
                    {
                        "name": "Maximilian K√§hler"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian K√§hler"
                },
                "author": "Maximilian K√§hler",
                "arxiv_comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21582v1",
                "updated": "2025-04-30T12:41:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    41,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    41,
                    51,
                    2,
                    120,
                    0
                ],
                "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large\n  Language Model Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large\n  Language Model Framework"
                },
                "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation."
                },
                "authors": [
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Xiangning Yu"
                    },
                    {
                        "name": "Zhiyu Zhao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "27 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21565v1",
                "updated": "2025-04-30T12:09:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    9,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:09:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    9,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Towards proactive self-adaptive AI for non-stationary environments with\n  dataset shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards proactive self-adaptive AI for non-stationary environments with\n  dataset shifts"
                },
                "summary": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health."
                },
                "authors": [
                    {
                        "name": "David Fern√°ndez Narro"
                    },
                    {
                        "name": "Pablo Ferri"
                    },
                    {
                        "name": "Juan M. Garc√≠a-G√≥mez"
                    },
                    {
                        "name": "Carlos S√°ez"
                    }
                ],
                "author_detail": {
                    "name": "Carlos S√°ez"
                },
                "author": "Carlos S√°ez",
                "arxiv_comment": "6 pages, 4 figures, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08165v2",
                "updated": "2025-04-30T12:02:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    2,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2024-11-12T20:15:58Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    20,
                    15,
                    58,
                    1,
                    317,
                    0
                ],
                "title": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for\n  Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for\n  Knowledge Graph Completion"
                },
                "summary": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets."
                },
                "authors": [
                    {
                        "name": "Muzhi Li"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "Accepted by NAACL2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21553v1",
                "updated": "2025-04-30T11:52:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    52,
                    18,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T11:52:18Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    52,
                    18,
                    2,
                    120,
                    0
                ],
                "title": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision\n  Quantization Strategy for LLaMA-based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision\n  Quantization Strategy for LLaMA-based Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06669v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06669v3",
                "updated": "2025-04-30T11:18:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    18,
                    40,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-09T15:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    15,
                    40,
                    29,
                    6,
                    68,
                    0
                ],
                "title": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems"
                },
                "summary": "We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence."
                },
                "authors": [
                    {
                        "name": "AgiBot-World-Contributors"
                    },
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Xiuqi Cui"
                    },
                    {
                        "name": "Yan Ding"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Shenyuan Gao"
                    },
                    {
                        "name": "Xindong He"
                    },
                    {
                        "name": "Xuan Hu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Shu Jiang"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Cheng Jing"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Chiming Liu"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yuxiang Lu"
                    },
                    {
                        "name": "Jianlan Luo"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Yuehan Niu"
                    },
                    {
                        "name": "Yixuan Pan"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Cheng Ruan"
                    },
                    {
                        "name": "Jiaqi Shan"
                    },
                    {
                        "name": "Yongjian Shen"
                    },
                    {
                        "name": "Chengshi Shi"
                    },
                    {
                        "name": "Mingkang Shi"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Chonghao Sima"
                    },
                    {
                        "name": "Jianheng Song"
                    },
                    {
                        "name": "Huijie Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dafeng Wei"
                    },
                    {
                        "name": "Chengen Xie"
                    },
                    {
                        "name": "Guo Xu"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Cunbiao Yang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Shukai Yang"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Chengyue Zhao"
                    },
                    {
                        "name": "Jiaqi Zhao"
                    },
                    {
                        "name": "Jianchao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianchao Zhu"
                },
                "author": "Jianchao Zhu",
                "arxiv_comment": "Project website: https://agibot-world.com/. Github repo:\n  https://github.com/OpenDriveLab/AgiBot-World. The author list is ordered\n  alphabetically by surname, with detailed contributions provided in the\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06669v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06669v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21518v2",
                "updated": "2025-05-01T07:09:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    7,
                    9,
                    22,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-30T11:13:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    13,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "Confidential Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Serverless Computing"
                },
                "summary": "Although serverless computing offers compelling cost and deployment\nsimplicity advantages, a significant challenge remains in securely managing\nsensitive data as it flows through the network of ephemeral function executions\nin serverless computing environments within untrusted clouds. While\nConfidential Virtual Machines (CVMs) offer a promising secure execution\nenvironment, their integration with serverless architectures currently faces\nfundamental limitations in key areas: security, performance, and resource\nefficiency. We present Hacher, a confidential computing system for secure\nserverless deployments to overcome these limitations. By employing nested\nconfidential execution and a decoupled guest OS within CVMs, Hacher runs each\nfunction in a minimal \"trustlet\", significantly improving security through a\nreduced Trusted Computing Base (TCB). Furthermore, by leveraging a data-centric\nI/O architecture built upon a lightweight LibOS, Hacher optimizes network\ncommunication to address performance and resource efficiency challenges. Our\nevaluation shows that compared to CVM-based deployments, Hacher has 4.3x\nsmaller TCB, improves end-to-end latency (15-93%), achieves higher function\ndensity (up to 907x), and reduces inter-function communication (up to 27x) and\nfunction chaining latency (16.7-30.2x); thus, Hacher offers a practical system\nfor confidential serverless computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although serverless computing offers compelling cost and deployment\nsimplicity advantages, a significant challenge remains in securely managing\nsensitive data as it flows through the network of ephemeral function executions\nin serverless computing environments within untrusted clouds. While\nConfidential Virtual Machines (CVMs) offer a promising secure execution\nenvironment, their integration with serverless architectures currently faces\nfundamental limitations in key areas: security, performance, and resource\nefficiency. We present Hacher, a confidential computing system for secure\nserverless deployments to overcome these limitations. By employing nested\nconfidential execution and a decoupled guest OS within CVMs, Hacher runs each\nfunction in a minimal \"trustlet\", significantly improving security through a\nreduced Trusted Computing Base (TCB). Furthermore, by leveraging a data-centric\nI/O architecture built upon a lightweight LibOS, Hacher optimizes network\ncommunication to address performance and resource efficiency challenges. Our\nevaluation shows that compared to CVM-based deployments, Hacher has 4.3x\nsmaller TCB, improves end-to-end latency (15-93%), achieves higher function\ndensity (up to 907x), and reduces inter-function communication (up to 27x) and\nfunction chaining latency (16.7-30.2x); thus, Hacher offers a practical system\nfor confidential serverless computing."
                },
                "authors": [
                    {
                        "name": "Patrick Sabanic"
                    },
                    {
                        "name": "Masanori Misono"
                    },
                    {
                        "name": "Teofil Bodea"
                    },
                    {
                        "name": "Julian Pritzi"
                    },
                    {
                        "name": "Michael Hackl"
                    },
                    {
                        "name": "Dimitrios Stavrakakis"
                    },
                    {
                        "name": "Pramod Bhatotia"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Bhatotia"
                },
                "author": "Pramod Bhatotia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10133v2",
                "updated": "2025-04-30T10:25:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    25,
                    22,
                    2,
                    120,
                    0
                ],
                "published": "2024-12-13T13:30:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    30,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects"
                },
                "summary": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "PUBLISHED AT ISSTA 2025",
                "arxiv_journal_ref": "ISSTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20462v2",
                "updated": "2025-04-30T10:20:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    20,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T06:50:48Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    50,
                    48,
                    1,
                    119,
                    0
                ],
                "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data"
                },
                "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Mingyi Li"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Mengbai Xiao"
                    },
                    {
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02304v4",
                "updated": "2025-04-30T10:18:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    18,
                    40,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-04T13:18:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    18,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb"
                },
                "summary": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power."
                },
                "authors": [
                    {
                        "name": "Fotis I. Giasemis"
                    },
                    {
                        "name": "Vladimir Lonƒçar"
                    },
                    {
                        "name": "Bertrand Granado"
                    },
                    {
                        "name": "Vladimir Vava Gligorov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Vava Gligorov"
                },
                "author": "Vladimir Vava Gligorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01713v2",
                "updated": "2025-04-30T09:32:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    32,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-03T16:25:58Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    16,
                    25,
                    58,
                    0,
                    62,
                    0
                ],
                "title": "SAGE: A Framework of Precise Retrieval for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: A Framework of Precise Retrieval for RAG"
                },
                "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Jinyang Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Su"
                },
                "author": "Jinyang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.11564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.11564v3",
                "updated": "2025-04-30T09:22:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    22,
                    25,
                    2,
                    120,
                    0
                ],
                "published": "2023-01-27T07:00:54Z",
                "published_parsed": [
                    2023,
                    1,
                    27,
                    7,
                    0,
                    54,
                    4,
                    27,
                    0
                ],
                "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance\n  Grounding"
                },
                "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape"
                },
                "authors": [
                    {
                        "name": "Yaoxian Song"
                    },
                    {
                        "name": "Penglei Sun"
                    },
                    {
                        "name": "Piaopiao Jin"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Tiefeng Li"
                    },
                    {
                        "name": "Jason Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jason Gu"
                },
                "author": "Jason Gu",
                "arxiv_comment": "15 pages, 8 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.11564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.11564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08349v3",
                "updated": "2025-04-30T09:11:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    11,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2023-10-12T14:18:13Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    14,
                    18,
                    13,
                    3,
                    285,
                    0
                ],
                "title": "Performativity and Prospective Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performativity and Prospective Fairness"
                },
                "summary": "Deploying an algorithmically informed policy is a significant intervention in\nthe structure of society. As is increasingly acknowledged, predictive\nalgorithms have performative effects: using them can shift the distribution of\nsocial outcomes away from the one on which the algorithms were trained.\nAlgorithmic fairness research is usually motivated by the worry that these\nperformative effects will exacerbate the structural inequalities that gave rise\nto the training data. However, standard retrospective fairness methodologies\nare ill-suited to predict these effects. They impose static fairness\nconstraints that hold after the predictive algorithm is trained, but before it\nis deployed and, therefore, before performative effects have had a chance to\nkick in. However, satisfying static fairness criteria after training is not\nsufficient to avoid exacerbating inequality after deployment. Addressing the\nfundamental worry that motivates algorithmic fairness requires explicitly\ncomparing the change in relevant structural inequalities before and after\ndeployment. We propose a prospective methodology for estimating this\npost-deployment change from pre-deployment data and knowledge about the\nalgorithmic policy. That requires a strategy for distinguishing between, and\naccounting for, different kinds of performative effects. In this paper, we\nfocus on the algorithmic effect on the causally downstream outcome variable.\nThroughout, we are guided by an application from public administration: the use\nof algorithms to (1) predict who among the recently unemployed will stay\nunemployed for the long term and (2) targeting them with labor market programs.\nWe illustrate our proposal by showing how to predict whether such policies will\nexacerbate gender inequalities in the labor market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying an algorithmically informed policy is a significant intervention in\nthe structure of society. As is increasingly acknowledged, predictive\nalgorithms have performative effects: using them can shift the distribution of\nsocial outcomes away from the one on which the algorithms were trained.\nAlgorithmic fairness research is usually motivated by the worry that these\nperformative effects will exacerbate the structural inequalities that gave rise\nto the training data. However, standard retrospective fairness methodologies\nare ill-suited to predict these effects. They impose static fairness\nconstraints that hold after the predictive algorithm is trained, but before it\nis deployed and, therefore, before performative effects have had a chance to\nkick in. However, satisfying static fairness criteria after training is not\nsufficient to avoid exacerbating inequality after deployment. Addressing the\nfundamental worry that motivates algorithmic fairness requires explicitly\ncomparing the change in relevant structural inequalities before and after\ndeployment. We propose a prospective methodology for estimating this\npost-deployment change from pre-deployment data and knowledge about the\nalgorithmic policy. That requires a strategy for distinguishing between, and\naccounting for, different kinds of performative effects. In this paper, we\nfocus on the algorithmic effect on the causally downstream outcome variable.\nThroughout, we are guided by an application from public administration: the use\nof algorithms to (1) predict who among the recently unemployed will stay\nunemployed for the long term and (2) targeting them with labor market programs.\nWe illustrate our proposal by showing how to predict whether such policies will\nexacerbate gender inequalities in the labor market."
                },
                "authors": [
                    {
                        "name": "Sebastian Zezulka"
                    },
                    {
                        "name": "Konstantin Genin"
                    }
                ],
                "author_detail": {
                    "name": "Konstantin Genin"
                },
                "author": "Konstantin Genin",
                "arxiv_comment": "Presented at NeurIPS 2023 Workshop \"Algorithmic Fairness through the\n  Lens of Time''",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.1; K.4.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21447v1",
                "updated": "2025-04-30T09:07:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    7,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T09:07:10Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    7,
                    10,
                    2,
                    120,
                    0
                ],
                "title": "Rethinking Visual Layer Selection in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Layer Selection in Multimodal LLMs"
                },
                "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs."
                },
                "authors": [
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Junyan Lin"
                    },
                    {
                        "name": "Xinhao Chen"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Jianfeng Dong"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "8 pages, 4 figures, submitted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21437v1",
                "updated": "2025-04-30T08:52:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    52,
                    32,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:52:32Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    52,
                    32,
                    2,
                    120,
                    0
                ],
                "title": "Identifying Critical Dependencies in Large-Scale Continuous Software\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Critical Dependencies in Large-Scale Continuous Software\n  Engineering"
                },
                "summary": "Continuous Software Engineering (CSE) is widely adopted in the industry,\nintegrating practices such as Continuous Integration and Continuous Deployment\n(CI/CD). Beyond technical aspects, CSE also encompasses business activities\nlike continuous planning, budgeting, and operational processes. Coordinating\nthese activities in large-scale product development involves multiple\nstakeholders, increasing complexity. This study aims to address this complexity\nby identifying and analyzing critical dependencies in large-scale CSE. Based on\n17 semi-structured interviews conducted at two Nordic fintech companies, our\npreliminary findings indicate that dependencies between software teams and\nsupport functions, as well as between software teams and external entities, are\nthe primary sources of delays and bottlenecks. As a next step, we plan to\nfurther refine our understanding of critical dependencies in large-scale CSE\nand explore coordination mechanisms that can better support software\ndevelopment teams in managing these challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Software Engineering (CSE) is widely adopted in the industry,\nintegrating practices such as Continuous Integration and Continuous Deployment\n(CI/CD). Beyond technical aspects, CSE also encompasses business activities\nlike continuous planning, budgeting, and operational processes. Coordinating\nthese activities in large-scale product development involves multiple\nstakeholders, increasing complexity. This study aims to address this complexity\nby identifying and analyzing critical dependencies in large-scale CSE. Based on\n17 semi-structured interviews conducted at two Nordic fintech companies, our\npreliminary findings indicate that dependencies between software teams and\nsupport functions, as well as between software teams and external entities, are\nthe primary sources of delays and bottlenecks. As a next step, we plan to\nfurther refine our understanding of critical dependencies in large-scale CSE\nand explore coordination mechanisms that can better support software\ndevelopment teams in managing these challenges."
                },
                "authors": [
                    {
                        "name": "Anastasiia Tkalich"
                    },
                    {
                        "name": "Eriks Klotins"
                    },
                    {
                        "name": "Nils Brede Moe"
                    }
                ],
                "author_detail": {
                    "name": "Nils Brede Moe"
                },
                "author": "Nils Brede Moe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21432v1",
                "updated": "2025-04-30T08:40:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    40,
                    47,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:40:47Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    40,
                    47,
                    2,
                    120,
                    0
                ],
                "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs"
                },
                "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy."
                },
                "authors": [
                    {
                        "name": "Pranav Saxena"
                    },
                    {
                        "name": "Nishant Raghuvanshi"
                    },
                    {
                        "name": "Neena Goveas"
                    }
                ],
                "author_detail": {
                    "name": "Neena Goveas"
                },
                "author": "Neena Goveas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15148v2",
                "updated": "2025-04-30T08:23:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    23,
                    8,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-25T09:12:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    9,
                    12,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "Finite and Asymptotic Key Analysis for CubeSat-Based BB84 QKD with\n  Elliptical Beam Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite and Asymptotic Key Analysis for CubeSat-Based BB84 QKD with\n  Elliptical Beam Approximation"
                },
                "summary": "Satellite and CubeSat-based quantum key distribution (QKD) presents a\npromising solution for secure long-distance communication by transmitting\nquantum keys through free space, with CubeSats offering a compact,\ncost-effective, and scalable platform for deployment. This study investigates\nthe performance of statistical techniques used to compute the finite-block and\nsingle-pass secret key lengths (SKL) for weak coherent pulse (WCP)-based\nefficient BB84 and standard decoy-state BB84 protocols in CubeSat-based\nsystems. An asymptotic key rate analysis is also conducted for both protocols,\nproviding deeper insights into their theoretical performance within the CubeSat\ncontext. The channel transmittance is modeled using an elliptical beam\napproximation, and the key rate performance is evaluated under varying weather\nconditions for the downlink scenario. The results demonstrate that the\nefficient BB84 protocol consistently outperforms the standard version across\ndifferent atmospheric conditions. Furthermore, the probability distribution of\nkey rates (PDR) for both implementations is analyzed, offering a comprehensive\nevaluation of their practical effectiveness in CubeSat-based QKD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite and CubeSat-based quantum key distribution (QKD) presents a\npromising solution for secure long-distance communication by transmitting\nquantum keys through free space, with CubeSats offering a compact,\ncost-effective, and scalable platform for deployment. This study investigates\nthe performance of statistical techniques used to compute the finite-block and\nsingle-pass secret key lengths (SKL) for weak coherent pulse (WCP)-based\nefficient BB84 and standard decoy-state BB84 protocols in CubeSat-based\nsystems. An asymptotic key rate analysis is also conducted for both protocols,\nproviding deeper insights into their theoretical performance within the CubeSat\ncontext. The channel transmittance is modeled using an elliptical beam\napproximation, and the key rate performance is evaluated under varying weather\nconditions for the downlink scenario. The results demonstrate that the\nefficient BB84 protocol consistently outperforms the standard version across\ndifferent atmospheric conditions. Furthermore, the probability distribution of\nkey rates (PDR) for both implementations is analyzed, offering a comprehensive\nevaluation of their practical effectiveness in CubeSat-based QKD applications."
                },
                "authors": [
                    {
                        "name": "Muskan"
                    },
                    {
                        "name": "Arindam Dutta"
                    },
                    {
                        "name": "Subhashish Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Subhashish Banerjee"
                },
                "author": "Subhashish Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13895v4",
                "updated": "2025-04-30T08:09:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    9,
                    18,
                    2,
                    120,
                    0
                ],
                "published": "2024-07-18T20:48:33Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    20,
                    48,
                    33,
                    3,
                    200,
                    0
                ],
                "title": "Improving the Robustness and Clinical Applicability of Automatic\n  Respiratory Sound Classification Using Deep Learning-Based Audio Enhancement:\n  Algorithm Development and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Robustness and Clinical Applicability of Automatic\n  Respiratory Sound Classification Using Deep Learning-Based Audio Enhancement:\n  Algorithm Development and Validation"
                },
                "summary": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions remains challenging for clinical\ndeployment. In addition, predicting signals with only background noise may\nreduce user trust in the system. This study explores the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement step\ninto automatic respiratory sound classification systems to improve robustness\nand clinical applicability. We conducted extensive experiments using various\naudio enhancement model architectures, including time-domain and\ntime-frequency-domain approaches, combined with multiple classification models\nto evaluate the module's effectiveness. The classification performance was\ncompared against the noise injection data augmentation method. These\nexperiments were carried out on two datasets: the ICBHI respiratory sound\ndataset and the FABS dataset. Furthermore, a physician validation study\nassessed the system's clinical utility. Integrating the audio enhancement\nmodule resulted in a 21.9% increase in the ICBHI classification score and a\n4.1% improvement on the FABS dataset in multi-class noisy scenarios.\nQuantitative analysis revealed efficiency gains, higher diagnostic confidence,\nand increased trust, with workflows using enhanced audio improving diagnostic\nsensitivity by 11.6% and enabling high-confidence diagnoses. Incorporating an\naudio enhancement algorithm boosts the robustness and clinical utility of\nautomatic respiratory sound classification systems, enhancing performance in\nnoisy environments and fostering greater trust among medical professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions remains challenging for clinical\ndeployment. In addition, predicting signals with only background noise may\nreduce user trust in the system. This study explores the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement step\ninto automatic respiratory sound classification systems to improve robustness\nand clinical applicability. We conducted extensive experiments using various\naudio enhancement model architectures, including time-domain and\ntime-frequency-domain approaches, combined with multiple classification models\nto evaluate the module's effectiveness. The classification performance was\ncompared against the noise injection data augmentation method. These\nexperiments were carried out on two datasets: the ICBHI respiratory sound\ndataset and the FABS dataset. Furthermore, a physician validation study\nassessed the system's clinical utility. Integrating the audio enhancement\nmodule resulted in a 21.9% increase in the ICBHI classification score and a\n4.1% improvement on the FABS dataset in multi-class noisy scenarios.\nQuantitative analysis revealed efficiency gains, higher diagnostic confidence,\nand increased trust, with workflows using enhanced audio improving diagnostic\nsensitivity by 11.6% and enabling high-confidence diagnoses. Incorporating an\naudio enhancement algorithm boosts the robustness and clinical utility of\nautomatic respiratory sound classification systems, enhancing performance in\nnoisy environments and fostering greater trust among medical professionals."
                },
                "authors": [
                    {
                        "name": "Jing-Tong Tzeng"
                    },
                    {
                        "name": "Jeng-Lin Li"
                    },
                    {
                        "name": "Huan-Yu Chen"
                    },
                    {
                        "name": "Chun-Hsiang Huang"
                    },
                    {
                        "name": "Chi-Hsin Chen"
                    },
                    {
                        "name": "Cheng-Yi Fan"
                    },
                    {
                        "name": "Edward Pei-Chuan Huang"
                    },
                    {
                        "name": "Chi-Chun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Chun Lee"
                },
                "author": "Chi-Chun Lee",
                "arxiv_doi": "10.2196/67239",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2196/67239",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on JMIR AI https://ai.jmir.org/2025/1/e67239. Demo website:\n  https://rogertzeng.github.io/ReSC-AE/",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04178v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04178v3",
                "updated": "2025-04-30T08:01:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    1,
                    26,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-05T13:48:33Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    13,
                    48,
                    33,
                    5,
                    95,
                    0
                ],
                "title": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender"
                },
                "summary": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yuegang Sun"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "arxiv_comment": "Accepted by SIGIR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04178v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04178v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21400v1",
                "updated": "2025-04-30T07:55:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    55,
                    52,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    55,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "Who Gets the Callback? Generative AI and Gender Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Gets the Callback? Generative AI and Gender Bias"
                },
                "summary": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms."
                },
                "authors": [
                    {
                        "name": "Sugat Chaturvedi"
                    },
                    {
                        "name": "Rochana Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Rochana Chaturvedi"
                },
                "author": "Rochana Chaturvedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21398v1",
                "updated": "2025-04-30T07:54:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    54,
                    4,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:54:04Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    54,
                    4,
                    2,
                    120,
                    0
                ],
                "title": "In a Few Words: Comparing Weak Supervision and LLMs for Short Query\n  Intent Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a Few Words: Comparing Weak Supervision and LLMs for Short Query\n  Intent Classification"
                },
                "summary": "User intent classification is an important task in information retrieval.\nPreviously, user intents were classified manually and automatically; the latter\nhelped to avoid hand labelling of large datasets. Recent studies explored\nwhether LLMs can reliably determine user intent. However, researchers have\nrecognized the limitations of using generative LLMs for classification tasks.\nIn this study, we empirically compare user intent classification into\ninformational, navigational, and transactional categories, using weak\nsupervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and\nLLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for\nfine-tuning, comparing their performance to an established baseline classifier\ntrained using weak supervision (ORCAS-I). Our results indicate that while LLMs\noutperform weak supervision in recall, they continue to struggle with\nprecision, which shows the need for improved methods to balance both metrics\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User intent classification is an important task in information retrieval.\nPreviously, user intents were classified manually and automatically; the latter\nhelped to avoid hand labelling of large datasets. Recent studies explored\nwhether LLMs can reliably determine user intent. However, researchers have\nrecognized the limitations of using generative LLMs for classification tasks.\nIn this study, we empirically compare user intent classification into\ninformational, navigational, and transactional categories, using weak\nsupervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and\nLLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for\nfine-tuning, comparing their performance to an established baseline classifier\ntrained using weak supervision (ORCAS-I). Our results indicate that while LLMs\noutperform weak supervision in recall, they continue to struggle with\nprecision, which shows the need for improved methods to balance both metrics\neffectively."
                },
                "authors": [
                    {
                        "name": "Daria Alexander"
                    },
                    {
                        "name": "Arjen P. de Vries"
                    }
                ],
                "author_detail": {
                    "name": "Arjen P. de Vries"
                },
                "author": "Arjen P. de Vries",
                "arxiv_doi": "10.1145/3726302.3730213",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730213",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.21398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted at International ACM SIGIR Conference on Research and\n  Development in Information Retrieval (SIGIR '25), July 13--18, 2025, Padua,\n  Italy",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21372v1",
                "updated": "2025-04-30T07:10:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    10,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T07:10:10Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    10,
                    10,
                    2,
                    120,
                    0
                ],
                "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction"
                },
                "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features."
                },
                "authors": [
                    {
                        "name": "M√°t√© Gedeon"
                    }
                ],
                "author_detail": {
                    "name": "M√°t√© Gedeon"
                },
                "author": "M√°t√© Gedeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21360v1",
                "updated": "2025-04-30T06:44:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    44,
                    14,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:44:14Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    44,
                    14,
                    2,
                    120,
                    0
                ],
                "title": "ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality"
                },
                "summary": "While augmented reality (AR) enables new ways to play, tell stories, and\nexplore ideas rooted in the physical world, authoring personalized AR content\nremains difficult for non-experts, often requiring professional tools and time.\nPrior systems have explored AI-driven XR design but typically rely on\nmanually-defined environments and fixed asset libraries, limiting creative\nflexibility and real-world relevance. We introduce ImaginateAR, a mobile\nAI-assisted AR authoring system that aims to let anyone build anything,\nanywhere -- simply by speaking their imagination. ImaginateAR is powered by\ncustom pipelines for offline scene understanding, fast 3D asset generation, and\nLLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\"\n(P7) and iteratively refine the scene using both AI and manual tools. Our\ntechnical evaluation shows that ImaginateAR produces more accurate outdoor\nscene graphs and generates 3D meshes faster than prior methods. A three-part\nuser study (N=20) revealed preferred roles for AI in authoring, what and how\nusers create in free-form use, and design implications for future AR authoring\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While augmented reality (AR) enables new ways to play, tell stories, and\nexplore ideas rooted in the physical world, authoring personalized AR content\nremains difficult for non-experts, often requiring professional tools and time.\nPrior systems have explored AI-driven XR design but typically rely on\nmanually-defined environments and fixed asset libraries, limiting creative\nflexibility and real-world relevance. We introduce ImaginateAR, a mobile\nAI-assisted AR authoring system that aims to let anyone build anything,\nanywhere -- simply by speaking their imagination. ImaginateAR is powered by\ncustom pipelines for offline scene understanding, fast 3D asset generation, and\nLLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\"\n(P7) and iteratively refine the scene using both AI and manual tools. Our\ntechnical evaluation shows that ImaginateAR produces more accurate outdoor\nscene graphs and generates 3D meshes faster than prior methods. A three-part\nuser study (N=20) revealed preferred roles for AI in authoring, what and how\nusers create in free-form use, and design implications for future AR authoring\ntools."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Filippo Aleotti"
                    },
                    {
                        "name": "Diego Mazala"
                    },
                    {
                        "name": "Guillermo Garcia-Hernando"
                    },
                    {
                        "name": "Sara Vicente"
                    },
                    {
                        "name": "Oliver James Johnston"
                    },
                    {
                        "name": "Isabel Kraus-Liang"
                    },
                    {
                        "name": "Jakub Powierza"
                    },
                    {
                        "name": "Donghoon Shin"
                    },
                    {
                        "name": "Jon E. Froehlich"
                    },
                    {
                        "name": "Gabriel Brostow"
                    },
                    {
                        "name": "Jessica Van Brummelen"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Van Brummelen"
                },
                "author": "Jessica Van Brummelen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06129v4",
                "updated": "2025-04-30T06:43:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    43,
                    45,
                    2,
                    120,
                    0
                ],
                "published": "2023-09-12T11:08:14Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    11,
                    8,
                    14,
                    1,
                    255,
                    0
                ],
                "title": "LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking\n  using Synthetic Eye Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking\n  using Synthetic Eye Images"
                },
                "summary": "Deep learning has bolstered gaze estimation techniques, but real-world\ndeployment has been impeded by inadequate training datasets. This problem is\nexacerbated by both hardware-induced variations in eye images and inherent\nbiological differences across the recorded participants, leading to both\nfeature and pixel-level variance that hinders the generalizability of models\ntrained on specific datasets. While synthetic datasets can be a solution, their\ncreation is both time and resource-intensive. To address this problem, we\npresent a framework called Light Eyes or \"LEyes\" which, unlike conventional\nphotorealistic methods, only models key image features required for video-based\neye tracking using simple light distributions. LEyes facilitates easy\nconfiguration for training neural networks across diverse gaze-estimation\ntasks. We demonstrate that models trained using LEyes are consistently on-par\nor outperform other state-of-the-art algorithms in terms of pupil and CR\nlocalization across well-known datasets. In addition, a LEyes trained model\noutperforms the industry standard eye tracker using significantly more\ncost-effective hardware. Going forward, we are confident that LEyes will\nrevolutionize synthetic data generation for gaze estimation models, and lead to\nsignificant improvements of the next generation video-based eye trackers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has bolstered gaze estimation techniques, but real-world\ndeployment has been impeded by inadequate training datasets. This problem is\nexacerbated by both hardware-induced variations in eye images and inherent\nbiological differences across the recorded participants, leading to both\nfeature and pixel-level variance that hinders the generalizability of models\ntrained on specific datasets. While synthetic datasets can be a solution, their\ncreation is both time and resource-intensive. To address this problem, we\npresent a framework called Light Eyes or \"LEyes\" which, unlike conventional\nphotorealistic methods, only models key image features required for video-based\neye tracking using simple light distributions. LEyes facilitates easy\nconfiguration for training neural networks across diverse gaze-estimation\ntasks. We demonstrate that models trained using LEyes are consistently on-par\nor outperform other state-of-the-art algorithms in terms of pupil and CR\nlocalization across well-known datasets. In addition, a LEyes trained model\noutperforms the industry standard eye tracker using significantly more\ncost-effective hardware. Going forward, we are confident that LEyes will\nrevolutionize synthetic data generation for gaze estimation models, and lead to\nsignificant improvements of the next generation video-based eye trackers."
                },
                "authors": [
                    {
                        "name": "Sean Anthony Byrne"
                    },
                    {
                        "name": "Virmarie Maquiling"
                    },
                    {
                        "name": "Marcus Nystr√∂m"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Diederick C. Niehorster"
                    }
                ],
                "author_detail": {
                    "name": "Diederick C. Niehorster"
                },
                "author": "Diederick C. Niehorster",
                "arxiv_doi": "10.3758/s13428-025-02645-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3758/s13428-025-02645-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.06129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21356v1",
                "updated": "2025-04-30T06:30:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    30,
                    48,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T06:30:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    30,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing"
                },
                "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."
                },
                "authors": [
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Zhongjie Duan"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21336v1",
                "updated": "2025-04-30T05:51:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    51,
                    48,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T05:51:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    51,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation"
                },
                "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis."
                },
                "authors": [
                    {
                        "name": "Linshan Wu"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Sunan He"
                    },
                    {
                        "name": "Jiaxin Zhuang"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "The first universal foundation model for grounded biomedical image\n  interpretation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21334v1",
                "updated": "2025-04-30T05:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    41,
                    43,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T05:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    41,
                    43,
                    2,
                    120,
                    0
                ],
                "title": "Simple Visual Artifact Detection in Sora-Generated Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Visual Artifact Detection in Sora-Generated Videos"
                },
                "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model\ndriven by natural language prompts, highlights a growing convergence between\nlarge language models (LLMs) and video synthesis. As these multimodal systems\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\nand interacting with visual content, understanding their limitations and\nensuring their safe deployment becomes essential. This study investigates\nvisual artifacts frequently found and reported in Sora-generated videos, which\ncan compromise quality, mislead viewers, or propagate disinformation. We\npropose a multi-label classification framework targeting four common artifact\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\nachieved an average multi-label classification accuracy of 94.14%. This work\nsupports the broader development of VidLLMs by contributing to (1) the creation\nof datasets for video quality evaluation, (2) interpretable artifact-based\nanalysis beyond language metrics, and (3) the identification of visual risks\nrelevant to factuality and safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The December 2024 release of OpenAI's Sora, a powerful video generation model\ndriven by natural language prompts, highlights a growing convergence between\nlarge language models (LLMs) and video synthesis. As these multimodal systems\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\nand interacting with visual content, understanding their limitations and\nensuring their safe deployment becomes essential. This study investigates\nvisual artifacts frequently found and reported in Sora-generated videos, which\ncan compromise quality, mislead viewers, or propagate disinformation. We\npropose a multi-label classification framework targeting four common artifact\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\nachieved an average multi-label classification accuracy of 94.14%. This work\nsupports the broader development of VidLLMs by contributing to (1) the creation\nof datasets for video quality evaluation, (2) interpretable artifact-based\nanalysis beyond language metrics, and (3) the identification of visual risks\nrelevant to factuality and safety."
                },
                "authors": [
                    {
                        "name": "Misora Sugiyama"
                    },
                    {
                        "name": "Hirokatsu Kataoka"
                    }
                ],
                "author_detail": {
                    "name": "Hirokatsu Kataoka"
                },
                "author": "Hirokatsu Kataoka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21330v1",
                "updated": "2025-04-30T05:36:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    36,
                    28,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T05:36:28Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    36,
                    28,
                    2,
                    120,
                    0
                ],
                "title": "Does the Prompt-based Large Language Model Recognize Students'\n  Demographics and Introduce Bias in Essay Scoring?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does the Prompt-based Large Language Model Recognize Students'\n  Demographics and Introduce Bias in Essay Scoring?"
                },
                "summary": "Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)\ndue to their ability to capture semantic meaning. Traditional fine-tuning\napproaches required technical expertise, limiting accessibility for educators\nwith limited technical backgrounds. However, prompt-based tools like ChatGPT\nhave made AES more accessible, enabling educators to obtain machine-generated\nscores using natural-language prompts (i.e., the prompt-based paradigm).\nDespite advancements, prior studies have shown bias in fine-tuned LLMs,\nparticularly against disadvantaged groups. It remains unclear whether such\nbiases persist or are amplified in the prompt-based paradigm with cutting-edge\ntools. Since such biases are believed to stem from the demographic information\nembedded in pre-trained models (i.e., the ability of LLMs' text embeddings to\npredict demographic attributes), this study explores the relationship between\nthe model's predictive power of students' demographic attributes based on their\nwritten works and its predictive bias in the scoring task in the prompt-based\nparadigm. Using a publicly available dataset of over 25,000 students'\nargumentative essays, we designed prompts to elicit demographic inferences\n(i.e., gender, first-language background) from GPT-4o and assessed fairness in\nautomated scoring. Then we conducted multivariate regression analysis to\nexplore the impact of the model's ability to predict demographics on its\nscoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat\ninfer students' demographics, particularly their first-language backgrounds,\nfrom their essays; (ii) scoring biases are more pronounced when the LLM\ncorrectly predicts students' first-language background than when it does not;\nand (iii) scoring error for non-native English speakers increases when the LLM\ncorrectly identifies them as non-native.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)\ndue to their ability to capture semantic meaning. Traditional fine-tuning\napproaches required technical expertise, limiting accessibility for educators\nwith limited technical backgrounds. However, prompt-based tools like ChatGPT\nhave made AES more accessible, enabling educators to obtain machine-generated\nscores using natural-language prompts (i.e., the prompt-based paradigm).\nDespite advancements, prior studies have shown bias in fine-tuned LLMs,\nparticularly against disadvantaged groups. It remains unclear whether such\nbiases persist or are amplified in the prompt-based paradigm with cutting-edge\ntools. Since such biases are believed to stem from the demographic information\nembedded in pre-trained models (i.e., the ability of LLMs' text embeddings to\npredict demographic attributes), this study explores the relationship between\nthe model's predictive power of students' demographic attributes based on their\nwritten works and its predictive bias in the scoring task in the prompt-based\nparadigm. Using a publicly available dataset of over 25,000 students'\nargumentative essays, we designed prompts to elicit demographic inferences\n(i.e., gender, first-language background) from GPT-4o and assessed fairness in\nautomated scoring. Then we conducted multivariate regression analysis to\nexplore the impact of the model's ability to predict demographics on its\nscoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat\ninfer students' demographics, particularly their first-language backgrounds,\nfrom their essays; (ii) scoring biases are more pronounced when the LLM\ncorrectly predicts students' first-language background than when it does not;\nand (iii) scoring error for non-native English speakers increases when the LLM\ncorrectly identifies them as non-native."
                },
                "authors": [
                    {
                        "name": "Kaixun Yang"
                    },
                    {
                        "name": "Mladen Rakoviƒá"
                    },
                    {
                        "name": "Dragan Ga≈°eviƒá"
                    },
                    {
                        "name": "Guanliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanliang Chen"
                },
                "author": "Guanliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13517v2",
                "updated": "2025-04-30T05:13:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    13,
                    56,
                    2,
                    120,
                    0
                ],
                "published": "2024-02-21T03:59:52Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    59,
                    52,
                    2,
                    52,
                    0
                ],
                "title": "Round Trip Translation Defence against Large Language Model Jailbreaking\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Trip Translation Defence against Large Language Model Jailbreaking\n  Attacks"
                },
                "summary": "Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence\n  This version of the article has been accepted for publication, after peer\nreview (when applicable) but is not the Version of Record and does not reflect\npost-acceptance improvements, or any corrections. The Version of Record is\navailable online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this\nAccepted Version is subject to the publisher's Accepted Manuscript terms of use\nhttps://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence\n  This version of the article has been accepted for publication, after peer\nreview (when applicable) but is not the Version of Record and does not reflect\npost-acceptance improvements, or any corrections. The Version of Record is\navailable online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this\nAccepted Version is subject to the publisher's Accepted Manuscript terms of use\nhttps://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms"
                },
                "authors": [
                    {
                        "name": "Canaan Yung"
                    },
                    {
                        "name": "Hadi Mohaghegh Dolatabadi"
                    },
                    {
                        "name": "Sarah Erfani"
                    },
                    {
                        "name": "Christopher Leckie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Leckie"
                },
                "author": "Christopher Leckie",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21317v1",
                "updated": "2025-04-30T05:04:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    4,
                    53,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T05:04:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    5,
                    4,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "Redundancy Analysis and Mitigation for Machine Learning-Based Process\n  Monitoring of Additive Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redundancy Analysis and Mitigation for Machine Learning-Based Process\n  Monitoring of Additive Manufacturing"
                },
                "summary": "The deployment of machine learning (ML)-based process monitoring systems has\nsignificantly advanced additive manufacturing (AM) by enabling real-time defect\ndetection, quality assessment, and process optimization. However, redundancy is\na critical yet often overlooked challenge in the deployment and operation of\nML-based AM process monitoring systems. Excessive redundancy leads to increased\nequipment costs, compromised model performance, and high computational\nrequirements, posing barriers to industrial adoption. However, existing\nresearch lacks a unified definition of redundancy and a systematic framework\nfor its evaluation and mitigation. This paper defines redundancy in ML-based AM\nprocess monitoring and categorizes it into sample-level, feature-level, and\nmodel-level redundancy. A comprehensive multi-level redundancy mitigation\n(MLRM) framework is proposed, incorporating advanced methods such as data\nregistration, downscaling, cross-modality knowledge transfer, and model pruning\nto systematically reduce redundancy while improving model performance. The\nframework is validated through an ML-based in-situ defect detection case study\nfor directed energy deposition (DED), demonstrating a 91% reduction in latency,\na 47% decrease in error rate, and a 99.4% reduction in storage requirements.\nAdditionally, the proposed approach lowers sensor costs and energy consumption,\nenabling a lightweight, cost-effective, and scalable monitoring system. By\ndefining redundancy and introducing a structured mitigation framework, this\nstudy establishes redundancy analysis and mitigation as a key enabler of\nefficient ML-based process monitoring in production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of machine learning (ML)-based process monitoring systems has\nsignificantly advanced additive manufacturing (AM) by enabling real-time defect\ndetection, quality assessment, and process optimization. However, redundancy is\na critical yet often overlooked challenge in the deployment and operation of\nML-based AM process monitoring systems. Excessive redundancy leads to increased\nequipment costs, compromised model performance, and high computational\nrequirements, posing barriers to industrial adoption. However, existing\nresearch lacks a unified definition of redundancy and a systematic framework\nfor its evaluation and mitigation. This paper defines redundancy in ML-based AM\nprocess monitoring and categorizes it into sample-level, feature-level, and\nmodel-level redundancy. A comprehensive multi-level redundancy mitigation\n(MLRM) framework is proposed, incorporating advanced methods such as data\nregistration, downscaling, cross-modality knowledge transfer, and model pruning\nto systematically reduce redundancy while improving model performance. The\nframework is validated through an ML-based in-situ defect detection case study\nfor directed energy deposition (DED), demonstrating a 91% reduction in latency,\na 47% decrease in error rate, and a 99.4% reduction in storage requirements.\nAdditionally, the proposed approach lowers sensor costs and energy consumption,\nenabling a lightweight, cost-effective, and scalable monitoring system. By\ndefining redundancy and introducing a structured mitigation framework, this\nstudy establishes redundancy analysis and mitigation as a key enabler of\nefficient ML-based process monitoring in production environments."
                },
                "authors": [
                    {
                        "name": "Jiarui Xie"
                    },
                    {
                        "name": "Yaoyao Fiona Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yaoyao Fiona Zhao"
                },
                "author": "Yaoyao Fiona Zhao",
                "arxiv_comment": "13 pages, 5 figures, 2 tables. Accepted by IDETC-CIE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21311v1",
                "updated": "2025-04-30T04:53:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    53,
                    11,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T04:53:11Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    53,
                    11,
                    2,
                    120,
                    0
                ],
                "title": "Covert Prompt Transmission for Secure Large Language Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covert Prompt Transmission for Secure Large Language Model Services"
                },
                "summary": "This paper investigates covert prompt transmission for secure and efficient\nlarge language model (LLM) services over wireless networks. We formulate a\nlatency minimization problem under fidelity and detectability constraints to\nensure confidential and covert communication by jointly optimizing the transmit\npower and prompt compression ratio. To solve this problem, we first propose a\nprompt compression and encryption (PCAE) framework, performing surprisal-guided\ncompression followed by lightweight permutation-based encryption. Specifically,\nPCAE employs a locally deployed small language model (SLM) to estimate\ntoken-level surprisal scores, selectively retaining semantically critical\ntokens while discarding redundant ones. This significantly reduces\ncomputational overhead and transmission duration. To further enhance covert\nwireless transmission, we then develop a group-based proximal policy\noptimization (GPPO) method that samples multiple candidate actions for each\nstate, selecting the optimal one within each group and incorporating a\nKullback-Leibler (KL) divergence penalty to improve policy stability and\nexploration. Simulation results show that PCAE achieves comparable LLM response\nfidelity to baseline methods while reducing preprocessing latency by over five\norders of magnitude, enabling real-time edge deployment. We further validate\nPCAE effectiveness across diverse LLM backbones, including DeepSeek-32B,\nQwen-32B, and their smaller variants. Moreover, GPPO reduces covert\ntransmission latency by up to 38.6\\% compared to existing reinforcement\nlearning strategies, with further analysis showing that increased transmit\npower provides additional latency benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates covert prompt transmission for secure and efficient\nlarge language model (LLM) services over wireless networks. We formulate a\nlatency minimization problem under fidelity and detectability constraints to\nensure confidential and covert communication by jointly optimizing the transmit\npower and prompt compression ratio. To solve this problem, we first propose a\nprompt compression and encryption (PCAE) framework, performing surprisal-guided\ncompression followed by lightweight permutation-based encryption. Specifically,\nPCAE employs a locally deployed small language model (SLM) to estimate\ntoken-level surprisal scores, selectively retaining semantically critical\ntokens while discarding redundant ones. This significantly reduces\ncomputational overhead and transmission duration. To further enhance covert\nwireless transmission, we then develop a group-based proximal policy\noptimization (GPPO) method that samples multiple candidate actions for each\nstate, selecting the optimal one within each group and incorporating a\nKullback-Leibler (KL) divergence penalty to improve policy stability and\nexploration. Simulation results show that PCAE achieves comparable LLM response\nfidelity to baseline methods while reducing preprocessing latency by over five\norders of magnitude, enabling real-time edge deployment. We further validate\nPCAE effectiveness across diverse LLM backbones, including DeepSeek-32B,\nQwen-32B, and their smaller variants. Moreover, GPPO reduces covert\ntransmission latency by up to 38.6\\% compared to existing reinforcement\nlearning strategies, with further analysis showing that increased transmit\npower provides additional latency benefits."
                },
                "authors": [
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Shunpu Tang"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Yonghui Li"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21304v1",
                "updated": "2025-04-30T04:26:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    26,
                    3,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T04:26:03Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    26,
                    3,
                    2,
                    120,
                    0
                ],
                "title": "Unsupervised Feature Transformation via In-context Generation,\n  Generator-critic LLM Agents, and Duet-play Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Feature Transformation via In-context Generation,\n  Generator-critic LLM Agents, and Duet-play Teaming"
                },
                "summary": "Feature transformation involves generating a new set of features from the\noriginal dataset to enhance the data's utility. In certain domains like\nmaterial performance screening, dimensionality is large and collecting labels\nis expensive and lengthy. It highly necessitates transforming feature spaces\nefficiently and without supervision to enhance data readiness and AI utility.\nHowever, existing methods fall short in efficient navigation of a vast space of\nfeature combinations, and are mostly designed for supervised settings. To fill\nthis gap, our unique perspective is to leverage a generator-critic duet-play\nteaming framework using LLM agents and in-context learning to derive\npseudo-supervision from unsupervised data. The framework consists of three\ninterconnected steps: (1) Critic agent diagnoses data to generate actionable\nadvice, (2) Generator agent produces tokenized feature transformations guided\nby the critic's advice, and (3) Iterative refinement ensures continuous\nimprovement through feedback between agents. The generator-critic framework can\nbe generalized to human-agent collaborative generation, by replacing the critic\nagent with human experts. Extensive experiments demonstrate that the proposed\nframework outperforms even supervised baselines in feature transformation\nefficiency, robustness, and practical applicability across diverse datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature transformation involves generating a new set of features from the\noriginal dataset to enhance the data's utility. In certain domains like\nmaterial performance screening, dimensionality is large and collecting labels\nis expensive and lengthy. It highly necessitates transforming feature spaces\nefficiently and without supervision to enhance data readiness and AI utility.\nHowever, existing methods fall short in efficient navigation of a vast space of\nfeature combinations, and are mostly designed for supervised settings. To fill\nthis gap, our unique perspective is to leverage a generator-critic duet-play\nteaming framework using LLM agents and in-context learning to derive\npseudo-supervision from unsupervised data. The framework consists of three\ninterconnected steps: (1) Critic agent diagnoses data to generate actionable\nadvice, (2) Generator agent produces tokenized feature transformations guided\nby the critic's advice, and (3) Iterative refinement ensures continuous\nimprovement through feedback between agents. The generator-critic framework can\nbe generalized to human-agent collaborative generation, by replacing the critic\nagent with human experts. Extensive experiments demonstrate that the proposed\nframework outperforms even supervised baselines in feature transformation\nefficiency, robustness, and practical applicability across diverse datasets."
                },
                "authors": [
                    {
                        "name": "Nanxu Gong"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Wangyang Ying"
                    },
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Accepted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21303v1",
                "updated": "2025-04-30T04:24:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    24,
                    50,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T04:24:50Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    24,
                    50,
                    2,
                    120,
                    0
                ],
                "title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to\n  Limited-Sample Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence in Large Language Model Evaluation: A Bayesian Approach to\n  Limited-Sample Challenges"
                },
                "summary": "Large language models (LLMs) exhibit probabilistic output characteristics,\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\nThis study introduces a Bayesian approach for LLM capability assessment that\nintegrates prior knowledge through probabilistic inference, addressing\nlimitations under limited-sample regimes. By treating model capabilities as\nlatent variables and leveraging a curated query set to induce discriminative\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\nover mutually exclusive capability intervals. Experimental evaluations with\nGPT-series models demonstrate that the proposed method achieves superior\ndiscrimination compared to conventional evaluation methods. Results indicate\nthat even with reduced sample sizes, the approach maintains statistical\nrobustness while providing actionable insights, such as probabilistic\nstatements about a model's likelihood of surpassing specific baselines. This\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\npractical constraints in real-world deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit probabilistic output characteristics,\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\nThis study introduces a Bayesian approach for LLM capability assessment that\nintegrates prior knowledge through probabilistic inference, addressing\nlimitations under limited-sample regimes. By treating model capabilities as\nlatent variables and leveraging a curated query set to induce discriminative\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\nover mutually exclusive capability intervals. Experimental evaluations with\nGPT-series models demonstrate that the proposed method achieves superior\ndiscrimination compared to conventional evaluation methods. Results indicate\nthat even with reduced sample sizes, the approach maintains statistical\nrobustness while providing actionable insights, such as probabilistic\nstatements about a model's likelihood of surpassing specific baselines. This\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\npractical constraints in real-world deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Xiao Xiao"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Sijing Zhang"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Yadong Chen"
                    },
                    {
                        "name": "Tian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tian Liu"
                },
                "author": "Tian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21299v1",
                "updated": "2025-04-30T04:13:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    13,
                    3,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T04:13:03Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    4,
                    13,
                    3,
                    2,
                    120,
                    0
                ],
                "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language\n  Models"
                },
                "summary": "Identifying bias in LLM-generated content is a crucial prerequisite for\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\nLLM-based judges, face limitations related to difficulties in understanding\nunderlying intentions and the lack of criteria for fairness judgment. In this\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\nanalyzes inputs and reasons through fairness specifications to provide accurate\njudgments. BiasGuard is implemented through a two-stage approach: the first\nstage initializes the model to explicitly reason based on fairness\nspecifications, while the second stage leverages reinforcement learning to\nenhance its reasoning and judgment capabilities. Our experiments, conducted\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\nthe importance of reasoning-enhanced decision-making and provide evidence for\nthe effectiveness of our two-stage optimization pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying bias in LLM-generated content is a crucial prerequisite for\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\nLLM-based judges, face limitations related to difficulties in understanding\nunderlying intentions and the lack of criteria for fairness judgment. In this\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\nanalyzes inputs and reasons through fairness specifications to provide accurate\njudgments. BiasGuard is implemented through a two-stage approach: the first\nstage initializes the model to explicitly reason based on fairness\nspecifications, while the second stage leverages reinforcement learning to\nenhance its reasoning and judgment capabilities. Our experiments, conducted\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\nthe importance of reasoning-enhanced decision-making and provide evidence for\nthe effectiveness of our two-stage optimization pipeline."
                },
                "authors": [
                    {
                        "name": "Zhiting Fan"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02009v2",
                "updated": "2025-04-30T03:53:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    53,
                    0,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-02T05:12:13Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    12,
                    13,
                    2,
                    92,
                    0
                ],
                "title": "Urban Computing in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban Computing in the Era of Large Language Models"
                },
                "summary": "Urban computing has emerged as a multidisciplinary field that harnesses\ndata-driven technologies to address challenges and improve urban living.\nTraditional approaches, while beneficial, often face challenges with\ngeneralization, scalability, and contextual understanding. The advent of Large\nLanguage Models (LLMs) offers transformative potential in this domain. This\nsurvey explores the intersection of LLMs and urban computing, emphasizing the\nimpact of LLMs in processing and analyzing urban data, enhancing\ndecision-making, and fostering citizen engagement. We provide a concise\noverview of the evolution and core technologies of LLMs. Additionally, we\nsurvey their applications across key urban domains, such as transportation,\npublic safety, and environmental monitoring, summarizing essential tasks and\nprior works in various urban contexts, while highlighting LLMs' functional\nroles and implementation patterns. Building on this, we propose potential\nLLM-based solutions to address unresolved challenges. To facilitate in-depth\nresearch, we compile a list of available datasets and tools applicable to\ndiverse urban scenarios. Finally, we discuss the limitations of current\napproaches and outline future directions for advancing LLMs in urban computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban computing has emerged as a multidisciplinary field that harnesses\ndata-driven technologies to address challenges and improve urban living.\nTraditional approaches, while beneficial, often face challenges with\ngeneralization, scalability, and contextual understanding. The advent of Large\nLanguage Models (LLMs) offers transformative potential in this domain. This\nsurvey explores the intersection of LLMs and urban computing, emphasizing the\nimpact of LLMs in processing and analyzing urban data, enhancing\ndecision-making, and fostering citizen engagement. We provide a concise\noverview of the evolution and core technologies of LLMs. Additionally, we\nsurvey their applications across key urban domains, such as transportation,\npublic safety, and environmental monitoring, summarizing essential tasks and\nprior works in various urban contexts, while highlighting LLMs' functional\nroles and implementation patterns. Building on this, we propose potential\nLLM-based solutions to address unresolved challenges. To facilitate in-depth\nresearch, we compile a list of available datasets and tools applicable to\ndiverse urban scenarios. Finally, we discuss the limitations of current\napproaches and outline future directions for advancing LLMs in urban computing."
                },
                "authors": [
                    {
                        "name": "Zhonghang Li"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Xubin Ren"
                    },
                    {
                        "name": "Jiabin Tang"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "https://github.com/HKUDS/Awesome-LLM4Urban-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14809v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14809v5",
                "updated": "2025-04-30T03:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    34,
                    34,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-21T02:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    19,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "vApps: Verifiable Applications at Internet Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vApps: Verifiable Applications at Internet Scale"
                },
                "summary": "Blockchain technology promises a decentralized, trustless, and interoperable\ninfrastructure. However, widespread adoption remains hindered by issues such as\nlimited scalability, high transaction costs, and the complexity of maintaining\ncoherent verification logic across different blockchain layers. This paper\nintroduces Verifiable Applications (vApps), a novel development framework\ndesigned to streamline the creation and deployment of verifiable blockchain\ncomputing applications. vApps offer a unified Rust-based Domain-Specific\nLanguage (DSL) within a comprehensive SDK, featuring modular abstractions for\nverification, proof generation, and inter-chain connectivity. This eases the\ndeveloper's burden in securing diverse software components, allowing them to\nfocus on application logic. The DSL also ensures that applications can\nautomatically take advantage of specialized precompiles and hardware\nacceleration to achieve consistently high performance with minimal developer\neffort, as demonstrated by benchmark results for zero-knowledge virtual\nmachines (zkVMs). Experiments show that native Rust execution eliminates\ninterpretation overhead, delivering up to an 197x cycle count improvement\ncompared to EVM-based approaches. Precompiled circuits can accelerate the proof\nby more than 95%, while GPU acceleration increases throughput by up to 30x and\nrecursion compresses the proof size by up to 230x, enabling succinct and\nefficient verification. The framework also supports seamless integration with\nthe Web2 and Web3 systems, enabling developers to focus solely on their\napplication logic. Through modular architecture, robust security guarantees,\nand composability, vApps pave the way toward a trust-minimized and verifiable\nInternet-scale application environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain technology promises a decentralized, trustless, and interoperable\ninfrastructure. However, widespread adoption remains hindered by issues such as\nlimited scalability, high transaction costs, and the complexity of maintaining\ncoherent verification logic across different blockchain layers. This paper\nintroduces Verifiable Applications (vApps), a novel development framework\ndesigned to streamline the creation and deployment of verifiable blockchain\ncomputing applications. vApps offer a unified Rust-based Domain-Specific\nLanguage (DSL) within a comprehensive SDK, featuring modular abstractions for\nverification, proof generation, and inter-chain connectivity. This eases the\ndeveloper's burden in securing diverse software components, allowing them to\nfocus on application logic. The DSL also ensures that applications can\nautomatically take advantage of specialized precompiles and hardware\nacceleration to achieve consistently high performance with minimal developer\neffort, as demonstrated by benchmark results for zero-knowledge virtual\nmachines (zkVMs). Experiments show that native Rust execution eliminates\ninterpretation overhead, delivering up to an 197x cycle count improvement\ncompared to EVM-based approaches. Precompiled circuits can accelerate the proof\nby more than 95%, while GPU acceleration increases throughput by up to 30x and\nrecursion compresses the proof size by up to 230x, enabling succinct and\nefficient verification. The framework also supports seamless integration with\nthe Web2 and Web3 systems, enabling developers to focus solely on their\napplication logic. Through modular architecture, robust security guarantees,\nand composability, vApps pave the way toward a trust-minimized and verifiable\nInternet-scale application environment."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Kshitij Kulkarni"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "John Guibas"
                    },
                    {
                        "name": "Uma Roy"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Ryan Zarick"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Zarick"
                },
                "author": "Ryan Zarick",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14809v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14809v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20519v2",
                "updated": "2025-04-30T03:22:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    22,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T07:59:46Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    59,
                    46,
                    1,
                    119,
                    0
                ],
                "title": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions\n  But Do Not Outperform Standard Public Health Messaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions\n  But Do Not Outperform Standard Public Health Messaging"
                },
                "summary": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies."
                },
                "authors": [
                    {
                        "name": "Neil K. R. Sehgal"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Anish K. Agarwal"
                    },
                    {
                        "name": "Joseph Cappella"
                    },
                    {
                        "name": "Melanie Kornides"
                    },
                    {
                        "name": "Lyle Ungar"
                    },
                    {
                        "name": "Alison Buttenheim"
                    },
                    {
                        "name": "Sharath Chandra Guntuku"
                    }
                ],
                "author_detail": {
                    "name": "Sharath Chandra Guntuku"
                },
                "author": "Sharath Chandra Guntuku",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21277v1",
                "updated": "2025-04-30T03:14:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    14,
                    28,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T03:14:28Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    14,
                    28,
                    2,
                    120,
                    0
                ],
                "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large\n  Language Models"
                },
                "summary": "The integration of reinforcement learning (RL) into the reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\na transformative research direction. While MLLMs significantly extend Large\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\nvideo, enabling robust reasoning across multimodal inputs remains a major\nchallenge. This survey systematically reviews recent advances in RL-based\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\ninnovations, and practical applications. We highlight two main RL\nparadigms--value-free and value-based methods--and analyze how RL enhances\nreasoning abilities by optimizing reasoning trajectories and aligning\nmultimodal information. Furthermore, we provide an extensive overview of\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\nfuture research directions to address current bottlenecks such as sparse\nrewards, inefficient cross-modal reasoning, and real-world deployment\nconstraints. Our goal is to offer a comprehensive and structured guide to\nresearchers interested in advancing RL-based reasoning in the multimodal era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of reinforcement learning (RL) into the reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\na transformative research direction. While MLLMs significantly extend Large\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\nvideo, enabling robust reasoning across multimodal inputs remains a major\nchallenge. This survey systematically reviews recent advances in RL-based\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\ninnovations, and practical applications. We highlight two main RL\nparadigms--value-free and value-based methods--and analyze how RL enhances\nreasoning abilities by optimizing reasoning trajectories and aligning\nmultimodal information. Furthermore, we provide an extensive overview of\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\nfuture research directions to address current bottlenecks such as sparse\nrewards, inefficient cross-modal reasoning, and real-world deployment\nconstraints. Our goal is to offer a comprehensive and structured guide to\nresearchers interested in advancing RL-based reasoning in the multimodal era."
                },
                "authors": [
                    {
                        "name": "Guanghao Zhou"
                    },
                    {
                        "name": "Panjia Qiu"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Zheming Yang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Minghui Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Minghui Qiu"
                },
                "author": "Minghui Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21276v1",
                "updated": "2025-04-30T03:11:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    11,
                    54,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T03:11:54Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    11,
                    54,
                    2,
                    120,
                    0
                ],
                "title": "Assessing LLM code generation quality through path planning tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing LLM code generation quality through path planning tasks"
                },
                "summary": "As LLM-generated code grows in popularity, more evaluation is needed to\nassess the risks of using such tools, especially for safety-critical\napplications such as path planning. Existing coding benchmarks are insufficient\nas they do not reflect the context and complexity of safety-critical\napplications. To this end, we assessed six LLMs' abilities to generate the code\nfor three different path-planning algorithms and tested them on three maps of\nvarious difficulties. Our results suggest that LLM-generated code presents\nserious hazards for path planning applications and should not be applied in\nsafety-critical contexts without rigorous testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-generated code grows in popularity, more evaluation is needed to\nassess the risks of using such tools, especially for safety-critical\napplications such as path planning. Existing coding benchmarks are insufficient\nas they do not reflect the context and complexity of safety-critical\napplications. To this end, we assessed six LLMs' abilities to generate the code\nfor three different path-planning algorithms and tested them on three maps of\nvarious difficulties. Our results suggest that LLM-generated code presents\nserious hazards for path planning applications and should not be applied in\nsafety-critical contexts without rigorous testing."
                },
                "authors": [
                    {
                        "name": "Wanyi Chen"
                    },
                    {
                        "name": "Meng-Wen Su"
                    },
                    {
                        "name": "Mary L. Cummings"
                    }
                ],
                "author_detail": {
                    "name": "Mary L. Cummings"
                },
                "author": "Mary L. Cummings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21266v1",
                "updated": "2025-04-30T02:50:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    2,
                    50,
                    24,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T02:50:24Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    2,
                    50,
                    24,
                    2,
                    120,
                    0
                ],
                "title": "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine\n  Text-Co-Guided Latent Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine\n  Text-Co-Guided Latent Diffusion"
                },
                "summary": "In action recognition tasks, feature diversity is essential for enhancing\nmodel generalization and performance. Existing methods typically promote\nfeature diversity by expanding the training data in the sample space, which\noften leads to inefficiencies and semantic inconsistencies. To overcome these\nproblems, we propose a novel Coarse-fine text co-guidance Diffusion model\n(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in\nthe latent space by leveraging diffusion and multi-granularity textual\nguidance. Specifically, our approach feeds spatio-temporal features extracted\nfrom skeleton sequences into a latent diffusion model to generate diverse\naction representations. Meanwhile, we introduce a coarse-fine text co-guided\nstrategy that leverages textual information from large language models (LLMs)\nto ensure semantic consistency between the generated features and the original\ninputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module\nduring training, incurring no additional inference cost. Extensive experiments\ndemonstrate that CoCoDiff achieves SOTA performance on skeleton-based action\nrecognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and\nKinetics-Skeleton.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In action recognition tasks, feature diversity is essential for enhancing\nmodel generalization and performance. Existing methods typically promote\nfeature diversity by expanding the training data in the sample space, which\noften leads to inefficiencies and semantic inconsistencies. To overcome these\nproblems, we propose a novel Coarse-fine text co-guidance Diffusion model\n(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in\nthe latent space by leveraging diffusion and multi-granularity textual\nguidance. Specifically, our approach feeds spatio-temporal features extracted\nfrom skeleton sequences into a latent diffusion model to generate diverse\naction representations. Meanwhile, we introduce a coarse-fine text co-guided\nstrategy that leverages textual information from large language models (LLMs)\nto ensure semantic consistency between the generated features and the original\ninputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module\nduring training, incurring no additional inference cost. Extensive experiments\ndemonstrate that CoCoDiff achieves SOTA performance on skeleton-based action\nrecognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and\nKinetics-Skeleton."
                },
                "authors": [
                    {
                        "name": "Zhifu Zhao"
                    },
                    {
                        "name": "Hanyang Hua"
                    },
                    {
                        "name": "Jianan Li"
                    },
                    {
                        "name": "Shaoxin Wu"
                    },
                    {
                        "name": "Fu Li"
                    },
                    {
                        "name": "Yangtao Zhou"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21934v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21934v4",
                "updated": "2025-04-30T02:42:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    2,
                    42,
                    44,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-27T19:21:05Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    19,
                    21,
                    5,
                    3,
                    86,
                    0
                ],
                "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad"
                },
                "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly: only Gemini-2.5-Pro\nachieves a non-trivial score of 25%, while all other models achieve less than\n5%. Through detailed analysis of reasoning traces, we identify the most common\nfailure modes and find several unwanted artifacts arising from the optimization\nstrategies employed during model training. Overall, our results suggest that\ncurrent LLMs are inadequate for rigorous mathematical reasoning tasks,\nhighlighting the need for substantial improvements in reasoning and proof\ngeneration capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly: only Gemini-2.5-Pro\nachieves a non-trivial score of 25%, while all other models achieve less than\n5%. Through detailed analysis of reasoning traces, we identify the most common\nfailure modes and find several unwanted artifacts arising from the optimization\nstrategies employed during model training. Overall, our results suggest that\ncurrent LLMs are inadequate for rigorous mathematical reasoning tasks,\nhighlighting the need for substantial improvements in reasoning and proof\ngeneration capabilities."
                },
                "authors": [
                    {
                        "name": "Ivo Petrov"
                    },
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Lyuben Baltadzhiev"
                    },
                    {
                        "name": "Maria Drencheva"
                    },
                    {
                        "name": "Kristian Minchev"
                    },
                    {
                        "name": "Mislav Balunoviƒá"
                    },
                    {
                        "name": "Nikola Jovanoviƒá"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21934v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21934v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20803v2",
                "updated": "2025-04-30T02:31:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    2,
                    31,
                    34,
                    2,
                    120,
                    0
                ],
                "published": "2025-03-24T14:44:55Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    44,
                    55,
                    0,
                    83,
                    0
                ],
                "title": "Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with\n  Machine Learning Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with\n  Machine Learning Classifiers"
                },
                "summary": "This paper assesses the performance of five machine learning classifiers:\nDecision Tree, Naive Bayes, LightGBM, Logistic Regression, and Random Forest\nusing latent representations learned by a Variational Autoencoder from malware\ndatasets. Results from the experiments conducted on different training-test\nsplits with different random seeds reveal that all the models perform well in\ndetecting malware with ensemble methods (LightGBM and Random Forest) performing\nslightly better than the rest. In addition, the use of latent features reduces\nthe computational cost of the model and the need for extensive hyperparameter\ntuning for improved efficiency of the model for deployment. Statistical tests\nshow that these improvements are significant, and thus, the practical relevance\nof integrating latent space representation with traditional classifiers for\neffective malware detection in cybersecurity is established.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper assesses the performance of five machine learning classifiers:\nDecision Tree, Naive Bayes, LightGBM, Logistic Regression, and Random Forest\nusing latent representations learned by a Variational Autoencoder from malware\ndatasets. Results from the experiments conducted on different training-test\nsplits with different random seeds reveal that all the models perform well in\ndetecting malware with ensemble methods (LightGBM and Random Forest) performing\nslightly better than the rest. In addition, the use of latent features reduces\nthe computational cost of the model and the need for extensive hyperparameter\ntuning for improved efficiency of the model for deployment. Statistical tests\nshow that these improvements are significant, and thus, the practical relevance\nof integrating latent space representation with traditional classifiers for\neffective malware detection in cybersecurity is established."
                },
                "authors": [
                    {
                        "name": "Bamidele Ajayi"
                    },
                    {
                        "name": "Basel Barakat"
                    },
                    {
                        "name": "Ken McGarry"
                    }
                ],
                "author_detail": {
                    "name": "Ken McGarry"
                },
                "author": "Ken McGarry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08298v2",
                "updated": "2025-04-30T02:21:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    2,
                    21,
                    1,
                    2,
                    120,
                    0
                ],
                "published": "2023-12-13T17:13:08Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    17,
                    13,
                    8,
                    2,
                    347,
                    0
                ],
                "title": "Venn: Resource Management for Collaborative Learning Jobs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Venn: Resource Management for Collaborative Learning Jobs"
                },
                "summary": "In recent years, collaborative learning (CL) has emerged as a promising\napproach for machine learning (ML) and data science across distributed edge\ndevices. As the deployment of CL jobs increases, they inevitably contend for\nlimited resources. However, efficient resource scheduling in this context is\nchallenging because of the ephemeral nature and resource heterogeneity of\ndevices, coupled with the overlapping resource requirements of diverse CL jobs.\nExisting resource managers often assign devices to CL jobs randomly for\nsimplicity and scalability, but this approach compromises job efficiency.\n  In this paper, we present Venn, a CL resource manager that efficiently\nschedules ephemeral, heterogeneous devices among multiple CL jobs to reduce the\naverage job completion time (JCT). Venn formulates the Intersection Resource\nScheduling (IRS) problem to identify complex resource contention among multiple\nCL jobs. It then proposes a contention-aware scheduling heuristic to minimize\nthe average scheduling delay. Furthermore, it proposes a resource-aware\ndevice-to-job matching heuristic to optimize response collection time by\nmitigating stragglers. Our evaluation shows that, compared to the\nstate-of-the-art CL resource managers, Venn improves the average JCT by up to\n1.88x. The code is available at https://github.com/SymbioticLab/Venn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, collaborative learning (CL) has emerged as a promising\napproach for machine learning (ML) and data science across distributed edge\ndevices. As the deployment of CL jobs increases, they inevitably contend for\nlimited resources. However, efficient resource scheduling in this context is\nchallenging because of the ephemeral nature and resource heterogeneity of\ndevices, coupled with the overlapping resource requirements of diverse CL jobs.\nExisting resource managers often assign devices to CL jobs randomly for\nsimplicity and scalability, but this approach compromises job efficiency.\n  In this paper, we present Venn, a CL resource manager that efficiently\nschedules ephemeral, heterogeneous devices among multiple CL jobs to reduce the\naverage job completion time (JCT). Venn formulates the Intersection Resource\nScheduling (IRS) problem to identify complex resource contention among multiple\nCL jobs. It then proposes a contention-aware scheduling heuristic to minimize\nthe average scheduling delay. Furthermore, it proposes a resource-aware\ndevice-to-job matching heuristic to optimize response collection time by\nmitigating stragglers. Our evaluation shows that, compared to the\nstate-of-the-art CL resource managers, Venn improves the average JCT by up to\n1.88x. The code is available at https://github.com/SymbioticLab/Venn."
                },
                "authors": [
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Ding Ding"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "arxiv_comment": "14 pages, 15 figrues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21252v1",
                "updated": "2025-04-30T01:37:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    1,
                    37,
                    44,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T01:37:44Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    1,
                    37,
                    44,
                    2,
                    120,
                    0
                ],
                "title": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in\n  Medical QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in\n  Medical QA"
                },
                "summary": "Medical question answering (QA) is a reasoning-intensive task that remains\nchallenging for large language models (LLMs) due to hallucinations and outdated\ndomain knowledge. Retrieval-Augmented Generation (RAG) provides a promising\npost-training solution by leveraging external knowledge. However, existing\nmedical RAG systems suffer from two key limitations: (1) a lack of modeling for\nhuman-like reasoning behaviors during information retrieval, and (2) reliance\non suboptimal medical corpora, which often results in the retrieval of\nirrelevant or noisy snippets. To overcome these challenges, we propose\nDiscuss-RAG, a plug-and-play module designed to enhance the medical QA RAG\nsystem through collaborative agent-based reasoning. Our method introduces a\nsummarizer agent that orchestrates a team of medical experts to emulate\nmulti-turn brainstorming, thereby improving the relevance of retrieved content.\nAdditionally, a decision-making agent evaluates the retrieved snippets before\ntheir final integration. Experimental results on four benchmark medical QA\ndatasets show that Discuss-RAG consistently outperforms MedRAG, especially\nsignificantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on\nPubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical question answering (QA) is a reasoning-intensive task that remains\nchallenging for large language models (LLMs) due to hallucinations and outdated\ndomain knowledge. Retrieval-Augmented Generation (RAG) provides a promising\npost-training solution by leveraging external knowledge. However, existing\nmedical RAG systems suffer from two key limitations: (1) a lack of modeling for\nhuman-like reasoning behaviors during information retrieval, and (2) reliance\non suboptimal medical corpora, which often results in the retrieval of\nirrelevant or noisy snippets. To overcome these challenges, we propose\nDiscuss-RAG, a plug-and-play module designed to enhance the medical QA RAG\nsystem through collaborative agent-based reasoning. Our method introduces a\nsummarizer agent that orchestrates a team of medical experts to emulate\nmulti-turn brainstorming, thereby improving the relevance of retrieved content.\nAdditionally, a decision-making agent evaluates the retrieved snippets before\ntheir final integration. Experimental results on four benchmark medical QA\ndatasets show that Discuss-RAG consistently outperforms MedRAG, especially\nsignificantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on\nPubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG."
                },
                "authors": [
                    {
                        "name": "Xuanzhao Dong"
                    },
                    {
                        "name": "Wenhui Zhu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Peijie Qiu"
                    },
                    {
                        "name": "Rui Yin"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yalin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yalin Wang"
                },
                "author": "Yalin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19915v2",
                "updated": "2025-04-30T01:26:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    1,
                    26,
                    23,
                    2,
                    120,
                    0
                ],
                "published": "2025-02-27T09:36:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    36,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty"
                },
                "summary": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Jiahui Cen"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Weixuan Zhong"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Aimin Yang"
                    },
                    {
                        "name": "Yongmei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yongmei Zhou"
                },
                "author": "Yongmei Zhou",
                "arxiv_comment": "During a careful review of our base-experiment results, we discovered\n  a possible error in the way some data were recorded. To ensure the integrity\n  and accuracy of our work, we must correct these results and revise the\n  corresponding analysis before making the manuscript publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15032v2",
                "updated": "2025-04-30T01:00:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    1,
                    0,
                    18,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-21T11:41:22Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    41,
                    22,
                    0,
                    111,
                    0
                ],
                "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation"
                },
                "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving precise control over\nindividual entities; and (3) An Entity-Consistency Constraint strategy that\npropagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis. The code is released in\nhttps://github.com/XiaoBuL/DyST-XL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving precise control over\nindividual entities; and (3) An Entity-Consistency Constraint strategy that\npropagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis. The code is released in\nhttps://github.com/XiaoBuL/DyST-XL."
                },
                "authors": [
                    {
                        "name": "Weijie He"
                    },
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Chao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wu"
                },
                "author": "Chao Wu",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21239v1",
                "updated": "2025-04-30T00:28:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    0,
                    28,
                    32,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T00:28:32Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    0,
                    28,
                    32,
                    2,
                    120,
                    0
                ],
                "title": "Memorization and Knowledge Injection in Gated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization and Knowledge Injection in Gated LLMs"
                },
                "summary": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain."
                },
                "authors": [
                    {
                        "name": "Xu Pan"
                    },
                    {
                        "name": "Ely Hahami"
                    },
                    {
                        "name": "Zechen Zhang"
                    },
                    {
                        "name": "Haim Sompolinsky"
                    }
                ],
                "author_detail": {
                    "name": "Haim Sompolinsky"
                },
                "author": "Haim Sompolinsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21233v1",
                "updated": "2025-04-30T00:04:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    0,
                    4,
                    35,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T00:04:35Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    0,
                    4,
                    35,
                    2,
                    120,
                    0
                ],
                "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math"
                },
                "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Hany Awadalla"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Yen-Chun Chen"
                    },
                    {
                        "name": "Mei Gao"
                    },
                    {
                        "name": "Young Jin Kim"
                    },
                    {
                        "name": "Yunsheng Li"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Weijian Xu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Weizhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weizhu Chen"
                },
                "author": "Weizhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18762v2",
                "updated": "2025-04-29T23:45:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    45,
                    37,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-26T01:42:22Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    1,
                    42,
                    22,
                    5,
                    116,
                    0
                ],
                "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning"
                },
                "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."
                },
                "authors": [
                    {
                        "name": "Ojasw Upadhyay"
                    },
                    {
                        "name": "Abishek Saravanakumar"
                    },
                    {
                        "name": "Ayman Ismail"
                    }
                ],
                "author_detail": {
                    "name": "Ayman Ismail"
                },
                "author": "Ayman Ismail",
                "arxiv_comment": "9 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v5",
                "updated": "2025-04-29T23:43:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    25,
                    1,
                    119,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doƒüru√∂z"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "Accepted to NAACL 2025 (findings), please cite ACL anthology\n  reference on https://aclanthology.org/2025.findings-naacl.85/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21227v1",
                "updated": "2025-04-29T23:41:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    41,
                    37,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:41:37Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    41,
                    37,
                    1,
                    119,
                    0
                ],
                "title": "Gradient Attention Map Based Verification of Deep Convolutional Neural\n  Networks with Application to X-ray Image Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Attention Map Based Verification of Deep Convolutional Neural\n  Networks with Application to X-ray Image Datasets"
                },
                "summary": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging."
                },
                "authors": [
                    {
                        "name": "Omid Halimi Milani"
                    },
                    {
                        "name": "Amanda Nikho"
                    },
                    {
                        "name": "Lauren Mills"
                    },
                    {
                        "name": "Marouane Tliba"
                    },
                    {
                        "name": "Ahmet Enis Cetin"
                    },
                    {
                        "name": "Mohammed H. Elnagar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed H. Elnagar"
                },
                "author": "Mohammed H. Elnagar",
                "arxiv_comment": "13 pages, 7 figures, accepted at IEEE VLSI Test Symposium (VTS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18521v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18521v4",
                "updated": "2025-04-29T23:31:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    31,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2024-07-26T05:34:34Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    5,
                    34,
                    34,
                    4,
                    208,
                    0
                ],
                "title": "Patched MOA: optimizing inference for diverse software development tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patched MOA: optimizing inference for diverse software development tasks"
                },
                "summary": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm."
                },
                "authors": [
                    {
                        "name": "Asankhaya Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Asankhaya Sharma"
                },
                "author": "Asankhaya Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18521v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18521v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16557v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16557v3",
                "updated": "2025-04-29T23:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    30,
                    3,
                    1,
                    119,
                    0
                ],
                "published": "2024-07-23T15:12:14Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    15,
                    12,
                    14,
                    1,
                    205,
                    0
                ],
                "title": "Patched RTC: evaluating LLMs for diverse software development tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patched RTC: evaluating LLMs for diverse software development tasks"
                },
                "summary": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows."
                },
                "authors": [
                    {
                        "name": "Asankhaya Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Asankhaya Sharma"
                },
                "author": "Asankhaya Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16557v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16557v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01698v2",
                "updated": "2025-04-29T23:25:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    25,
                    27,
                    1,
                    119,
                    0
                ],
                "published": "2024-06-03T18:00:50Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    0,
                    50,
                    0,
                    155,
                    0
                ],
                "title": "Demystifying AI Platform Design for Distributed Inference of\n  Next-Generation LLM models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying AI Platform Design for Distributed Inference of\n  Next-Generation LLM models"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of applications, often outperforming human experts. However, deploying\nthese gigantic models efficiently for diverse inference use cases requires\ncarefully designed hardware platforms with ample computing, memory, and network\nresources. With constant innovation in LLM serving optimizations and model\narchitecture evolving at breakneck speed, the hardware requirements to meet\nService Level Objectives (SLOs) remain an open research question.\n  To answer the question, we present an analytical tool, GenZ, to efficiently\nnavigate the relationship between diverse LLM model architectures(Dense, GQA,\nMoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding,\nquanitization), and AI platform design parameters. Our tool estimates LLM\ninference performance metrics for the given scenario. We have validated against\nreal hardware platforms running various different LLM models, achieving a max\ngeomean error of 5.82.We use GenZ to identify compute, memory capacity, memory\nbandwidth, network latency, and network bandwidth requirements across diverse\nLLM inference use cases. We also study diverse architectural choices in use\ntoday (inspired by LLM serving platforms from several vendors) to help inform\ncomputer architects designing next-generation AI hardware accelerators and\nplatforms. The trends and insights derived from GenZ can guide AI engineers\ndeploying LLMs as well as computer architects designing next-generation\nhardware accelerators and platforms. Ultimately, this work sheds light on the\nplatform design considerations for unlocking the full potential of large\nlanguage models across a spectrum of applications. The source code is available\nat https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be\ntried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on\nyour web browser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of applications, often outperforming human experts. However, deploying\nthese gigantic models efficiently for diverse inference use cases requires\ncarefully designed hardware platforms with ample computing, memory, and network\nresources. With constant innovation in LLM serving optimizations and model\narchitecture evolving at breakneck speed, the hardware requirements to meet\nService Level Objectives (SLOs) remain an open research question.\n  To answer the question, we present an analytical tool, GenZ, to efficiently\nnavigate the relationship between diverse LLM model architectures(Dense, GQA,\nMoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding,\nquanitization), and AI platform design parameters. Our tool estimates LLM\ninference performance metrics for the given scenario. We have validated against\nreal hardware platforms running various different LLM models, achieving a max\ngeomean error of 5.82.We use GenZ to identify compute, memory capacity, memory\nbandwidth, network latency, and network bandwidth requirements across diverse\nLLM inference use cases. We also study diverse architectural choices in use\ntoday (inspired by LLM serving platforms from several vendors) to help inform\ncomputer architects designing next-generation AI hardware accelerators and\nplatforms. The trends and insights derived from GenZ can guide AI engineers\ndeploying LLMs as well as computer architects designing next-generation\nhardware accelerators and platforms. Ultimately, this work sheds light on the\nplatform design considerations for unlocking the full potential of large\nlanguage models across a spectrum of applications. The source code is available\nat https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be\ntried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on\nyour web browser."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Bambhaniya"
                    },
                    {
                        "name": "Ritik Raj"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "19 Pages, https://github.com/abhibambhaniya/GenZ-LLM-Analyzer,\n  https://genz-llm-analyzer.streamlit.app/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21211v1",
                "updated": "2025-04-29T22:34:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    34,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T22:34:42Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    34,
                    42,
                    1,
                    119,
                    0
                ],
                "title": "A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in\n  Online Marketplaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in\n  Online Marketplaces"
                },
                "summary": "Wildlife trafficking remains a critical global issue, significantly impacting\nbiodiversity, ecological stability, and public health. Despite efforts to\ncombat this illicit trade, the rise of e-commerce platforms has made it easier\nto sell wildlife products, putting new pressure on wild populations of\nendangered and threatened species. The use of these platforms also opens a new\nopportunity: as criminals sell wildlife products online, they leave digital\ntraces of their activity that can provide insights into trafficking activities\nas well as how they can be disrupted. The challenge lies in finding these\ntraces. Online marketplaces publish ads for a plethora of products, and\nidentifying ads for wildlife-related products is like finding a needle in a\nhaystack. Learning classifiers can automate ad identification, but creating\nthem requires costly, time-consuming data labeling that hinders support for\ndiverse ads and research questions. This paper addresses a critical challenge\nin the data science pipeline for wildlife trafficking analytics: generating\nquality labeled data for classifiers that select relevant data. While large\nlanguage models (LLMs) can directly label advertisements, doing so at scale is\nprohibitively expensive. We propose a cost-effective strategy that leverages\nLLMs to generate pseudo labels for a small sample of the data and uses these\nlabels to create specialized classification models. Our novel method\nautomatically gathers diverse and representative samples to be labeled while\nminimizing the labeling costs. Our experimental evaluation shows that our\nclassifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We\npresent real use cases that demonstrate the effectiveness of our approach in\nenabling analyses of different aspects of wildlife trafficking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wildlife trafficking remains a critical global issue, significantly impacting\nbiodiversity, ecological stability, and public health. Despite efforts to\ncombat this illicit trade, the rise of e-commerce platforms has made it easier\nto sell wildlife products, putting new pressure on wild populations of\nendangered and threatened species. The use of these platforms also opens a new\nopportunity: as criminals sell wildlife products online, they leave digital\ntraces of their activity that can provide insights into trafficking activities\nas well as how they can be disrupted. The challenge lies in finding these\ntraces. Online marketplaces publish ads for a plethora of products, and\nidentifying ads for wildlife-related products is like finding a needle in a\nhaystack. Learning classifiers can automate ad identification, but creating\nthem requires costly, time-consuming data labeling that hinders support for\ndiverse ads and research questions. This paper addresses a critical challenge\nin the data science pipeline for wildlife trafficking analytics: generating\nquality labeled data for classifiers that select relevant data. While large\nlanguage models (LLMs) can directly label advertisements, doing so at scale is\nprohibitively expensive. We propose a cost-effective strategy that leverages\nLLMs to generate pseudo labels for a small sample of the data and uses these\nlabels to create specialized classification models. Our novel method\nautomatically gathers diverse and representative samples to be labeled while\nminimizing the labeling costs. Our experimental evaluation shows that our\nclassifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We\npresent real use cases that demonstrate the effectiveness of our approach in\nenabling analyses of different aspects of wildlife trafficking."
                },
                "authors": [
                    {
                        "name": "Juliana Barbosa"
                    },
                    {
                        "name": "Ulhas Gondhali"
                    },
                    {
                        "name": "Gohar Petrossian"
                    },
                    {
                        "name": "Kinshuk Sharma"
                    },
                    {
                        "name": "Sunandan Chakraborty"
                    },
                    {
                        "name": "Jennifer Jacquet"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "arxiv_doi": "10.1145/3725256",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725256",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.21211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09689v3",
                "updated": "2025-04-29T22:29:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    29,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-13T18:47:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    18,
                    47,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety"
                },
                "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent"
                },
                "authors": [
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Yinghui He"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Yimin Wang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Zixin Yao"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21205v1",
                "updated": "2025-04-29T22:22:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    22,
                    44,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T22:22:44Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    22,
                    44,
                    1,
                    119,
                    0
                ],
                "title": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World\n  Repositories"
                },
                "summary": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories."
                },
                "authors": [
                    {
                        "name": "Connor Dilgren"
                    },
                    {
                        "name": "Purva Chiniya"
                    },
                    {
                        "name": "Luke Griffith"
                    },
                    {
                        "name": "Yu Ding"
                    },
                    {
                        "name": "Yizheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yizheng Chen"
                },
                "author": "Yizheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21202v1",
                "updated": "2025-04-29T22:16:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    16,
                    39,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T22:16:39Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    16,
                    39,
                    1,
                    119,
                    0
                ],
                "title": "Automatic Legal Writing Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Legal Writing Evaluation of LLMs"
                },
                "summary": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available."
                },
                "authors": [
                    {
                        "name": "Ramon Pires"
                    },
                    {
                        "name": "Roseval Malaquias Junior"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Nogueira"
                },
                "author": "Rodrigo Nogueira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19394v2",
                "updated": "2025-04-29T22:15:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    15,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-27T23:59:39Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    23,
                    59,
                    39,
                    6,
                    117,
                    0
                ],
                "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Engineering: Teaching Models to Design High Powered Rockets"
                },
                "summary": "Large Language Models (LLMs) have transformed software engineering, but their\napplication to physical engineering domains remains underexplored. This paper\nevaluates LLMs' capabilities in high-powered rocketry design through\nRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.\nWe test models on two increasingly complex design tasks: target altitude\noptimization and precision landing challenges. Our findings reveal that while\nstate-of-the-art LLMs demonstrate strong baseline engineering knowledge, they\nstruggle to iterate on their designs when given simulation results and\nultimately plateau below human performance levels. However, when enhanced with\nreinforcement learning (RL), we show that a 7B parameter model outperforms both\nSoTA foundation models and human experts. This research demonstrates that\nRL-trained LLMs can serve as effective tools for complex engineering\noptimization, potentially transforming engineering domains beyond software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed software engineering, but their\napplication to physical engineering domains remains underexplored. This paper\nevaluates LLMs' capabilities in high-powered rocketry design through\nRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.\nWe test models on two increasingly complex design tasks: target altitude\noptimization and precision landing challenges. Our findings reveal that while\nstate-of-the-art LLMs demonstrate strong baseline engineering knowledge, they\nstruggle to iterate on their designs when given simulation results and\nultimately plateau below human performance levels. However, when enhanced with\nreinforcement learning (RL), we show that a 7B parameter model outperforms both\nSoTA foundation models and human experts. This research demonstrates that\nRL-trained LLMs can serve as effective tools for complex engineering\noptimization, potentially transforming engineering domains beyond software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Toby Simonds"
                    }
                ],
                "author_detail": {
                    "name": "Toby Simonds"
                },
                "author": "Toby Simonds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21198v1",
                "updated": "2025-04-29T22:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    4,
                    30,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T22:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    22,
                    4,
                    30,
                    1,
                    119,
                    0
                ],
                "title": "Graph Synthetic Out-of-Distribution Exposure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Synthetic Out-of-Distribution Exposure with Large Language Models"
                },
                "summary": "Out-of-distribution (OOD) detection in graphs is critical for ensuring model\nrobustness in open-world and safety-sensitive applications. Existing approaches\nto graph OOD detection typically involve training an in-distribution (ID)\nclassifier using only ID data, followed by the application of post-hoc OOD\nscoring techniques. Although OOD exposure - introducing auxiliary OOD samples\nduring training - has proven to be an effective strategy for enhancing\ndetection performance, current methods in the graph domain generally assume\naccess to a set of real OOD nodes. This assumption, however, is often\nimpractical due to the difficulty and cost of acquiring representative OOD\nsamples. In this paper, we introduce GOE-LLM, a novel framework that leverages\nLarge Language Models (LLMs) for OOD exposure in graph OOD detection without\nrequiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying\npseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM\nannotations, and (2) generating semantically informative synthetic OOD nodes\nvia LLM-prompted text generation. These pseudo-OOD nodes are then used to\nregularize the training of the ID classifier for improved OOD awareness. We\nevaluate our approach across multiple benchmark datasets, showing that GOE-LLM\nsignificantly outperforms state-of-the-art graph OOD detection methods that do\nnot use OOD exposure and achieves comparable performance to those relying on\nreal OOD data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection in graphs is critical for ensuring model\nrobustness in open-world and safety-sensitive applications. Existing approaches\nto graph OOD detection typically involve training an in-distribution (ID)\nclassifier using only ID data, followed by the application of post-hoc OOD\nscoring techniques. Although OOD exposure - introducing auxiliary OOD samples\nduring training - has proven to be an effective strategy for enhancing\ndetection performance, current methods in the graph domain generally assume\naccess to a set of real OOD nodes. This assumption, however, is often\nimpractical due to the difficulty and cost of acquiring representative OOD\nsamples. In this paper, we introduce GOE-LLM, a novel framework that leverages\nLarge Language Models (LLMs) for OOD exposure in graph OOD detection without\nrequiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying\npseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM\nannotations, and (2) generating semantically informative synthetic OOD nodes\nvia LLM-prompted text generation. These pseudo-OOD nodes are then used to\nregularize the training of the ID classifier for improved OOD awareness. We\nevaluate our approach across multiple benchmark datasets, showing that GOE-LLM\nsignificantly outperforms state-of-the-art graph OOD detection methods that do\nnot use OOD exposure and achieves comparable performance to those relying on\nreal OOD data."
                },
                "authors": [
                    {
                        "name": "Haoyan Xu"
                    },
                    {
                        "name": "Zhengtao Yao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Zhan Cheng"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Mengyuan Li"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21191v1",
                "updated": "2025-04-29T21:50:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    50,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T21:50:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    50,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice\n  for Specialized Applications in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice\n  for Specialized Applications in Healthcare"
                },
                "summary": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs."
                },
                "authors": [
                    {
                        "name": "Lovedeep Gondara"
                    },
                    {
                        "name": "Jonathan Simkin"
                    },
                    {
                        "name": "Graham Sayle"
                    },
                    {
                        "name": "Shebnum Devji"
                    },
                    {
                        "name": "Gregory Arbour"
                    },
                    {
                        "name": "Raymond Ng"
                    }
                ],
                "author_detail": {
                    "name": "Raymond Ng"
                },
                "author": "Raymond Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21190v1",
                "updated": "2025-04-29T21:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    46,
                    43,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T21:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    46,
                    43,
                    1,
                    119,
                    0
                ],
                "title": "TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse\n  Mixture-of-Experts"
                },
                "summary": "We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA\nMoE), a novel computational framework integrating Parameter-Efficient\nFine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in\nlarge model deployments. Unlike traditional MoE approaches, which face\nsubstantial computational overhead as expert counts grow, TT-LoRA MoE\ndecomposes training into two distinct, optimized stages. First, we\nindependently train lightweight, tensorized low-rank adapters (TT-LoRA\nexperts), each specialized for specific tasks. Subsequently, these expert\nadapters remain frozen, eliminating inter-task interference and catastrophic\nforgetting in multi-task setting. A sparse MoE router, trained separately,\ndynamically leverages base model representations to select exactly one\nspecialized adapter per input at inference time, automating expert selection\nwithout explicit task specification. Comprehensive experiments confirm our\narchitecture retains the memory efficiency of low-rank adapters, seamlessly\nscales to large expert pools, and achieves robust task-level optimization. This\nstructured decoupling significantly enhances computational efficiency and\nflexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion\nparameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling\npractical and scalable multi-task inference deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA\nMoE), a novel computational framework integrating Parameter-Efficient\nFine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in\nlarge model deployments. Unlike traditional MoE approaches, which face\nsubstantial computational overhead as expert counts grow, TT-LoRA MoE\ndecomposes training into two distinct, optimized stages. First, we\nindependently train lightweight, tensorized low-rank adapters (TT-LoRA\nexperts), each specialized for specific tasks. Subsequently, these expert\nadapters remain frozen, eliminating inter-task interference and catastrophic\nforgetting in multi-task setting. A sparse MoE router, trained separately,\ndynamically leverages base model representations to select exactly one\nspecialized adapter per input at inference time, automating expert selection\nwithout explicit task specification. Comprehensive experiments confirm our\narchitecture retains the memory efficiency of low-rank adapters, seamlessly\nscales to large expert pools, and achieves robust task-level optimization. This\nstructured decoupling significantly enhances computational efficiency and\nflexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion\nparameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling\npractical and scalable multi-task inference deployments."
                },
                "authors": [
                    {
                        "name": "Pradip Kunwar"
                    },
                    {
                        "name": "Minh N. Vu"
                    },
                    {
                        "name": "Maanak Gupta"
                    },
                    {
                        "name": "Mahmoud Abdelsalam"
                    },
                    {
                        "name": "Manish Bhattarai"
                    }
                ],
                "author_detail": {
                    "name": "Manish Bhattarai"
                },
                "author": "Manish Bhattarai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21187v1",
                "updated": "2025-04-29T21:42:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    42,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T21:42:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    42,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning"
                },
                "summary": "FPGAs are increasingly adopted in datacenter environments for their\nreconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have\neased FPGA programming by raising the abstraction level from RTL to untimed\nC/C++, yet attaining high performance still demands expert knowledge and\niterative manual insertion of optimization pragmas to modify the\nmicroarchitecture. To address this challenge, we propose LIFT, a large language\nmodel (LLM)-based coding assistant for HLS that automatically generates\nperformance-critical pragmas given a C/C++ design. We fine-tune the LLM by\ntightly integrating and supervising the training process with a graph neural\nnetwork (GNN), combining the sequential modeling capabilities of LLMs with the\nstructural and semantic understanding of GNNs necessary for reasoning over code\nand its control/data dependencies. On average, LIFT produces designs that\nimprove performance by 3.52x and 2.16x than prior state-of the art AutoDSE and\nHARP respectively, and 66x than GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPGAs are increasingly adopted in datacenter environments for their\nreconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have\neased FPGA programming by raising the abstraction level from RTL to untimed\nC/C++, yet attaining high performance still demands expert knowledge and\niterative manual insertion of optimization pragmas to modify the\nmicroarchitecture. To address this challenge, we propose LIFT, a large language\nmodel (LLM)-based coding assistant for HLS that automatically generates\nperformance-critical pragmas given a C/C++ design. We fine-tune the LLM by\ntightly integrating and supervising the training process with a graph neural\nnetwork (GNN), combining the sequential modeling capabilities of LLMs with the\nstructural and semantic understanding of GNNs necessary for reasoning over code\nand its control/data dependencies. On average, LIFT produces designs that\nimprove performance by 3.52x and 2.16x than prior state-of the art AutoDSE and\nHARP respectively, and 66x than GPT-4o."
                },
                "authors": [
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Zijian Ding"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21186v1",
                "updated": "2025-04-29T21:42:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    42,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T21:42:54Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    42,
                    54,
                    1,
                    119,
                    0
                ],
                "title": "GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model"
                },
                "summary": "Out-of-distribution (OOD) detection is critical for ensuring the safety and\nreliability of machine learning systems, particularly in dynamic and open-world\nenvironments. In the vision and text domains, zero-shot OOD detection - which\nrequires no training on in-distribution (ID) data - has made significant\nprogress through the use of large-scale pretrained models such as\nvision-language models (VLMs) and large language models (LLMs). However,\nzero-shot OOD detection in graph-structured data remains largely unexplored,\nprimarily due to the challenges posed by complex relational structures and the\nabsence of powerful, large-scale pretrained models for graphs. In this work, we\ntake the first step toward enabling zero-shot graph OOD detection by leveraging\na graph foundation model (GFM). We show that, when provided only with class\nlabel names, the GFM can perform OOD detection without any node-level\nsupervision - outperforming existing supervised methods across multiple\ndatasets. To address the more practical setting where OOD label names are\nunavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to\ngenerate semantically informative pseudo-OOD labels from unlabeled data. These\nlabels enable the GFM to capture nuanced semantic boundaries between ID and OOD\nclasses and perform fine-grained OOD detection - without requiring any labeled\nnodes. Our approach is the first to enable node-level graph OOD detection in a\nfully zero-shot setting, and achieves state-of-the-art performance on four\nbenchmark text-attributed graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is critical for ensuring the safety and\nreliability of machine learning systems, particularly in dynamic and open-world\nenvironments. In the vision and text domains, zero-shot OOD detection - which\nrequires no training on in-distribution (ID) data - has made significant\nprogress through the use of large-scale pretrained models such as\nvision-language models (VLMs) and large language models (LLMs). However,\nzero-shot OOD detection in graph-structured data remains largely unexplored,\nprimarily due to the challenges posed by complex relational structures and the\nabsence of powerful, large-scale pretrained models for graphs. In this work, we\ntake the first step toward enabling zero-shot graph OOD detection by leveraging\na graph foundation model (GFM). We show that, when provided only with class\nlabel names, the GFM can perform OOD detection without any node-level\nsupervision - outperforming existing supervised methods across multiple\ndatasets. To address the more practical setting where OOD label names are\nunavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to\ngenerate semantically informative pseudo-OOD labels from unlabeled data. These\nlabels enable the GFM to capture nuanced semantic boundaries between ID and OOD\nclasses and perform fine-grained OOD detection - without requiring any labeled\nnodes. Our approach is the first to enable node-level graph OOD detection in a\nfully zero-shot setting, and achieves state-of-the-art performance on four\nbenchmark text-attributed graph datasets."
                },
                "authors": [
                    {
                        "name": "Haoyan Xu"
                    },
                    {
                        "name": "Zhengtao Yao"
                    },
                    {
                        "name": "Xuzhi Zhang"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Mengyuan Li"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21185v1",
                "updated": "2025-04-29T21:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    42,
                    2,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T21:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    21,
                    42,
                    2,
                    1,
                    119,
                    0
                ],
                "title": "AI-in-the-Loop Planning for Transportation Electrification: Case Studies\n  from Austin, Texas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-in-the-Loop Planning for Transportation Electrification: Case Studies\n  from Austin, Texas"
                },
                "summary": "This study explores the integration of AI in transportation electrification\nplanning in Austin, TX, focusing on the use of Geospatial AI (GeoAI),\nGenerative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site\nselection, localized GenAI models support meta-level estimations, and LLMs\nenable scenario simulations. These AI applications require human oversight.\nGeoAI outputs must be evaluated with land use data, GenAI models are not always\naccurate, and LLMs are prone to hallucinations. To ensure accountable planning,\nhuman planners must work alongside AI agents. Establishing a community feedback\nloop is essential to audit automated decisions. Planners should place Community\nExperience (CX) at the center of Urban Planning AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the integration of AI in transportation electrification\nplanning in Austin, TX, focusing on the use of Geospatial AI (GeoAI),\nGenerative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site\nselection, localized GenAI models support meta-level estimations, and LLMs\nenable scenario simulations. These AI applications require human oversight.\nGeoAI outputs must be evaluated with land use data, GenAI models are not always\naccurate, and LLMs are prone to hallucinations. To ensure accountable planning,\nhuman planners must work alongside AI agents. Establishing a community feedback\nloop is essential to audit automated decisions. Planners should place Community\nExperience (CX) at the center of Urban Planning AI."
                },
                "authors": [
                    {
                        "name": "Seung Jun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Seung Jun Choi"
                },
                "author": "Seung Jun Choi",
                "arxiv_comment": "10 pages, 7 figures. This manuscript is a revised version of Seung\n  Jun Choi's doctoral dissertation, completed at The University of Texas at\n  Austin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]